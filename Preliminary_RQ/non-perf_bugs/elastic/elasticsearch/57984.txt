{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/57984","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/57984/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/57984/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/57984/events","html_url":"https://github.com/elastic/elasticsearch/issues/57984","id":636979775,"node_id":"MDU6SXNzdWU2MzY5Nzk3NzU=","number":57984,"title":"Length filter doesnt work with keyword normalizer","user":{"login":"SLavrynenko","id":16493827,"node_id":"MDQ6VXNlcjE2NDkzODI3","avatar_url":"https://avatars2.githubusercontent.com/u/16493827?v=4","gravatar_id":"","url":"https://api.github.com/users/SLavrynenko","html_url":"https://github.com/SLavrynenko","followers_url":"https://api.github.com/users/SLavrynenko/followers","following_url":"https://api.github.com/users/SLavrynenko/following{/other_user}","gists_url":"https://api.github.com/users/SLavrynenko/gists{/gist_id}","starred_url":"https://api.github.com/users/SLavrynenko/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SLavrynenko/subscriptions","organizations_url":"https://api.github.com/users/SLavrynenko/orgs","repos_url":"https://api.github.com/users/SLavrynenko/repos","events_url":"https://api.github.com/users/SLavrynenko/events{/privacy}","received_events_url":"https://api.github.com/users/SLavrynenko/received_events","type":"User","site_admin":false},"labels":[{"id":142001965,"node_id":"MDU6TGFiZWwxNDIwMDE5NjU=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Analysis","name":":Search/Analysis","color":"0e8a16","default":false,"description":"How text is split into tokens"},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":14,"created_at":"2020-06-11T12:40:10Z","updated_at":"2020-07-29T08:44:07Z","closed_at":"2020-07-29T08:44:07Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\r\nGitHub is reserved for bug reports and feature requests; it is not the place\r\nfor general questions. If you have a question or an unconfirmed bug , please\r\nvisit the [forums](https://discuss.elastic.co/c/elasticsearch).  Please also\r\ncheck your OS is [supported](https://www.elastic.co/support/matrix#show_os).\r\nIf it is not, the issue is likely to be closed.\r\n\r\nFor security vulnerabilities please only send reports to security@elastic.co.\r\nSee https://www.elastic.co/community/security for more information.\r\n\r\nPlease fill in the following details to help us reproduce the bug:\r\n-->\r\n\r\n**Elasticsearch version** 7.7.0\r\n\r\n**Steps to reproduce**:\r\n\r\nWhen I try to create an index with the following script:\r\n\r\n`{\r\n  \"settings\": {\r\n    \"index\": {\r\n      \"number_of_shards\": 3,\r\n      \"number_of_replicas\": 0\r\n    },\r\n    \"analysis\": {\r\n      \"analyzer\": {\r\n        \"text\": {\r\n          \"tokenizer\": \"my_tokenizer\",\r\n          \"filter\": [\r\n            \"lowercase\",\r\n            \"trim\",\r\n            \"snowball\",\r\n            \"unicode_truncate\"\r\n          ],\r\n          \"char_filter\": [\r\n            \"html_strip\",\r\n            \"quotes\"\r\n          ]\r\n        },\r\n        \"custom_analyzer\": {\r\n          \"tokenizer\": \"my_tokenizer\",\r\n          \"filter\": [\r\n            \"lowercase\",\r\n            \"trim\",\r\n            \"unicode_truncate\"\r\n          ],\r\n          \"char_filter\": [\r\n            \"html_strip\",\r\n            \"quotes\"\r\n          ]\r\n        },\r\n        \"analyzer_keyword\": {\r\n          \"tokenizer\": \"keyword\",\r\n          \"filter\": [\r\n            \"lowercase\",\r\n            \"trim\",\r\n            \"unicode_truncate\"\r\n          ]\r\n        }\r\n      },\r\n      \"normalizer\": {\r\n        \"keyword_normalizer\": {\r\n          \"type\": \"custom\",\r\n          \"filter\": [\r\n            \"lowercase\",\r\n            \"trim\",\r\n\t\t\t\"unicode_truncate\"\r\n          ]\r\n        }\r\n      },\r\n      \"filter\": {\r\n        \"unicode_truncate\": {\r\n          \"type\": \"length\",\r\n          \"max\": 8191\r\n        }\r\n      },\r\n      \"char_filter\": {\r\n        \"quotes\": {\r\n          \"mappings\": [\r\n            \"\\\\u0091=>'\",\r\n            \"\\\\u0092=>'\",\r\n            \"\\\\u2018=>'\",\r\n            \"\\\\u2019=>'\",\r\n            \"\\\\uf0b7=>\\\\u0020\"\r\n          ],\r\n          \"type\": \"mapping\"\r\n        }\r\n      },\r\n      \"tokenizer\": {\r\n        \"my_tokenizer\": {\r\n          \"type\": \"pattern\",\r\n          \"pattern\": \"[.;:\\\\s]*[ ,!?;\\n\\t\\r]\"\r\n        }\r\n      }\r\n    }\r\n  },\r\n  \"mappings\": {\r\n    \"dynamic\": false,\r\n    \"properties\": {\r\n      \"Status\": {\r\n        \"type\": \"text\",\r\n        \"analyzer\": \"text\",\r\n        \"fields\": {\r\n          \"raw\": {\r\n            \"type\": \"text\",\r\n            \"analyzer\": \"analyzer_keyword\"\r\n          }\r\n        }\r\n      },\r\n      \"Ts\": {\r\n        \"type\": \"long\"\r\n      },\r\n      \"AddedBy\": {\r\n        \"type\": \"integer\"\r\n      },\r\n      \"AddedOn\": {\r\n        \"type\": \"date\",\r\n        \"format\": \"strict_date_optional_time||epoch_millis\"\r\n      },\r\n      \"Address1\": {\r\n        \"type\": \"text\",\r\n        \"analyzer\": \"analyzer_keyword\"\r\n      },\r\n      \"Address2\": {\r\n        \"type\": \"text\",\r\n        \"analyzer\": \"analyzer_keyword\"\r\n      },\r\n      \"City\": {\r\n        \"type\": \"text\",\r\n        \"analyzer\": \"custom_analyzer\",\r\n        \"fields\": {\r\n          \"raw\": {\r\n            \"type\": \"text\",\r\n            \"analyzer\": \"analyzer_keyword\"\r\n          }\r\n        }\r\n      }\r\n   }\r\n  }\r\n}`\r\n\r\nit gives me an error, saying:\r\n\r\n`{\r\n  \"error\": {\r\n    \"root_cause\": [\r\n      {\r\n        \"type\": \"illegal_argument_exception\",\r\n        \"reason\": \"Custom normalizer [keyword_normalizer] may not use filter [unicode_truncate]\"\r\n      }\r\n    ],\r\n    \"type\": \"illegal_argument_exception\",\r\n    \"reason\": \"Custom normalizer [keyword_normalizer] may not use filter [unicode_truncate]\"\r\n  },\r\n  \"status\": 400\r\n}`\r\n\r\nI believe that there is no reason of why length filter shouldnt work for normalizer","closed_by":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"performed_via_github_app":null}