[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/301899891","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-301899891","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":301899891,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMTg5OTg5MQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-05-16T20:07:07Z","updated_at":"2017-05-16T20:07:07Z","author_association":"MEMBER","body":"Please see my previous [comment](https://github.com/elastic/elasticsearch/issues/22406#issuecomment-301899130) to you on this subject. If you need help, open a topic on the [forum](https://discuss.elastic.co), we reserve GitHub for verified bug reports and feature requests.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/302609359","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-302609359","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":302609359,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMjYwOTM1OQ==","user":{"login":"ninadvps","id":19671720,"node_id":"MDQ6VXNlcjE5NjcxNzIw","avatar_url":"https://avatars3.githubusercontent.com/u/19671720?v=4","gravatar_id":"","url":"https://api.github.com/users/ninadvps","html_url":"https://github.com/ninadvps","followers_url":"https://api.github.com/users/ninadvps/followers","following_url":"https://api.github.com/users/ninadvps/following{/other_user}","gists_url":"https://api.github.com/users/ninadvps/gists{/gist_id}","starred_url":"https://api.github.com/users/ninadvps/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ninadvps/subscriptions","organizations_url":"https://api.github.com/users/ninadvps/orgs","repos_url":"https://api.github.com/users/ninadvps/repos","events_url":"https://api.github.com/users/ninadvps/events{/privacy}","received_events_url":"https://api.github.com/users/ninadvps/received_events","type":"User","site_admin":false},"created_at":"2017-05-19T05:04:06Z","updated_at":"2017-05-19T05:04:06Z","author_association":"NONE","body":"@jasontedor \"please see my previous comment\" didnt help at all. Importantly, this issue we are facing is in the es client not the server, Ive tried with versions 5.3.1 and 5.4.0, also there are multiple people facing the same issue, how does one get into a verified bug report? Can you please take a look the logs Ive sent, before discounting it?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/302691889","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-302691889","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":302691889,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMjY5MTg4OQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-05-19T12:39:50Z","updated_at":"2017-05-19T12:39:50Z","author_association":"MEMBER","body":"@ninadvps There are many reasons that an Elasticsearch client or Elasticsearch server can run into an out of memory error on the network layer. Just because you see the same stack trace here that appears on another issue does not mean that the cause is the same, or that they are even related. There was an issue with early versions of Elasticsearch the server in the 5.x series that were prone to out of memory errors due to issues with a specialized component in the framework that we use for networking. We now ship with this component disabled to reduce the likelihood that users run into out of memory errors on the network layer. It does not mean that all causes of out of memory errors on the network layer were addressed by this fix, only that a very common cause that was not the fault of the user was resolved. It also does not mean that there are not other causes that are not the fault of the user. Summarizing my points and the previous discussion then:\r\n - on the client, the recycler is *not* disabled, only for the server; you have to add the startup flag `-Dio.netty.recycler.maxCapacityPerThread=0` to your JVM startup flags (we probably should do this, though)\r\n - if this does not resolve your issue, *you* have to take a heap dump and analyze it to see what is consuming the heap in *your* application\r\n\r\nExpanding on this last point, it could be that your heap is consumed by objects that come from *your* application. It could be that there is a leak in the Elasticsearch transport client. It could be that there is another issue with Netty, the underlying framework that we use for networking in both the Elasticsearch client and Elasticsearch the server. Since this is client-side, until we have some evidence that there is an issue with Elasticsearch that is causing this, we will not treat it as a bug.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/302837875","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-302837875","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":302837875,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMjgzNzg3NQ==","user":{"login":"ninadvps","id":19671720,"node_id":"MDQ6VXNlcjE5NjcxNzIw","avatar_url":"https://avatars3.githubusercontent.com/u/19671720?v=4","gravatar_id":"","url":"https://api.github.com/users/ninadvps","html_url":"https://github.com/ninadvps","followers_url":"https://api.github.com/users/ninadvps/followers","following_url":"https://api.github.com/users/ninadvps/following{/other_user}","gists_url":"https://api.github.com/users/ninadvps/gists{/gist_id}","starred_url":"https://api.github.com/users/ninadvps/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ninadvps/subscriptions","organizations_url":"https://api.github.com/users/ninadvps/orgs","repos_url":"https://api.github.com/users/ninadvps/repos","events_url":"https://api.github.com/users/ninadvps/events{/privacy}","received_events_url":"https://api.github.com/users/ninadvps/received_events","type":"User","site_admin":false},"created_at":"2017-05-20T00:23:58Z","updated_at":"2017-05-20T00:23:58Z","author_association":"NONE","body":"First of all thank you very much for getting back, I know its painful to respond to all our queries. Also that was an excellent explanation, I will do all my due diligence before I do decide to come back. As an fyi, yes I had tried disabling the recyler from the client side as well before with no joy. Anyways I will post what I found later. Again appreciate your response.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/302838609","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-302838609","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":302838609,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMjgzODYwOQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-05-20T00:32:31Z","updated_at":"2017-05-20T00:32:31Z","author_association":"MEMBER","body":"> I know its painful to respond to all our queries.\r\n\r\nIt's not painful, we are here for and because of users. It's only time consuming. :smile:\r\n\r\n> As an fyi, yes I had tried disabling the recyler from the client side as well before with no joy. Anyways I will post what I found later. \r\n\r\nOkay, let's see where this goes. I will be happy to help you if this is an issue on our side.\r\n\r\n> Again appreciate your response.\r\n\r\nYou're very welcome.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/303206263","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-303206263","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":303206263,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMzIwNjI2Mw==","user":{"login":"ninadvps","id":19671720,"node_id":"MDQ6VXNlcjE5NjcxNzIw","avatar_url":"https://avatars3.githubusercontent.com/u/19671720?v=4","gravatar_id":"","url":"https://api.github.com/users/ninadvps","html_url":"https://github.com/ninadvps","followers_url":"https://api.github.com/users/ninadvps/followers","following_url":"https://api.github.com/users/ninadvps/following{/other_user}","gists_url":"https://api.github.com/users/ninadvps/gists{/gist_id}","starred_url":"https://api.github.com/users/ninadvps/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ninadvps/subscriptions","organizations_url":"https://api.github.com/users/ninadvps/orgs","repos_url":"https://api.github.com/users/ninadvps/repos","events_url":"https://api.github.com/users/ninadvps/events{/privacy}","received_events_url":"https://api.github.com/users/ninadvps/received_events","type":"User","site_admin":false},"created_at":"2017-05-22T20:08:35Z","updated_at":"2017-05-22T20:08:35Z","author_association":"NONE","body":"@jasontedor https://drive.google.com/file/d/0B-AtWs3naQrSejYwRThURC1ETWM/view?usp=sharing\r\nhere the link to the code.... just follow the below steps to generate the crash, or you can examine the heap dump in it... \r\n1) cd esTool\r\n2) sbt;\r\n3) in the sbt console: testOnly *TestEsClient*, rerun this command 8 to 12 times. \r\nThe crash doesnt happen if u simply run it from a main method only from sbt.\r\n\r\n4) Inside the esTool folder I have esdump.hprof  and also esdump_Leak_Suspects.zip which has an index.html which points you to io.netty.buffer.PoolArena$HeapArena ....\r\nIve played with many of the netty setting etc, with no luck also increasing the heap memory of sbt only delays the problem. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/303848877","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-303848877","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":303848877,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMzg0ODg3Nw==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-05-24T20:54:53Z","updated_at":"2017-05-25T21:33:57Z","author_association":"MEMBER","body":"I looked at the heap dump. I noticed that you have *not* disabled the Netty recycler as I have indicated many times needs to be done. I also looked at your sbt project, and the issue you report does not reproduce for me (I ran it 32 times) but I see this in the logs which again indicates that you're not disabling the recycler:\r\n\r\n```\r\n16:53:26.306 [pool-40-thread-6-ScalaTest-running-TestEsClient] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.maxCapacityPerThread: 32768\r\n16:53:26.306 [pool-40-thread-6-ScalaTest-running-TestEsClient] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.maxSharedCapacityFactor: 2\r\n16:53:26.306 [pool-40-thread-6-ScalaTest-running-TestEsClient] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.linkCapacity: 16\r\n16:53:26.306 [pool-40-thread-6-ScalaTest-running-TestEsClient] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.ratio: 8\r\n```\r\n\r\nPlease disable the recycler and try again.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/303871353","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-303871353","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":303871353,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMzg3MTM1Mw==","user":{"login":"ninadvps","id":19671720,"node_id":"MDQ6VXNlcjE5NjcxNzIw","avatar_url":"https://avatars3.githubusercontent.com/u/19671720?v=4","gravatar_id":"","url":"https://api.github.com/users/ninadvps","html_url":"https://github.com/ninadvps","followers_url":"https://api.github.com/users/ninadvps/followers","following_url":"https://api.github.com/users/ninadvps/following{/other_user}","gists_url":"https://api.github.com/users/ninadvps/gists{/gist_id}","starred_url":"https://api.github.com/users/ninadvps/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ninadvps/subscriptions","organizations_url":"https://api.github.com/users/ninadvps/orgs","repos_url":"https://api.github.com/users/ninadvps/repos","events_url":"https://api.github.com/users/ninadvps/events{/privacy}","received_events_url":"https://api.github.com/users/ninadvps/received_events","type":"User","site_admin":false},"created_at":"2017-05-24T22:36:08Z","updated_at":"2017-05-24T22:36:08Z","author_association":"NONE","body":"Yes disabling the recycler doesn't seem to help, so I kept it, with no joy. Anyways...\r\nHeres the stack trace with the recycler disabled... FYI we are running this on a mac.\r\n\r\n\r\n\r\n5:33:17.345 [pool-18-thread-4-ScalaTest-running-TestEsClient] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.maxCapacityPerThread: disabled\r\n15:33:17.345 [pool-18-thread-4-ScalaTest-running-TestEsClient] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.maxSharedCapacityFactor: disabled\r\n15:33:17.345 [pool-18-thread-4-ScalaTest-running-TestEsClient] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.linkCapacity: disabled\r\n15:33:17.345 [pool-18-thread-4-ScalaTest-running-TestEsClient] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.ratio: disabled\r\n15:33:17.394 [pool-18-thread-4-ScalaTest-running-TestEsClient] DEBUG o.e.transport.netty4.Netty4Transport - connected to node [{#transport#-1}{WUtf0HWcR-u-L-7aSQU_kg}{127.0.0.1}{127.0.0.1:9300}]\r\n15:33:17.420 [pool-18-thread-4-ScalaTest-running-TestEsClient] DEBUG o.e.transport.netty4.Netty4Transport - connected to node [{test-0}{Sa-oBtfbRXWfhJt_9wuNBA}{WBJhqt7LT-2RUyqnXjntSA}{127.0.0.1}{127.0.0.1:9300}]\r\n15:33:17.420 [pool-18-thread-4-ScalaTest-running-TestEsClient] DEBUG o.e.transport.netty4.Netty4Transport - disconnecting from [{test-0}{Sa-oBtfbRXWfhJt_9wuNBA}{WBJhqt7LT-2RUyqnXjntSA}{127.0.0.1}{127.0.0.1:9300}], due to explicit disconnect call\r\n15:33:17.424 [pool-18-thread-4-ScalaTest-running-TestEsClient] DEBUG o.e.transport.netty4.Netty4Transport - disconnecting from [{#transport#-1}{WUtf0HWcR-u-L-7aSQU_kg}{127.0.0.1}{127.0.0.1:9300}], due to explicit disconnect call\r\n15:33:18.352 [threadDeathWatcher-2-1] DEBUG io.netty.buffer.PoolThreadCache - Freed 2 thread-local buffer(s) from thread: threadDeathWatcher-2-1\r\n15:33:18.353 [threadDeathWatcher-2-1] DEBUG io.netty.buffer.PoolThreadCache - Freed 1 thread-local buffer(s) from thread: threadDeathWatcher-2-1\r\n[info] TestEsClient:\r\n[info] es\r\n[info] - should connect and disconnect\r\n[info] ScalaTest\r\n[info] Run completed in 3 seconds, 219 milliseconds.\r\n[info] Total number of tests run: 1\r\n[info] Suites: completed 1, aborted 0\r\n[info] Tests: succeeded 1, failed 0, canceled 0, ignored 0, pending 0\r\n[info] All tests passed.\r\n[info] Passed: Total 1, Failed 0, Errors 0, Passed 1\r\n[success] Total time: 3 s, completed May 24, 2017 3:33:19 PM\r\n> testOnly *TestEsClient*\r\n[info] Passed: Total 0, Failed 0, Errors 0, Passed 0\r\n[info] No tests to run for estool/test:testOnly\r\n[warn] javaOptions will be ignored, fork is set to false\r\n15:33:20.952 [pool-20-thread-6-ScalaTest-running-TestEsClient] INFO  o.e.plugins.PluginsService - no modules loaded\r\n15:33:20.955 [pool-20-thread-6-ScalaTest-running-TestEsClient] INFO  o.e.plugins.PluginsService - loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]\r\n15:33:20.955 [pool-20-thread-6-ScalaTest-running-TestEsClient] INFO  o.e.plugins.PluginsService - loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]\r\n15:33:20.955 [pool-20-thread-6-ScalaTest-running-TestEsClient] INFO  o.e.plugins.PluginsService - loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]\r\n15:33:20.955 [pool-20-thread-6-ScalaTest-running-TestEsClient] INFO  o.e.plugins.PluginsService - loaded plugin [org.elasticsearch.transport.Netty3Plugin]\r\n15:33:20.955 [pool-20-thread-6-ScalaTest-running-TestEsClient] INFO  o.e.plugins.PluginsService - loaded plugin [org.elasticsearch.transport.Netty4Plugin]\r\n15:33:20.986 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [force_merge], size [1], queue size [unbounded]\r\n15:33:20.987 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]\r\n15:33:20.987 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [listener], size [4], queue size [unbounded]\r\n15:33:20.990 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [index], size [8], queue size [200]\r\n15:33:20.990 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [refresh], core [1], max [4], keep alive [5m]\r\n15:33:20.991 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [generic], core [4], max [128], keep alive [30s]\r\n15:33:20.991 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [warmer], core [1], max [4], keep alive [5m]\r\n15:33:20.991 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [search], size [13], queue size [1k]\r\n15:33:20.991 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [flush], core [1], max [4], keep alive [5m]\r\n15:33:20.991 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]\r\n15:33:20.991 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [management], core [1], max [5], keep alive [5m]\r\n15:33:20.991 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [get], size [8], queue size [1k]\r\n15:33:20.991 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [bulk], size [8], queue size [200]\r\n15:33:20.991 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.threadpool.ThreadPool - created thread pool: name [snapshot], core [1], max [4], keep alive [5m]\r\n15:33:20.996 [pool-20-thread-6-ScalaTest-running-TestEsClient] DEBUG o.e.common.network.IfConfig - configuration:\r\n\r\nlo0\r\n        inet 127.0.0.1 netmask:255.0.0.0 scope:host\r\n        inet6 fe80::1 prefixlen:64 scope:link\r\n        inet6 ::1 prefixlen:128 scope:host\r\n        UP MULTICAST LOOPBACK mtu:16384 index:1\r\n\r\nen0\r\n        inet 10.10.10.63 netmask:255.255.255.0 broadcast:10.10.10.255 scope:site\r\n        inet6 fe80::f65c:89ff:fecc:ab7b prefixlen:64 scope:link\r\n        hardware F4:5C:89:CC:AB:7B\r\n        UP MULTICAST mtu:1500 index:4\r\n\r\nawdl0\r\n        inet6 fe80::9cc3:44ff:fe7d:368f prefixlen:64 scope:link\r\n        hardware 9E:C3:44:7D:36:8F\r\n        UP MULTICAST mtu:1484 index:10\r\n\r\nutun0\r\n        inet6 fe80::ba01:faad:3a88:e606 prefixlen:64 scope:link\r\n        UP MULTICAST POINTOPOINT mtu:1500 index:12\r\n\r\n[info] TestEsClient:\r\n[info] es\r\n[trace] Stack trace suppressed: run last esmain/test:testOnly for the full output.\r\n[error] Could not run test TestEsClient: java.lang.OutOfMemoryError: Metaspace\r\njava.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Metaspace\r\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\r\n\tat sbt.ConcurrentRestrictions$$anon$4.take(ConcurrentRestrictions.scala:188)\r\n\tat sbt.Execute.next$1(Execute.scala:85)\r\n\tat sbt.Execute.processAll(Execute.scala:88)\r\n\tat sbt.Execute.runKeep(Execute.scala:68)\r\n\tat sbt.EvaluateTask$.liftedTree1$1(EvaluateTask.scala:359)\r\n\tat sbt.EvaluateTask$.run$1(EvaluateTask.scala:358)\r\n\tat sbt.EvaluateTask$.runTask(EvaluateTask.scala:378)\r\n\tat sbt.Aggregation$$anonfun$3.apply(Aggregation.scala:69)\r\n\tat sbt.Aggregation$$anonfun$3.apply(Aggregation.scala:67)\r\n\tat sbt.EvaluateTask$.withStreams(EvaluateTask.scala:314)\r\n\tat sbt.Aggregation$.timedRun(Aggregation.scala:67)\r\n\tat sbt.Aggregation$.runTasks(Aggregation.scala:76)\r\n\tat sbt.Aggregation$$anonfun$applyDynamicTasks$1.apply(Aggregation.scala:117)\r\n\tat sbt.Aggregation$$anonfun$applyDynamicTasks$1.apply(Aggregation.scala:115)\r\n\tat sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:61)\r\n\tat sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:61)\r\n\tat sbt.Act$$anonfun$sbt$Act$$actParser0$1$$anonfun$sbt$Act$$anonfun$$evaluate$1$1$$anonfun$apply$10.apply(Act.scala:259)\r\n\tat sbt.Act$$anonfun$sbt$Act$$actParser0$1$$anonfun$sbt$Act$$anonfun$$evaluate$1$1$$anonfun$apply$10.apply(Act.scala:256)\r\n\tat sbt.Command$.process(Command.scala:93)\r\n\tat sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:96)\r\n\tat sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:96)\r\n\tat sbt.State$$anon$1.process(State.scala:184)\r\n\tat sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:96)\r\n\tat sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:96)\r\n\tat sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)\r\n\tat sbt.MainLoop$.next(MainLoop.scala:96)\r\n\tat sbt.MainLoop$.run(MainLoop.scala:89)\r\n\tat sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:68)\r\n\tat sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:63)\r\n\tat sbt.Using.apply(Using.scala:24)\r\n\tat sbt.MainLoop$.runWithNewLog(MainLoop.scala:63)\r\n\tat sbt.MainLoop$.runAndClearLast(MainLoop.scala:46)\r\n\tat sbt.MainLoop$.runLoggedLoop(MainLoop.scala:30)\r\n\tat sbt.MainLoop$.runLogged(MainLoop.scala:22)\r\n\tat sbt.StandardMain$.runManaged(Main.scala:57)\r\n\tat sbt.xMain.run(Main.scala:29)\r\n\tat xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:109)\r\n\tat xsbt.boot.Launch$.withContextLoader(Launch.scala:128)\r\n\tat xsbt.boot.Launch$.run(Launch.scala:109)\r\n\tat xsbt.boot.Launch$$anonfun$apply$1.apply(Launch.scala:35)\r\n\tat xsbt.boot.Launch$.launch(Launch.scala:117)\r\n\tat xsbt.boot.Launch$.apply(Launch.scala:18)\r\n\tat xsbt.boot.Boot$.runImpl(Boot.scala:41)\r\n\tat xsbt.boot.Boot$.main(Boot.scala:17)\r\n\tat xsbt.boot.Boot.main(Boot.scala)\r\nCaused by: java.lang.OutOfMemoryError: Metaspace\r\n[error] java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Metaspace\r\n[error] Use 'last' for the full log.\r\n> ninad@Virtuals-MacBook-Pro-4:~/projects/DemoSnmp/test/esTool> java -version\r\njava version \"1.8.0_91\"\r\nJava(TM) SE Runtime Environment (build 1.8.0_91-b14)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/303879697","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-303879697","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":303879697,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMzg3OTY5Nw==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-05-24T23:20:16Z","updated_at":"2017-05-25T21:33:48Z","author_association":"MEMBER","body":"That's sbt that is running out of metaspace. This is a well-known problem with sbt.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/303884859","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-303884859","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":303884859,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMzg4NDg1OQ==","user":{"login":"ninadvps","id":19671720,"node_id":"MDQ6VXNlcjE5NjcxNzIw","avatar_url":"https://avatars3.githubusercontent.com/u/19671720?v=4","gravatar_id":"","url":"https://api.github.com/users/ninadvps","html_url":"https://github.com/ninadvps","followers_url":"https://api.github.com/users/ninadvps/followers","following_url":"https://api.github.com/users/ninadvps/following{/other_user}","gists_url":"https://api.github.com/users/ninadvps/gists{/gist_id}","starred_url":"https://api.github.com/users/ninadvps/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ninadvps/subscriptions","organizations_url":"https://api.github.com/users/ninadvps/orgs","repos_url":"https://api.github.com/users/ninadvps/repos","events_url":"https://api.github.com/users/ninadvps/events{/privacy}","received_events_url":"https://api.github.com/users/ninadvps/received_events","type":"User","site_admin":false},"created_at":"2017-05-24T23:55:04Z","updated_at":"2017-05-24T23:55:04Z","author_association":"NONE","body":"with only elastic client this runs out of metaspace nothing else... why does it happen only with elasticsearch client? And the heap stack shows \r\n<img width=\"1335\" alt=\"screen shot 2017-05-19 at 11 27 28 pm\" src=\"https://cloud.githubusercontent.com/assets/19671720/26430391/b4a9a608-40a1-11e7-9442-101b6ea31f6b.png\">\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/303917375","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-303917375","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":303917375,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMzkxNzM3NQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-05-25T04:10:08Z","updated_at":"2017-05-25T04:10:08Z","author_association":"MEMBER","body":"The heap is irrelevant when you're running out of metaspace. This is not an Elasticsearch issue.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/304076054","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-304076054","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":304076054,"node_id":"MDEyOklzc3VlQ29tbWVudDMwNDA3NjA1NA==","user":{"login":"ninadvps","id":19671720,"node_id":"MDQ6VXNlcjE5NjcxNzIw","avatar_url":"https://avatars3.githubusercontent.com/u/19671720?v=4","gravatar_id":"","url":"https://api.github.com/users/ninadvps","html_url":"https://github.com/ninadvps","followers_url":"https://api.github.com/users/ninadvps/followers","following_url":"https://api.github.com/users/ninadvps/following{/other_user}","gists_url":"https://api.github.com/users/ninadvps/gists{/gist_id}","starred_url":"https://api.github.com/users/ninadvps/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ninadvps/subscriptions","organizations_url":"https://api.github.com/users/ninadvps/orgs","repos_url":"https://api.github.com/users/ninadvps/repos","events_url":"https://api.github.com/users/ninadvps/events{/privacy}","received_events_url":"https://api.github.com/users/ninadvps/received_events","type":"User","site_admin":false},"created_at":"2017-05-25T17:49:08Z","updated_at":"2017-05-25T17:49:08Z","author_association":"NONE","body":"Ok fine Ill deal with it differently, but the metaspace runs out only with es client, we run mongo, or any other thing, we dont face an issue.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/304123942","html_url":"https://github.com/elastic/elasticsearch/issues/24721#issuecomment-304123942","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24721","id":304123942,"node_id":"MDEyOklzc3VlQ29tbWVudDMwNDEyMzk0Mg==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-05-25T21:03:57Z","updated_at":"2017-05-25T21:33:35Z","author_association":"MEMBER","body":">  the metaspace runs out only with es client, we run mongo, or any other thing, we dont face an issue\r\n\r\nThat's irrelevant. The JVM defaults to unlimited metaspace (more precisely 16 exabytes, but that's effectively unlimited) but sbt defaults to limiting the metaspace to 256m. This is why running out of a metaspace is a common problem with sbt.","performed_via_github_app":null}]