{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/13710","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13710/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13710/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13710/events","html_url":"https://github.com/elastic/elasticsearch/issues/13710","id":107688559,"node_id":"MDU6SXNzdWUxMDc2ODg1NTk=","number":13710,"title":"NGram Tokenizer ignoring search terms smaller than n","user":{"login":"mrdotnic","id":11489835,"node_id":"MDQ6VXNlcjExNDg5ODM1","avatar_url":"https://avatars3.githubusercontent.com/u/11489835?v=4","gravatar_id":"","url":"https://api.github.com/users/mrdotnic","html_url":"https://github.com/mrdotnic","followers_url":"https://api.github.com/users/mrdotnic/followers","following_url":"https://api.github.com/users/mrdotnic/following{/other_user}","gists_url":"https://api.github.com/users/mrdotnic/gists{/gist_id}","starred_url":"https://api.github.com/users/mrdotnic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mrdotnic/subscriptions","organizations_url":"https://api.github.com/users/mrdotnic/orgs","repos_url":"https://api.github.com/users/mrdotnic/repos","events_url":"https://api.github.com/users/mrdotnic/events{/privacy}","received_events_url":"https://api.github.com/users/mrdotnic/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2015-09-22T10:46:10Z","updated_at":"2019-07-02T10:58:53Z","closed_at":"2015-09-22T13:44:28Z","author_association":"NONE","active_lock_reason":null,"body":"NGram Tokenizers are a very useful filter to match parts of compound words. Compound words can be commonly found p.e. in the German language, but occur in English as well (p.e. \"thunderstorm\").\nhttps://www.elastic.co/guide/en/elasticsearch/guide/current/ngrams-compound-words.html\n\nNGram (also called n-gram) Tokenizers split up words into token sequences. Each token has a specified minimum gram length (min_gram) and maximum gram length (max_gram). Tokens consisting of three letters p.e. are called trigrams.\nUsing trigrams as our starting point, the word \"peanut\" for example would be divided into the three-letter-tokens \"pea\", \"ean\", \"anu\" and \"nut\".\nSearching for \"peanut\" would therefore also (partially) match words like \"**pea**ce\", \"S**ean**\", \"**Anu**bis\" and \"**nut**rition\". They all contain one of these tokens, but might not be words (or the document if they are all in one) we want to see as our results.\nStatistically speaking we would then be suffering type I errors (false positives).\nhttps://en.wikipedia.org/wiki/Type_I_and_type_II_errors\n\nObviously we might then try longer min_gram lengths, p.e. four or five letters long.\nWith five-grams, \"peanut\" would be split up in \"peanu\" and \"eanut\", giving us far more exact results.\nOne might come to the conclusion, the longer the n-gram, the better.\nThough this is be true to some extent and for some purposes, we will run into another problem.\n\nElasticsearch's NGram Tokenizers ignore search terms with a length smaller than the specified min_gram!\nThat means the actual search term will often be significantly shorter and more vague than intended. If there will be a search term left at all.\n\nWe might p.e. search for \"step by step\", trying to find backpacks of this company. Here we have multiple query strings, combined by the AND operator.\nIf we use our five-gram Tokenizer every word will be ignored. We will have no results resulting in type II errors (false negatives).\nBigrams would respect every word, but lead to a lot of type I errors (see above the \"peanut\" example).\nTrigrams would only exclude the word \"by\" and respect the rest while being more accurate. But trigrams match the unwanted \"Steckdosenleiste\" (= multi-outlet power strip). **\"Ste\"**, \"tep\", **\"Ste\"**, \"tep\" on the one hand and **\"ste\"**, \"tec\", \"eck\", \"ckd\", \"kdo\", \"dos\", \"ose\", \"sen\", \"enl\", \"nle\", \"lei\", \"eis\", \"ist\", **\"ste\"** on the other means 50% of the search terms are matched!\nSo we would might opt for a four-gram Tokenizer, bringing us good results this time.\nBut then searching for \"Jay Smith\" would also give us somebody called \"John Smith\" with the same exact match percentage (type I error).\n\nEven if we wouldn't care for words smaller than four letters, sure enough one can come up with an example where five-grams would be needed.\nSearching for \"Recht\" (= law), a four-gram Tokenizer will match \"Echt + rechnung\" (= calculation/bill + real):\n\"Recht\" splits up into **\"rech\"**, **\"echt\"** while \"Echt + rechnung\" splits up into **\"echt\"** and **\"rech\"**, \"echn\", \"chnu\", \"hnun\", \"nung\", presenting a perfect match. This might come as a surprise, as those words are in two differents fields (the fields \"name\" and \"description\"), making a good example of how unintended and undesired a match can be.\nRespecting the token order (\"Rech\" before \"echt\") of our search term would help here, but is not supported. Same goes for the \"step by step\" example.\n\nIf it were possible for the Elasticsearch NGram Tokenizer to fall back to smaller sized n-grams, using five-grams for \"Smith\" and trigrams for \"Jay\" (instead of ignoring \"Jay completely), our problems described above would be gone. We would have relative n-gram precision according to the length of our search terms. But as it stands, we are forced to choose between lots of type I or lots of type II errors.\n\nTo sum the examples we have the following situation:\n1. A gram size smaller than five leads to problems with our search term \"Recht\" (type I errors).\n2. A gram size larger than three ignores \"jay\" in \"jay smith\" (type I error).\n3. A gram size larger than four ignores \"step by step\" (type II error).\n\nThis is why a combination of three to five grams (p.e. min_gram=3 and max_gram=5) also fails.\n\nFeel free to play around with the examples in my gist: https://www.found.no/play/gist/6d27486e51bdda88dd84\nEdit the min_gram and max_gram sizes to your liking in the \"Analysis\" section (center left), rerun the queries (\"Run\" in the top right) and see the altered \"Results\" section (bottom right) with our Searches # 1, # 2 and # 3.\nSearching for \"jay smith\" corresponds to Search # 1, searching for \"step by step\" to Search # 2 and finally searching for \"recht\" to Search # 3. To try out other search terms, edit the queries in the \"Searches\" section (top right).\nOr edit the \"Documents\" section (top left), to experiment with different words to be searched for.\n\nTo summarize: \nIf we want to match search terms to compound words (\"thunderstorm\"), NGram Tokenizers are a fine tool. They split up words into tokens of a predefined size. \nBut Elasticsearch's fixed size NGram Tokenizers force us to choose between either lots of type I or type II errors. What works well for short words like \"Jay\" won't work for longer words like \"Recht\" (German for law) and vice versa.\nDesired would be a flexible NGram Tokenizer. This means being able to switch from longer NGram Tokenizers (p.e. five-grams) to shorter (p.e. trigrams), as the search terms become shorter. Another, maybe more complicated way could be to make the NGram Tokenizer respect the order of the tokens.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}