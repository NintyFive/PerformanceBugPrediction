[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/8201711","html_url":"https://github.com/elastic/elasticsearch/issues/2224#issuecomment-8201711","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224","id":8201711,"node_id":"MDEyOklzc3VlQ29tbWVudDgyMDE3MTE=","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2012-08-31T18:55:19Z","updated_at":"2012-08-31T18:55:19Z","author_association":"MEMBER","body":"How many nodes are you running? Is there a shard being stuck in recovery by any chance from the mentioned shard 3 of the specified index? Can you issue a `jstack` on the node and gist the response?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/8386737","html_url":"https://github.com/elastic/elasticsearch/issues/2224#issuecomment-8386737","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224","id":8386737,"node_id":"MDEyOklzc3VlQ29tbWVudDgzODY3Mzc=","user":{"login":"awick","id":427321,"node_id":"MDQ6VXNlcjQyNzMyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/427321?v=4","gravatar_id":"","url":"https://api.github.com/users/awick","html_url":"https://github.com/awick","followers_url":"https://api.github.com/users/awick/followers","following_url":"https://api.github.com/users/awick/following{/other_user}","gists_url":"https://api.github.com/users/awick/gists{/gist_id}","starred_url":"https://api.github.com/users/awick/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/awick/subscriptions","organizations_url":"https://api.github.com/users/awick/orgs","repos_url":"https://api.github.com/users/awick/repos","events_url":"https://api.github.com/users/awick/events{/privacy}","received_events_url":"https://api.github.com/users/awick/received_events","type":"User","site_admin":false},"created_at":"2012-09-08T11:30:27Z","updated_at":"2012-09-08T11:30:27Z","author_association":"NONE","body":"15 (64G) machines running 30 nodes (22G Elastic Search Memory)\n\nRight now i was using 10 shards (with \"index.routing.allocation.total_shards_per_node\" :  1) if I increase the number of shards used the problem doesn't seem to happen as much.  Which I guess is an ok solution, but still seems like there is some deadlock issue since I'm not doing too much.\n\nI'm pretty sure not iops or cpu related since these are decent machines only running elasticsearch.\n\nIf I try and flush while this hang is happening I get\n\"reason\" : \"BroadcastShardOperationFailedException[[sessions-120908][2] ]; nested: RemoteTransportException[[moloches-m15b][inet[/X.X.X.X:9301]][indices/flush/s]]; nested: FlushNotAllowedEngineException[[sessions-120908][2] Already flushing...]; \"\n\nI got a jstack while the hang was happening. https://gist.github.com/3673632\n\nTons of entries like\n\n\"elasticsearch[moloches-m15b][bulk][T#3597]\" daemon prio=10 tid=0x00002aaad5aa0800 nid=0x4b94 waiting for monitor entry [0x0000000043fbd000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3562)\n        - waiting to lock <0x000000034c25d708> (a org.apache.lucene.index.XIndexWriter)\n        at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3552)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2067)\n        at org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:581)\n        - locked <0x000000030e0a9a40> (a java.lang.Object)\n        at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\n\n   Locked ownable synchronizers:\n        - <0x000000054f85c4e0> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\n\n\"elasticsearch[moloches-m15b][bulk][T#3561]\" daemon prio=10 tid=0x00002aaad4256000 nid=0x4a53 in Object.wait() [0x00000000476d6000]\n   java.lang.Thread.State: WAITING (on object monitor)\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Object.wait(Object.java:485)\n        at org.apache.lucene.index.IndexWriter$FlushControl.waitUpdate(IndexWriter.java:4884)\n        - locked <0x00000003110100e8> (a org.apache.lucene.index.IndexWriter$FlushControl)\n        at org.apache.lucene.index.IndexWriter$FlushControl.waitUpdate(IndexWriter.java:4878)\n        - locked <0x00000003110100e8> (a org.apache.lucene.index.IndexWriter$FlushControl)\n        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:751)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2060)\n        at org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:581)\n        - locked <0x000000030e0a7ac0> (a java.lang.Object)\n        at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/8386972","html_url":"https://github.com/elastic/elasticsearch/issues/2224#issuecomment-8386972","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224","id":8386972,"node_id":"MDEyOklzc3VlQ29tbWVudDgzODY5NzI=","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2012-09-08T12:10:21Z","updated_at":"2012-09-08T12:16:54Z","author_association":"MEMBER","body":"Just a question outside the scope of your concern : why do you launch 2 nodes per machine ?\n\nIMHO, the \"best\" setup is to have one node per machine with one single shard (or replica) per node.\nThis node should have about half of the total physical memory. Also a single node will open less files than two.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/8387148","html_url":"https://github.com/elastic/elasticsearch/issues/2224#issuecomment-8387148","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224","id":8387148,"node_id":"MDEyOklzc3VlQ29tbWVudDgzODcxNDg=","user":{"login":"awick","id":427321,"node_id":"MDQ6VXNlcjQyNzMyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/427321?v=4","gravatar_id":"","url":"https://api.github.com/users/awick","html_url":"https://github.com/awick","followers_url":"https://api.github.com/users/awick/followers","following_url":"https://api.github.com/users/awick/following{/other_user}","gists_url":"https://api.github.com/users/awick/gists{/gist_id}","starred_url":"https://api.github.com/users/awick/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/awick/subscriptions","organizations_url":"https://api.github.com/users/awick/orgs","repos_url":"https://api.github.com/users/awick/repos","events_url":"https://api.github.com/users/awick/events{/privacy}","received_events_url":"https://api.github.com/users/awick/received_events","type":"User","site_admin":false},"created_at":"2012-09-08T12:35:32Z","updated_at":"2012-09-08T12:35:32Z","author_association":"NONE","body":"My app is high write, extremely low read (with mullti second query times normal), and most importantly high facet using.  That last part is key, it means that elastic needs the memory, not the OS disk cache.  I run multiple nodes since I want to use UseCompressedOops, since I assume (hopefully correctly) that will leave memory memory for facets.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/8391543","html_url":"https://github.com/elastic/elasticsearch/issues/2224#issuecomment-8391543","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224","id":8391543,"node_id":"MDEyOklzc3VlQ29tbWVudDgzOTE1NDM=","user":{"login":"awick","id":427321,"node_id":"MDQ6VXNlcjQyNzMyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/427321?v=4","gravatar_id":"","url":"https://api.github.com/users/awick","html_url":"https://github.com/awick","followers_url":"https://api.github.com/users/awick/followers","following_url":"https://api.github.com/users/awick/following{/other_user}","gists_url":"https://api.github.com/users/awick/gists{/gist_id}","starred_url":"https://api.github.com/users/awick/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/awick/subscriptions","organizations_url":"https://api.github.com/users/awick/orgs","repos_url":"https://api.github.com/users/awick/repos","events_url":"https://api.github.com/users/awick/events{/privacy}","received_events_url":"https://api.github.com/users/awick/received_events","type":"User","site_admin":false},"created_at":"2012-09-08T17:08:29Z","updated_at":"2012-09-08T17:08:29Z","author_association":"NONE","body":"So after several hours it still didn't recover.\n\nI did a curl -XPOST http://localhost:9201/_cluster/nodes/_local/_shutdown\n\nwhich stopped the HTTP interface but didn't stop the process\n\n[2012-09-08 12:54:38,524][INFO ][cluster.service          ] [moloches-m15b] removed {[moloches-m15a][K9NUdozvShuhrnaC8axN8w][inet[/X.X.X.X:9300]]{max_local_storage_nodes=2},}, reason: zen-disco-receive(from master [[moloches-m03a][22ar_oX6S4qvv37_KIuEHQ][inet[/X.X.X.X:9300]]{max_local_storage_nodes=2}])\n[2012-09-08 12:54:46,664][INFO ][action.admin.cluster.node.shutdown] [moloches-m15b] shutting down in [200ms]\n[2012-09-08 12:54:46,867][INFO ][action.admin.cluster.node.shutdown] [moloches-m15b] initiating requested shutdown...\n[2012-09-08 12:54:46,868][INFO ][node                     ] [moloches-m15b] {0.19.9}[11618]: stopping ...\n[2012-09-08 12:56:24,702][INFO ][node                     ] [moloches-m15b] {0.19.9}[11618]: closing ...\n[2012-09-08 12:56:24,737][DEBUG][discovery.zen.fd         ] [moloches-m15b] [master] stopping fault detection against master [[moloches-m03a][22ar_oX6S4qvv37_KIuEHQ][inet[/X.X.X.X:9300]]{max_local_storage_nodes=2}], reason [zen disco stop]\n[2012-09-08 12:56:25,718][DEBUG][action.bulk              ] [moloches-m15b] [sessions-120908][2] failed to execute bulk item (index) index {[sessions-120908][session][120908-UxuT1bVU_J1O24xuOSgwy3s1], source[{\"fp\":1347095588,\"lp\":1347095595,\"a1\":2894203953,\"g1\":\"USA\",\"p1\":39596,\"a2\":3305523277,\"g2\":\"TUN\",\"p2\":11301,\"pr\":17,\"pa\":8,\"by\":2640,\"db\":2576,\"ps\":[2648320844911980,2648320896463782,2648320897664843,2648320962516726,2648320965823569,2648320966451606,2648321009095386,2648321009962153],\"ta\":[2,4,25675],\"fs\":[38538]}]}\norg.elasticsearch.index.engine.IndexFailedEngineException: [sessions-120908][2] Index failed for [session#120908-UxuT1bVU_J1O24xuOSgwy3s1]\n    at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:506)\n    at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:662)\nCaused by: org.apache.lucene.store.AlreadyClosedException: this Analyzer is closed\n    at org.apache.lucene.analysis.Analyzer.getPreviousTokenStream(Analyzer.java:93)\n    at org.apache.lucene.analysis.snowball.SnowballAnalyzer.reusableTokenStream(SnowballAnalyzer.java:109)\n    at org.elasticsearch.index.analysis.NamedAnalyzer.reusableTokenStream(NamedAnalyzer.java:79)\n    at org.elasticsearch.index.analysis.FieldNameAnalyzer.reusableTokenStream(FieldNameAnalyzer.java:60)\n    at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:132)\n    at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:276)\n    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:766)\n    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2060)\n    at org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:581)\n    at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)\n    ... 7 more\n[2012-09-08 12:56:25,725][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.\njava.util.concurrent.RejectedExecutionException\n    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1768)\n    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)\n    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)\n    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker.start(DeadLockProofWorker.java:38)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.start(AbstractNioWorker.java:184)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:330)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:35)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:313)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:35)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:637)\n    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:504)\n    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:47)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:659)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:578)\n    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:712)\n    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:679)\n    at org.elasticsearch.common.netty.channel.AbstractChannel.write(AbstractChannel.java:246)\n    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:104)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$OperationTransportHandler$1.onFailure(TransportShardReplicationOperationAction.java:232)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$3.onClose(TransportShardReplicationOperationAction.java:497)\n    at org.elasticsearch.cluster.service.InternalClusterService.add(InternalClusterService.java:180)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.retry(TransportShardReplicationOperationAction.java:485)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:538)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:662)\n\nI eventually kill -9 the process, and restart it.  Which causes it to hang right away.  I stop the process, remove the shard off the disk, copy a empty version on top, and now everything is fine.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/8402781","html_url":"https://github.com/elastic/elasticsearch/issues/2224#issuecomment-8402781","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224","id":8402781,"node_id":"MDEyOklzc3VlQ29tbWVudDg0MDI3ODE=","user":{"login":"awick","id":427321,"node_id":"MDQ6VXNlcjQyNzMyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/427321?v=4","gravatar_id":"","url":"https://api.github.com/users/awick","html_url":"https://github.com/awick","followers_url":"https://api.github.com/users/awick/followers","following_url":"https://api.github.com/users/awick/following{/other_user}","gists_url":"https://api.github.com/users/awick/gists{/gist_id}","starred_url":"https://api.github.com/users/awick/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/awick/subscriptions","organizations_url":"https://api.github.com/users/awick/orgs","repos_url":"https://api.github.com/users/awick/repos","events_url":"https://api.github.com/users/awick/events{/privacy}","received_events_url":"https://api.github.com/users/awick/received_events","type":"User","site_admin":false},"created_at":"2012-09-09T11:28:51Z","updated_at":"2012-09-09T11:28:51Z","author_association":"NONE","body":"Spoke to soon about raising the shards helping.  I went to 30 shards and this morning woke up to another flush hang.  Different machine/shard number.\n\ncurl http://localhost:9200/_flush\\?pretty\n{\n  \"ok\" : true,\n  \"_shards\" : {\n    \"total\" : 366,\n    \"successful\" : 365,\n    \"failed\" : 1,\n    \"failures\" : [ {\n      \"index\" : \"sessions-120909\",\n      \"shard\" : 23,\n      \"reason\" : \"BroadcastShardOperationFailedException[[sessions-120909][23] ]; nested: FlushNotAllowedEngineException[[sessions-120909][23] Already flushing...]; \"\n    } ]\n  }\n}\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/8403422","html_url":"https://github.com/elastic/elasticsearch/issues/2224#issuecomment-8403422","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224","id":8403422,"node_id":"MDEyOklzc3VlQ29tbWVudDg0MDM0MjI=","user":{"login":"awick","id":427321,"node_id":"MDQ6VXNlcjQyNzMyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/427321?v=4","gravatar_id":"","url":"https://api.github.com/users/awick","html_url":"https://github.com/awick","followers_url":"https://api.github.com/users/awick/followers","following_url":"https://api.github.com/users/awick/following{/other_user}","gists_url":"https://api.github.com/users/awick/gists{/gist_id}","starred_url":"https://api.github.com/users/awick/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/awick/subscriptions","organizations_url":"https://api.github.com/users/awick/orgs","repos_url":"https://api.github.com/users/awick/repos","events_url":"https://api.github.com/users/awick/events{/privacy}","received_events_url":"https://api.github.com/users/awick/received_events","type":"User","site_admin":false},"created_at":"2012-09-09T12:59:50Z","updated_at":"2012-09-09T12:59:50Z","author_association":"NONE","body":"Here is the status for the shard that is hung.  Merge section looks bad - 6 current merges for 12.2 hours (the shard was created a little less then 13 hours ago)\n\n23: [\n{\nrouting: {\nstate: STARTED\nprimary: true\nnode: P9HExwI2TqOYRszOsTeGmQ\nrelocating_node: null\nshard: 23\nindex: sessions-120909\n}\nstate: STARTED\nindex: {\nsize: 9.8gb\nsize_in_bytes: 10612211214\n}\ntranslog: {\nid: 1347148801443\noperations: 25759\n}\ndocs: {\nnum_docs: 7410594\nmax_doc: 7410594\ndeleted_docs: 0\n}\nmerges: {\ncurrent: 6\ncurrent_docs: 54\ncurrent_size: 149.4kb\ncurrent_size_in_bytes: 153024\ntotal: 49880\ntotal_time: 12.2h\ntotal_time_in_millis: 44246930\ntotal_docs: 45288021\ntotal_size: 64.4gb\ntotal_size_in_bytes: 69236703248\n}\nrefresh: {\ntotal: 394937\ntotal_time: 5.1h\ntotal_time_in_millis: 18469193\n}\nflush: {\ntotal: 1288\ntotal_time: 2.7m\ntotal_time_in_millis: 167848\n}\n\nHere is similar output for a shard that isn't having the issue.  The merge section looks fine.\n22: [\n{\nrouting: {\nstate: STARTED\nprimary: true\nnode: 2GcOSrjdQm25k37jpUVXCA\nrelocating_node: null\nshard: 22\nindex: sessions-120909\n}\nstate: STARTED\nindex: {\nsize: 9.8gb\nsize_in_bytes: 10568233662\n}\ntranslog: {\nid: 1347148801525\noperations: 90\n}\ndocs: {\nnum_docs: 7414334\nmax_doc: 7414334\ndeleted_docs: 0\n}\nmerges: {\ncurrent: 0\ncurrent_docs: 0\ncurrent_size: 0b\ncurrent_size_in_bytes: 0\ntotal: 50044\ntotal_time: 31m\ntotal_time_in_millis: 1863813\ntotal_docs: 45742651\ntotal_size: 65.1gb\ntotal_size_in_bytes: 69905301434\n}\nrefresh: {\ntotal: 399587\ntotal_time: 33.9m\ntotal_time_in_millis: 2039999\n}\nflush: {\ntotal: 1307\ntotal_time: 2.5m\ntotal_time_in_millis: 155000\n}\n}\n]\n","performed_via_github_app":null}]