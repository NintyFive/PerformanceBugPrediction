[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/346162063","html_url":"https://github.com/elastic/elasticsearch/issues/27038#issuecomment-346162063","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27038","id":346162063,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NjE2MjA2Mw==","user":{"login":"mayya-sharipova","id":5738841,"node_id":"MDQ6VXNlcjU3Mzg4NDE=","avatar_url":"https://avatars1.githubusercontent.com/u/5738841?v=4","gravatar_id":"","url":"https://api.github.com/users/mayya-sharipova","html_url":"https://github.com/mayya-sharipova","followers_url":"https://api.github.com/users/mayya-sharipova/followers","following_url":"https://api.github.com/users/mayya-sharipova/following{/other_user}","gists_url":"https://api.github.com/users/mayya-sharipova/gists{/gist_id}","starred_url":"https://api.github.com/users/mayya-sharipova/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mayya-sharipova/subscriptions","organizations_url":"https://api.github.com/users/mayya-sharipova/orgs","repos_url":"https://api.github.com/users/mayya-sharipova/repos","events_url":"https://api.github.com/users/mayya-sharipova/events{/privacy}","received_events_url":"https://api.github.com/users/mayya-sharipova/received_events","type":"User","site_admin":false},"created_at":"2017-11-21T21:10:40Z","updated_at":"2017-11-21T22:45:03Z","author_association":"CONTRIBUTOR","body":"@colings86 What should a reasonable default limit on number of tokens? Will 10000 be a good limit?\r\nAlso, should we throw an error when number of generated tokens exceeds this limit, or should we silently stop consuming more tokens when the limit is exceeded?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/346265024","html_url":"https://github.com/elastic/elasticsearch/issues/27038#issuecomment-346265024","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27038","id":346265024,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NjI2NTAyNA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-11-22T07:25:09Z","updated_at":"2017-11-22T07:25:09Z","author_association":"CONTRIBUTOR","body":"> Will 10000 be a good limit?\r\n\r\n10k sounds good to me.\r\n\r\n> should we throw an error when number of generated tokens exceeds this limit, or should we silently stop consuming more tokens when the limit is exceeded?\r\n\r\nIgnoring tokens silently sounds trappy to me. I would rather fail the request.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/346353272","html_url":"https://github.com/elastic/elasticsearch/issues/27038#issuecomment-346353272","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27038","id":346353272,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NjM1MzI3Mg==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-11-22T13:42:51Z","updated_at":"2017-11-22T13:42:51Z","author_association":"CONTRIBUTOR","body":"what is the plan here to use this limit. will we only limit highlighting usecases of also on indexing? I wonder if we should only do this for highlighting for now.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/346354098","html_url":"https://github.com/elastic/elasticsearch/issues/27038#issuecomment-346354098","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27038","id":346354098,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NjM1NDA5OA==","user":{"login":"mayya-sharipova","id":5738841,"node_id":"MDQ6VXNlcjU3Mzg4NDE=","avatar_url":"https://avatars1.githubusercontent.com/u/5738841?v=4","gravatar_id":"","url":"https://api.github.com/users/mayya-sharipova","html_url":"https://github.com/mayya-sharipova","followers_url":"https://api.github.com/users/mayya-sharipova/followers","following_url":"https://api.github.com/users/mayya-sharipova/following{/other_user}","gists_url":"https://api.github.com/users/mayya-sharipova/gists{/gist_id}","starred_url":"https://api.github.com/users/mayya-sharipova/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mayya-sharipova/subscriptions","organizations_url":"https://api.github.com/users/mayya-sharipova/orgs","repos_url":"https://api.github.com/users/mayya-sharipova/repos","events_url":"https://api.github.com/users/mayya-sharipova/events{/privacy}","received_events_url":"https://api.github.com/users/mayya-sharipova/received_events","type":"User","site_admin":false},"created_at":"2017-11-22T13:46:19Z","updated_at":"2017-11-22T13:46:19Z","author_association":"CONTRIBUTOR","body":"I was thinking it only applies to `_analyze` API, as it is originated from this issue.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/346554827","html_url":"https://github.com/elastic/elasticsearch/issues/27038#issuecomment-346554827","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27038","id":346554827,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NjU1NDgyNw==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2017-11-23T08:25:47Z","updated_at":"2017-11-23T08:25:47Z","author_association":"MEMBER","body":"I was thinking that this should apply to anywhere we use analyzers at search time. So the `query_string_query`, `match` query` , etc. would apply this limit when analysing the query to stop queries that expand into massive number of tokens from running, and it would also apply to highlighting as @s1monw mentioned and the _analyze API could use this limit too as @mayya-sharipova mentioned","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/346820054","html_url":"https://github.com/elastic/elasticsearch/issues/27038#issuecomment-346820054","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27038","id":346820054,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NjgyMDA1NA==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2017-11-24T12:39:29Z","updated_at":"2017-11-24T12:39:29Z","author_association":"MEMBER","body":"We had a chat with @colings86 and @mayya-sharipova about this and decided that it would be difficult to apply the same limit to queries and highlighters.\r\nFor queries, it should be simpler to limit the size of the input to a few kbytes and get done with it since we don't want/need to handle big queries in `query_string` or `match` queries.\r\nFor highlighters, the main issue is when the field is not indexed with `offsets` nor `term_vectors`. In such case we need to re-analyze the text to extract the offsets of the matching terms and this can create a lot of tokens if the text to analyze is big. So instead of adding a soft limit, we've decided to add a new feature in the highlighter that would limit the number of tokens that should be considered during the re-analysis. When this limit is reached the remaining tokens are discarded and a flag should be added in the response to indicate that the highlighting response for this hit is partial.\r\nThis issue originated from an highlighting issue with big texts so I am closing it in favor of https://github.com/elastic/elasticsearch/issues/27517 that will try to address the problem differently.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/346821413","html_url":"https://github.com/elastic/elasticsearch/issues/27038#issuecomment-346821413","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27038","id":346821413,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NjgyMTQxMw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-11-24T12:47:20Z","updated_at":"2017-11-24T12:47:20Z","author_association":"CONTRIBUTOR","body":"> This issue originated from an highlighting issue with big texts\r\n\r\nThis is not correct. :) This issue originated from an admin who wanted to prevent callers of the `_analyze` API from making the cluster run out-of-memory. So I think we should keep it open and address it?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/346821954","html_url":"https://github.com/elastic/elasticsearch/issues/27038#issuecomment-346821954","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27038","id":346821954,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NjgyMTk1NA==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2017-11-24T12:50:14Z","updated_at":"2017-11-24T12:50:14Z","author_association":"MEMBER","body":"Oups I mixed two cases then sorry. Let's reopen, sorry for the noise I got confused with the comments.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/346823655","html_url":"https://github.com/elastic/elasticsearch/issues/27038#issuecomment-346823655","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27038","id":346823655,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NjgyMzY1NQ==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2017-11-24T12:59:45Z","updated_at":"2017-11-24T12:59:45Z","author_association":"MEMBER","body":"So let's just add a limit for the `_analyze` endpoint and fails the request if the number of produced tokens hits the limit:\r\n@mayya-sharipova @colings86 ^^","performed_via_github_app":null}]