[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/501641295","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-501641295","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":501641295,"node_id":"MDEyOklzc3VlQ29tbWVudDUwMTY0MTI5NQ==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-06-13T10:14:22Z","updated_at":"2019-06-13T10:14:22Z","author_association":"COLLABORATOR","body":"Pinging @elastic/ml-core","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/501695060","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-501695060","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":501695060,"node_id":"MDEyOklzc3VlQ29tbWVudDUwMTY5NTA2MA==","user":{"login":"davidkyle","id":2353640,"node_id":"MDQ6VXNlcjIzNTM2NDA=","avatar_url":"https://avatars1.githubusercontent.com/u/2353640?v=4","gravatar_id":"","url":"https://api.github.com/users/davidkyle","html_url":"https://github.com/davidkyle","followers_url":"https://api.github.com/users/davidkyle/followers","following_url":"https://api.github.com/users/davidkyle/following{/other_user}","gists_url":"https://api.github.com/users/davidkyle/gists{/gist_id}","starred_url":"https://api.github.com/users/davidkyle/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidkyle/subscriptions","organizations_url":"https://api.github.com/users/davidkyle/orgs","repos_url":"https://api.github.com/users/davidkyle/repos","events_url":"https://api.github.com/users/davidkyle/events{/privacy}","received_events_url":"https://api.github.com/users/davidkyle/received_events","type":"User","site_admin":false},"created_at":"2019-06-13T13:08:15Z","updated_at":"2019-06-13T13:08:15Z","author_association":"MEMBER","body":"> The data frame silently fails to index these values resulting in an entirely empty dest index.\r\n\r\nI can't reproduce this on master or the 7.2 bc6. I can see documents written to the index with a `null` value for the `mean_respose` field:\r\n\r\n```\r\n      \"_source\" : {\r\n          \"last_updated\" : null,\r\n          \"count\" : 8728.0,\r\n          \"mean_response\" : null,\r\n          \"airline\" : \"AAL\"\r\n        }\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/501779374","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-501779374","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":501779374,"node_id":"MDEyOklzc3VlQ29tbWVudDUwMTc3OTM3NA==","user":{"login":"sophiec20","id":4185750,"node_id":"MDQ6VXNlcjQxODU3NTA=","avatar_url":"https://avatars2.githubusercontent.com/u/4185750?v=4","gravatar_id":"","url":"https://api.github.com/users/sophiec20","html_url":"https://github.com/sophiec20","followers_url":"https://api.github.com/users/sophiec20/followers","following_url":"https://api.github.com/users/sophiec20/following{/other_user}","gists_url":"https://api.github.com/users/sophiec20/gists{/gist_id}","starred_url":"https://api.github.com/users/sophiec20/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sophiec20/subscriptions","organizations_url":"https://api.github.com/users/sophiec20/orgs","repos_url":"https://api.github.com/users/sophiec20/repos","events_url":"https://api.github.com/users/sophiec20/events{/privacy}","received_events_url":"https://api.github.com/users/sophiec20/received_events","type":"User","site_admin":false},"created_at":"2019-06-13T16:34:45Z","updated_at":"2019-06-13T16:34:45Z","author_association":"NONE","body":"I'll retry on BC6.\r\n\r\nNote that we have also seen silent failures when the deduced mapping was `half_float` and the `sum(value)` overflowed. cc @benwtrent ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/501858539","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-501858539","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":501858539,"node_id":"MDEyOklzc3VlQ29tbWVudDUwMTg1ODUzOQ==","user":{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false},"created_at":"2019-06-13T20:04:04Z","updated_at":"2019-06-13T20:04:04Z","author_association":"MEMBER","body":"@sophiec20 yes, 7.3 is now writing an audit when there are bulk indexing issues. If desired, it can be backported to 7.2. However, I am not sure that is necessary because the UI is not displaying audit messages yet. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/502019787","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-502019787","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":502019787,"node_id":"MDEyOklzc3VlQ29tbWVudDUwMjAxOTc4Nw==","user":{"login":"sophiec20","id":4185750,"node_id":"MDQ6VXNlcjQxODU3NTA=","avatar_url":"https://avatars2.githubusercontent.com/u/4185750?v=4","gravatar_id":"","url":"https://api.github.com/users/sophiec20","html_url":"https://github.com/sophiec20","followers_url":"https://api.github.com/users/sophiec20/followers","following_url":"https://api.github.com/users/sophiec20/following{/other_user}","gists_url":"https://api.github.com/users/sophiec20/gists{/gist_id}","starred_url":"https://api.github.com/users/sophiec20/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sophiec20/subscriptions","organizations_url":"https://api.github.com/users/sophiec20/orgs","repos_url":"https://api.github.com/users/sophiec20/repos","events_url":"https://api.github.com/users/sophiec20/events{/privacy}","received_events_url":"https://api.github.com/users/sophiec20/received_events","type":"User","site_admin":false},"created_at":"2019-06-14T08:29:33Z","updated_at":"2019-06-14T08:29:33Z","author_association":"NONE","body":"@benwtrent Not worth a backport to 7.2 at this time. Do these failures get counted by `_stats`?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/502086482","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-502086482","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":502086482,"node_id":"MDEyOklzc3VlQ29tbWVudDUwMjA4NjQ4Mg==","user":{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false},"created_at":"2019-06-14T12:15:54Z","updated_at":"2019-06-14T12:15:54Z","author_association":"MEMBER","body":"@sophiec20 no, it will only write a `warning` log entry.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/504138008","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-504138008","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":504138008,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDEzODAwOA==","user":{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false},"created_at":"2019-06-20T18:42:19Z","updated_at":"2019-06-20T18:42:19Z","author_association":"MEMBER","body":"For the sum specific issue: https://github.com/elastic/elasticsearch/pull/43213\r\nAlerting when there are bulk index issues: https://github.com/elastic/elasticsearch/pull/43106\r\nHandling `null` values better for aggs: https://github.com/elastic/elasticsearch/pull/42966\r\n\r\n@sophiec20 and @davidkyle I think these three PRs address the concerns in this issue. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/504396333","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-504396333","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":504396333,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDM5NjMzMw==","user":{"login":"sophiec20","id":4185750,"node_id":"MDQ6VXNlcjQxODU3NTA=","avatar_url":"https://avatars2.githubusercontent.com/u/4185750?v=4","gravatar_id":"","url":"https://api.github.com/users/sophiec20","html_url":"https://github.com/sophiec20","followers_url":"https://api.github.com/users/sophiec20/followers","following_url":"https://api.github.com/users/sophiec20/following{/other_user}","gists_url":"https://api.github.com/users/sophiec20/gists{/gist_id}","starred_url":"https://api.github.com/users/sophiec20/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sophiec20/subscriptions","organizations_url":"https://api.github.com/users/sophiec20/orgs","repos_url":"https://api.github.com/users/sophiec20/repos","events_url":"https://api.github.com/users/sophiec20/events{/privacy}","received_events_url":"https://api.github.com/users/sophiec20/received_events","type":"User","site_admin":false},"created_at":"2019-06-21T11:43:47Z","updated_at":"2019-06-21T11:43:47Z","author_association":"NONE","body":"@benwtrent yes, those 3 PRs do address many of the concerns around silently failing..\r\n\r\nBefore we close this I have one more question, under what conditions does stats `index_failures` get incremented?\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/504402744","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-504402744","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":504402744,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDQwMjc0NA==","user":{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false},"created_at":"2019-06-21T12:09:25Z","updated_at":"2019-06-21T12:09:25Z","author_association":"MEMBER","body":"`index_failures` gets incremented if there is an exception thrown during the bulk indexing process. I am not 100% sure why this type of behavior is the way it is, but the Rollups folks may have better input as to why we don't count a bulk indexing failure as an `index_failure` (it seems to me like we should). ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/504406049","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-504406049","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":504406049,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDQwNjA0OQ==","user":{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false},"created_at":"2019-06-21T12:21:59Z","updated_at":"2019-06-21T12:21:59Z","author_association":"MEMBER","body":"@polyfractal @jimczi what do y'all think? Should BulkIndex failures increment the `index_failure` statistic for the `AsyncTwoPhaseIndexer`? It seems to me like they should. Otherwise there is no obvious indication that such a failure happened.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/504411848","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-504411848","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":504411848,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDQxMTg0OA==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2019-06-21T12:43:18Z","updated_at":"2019-06-21T12:43:18Z","author_association":"MEMBER","body":"> Should BulkIndex failures increment the index_failure statistic for the AsyncTwoPhaseIndexer? \r\n\r\nI am not sure because it currently contains the number of times that the job failed due to a failure in bulk indexing. If the entire bulk request fails we stop the job and increment the counter whereas if only individual documents failed we ignore and continue the processing. So maybe we should treat all indexing errors as fatal and stop the job ? Most of these errors should be transient so resuming a job a with a backoff policy should be enough. For rollups it is acceptable to loose some documents (debatable but we've made this decision long time ago ;)) but there is no way to go back in time to replay those missing documents so I'd prefer to fail rather than incrementing a counter and move to indexer state. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/504423963","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-504423963","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":504423963,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDQyMzk2Mw==","user":{"login":"sophiec20","id":4185750,"node_id":"MDQ6VXNlcjQxODU3NTA=","avatar_url":"https://avatars2.githubusercontent.com/u/4185750?v=4","gravatar_id":"","url":"https://api.github.com/users/sophiec20","html_url":"https://github.com/sophiec20","followers_url":"https://api.github.com/users/sophiec20/followers","following_url":"https://api.github.com/users/sophiec20/following{/other_user}","gists_url":"https://api.github.com/users/sophiec20/gists{/gist_id}","starred_url":"https://api.github.com/users/sophiec20/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sophiec20/subscriptions","organizations_url":"https://api.github.com/users/sophiec20/orgs","repos_url":"https://api.github.com/users/sophiec20/repos","events_url":"https://api.github.com/users/sophiec20/events{/privacy}","received_events_url":"https://api.github.com/users/sophiec20/received_events","type":"User","site_admin":false},"created_at":"2019-06-21T13:23:27Z","updated_at":"2019-06-21T13:23:27Z","author_association":"NONE","body":"> ... it is acceptable to loose some documents ...\r\n\r\nIt depends, yes and no, depending on the use case. Being able to quantify how many documents are lost in bulk allows the user to somewhat quantify this impact and they can make a choice. I don't recall being able to easily tell how many documents were lost in `_bulk` .. can we tell?\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/504433128","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-504433128","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":504433128,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDQzMzEyOA==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2019-06-21T13:50:59Z","updated_at":"2019-06-21T13:50:59Z","author_association":"MEMBER","body":"> I don't recall being able to easily tell how many documents were lost in _bulk .. can we tell?\r\n\r\nYes that's what we ignore here. The bulk response contains the list of success and failures so we can increment the counter of missing documents easily. My point here is that we don't have a way to tell a job to retry from a specific point so if you have missing documents and you're not ok with it the only solution currently is to remove the job and start again. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/504433547","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-504433547","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":504433547,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDQzMzU0Nw==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2019-06-21T13:52:05Z","updated_at":"2019-06-21T13:52:05Z","author_association":"MEMBER","body":"> I don't recall being able to easily tell how many documents were lost in _bulk .. can we tell?\r\n\r\nThere's not a specific counter, no.  The bulk response has a high-level property that tells you if there are failures.  But if you want the total count and how they failed, you have to iterate over the response and count/collect the individual failures.\r\n\r\n> So maybe we should treat all indexing errors as fatal and stop the job ?  [...] but there is no way to go back in time to replay those missing documents so I'd prefer to fail rather than incrementing a counter and move to indexer state.\r\n\r\nYeah I'd be fine with failing the current indexer run if there are bulk failures, which will force the indexer to backtrack and re-run on the next trigger.  There's a `todo` note in the bulk section which basically says \"what should we do here?\" :)\r\n\r\nAnother solution is to re-index the failed documents before continuing.  We didn't implement that because it opens up a whole host of non-trivial edge-cases (what happens if those fail too? How do you chain it into the existing indexer cycle? etc etc).\r\n\r\n> Handling null values better for aggs: #42966\r\n\r\nJust an FYI, we [implemented some \"inspection helpers\"](\r\nhttps://github.com/elastic/elasticsearch/pull/36020) for SQL a while ago.  Given an internal agg result, the helpers will tell you if they have a value, using the specific rules for each type of agg (`min` uses `+Infinity`, etc).  Probably the same as what you implemented, just maintained by the agg framework so you don't have to :)\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/504451861","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-504451861","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":504451861,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDQ1MTg2MQ==","user":{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false},"created_at":"2019-06-21T14:44:05Z","updated_at":"2019-06-21T14:44:05Z","author_association":"MEMBER","body":"Awesome, I (or somebody else) will write up a PR so that we raise when there is a bulk index failure. This will simplify other aspects. \r\n\r\nI will in parallel look into the `inspection helpers`. I was unsure how to get to the `InternalAggregation` class from the `Aggregation` value in the search response. Will dig around to see what I find. \r\n\r\n\r\n\r\nI do not think `AsyncTwoPhaseIndexer` should auto-retry. I do not want to stick all users and future users of the class with a sub-optimal abstraction. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/509563116","html_url":"https://github.com/elastic/elasticsearch/issues/43194#issuecomment-509563116","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43194","id":509563116,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTU2MzExNg==","user":{"login":"hendrikmuhs","id":7126422,"node_id":"MDQ6VXNlcjcxMjY0MjI=","avatar_url":"https://avatars3.githubusercontent.com/u/7126422?v=4","gravatar_id":"","url":"https://api.github.com/users/hendrikmuhs","html_url":"https://github.com/hendrikmuhs","followers_url":"https://api.github.com/users/hendrikmuhs/followers","following_url":"https://api.github.com/users/hendrikmuhs/following{/other_user}","gists_url":"https://api.github.com/users/hendrikmuhs/gists{/gist_id}","starred_url":"https://api.github.com/users/hendrikmuhs/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hendrikmuhs/subscriptions","organizations_url":"https://api.github.com/users/hendrikmuhs/orgs","repos_url":"https://api.github.com/users/hendrikmuhs/repos","events_url":"https://api.github.com/users/hendrikmuhs/events{/privacy}","received_events_url":"https://api.github.com/users/hendrikmuhs/received_events","type":"User","site_admin":false},"created_at":"2019-07-09T09:18:02Z","updated_at":"2019-07-09T09:18:02Z","author_association":"CONTRIBUTOR","body":"I created #44101 as a follow up regarding behavior on bulk index failures.\r\n\r\nBest to my knowledge this issue can be closed now as the original issue has been solved and #44101 covers the rest.\r\n\r\n@benwtrent please re-open if I misunderstood something","performed_via_github_app":null}]