{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/20631","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20631/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20631/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20631/events","html_url":"https://github.com/elastic/elasticsearch/issues/20631","id":178586906,"node_id":"MDU6SXNzdWUxNzg1ODY5MDY=","number":20631,"title":"Node cannot recover from restart: Failed to deserialize response of type [org.elasticsearch.xpack.action.XPackUsageResponse]","user":{"login":"tomas-mazak","id":2614161,"node_id":"MDQ6VXNlcjI2MTQxNjE=","avatar_url":"https://avatars1.githubusercontent.com/u/2614161?v=4","gravatar_id":"","url":"https://api.github.com/users/tomas-mazak","html_url":"https://github.com/tomas-mazak","followers_url":"https://api.github.com/users/tomas-mazak/followers","following_url":"https://api.github.com/users/tomas-mazak/following{/other_user}","gists_url":"https://api.github.com/users/tomas-mazak/gists{/gist_id}","starred_url":"https://api.github.com/users/tomas-mazak/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tomas-mazak/subscriptions","organizations_url":"https://api.github.com/users/tomas-mazak/orgs","repos_url":"https://api.github.com/users/tomas-mazak/repos","events_url":"https://api.github.com/users/tomas-mazak/events{/privacy}","received_events_url":"https://api.github.com/users/tomas-mazak/received_events","type":"User","site_admin":false},"labels":[{"id":152510590,"node_id":"MDU6TGFiZWwxNTI1MTA1OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Recovery","name":":Distributed/Recovery","color":"0e8a16","default":false,"description":"Anything around constructing a new shard, either from a local or a remote source."},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2016-09-22T11:39:10Z","updated_at":"2017-05-09T08:14:52Z","closed_at":"2016-10-17T08:50:10Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\nGitHub is reserved for bug reports and feature requests. The best place\nto ask a general question is at the Elastic Discourse forums at\nhttps://discuss.elastic.co. If you are in fact posting a bug report or\na feature request, please include one and only one of the below blocks\nin your new issue. Note that whether you're filing a bug report or a\nfeature request, ensure that your submission is for an\n[OS that we support](https://www.elastic.co/support/matrix#show_os).\nBug reports on an OS that we do not support or feature requests\nspecific to an OS that we do not support will be closed.\n-->\n\n<!--\nIf you are filing a bug report, please remove the below feature\nrequest block and provide responses for all of the below items.\n-->\n\n**Elasticsearch version**: 5.0.0-alpha5\n\n**Plugins installed**: [x-pack 5.0.0-alpha5]\n\n**JVM version**: openjdk-8-jre:amd64                 8u102-b14.1-1~bpo8+1\n\n**OS version**: Debian Jessie amd64\n\n**Description of the problem including expected versus actual behavior**:\nAfter restarting, the node ends up continually leaving and joining the cluster, not able to start working properly (see the log below). After deleting all data on the affected node and restarting elasticsearch again, the node joins the cluster and starts working properly.\n\n**Steps to reproduce**:\nIn order to reproduce this problem, I need to reinstall the broken version of elasticsearch data dir and restart elasticsearch. As it happened twice, I have two instances of \"broken data\". The data is large and contains confidential information, so I cannot provide it here. However, if somebody is interested in looking at this issue, I can help analyzing the data.\n\n**Provide logs (if relevant)**:\n[2016-09-22 13:18:16,621][INFO ][cluster.service          ] [epyk-node1] detected_master {epyk-arbiter}{06_SMr8iSpuyzPG0n1dXCg}{vZiHtdyZS_OicIxqI7lXjw}{10.32.56.22}{10.32.56.22:9300}, added {{epyk-arbiter}{06_SMr8iSpuyzPG0n1dXCg}{vZiHtdyZS_OicIxqI7lXjw}{10.32.56.22}{10.32.56.22:9300},}, reason: zen-disco-receive(from master [master {epyk-arbiter}{06_SMr8iSpuyzPG0n1dXCg}{vZiHtdyZS_OicIxqI7lXjw}{10.32.56.22}{10.32.56.22:9300} committed version [1498]])\n[2016-09-22 13:18:18,653][WARN ][rest.suppressed          ] path: _xpack/usage, params: {}\nRemoteTransportException[[Failed to deserialize response of type [org.elasticsearch.xpack.action.XPackUsageResponse]]]; nested: TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.xpack.action.XPackUsageResponse]]; nested: IOException[Can't read unknown type [97]];\nCaused by: TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.xpack.action.XPackUsageResponse]]; nested: IOException[Can't read unknown type [97]];\n    at org.elasticsearch.transport.TcpTransport.handleResponse(TcpTransport.java:1234)\n    at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1206)\n    at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:74)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:280)\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:396)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:571)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:474)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:428)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:398)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:877)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Can't read unknown type [97]\n    at org.elasticsearch.common.io.stream.StreamInput.readGenericValue(StreamInput.java:509)\n    at org.elasticsearch.common.io.stream.StreamInput.readArrayList(StreamInput.java:518)\n    at org.elasticsearch.common.io.stream.StreamInput.readGenericValue(StreamInput.java:477)\n    at org.elasticsearch.common.io.stream.StreamInput.readMap(StreamInput.java:437)\n    at org.elasticsearch.xpack.security.SecurityFeatureSet$Usage.<init>(SecurityFeatureSet.java:178)\n    at org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput.readNamedWriteable(NamedWriteableAwareStreamInput.java:40)\n    at org.elasticsearch.xpack.action.XPackUsageResponse.readFrom(XPackUsageResponse.java:60)\n    at org.elasticsearch.transport.TcpTransport.handleResponse(TcpTransport.java:1231)\n    ... 23 more\n[2016-09-22 13:18:18,662][WARN ][transport.netty4         ] [epyk-node1] exception caught on transport layer [[id: 0xe26113ed, L:/10.32.56.24:43659 - R:/10.32.56.22:9300]], closing connection\njava.lang.IllegalStateException: Message not fully read (response) for requestId [1570], handler [org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler@50936fba], error [false]; resetting\n    at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1213)\n    at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:74)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:280)\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:396)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:571)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:474)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:428)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:398)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:877)\n    at java.lang.Thread.run(Thread.java:745)\n[2016-09-22 13:18:18,665][INFO ][discovery.zen            ] [epyk-node1] master_left [{epyk-arbiter}{06_SMr8iSpuyzPG0n1dXCg}{vZiHtdyZS_OicIxqI7lXjw}{10.32.56.22}{10.32.56.22:9300}], reason [transport disconnected]\n[2016-09-22 13:18:18,666][WARN ][discovery.zen            ] [epyk-node1] master left (reason = transport disconnected), current nodes: {{epyk-node1}{bEQdMB_2RuiVQrQbUdcYsg}{RGqPjcwTS-6F-H8K9X6wxg}{10.32.56.24}{10.32.56.24:9300},{epyk-node2}{rx6eFW2zQ8WQkUrLKRe_ZQ}{4XQPgbKFTwCSJqceIR3N8Q}{10.32.56.23}{10.32.56.23:9300},}\n[2016-09-22 13:18:18,666][INFO ][cluster.service          ] [epyk-node1] removed {{epyk-arbiter}{06_SMr8iSpuyzPG0n1dXCg}{vZiHtdyZS_OicIxqI7lXjw}{10.32.56.22}{10.32.56.22:9300},}, reason: master_failed ({epyk-arbiter}{06_SMr8iSpuyzPG0n1dXCg}{vZiHtdyZS_OicIxqI7lXjw}{10.32.56.22}{10.32.56.22:9300})\n[2016-09-22 13:18:20,215][ERROR][xpack.monitoring.agent   ] [epyk-node1] exception when exporting documents\nExportException[failed to flush export bulks]\n    at org.elasticsearch.xpack.monitoring.agent.exporter.ExportBulk$Compound.doFlush(ExportBulk.java:143)\n    at org.elasticsearch.xpack.monitoring.agent.exporter.ExportBulk.close(ExportBulk.java:72)\n    at org.elasticsearch.xpack.monitoring.agent.exporter.Exporters.export(Exporters.java:198)\n    at org.elasticsearch.xpack.monitoring.agent.AgentService$ExportingWorker.run(AgentService.java:209)\n    at java.lang.Thread.run(Thread.java:745)\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}