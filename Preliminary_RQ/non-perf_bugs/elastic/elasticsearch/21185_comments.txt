[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/257128849","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-257128849","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":257128849,"node_id":"MDEyOklzc3VlQ29tbWVudDI1NzEyODg0OQ==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-10-30T02:54:01Z","updated_at":"2016-10-30T02:54:01Z","author_association":"CONTRIBUTOR","body":"I'll add this to my list of things to investigate on Monday. You can work around it by using a smaller batch size which looks like:\n\n```\nPOST _reindex\n{\n  \"source\": {\n    \"index\": \"source\",\n    \"size\": 100\n  },\n  \"dest\": {\n    \"index\": \"dest\"\n  }\n}\n```\n\nMy instinct is that a buffer limit of 100mb is more appropriate than 10mb for reindex-from-remote and if you want to go beyond that you should use a smaller batch size. Buffers larger than 100mb are likely to send similarly sized bulks and we know bulk performance suffers with batches that large.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/257400663","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-257400663","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":257400663,"node_id":"MDEyOklzc3VlQ29tbWVudDI1NzQwMDY2Mw==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-10-31T19:50:45Z","updated_at":"2016-10-31T19:50:45Z","author_association":"CONTRIBUTOR","body":"[reproduction](https://gist.github.com/nik9000/526a45ea9a231d01598d8dafa0704ca5)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/270444406","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-270444406","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":270444406,"node_id":"MDEyOklzc3VlQ29tbWVudDI3MDQ0NDQwNg==","user":{"login":"PhaedrusTheGreek","id":4387023,"node_id":"MDQ6VXNlcjQzODcwMjM=","avatar_url":"https://avatars0.githubusercontent.com/u/4387023?v=4","gravatar_id":"","url":"https://api.github.com/users/PhaedrusTheGreek","html_url":"https://github.com/PhaedrusTheGreek","followers_url":"https://api.github.com/users/PhaedrusTheGreek/followers","following_url":"https://api.github.com/users/PhaedrusTheGreek/following{/other_user}","gists_url":"https://api.github.com/users/PhaedrusTheGreek/gists{/gist_id}","starred_url":"https://api.github.com/users/PhaedrusTheGreek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/PhaedrusTheGreek/subscriptions","organizations_url":"https://api.github.com/users/PhaedrusTheGreek/orgs","repos_url":"https://api.github.com/users/PhaedrusTheGreek/repos","events_url":"https://api.github.com/users/PhaedrusTheGreek/events{/privacy}","received_events_url":"https://api.github.com/users/PhaedrusTheGreek/received_events","type":"User","site_admin":false},"created_at":"2017-01-04T18:17:57Z","updated_at":"2017-01-04T18:17:57Z","author_association":"CONTRIBUTOR","body":"@nik9000 , [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html) are showing that the buffer size is 200mb in 5.x, but i'm hitting a 100mb limit in 5.1.1:\r\n\r\n```\r\n{\"error\":{\"root_cause\":[{\"type\":\"illegal_argument_exception\",\"reason\":\"Remote responded with a chunk that was too large. Use a smaller batch size.\"}],\"type\":\"illegal_argument_exception\",\"reason\":\"Remote responded with a chunk that was too large. Use a smaller batch size.\",\"caused_by\":{\"type\":\"content_too_long_exception\",\"reason\":\"entity content is too long [245498346] for the configured buffer limit [104857600]\"}},\"status\":400}Done with <removed>, moving to the next one...\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/270451591","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-270451591","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":270451591,"node_id":"MDEyOklzc3VlQ29tbWVudDI3MDQ1MTU5MQ==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-01-04T18:46:36Z","updated_at":"2017-01-04T18:46:36Z","author_association":"CONTRIBUTOR","body":"> @nik9000 , docs are showing that the buffer size is 200mb in 5.x, but i'm hitting a 100mb limit in 5.1.1:\r\n\r\nYeah, I remember fixing that but I it looks like I fixed it in a PR that went only to 5.2+.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/270718695","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-270718695","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":270718695,"node_id":"MDEyOklzc3VlQ29tbWVudDI3MDcxODY5NQ==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-01-05T18:28:28Z","updated_at":"2017-01-05T18:28:28Z","author_association":"CONTRIBUTOR","body":"@PhaedrusTheGreek, I pushed 1294035aa7b4549beeeaff22b5403c3479c1bc5d to 5.1 to fix the docs in 5.1. It looks right in 5.x.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/285842478","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-285842478","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":285842478,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NTg0MjQ3OA==","user":{"login":"egyptianbman","id":323600,"node_id":"MDQ6VXNlcjMyMzYwMA==","avatar_url":"https://avatars3.githubusercontent.com/u/323600?v=4","gravatar_id":"","url":"https://api.github.com/users/egyptianbman","html_url":"https://github.com/egyptianbman","followers_url":"https://api.github.com/users/egyptianbman/followers","following_url":"https://api.github.com/users/egyptianbman/following{/other_user}","gists_url":"https://api.github.com/users/egyptianbman/gists{/gist_id}","starred_url":"https://api.github.com/users/egyptianbman/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/egyptianbman/subscriptions","organizations_url":"https://api.github.com/users/egyptianbman/orgs","repos_url":"https://api.github.com/users/egyptianbman/repos","events_url":"https://api.github.com/users/egyptianbman/events{/privacy}","received_events_url":"https://api.github.com/users/egyptianbman/received_events","type":"User","site_admin":false},"created_at":"2017-03-11T04:54:59Z","updated_at":"2017-03-11T04:54:59Z","author_association":"NONE","body":"This memory limit **really** needs to be configurable. The limit that's currently in place makes remote reindexing a nightmare. I have one of two options:\r\n\r\nOption 1:\r\nReindex all the indexes with a `size` of 1 to ensure I don't hit this limit. This will take an immense amount of time because of how slow it will be.\r\n\r\nOption 2:\r\nRun the reindex with size of 3000. If it fails, try again with 1000. If that fails, try again with 500. If that fails, try again with 100. If that fails, try again with 50. If that fails, try again with 25......... (the numbers here don't matter but the point is -- with an index dozens of gigabytes, I can't be sure what the maximum size that I can use is without actually running the reindex).\r\n\r\nIf the limit can't be changed due to other issues that it would cause, then is it possible to add the ability to resume a reindex instead of having to start over every time? Thank you!","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/287114580","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-287114580","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":287114580,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NzExNDU4MA==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-03-16T16:32:42Z","updated_at":"2017-03-16T16:32:42Z","author_association":"CONTRIBUTOR","body":"> If the limit can't be changed due to other issues that it would cause, then is it possible to add the ability to resume a reindex instead of having to start over every time? Thank you!\r\n\r\nNot really.... Not with the tools we have now. With tools we're building, maybe.\r\n\r\nI wonder how expensive it'd be to have a script that filtered out documents that are huge. Then you can do them in two passes. That'd be a thing you could do right now. It'd be better for the nodes then allowing the buffer to balloon up uncontrollably. Not great for usability, obviously, but better than nothing.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/326233305","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-326233305","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":326233305,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNjIzMzMwNQ==","user":{"login":"zqc0512","id":25192261,"node_id":"MDQ6VXNlcjI1MTkyMjYx","avatar_url":"https://avatars1.githubusercontent.com/u/25192261?v=4","gravatar_id":"","url":"https://api.github.com/users/zqc0512","html_url":"https://github.com/zqc0512","followers_url":"https://api.github.com/users/zqc0512/followers","following_url":"https://api.github.com/users/zqc0512/following{/other_user}","gists_url":"https://api.github.com/users/zqc0512/gists{/gist_id}","starred_url":"https://api.github.com/users/zqc0512/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zqc0512/subscriptions","organizations_url":"https://api.github.com/users/zqc0512/orgs","repos_url":"https://api.github.com/users/zqc0512/repos","events_url":"https://api.github.com/users/zqc0512/events{/privacy}","received_events_url":"https://api.github.com/users/zqc0512/received_events","type":"User","site_admin":false},"created_at":"2017-08-31T08:53:11Z","updated_at":"2017-08-31T08:53:11Z","author_association":"NONE","body":"can change the configured buffer limit  with configure file or curl?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328323133","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-328323133","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":328323133,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODMyMzEzMw==","user":{"login":"sysr-q","id":1192957,"node_id":"MDQ6VXNlcjExOTI5NTc=","avatar_url":"https://avatars3.githubusercontent.com/u/1192957?v=4","gravatar_id":"","url":"https://api.github.com/users/sysr-q","html_url":"https://github.com/sysr-q","followers_url":"https://api.github.com/users/sysr-q/followers","following_url":"https://api.github.com/users/sysr-q/following{/other_user}","gists_url":"https://api.github.com/users/sysr-q/gists{/gist_id}","starred_url":"https://api.github.com/users/sysr-q/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sysr-q/subscriptions","organizations_url":"https://api.github.com/users/sysr-q/orgs","repos_url":"https://api.github.com/users/sysr-q/repos","events_url":"https://api.github.com/users/sysr-q/events{/privacy}","received_events_url":"https://api.github.com/users/sysr-q/received_events","type":"User","site_admin":false},"created_at":"2017-09-10T06:30:06Z","updated_at":"2017-09-10T06:30:06Z","author_association":"NONE","body":"Big :+1: from me on configurable buffer size. Have been doing a reindex-from-remote of 21million odd documents, where most are tiny, but there are a few big ones peppered in to make my life fun. Have to run a small batch size -- like egyptianbman, constantly trying again with smaller and smaller until I find one that doesn't eventually die -- as well as babysit it to make sure it doesn't stop without me noticing. Less than ideal.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/403225428","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-403225428","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":403225428,"node_id":"MDEyOklzc3VlQ29tbWVudDQwMzIyNTQyOA==","user":{"login":"timtutt","id":1517492,"node_id":"MDQ6VXNlcjE1MTc0OTI=","avatar_url":"https://avatars2.githubusercontent.com/u/1517492?v=4","gravatar_id":"","url":"https://api.github.com/users/timtutt","html_url":"https://github.com/timtutt","followers_url":"https://api.github.com/users/timtutt/followers","following_url":"https://api.github.com/users/timtutt/following{/other_user}","gists_url":"https://api.github.com/users/timtutt/gists{/gist_id}","starred_url":"https://api.github.com/users/timtutt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/timtutt/subscriptions","organizations_url":"https://api.github.com/users/timtutt/orgs","repos_url":"https://api.github.com/users/timtutt/repos","events_url":"https://api.github.com/users/timtutt/events{/privacy}","received_events_url":"https://api.github.com/users/timtutt/received_events","type":"User","site_admin":false},"created_at":"2018-07-07T16:01:55Z","updated_at":"2018-07-07T16:01:55Z","author_association":"NONE","body":"Would really love it if this was looked into again. The buffer limit of 100mb just seems to be way too low. When reindexing from remote this seems to be my bottleneck for indexing rate. I even increased by node count 3x and still received the same performance because I'm having to use a size of 10 just to get docs indexed. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/580790201","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-580790201","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":580790201,"node_id":"MDEyOklzc3VlQ29tbWVudDU4MDc5MDIwMQ==","user":{"login":"fhalde","id":7455872,"node_id":"MDQ6VXNlcjc0NTU4NzI=","avatar_url":"https://avatars2.githubusercontent.com/u/7455872?v=4","gravatar_id":"","url":"https://api.github.com/users/fhalde","html_url":"https://github.com/fhalde","followers_url":"https://api.github.com/users/fhalde/followers","following_url":"https://api.github.com/users/fhalde/following{/other_user}","gists_url":"https://api.github.com/users/fhalde/gists{/gist_id}","starred_url":"https://api.github.com/users/fhalde/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fhalde/subscriptions","organizations_url":"https://api.github.com/users/fhalde/orgs","repos_url":"https://api.github.com/users/fhalde/repos","events_url":"https://api.github.com/users/fhalde/events{/privacy}","received_events_url":"https://api.github.com/users/fhalde/received_events","type":"User","site_admin":false},"created_at":"2020-01-31T15:55:09Z","updated_at":"2020-01-31T15:55:09Z","author_association":"NONE","body":"was anything done for this?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/606448462","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-606448462","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":606448462,"node_id":"MDEyOklzc3VlQ29tbWVudDYwNjQ0ODQ2Mg==","user":{"login":"DylanGriffith","id":1002737,"node_id":"MDQ6VXNlcjEwMDI3Mzc=","avatar_url":"https://avatars2.githubusercontent.com/u/1002737?v=4","gravatar_id":"","url":"https://api.github.com/users/DylanGriffith","html_url":"https://github.com/DylanGriffith","followers_url":"https://api.github.com/users/DylanGriffith/followers","following_url":"https://api.github.com/users/DylanGriffith/following{/other_user}","gists_url":"https://api.github.com/users/DylanGriffith/gists{/gist_id}","starred_url":"https://api.github.com/users/DylanGriffith/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DylanGriffith/subscriptions","organizations_url":"https://api.github.com/users/DylanGriffith/orgs","repos_url":"https://api.github.com/users/DylanGriffith/repos","events_url":"https://api.github.com/users/DylanGriffith/events{/privacy}","received_events_url":"https://api.github.com/users/DylanGriffith/received_events","type":"User","site_admin":false},"created_at":"2020-03-31T07:22:43Z","updated_at":"2020-03-31T07:22:43Z","author_association":"NONE","body":"Is there any way to configure this yet? It appears to be the bottleneck for us doing remote reindex as well and thus we are forced to use a very low batch size considering our cluster size.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/656737299","html_url":"https://github.com/elastic/elasticsearch/issues/21185#issuecomment-656737299","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21185","id":656737299,"node_id":"MDEyOklzc3VlQ29tbWVudDY1NjczNzI5OQ==","user":{"login":"luispollo","id":1323478,"node_id":"MDQ6VXNlcjEzMjM0Nzg=","avatar_url":"https://avatars3.githubusercontent.com/u/1323478?v=4","gravatar_id":"","url":"https://api.github.com/users/luispollo","html_url":"https://github.com/luispollo","followers_url":"https://api.github.com/users/luispollo/followers","following_url":"https://api.github.com/users/luispollo/following{/other_user}","gists_url":"https://api.github.com/users/luispollo/gists{/gist_id}","starred_url":"https://api.github.com/users/luispollo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/luispollo/subscriptions","organizations_url":"https://api.github.com/users/luispollo/orgs","repos_url":"https://api.github.com/users/luispollo/repos","events_url":"https://api.github.com/users/luispollo/events{/privacy}","received_events_url":"https://api.github.com/users/luispollo/received_events","type":"User","site_admin":false},"created_at":"2020-07-10T15:32:01Z","updated_at":"2020-07-10T15:32:01Z","author_association":"NONE","body":"Piling on here... I ran into this issue during a migration from ES 2.x to 6.x, and was unable to complete the reindexing process even with a batch size of `1`. I know it's ridiculous to have a document that large in the index, but that's another discussion.\r\n\r\nIf nothing else, I'd suggest adjusting this bit of the docs:\r\n> Reindexing from a remote server uses an on-heap buffer that defaults to a maximum size of 100mb.\r\n\r\nThe word \"defaults\" there is misleading, since there's no way to configure/override that limit. In other words, it is effectively a hard limit, and the documentation should reflect that.","performed_via_github_app":null}]