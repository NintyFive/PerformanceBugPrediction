{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/15465","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15465/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15465/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15465/events","html_url":"https://github.com/elastic/elasticsearch/issues/15465","id":122406273,"node_id":"MDU6SXNzdWUxMjI0MDYyNzM=","number":15465,"title":"Apparent degradation in handling of large concurrent index job on update from 1.3.4 to 2.1.0","user":{"login":"hronik1","id":1770336,"node_id":"MDQ6VXNlcjE3NzAzMzY=","avatar_url":"https://avatars0.githubusercontent.com/u/1770336?v=4","gravatar_id":"","url":"https://api.github.com/users/hronik1","html_url":"https://github.com/hronik1","followers_url":"https://api.github.com/users/hronik1/followers","following_url":"https://api.github.com/users/hronik1/following{/other_user}","gists_url":"https://api.github.com/users/hronik1/gists{/gist_id}","starred_url":"https://api.github.com/users/hronik1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hronik1/subscriptions","organizations_url":"https://api.github.com/users/hronik1/orgs","repos_url":"https://api.github.com/users/hronik1/repos","events_url":"https://api.github.com/users/hronik1/events{/privacy}","received_events_url":"https://api.github.com/users/hronik1/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2015-12-16T01:25:46Z","updated_at":"2016-12-31T05:06:05Z","closed_at":"2015-12-16T02:15:17Z","author_association":"NONE","active_lock_reason":null,"body":"Scoping out potential issues from upgrading from 1.3.4 to 2.1.0 since 1.3.4 is swiftly approaching end of life.\n\nI am standing up identical spec 1.3.4 and 2.1.0 single node clusters, and attempting to reindex an existing index from the current staging environment into them(doing this to both to see how the versions compare when being overloaded as well as to get some test data into them). I am launching a spark job which uses elasticsearch-hadoop-2.2.0-m1 and is indexing into it using 16 executors.\n\nTo my surprise, the 1.3.4 index got about 200 times more data into it before the index job failed, compared to the 2.1.0 index. While indexing into both seemed to face error conditions along the way(will post errors below), the 2.1.0 index job seemed to fail almost immediately while the 1.3.4 seems to attempt recovery and continue indexing for quite some time before failing.\n\nRecurring 1.3.4 error(seen repeatedly, but indexing continues):\n\n```\n15/12/15 16:33:51 ERROR Executor: Exception in task 8.0 in stage 0.0 (TID 8)\norg.elasticsearch.hadoop.EsHadoopException: Could not write all entries [6/1047872] (maybe ES was overloaded?). Bailing out...\n    at org.elasticsearch.hadoop.rest.RestRepository.flush(RestRepository.java:239)\n    at org.elasticsearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:176)\n    at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:152)\n    at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:49)\n    at org.elasticsearch.spark.rdd.EsSpark$$anonfun$saveToEsWithMeta$1.apply(EsSpark.scala:86)\n    at org.elasticsearch.spark.rdd.EsSpark$$anonfun$saveToEsWithMeta$1.apply(EsSpark.scala:86)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n    at org.apache.spark.scheduler.Task.run(Task.scala:88)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n```\n\nError in 2.1.0(fails almost immediately after encountering):\n\n```\n15/12/15 16:58:29 ERROR Executor: Exception in task 8.0 in stage 0.0 (TID 8)\norg.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: Found unrecoverable error [Too Many Requests(429) - [rejected execution of org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1@250a2a1c on EsThreadPoolExecutor[bulk, queue capacity = 50, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@75bb8775[Running, pool size = 2, active threads = 2, queued tasks = 50, completed tasks = 0]]]]; Bailing out..\n    at org.elasticsearch.hadoop.rest.RestClient.retryFailedEntries(RestClient.java:215)\n    at org.elasticsearch.hadoop.rest.RestClient.bulk(RestClient.java:169)\n    at org.elasticsearch.hadoop.rest.RestRepository.tryFlush(RestRepository.java:214)\n    at org.elasticsearch.hadoop.rest.RestRepository.flush(RestRepository.java:237)\n    at org.elasticsearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:176)\n    at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:152)\n    at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:49)\n    at org.elasticsearch.spark.rdd.EsSpark$$anonfun$saveToEsWithMeta$1.apply(EsSpark.scala:86)\n    at org.elasticsearch.spark.rdd.EsSpark$$anonfun$saveToEsWithMeta$1.apply(EsSpark.scala:86)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n    at org.apache.spark.scheduler.Task.run(Task.scala:88)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n```\n","closed_by":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"performed_via_github_app":null}