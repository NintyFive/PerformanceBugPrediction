{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/6305","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6305/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6305/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6305/events","html_url":"https://github.com/elastic/elasticsearch/issues/6305","id":34255750,"node_id":"MDU6SXNzdWUzNDI1NTc1MA==","number":6305,"title":"Elasticsearch does not respond with huge data","user":{"login":"umttt","id":4945980,"node_id":"MDQ6VXNlcjQ5NDU5ODA=","avatar_url":"https://avatars1.githubusercontent.com/u/4945980?v=4","gravatar_id":"","url":"https://api.github.com/users/umttt","html_url":"https://github.com/umttt","followers_url":"https://api.github.com/users/umttt/followers","following_url":"https://api.github.com/users/umttt/following{/other_user}","gists_url":"https://api.github.com/users/umttt/gists{/gist_id}","starred_url":"https://api.github.com/users/umttt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umttt/subscriptions","organizations_url":"https://api.github.com/users/umttt/orgs","repos_url":"https://api.github.com/users/umttt/repos","events_url":"https://api.github.com/users/umttt/events{/privacy}","received_events_url":"https://api.github.com/users/umttt/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2014-05-25T06:33:18Z","updated_at":"2014-05-25T08:39:49Z","closed_at":"2014-05-25T08:39:49Z","author_association":"NONE","active_lock_reason":null,"body":"  Hi,\n\n   I am working on centos5 and I run elasticsearch with version 1.0.0 with -Xms808m -Xmx808m -Xss256kparameters. There are 17 index and total 30200583 docs. Each index's docs count between 1000000 and 2000000. I create request query like ( each index have date field );\n\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"range\": {\n            \"date\": {\n              \"to\": \"2014-06-01 14:14:00\",\n              \"from\": \"2014-04-01 00:00:00\"\n            }\n          }\n        }\n      ],\n      \"should\": [],\n      \"must_not\": [],\n      \"minimum_number_should_match\": 1\n    }\n  },\n  \"from\": 0,\n  \"size\": \"50\"\n}\n\nIt give response;\n\n{\n   took: 5903\n   timed_out: false\n   _shards: {\n      total: 17\n      successful: 17\n      failed: 0\n   },\n   hits: {\n   total: 30200583\n...\n...\n...}\n\nHowever when I send query on elasticsearch-head tool for last 50 rows like;\n\n{\n  ...\n  ...\n  ...\n  \"from\": 30200533,\n  \"size\": \"50\"\n}\nIt does not give a response and throw exception like;\n\nava.lang.OutOfMemoryError: Java heap space\n        at org.apache.lucene.store.DataOutput.copyBytes(DataOutput.java:247)\n        at org.apache.lucene.store.Directory.copy(Directory.java:186)\n        at org.elasticsearch.index.store.Store$StoreDirectory.copy(Store.java:348)\n        at org.apache.lucene.store.TrackingDirectoryWrapper.copy(TrackingDirectoryWrapper.java:50)\n        at org.apache.lucene.index.IndexWriter.createCompoundFile(IndexWriter.java:4596)\n        at org.apache.lucene.index.DocumentsWriterPerThread.sealFlushedSegment(DocumentsWriterPerThread.java:535)\n        at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:502)\n        at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:506)\n        at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:616)\n        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:370)\n        at org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:285)\n        at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:260)\n        at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:250)\n        at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:170)\n        at org.apache.lucene.search.XSearcherManager.refreshIfNeeded(XSearcherManager.java:123)\n        at org.apache.lucene.search.XSearcherManager.refreshIfNeeded(XSearcherManager.java:59)\n        at org.apache.lucene.search.XReferenceManager.doMaybeRefresh(XReferenceManager.java:180)\n        at org.apache.lucene.search.XReferenceManager.maybeRefresh(XReferenceManager.java:229)\n        at org.elasticsearch.index.engine.internal.InternalEngine.refresh(InternalEngine.java:730)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:477)\n        at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:924)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\nWhat is the problem? Is it not enough java heap space or does my query cause this heap space error? \n\nI asked same question in stackoverflow. Soutions ,which recommended in stackoverflow, are not applicable for me. Anyone can give another solutions for this problem?\n","closed_by":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"performed_via_github_app":null}