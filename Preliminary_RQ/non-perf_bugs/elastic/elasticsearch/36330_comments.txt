[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/445001973","html_url":"https://github.com/elastic/elasticsearch/issues/36330#issuecomment-445001973","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36330","id":445001973,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTAwMTk3Mw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-12-06T19:37:16Z","updated_at":"2018-12-06T19:37:16Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/445002842","html_url":"https://github.com/elastic/elasticsearch/issues/36330#issuecomment-445002842","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36330","id":445002842,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTAwMjg0Mg==","user":{"login":"tbrooks8","id":862472,"node_id":"MDQ6VXNlcjg2MjQ3Mg==","avatar_url":"https://avatars3.githubusercontent.com/u/862472?v=4","gravatar_id":"","url":"https://api.github.com/users/tbrooks8","html_url":"https://github.com/tbrooks8","followers_url":"https://api.github.com/users/tbrooks8/followers","following_url":"https://api.github.com/users/tbrooks8/following{/other_user}","gists_url":"https://api.github.com/users/tbrooks8/gists{/gist_id}","starred_url":"https://api.github.com/users/tbrooks8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tbrooks8/subscriptions","organizations_url":"https://api.github.com/users/tbrooks8/orgs","repos_url":"https://api.github.com/users/tbrooks8/repos","events_url":"https://api.github.com/users/tbrooks8/events{/privacy}","received_events_url":"https://api.github.com/users/tbrooks8/received_events","type":"User","site_admin":false},"created_at":"2018-12-06T19:39:58Z","updated_at":"2018-12-06T19:39:58Z","author_association":"CONTRIBUTOR","body":"This seems like it might be similar to https://github.com/elastic/elasticsearch/issues/36276?\r\n\r\nAt least same test suite and started failing recently.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/445003656","html_url":"https://github.com/elastic/elasticsearch/issues/36330#issuecomment-445003656","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36330","id":445003656,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTAwMzY1Ng==","user":{"login":"jdconrad","id":2126764,"node_id":"MDQ6VXNlcjIxMjY3NjQ=","avatar_url":"https://avatars2.githubusercontent.com/u/2126764?v=4","gravatar_id":"","url":"https://api.github.com/users/jdconrad","html_url":"https://github.com/jdconrad","followers_url":"https://api.github.com/users/jdconrad/followers","following_url":"https://api.github.com/users/jdconrad/following{/other_user}","gists_url":"https://api.github.com/users/jdconrad/gists{/gist_id}","starred_url":"https://api.github.com/users/jdconrad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jdconrad/subscriptions","organizations_url":"https://api.github.com/users/jdconrad/orgs","repos_url":"https://api.github.com/users/jdconrad/repos","events_url":"https://api.github.com/users/jdconrad/events{/privacy}","received_events_url":"https://api.github.com/users/jdconrad/received_events","type":"User","site_admin":false},"created_at":"2018-12-06T19:42:21Z","updated_at":"2018-12-06T19:42:28Z","author_association":"CONTRIBUTOR","body":"@tbrooks8 I looked at the other issue as well, and maybe it's the same problem, but definitely different error messages I believe.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/445074806","html_url":"https://github.com/elastic/elasticsearch/issues/36330#issuecomment-445074806","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36330","id":445074806,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTA3NDgwNg==","user":{"login":"tbrooks8","id":862472,"node_id":"MDQ6VXNlcjg2MjQ3Mg==","avatar_url":"https://avatars3.githubusercontent.com/u/862472?v=4","gravatar_id":"","url":"https://api.github.com/users/tbrooks8","html_url":"https://github.com/tbrooks8","followers_url":"https://api.github.com/users/tbrooks8/followers","following_url":"https://api.github.com/users/tbrooks8/following{/other_user}","gists_url":"https://api.github.com/users/tbrooks8/gists{/gist_id}","starred_url":"https://api.github.com/users/tbrooks8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tbrooks8/subscriptions","organizations_url":"https://api.github.com/users/tbrooks8/orgs","repos_url":"https://api.github.com/users/tbrooks8/repos","events_url":"https://api.github.com/users/tbrooks8/events{/privacy}","received_events_url":"https://api.github.com/users/tbrooks8/received_events","type":"User","site_admin":false},"created_at":"2018-12-07T00:01:24Z","updated_at":"2018-12-07T00:01:24Z","author_association":"CONTRIBUTOR","body":"After some investigation I think the issue is the same as #36276. There are stack traces that come before the timeout waiting for green:\r\n\r\n```\r\n[2018-12-06T23:29:17,024][WARN ][o.e.i.r.PeerRecoveryTargetService] [node_s0] error while listing local files, recovering as if there are none\r\norg.apache.lucene.store.LockObtainFailedException: Lock held by this virtual machine: /private/var/folders/vj/_qsxqj6n6z14dpr0crpw2rbr0000gn/T/org.elasticsearch.snapshots.SourceOnlySnapshotIT_3F6F96908F439A68-058/tempDir-002/custom/XVxgoOZJNO/0/zxnpJEqtTCa3lzR8Ud7Z0Q/0/index/write.lock\r\n\tat org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:139) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.FilterDirectory.obtainLock(FilterDirectory.java:105) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.MockDirectoryWrapper.obtainLock(MockDirectoryWrapper.java:1046) ~[lucene-test-framework-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.FilterDirectory.obtainLock(FilterDirectory.java:105) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.FilterDirectory.obtainLock(FilterDirectory.java:105) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.elasticsearch.index.store.Store.getMetadata(Store.java:291) ~[main/:?]\r\n\tat org.elasticsearch.index.shard.IndexShard.snapshotStoreMetadata(IndexShard.java:1143) ~[main/:?]\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.getStoreMetadataSnapshot(PeerRecoveryTargetService.java:300) [main/:?]\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.getStartRecoveryRequest(PeerRecoveryTargetService.java:321) [main/:?]\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.doRecovery(PeerRecoveryTargetService.java:182) [main/:?]\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.access$500(PeerRecoveryTargetService.java:82) [main/:?]\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService$RecoveryRunner.doRun(PeerRecoveryTargetService.java:633) [main/:?]\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:759) [main/:?]\r\n\tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [main/:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\r\n\tat java.lang.Thread.run(Thread.java:834) [?:?]\r\n```\r\n\r\nAdditionally, I think I have isolated what is the underlying issue. Although I do not yet know that is causing it.\r\n\r\nWhat I see is that we start a recovery from snapshot.\r\n```\r\njava.lang.Throwable: start snapshot recovery\r\n\tat org.elasticsearch.index.shard.IndexShard.startRecovery(IndexShard.java:2138)\r\n\tat org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:611)\r\n\tat org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:155)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.createShard(IndicesClusterStateService.java:560)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.createOrUpdateShards(IndicesClusterStateService.java:537)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.applyClusterState(IndicesClusterStateService.java:234)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.lambda$callClusterStateAppliers$6(ClusterApplierService.java:483)\r\n\tat java.base/java.lang.Iterable.forEach(Iterable.java:75)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.callClusterStateAppliers(ClusterApplierService.java:480)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.applyChanges(ClusterApplierService.java:467)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.runTask(ClusterApplierService.java:418)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService$UpdateTask.run(ClusterApplierService.java:162)\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:660)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:244)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:207)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n```\r\n\r\nThis obtains a filesystem lock in the `ReadOnlyEngine`\r\n\r\n```\r\njava.lang.Throwable: Obtain\r\n\tat org.elasticsearch.index.engine.ReadOnlyEngine.<init>(ReadOnlyEngine.java:105)\r\n\tat org.elasticsearch.snapshots.SourceOnlySnapshotRepository.lambda$getEngineFactory$1(SourceOnlySnapshotRepository.java:149)\r\n\tat org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:2251)\r\n\tat org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:2233)\r\n\tat org.elasticsearch.index.shard.IndexShard.innerOpenEngineAndTranslog(IndexShard.java:1389)\r\n\tat org.elasticsearch.index.shard.IndexShard.openEngineAndRecoverFromTranslog(IndexShard.java:1346)\r\n\tat org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:474)\r\n\tat org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$5(StoreRecovery.java:279)\r\n\tat org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:302)\r\n\tat org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:277)\r\n\tat org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1631)\r\n\tat org.elasticsearch.index.shard.IndexShard.lambda$startRecovery$7(IndexShard.java:2142)\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:660)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n```\r\n\r\nThis lock will be held until `ReadOnlyEngine#closeNoLock` is called. That will happen when the engine is closed or failed. \r\n\r\nIn the test we update the number of replicas.\r\n\r\n```\r\n[2018-12-06T23:41:23,940][INFO ][o.e.c.m.MetaDataUpdateSettingsService] [node_s0] updating number_of_replicas to [1] for indices [test-idx]\r\n```\r\n\r\nAt this point I see another recovery start. However, this is a `PEER` recovery. The `ReadOnlyEngine` is still open and holding the filesystem lock.\r\n\r\n```\r\njava.lang.Throwable: start Recovery\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.startRecovery(PeerRecoveryTargetService.java:145)\r\n\tat org.elasticsearch.index.shard.IndexShard.startRecovery(IndexShard.java:2128)\r\n\tat org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:611)\r\n\tat org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:155)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.createShard(IndicesClusterStateService.java:560)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.createOrUpdateShards(IndicesClusterStateService.java:537)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.applyClusterState(IndicesClusterStateService.java:234)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.lambda$callClusterStateAppliers$6(ClusterApplierService.java:483)\r\n\tat java.base/java.lang.Iterable.forEach(Iterable.java:75)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.callClusterStateAppliers(ClusterApplierService.java:480)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.applyChanges(ClusterApplierService.java:467)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.runTask(ClusterApplierService.java:418)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService$UpdateTask.run(ClusterApplierService.java:162)\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:660)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:244)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:207)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n```\r\n\r\nThe peer recovery throws an exception when it goes to get the `Store.MetadataSnapshot` for the index.\r\n\r\n```\r\norg.apache.lucene.store.LockObtainFailedException: Lock held by this virtual machine: /private/var/folders/vj/_qsxqj6n6z14dpr0crpw2rbr0000gn/T/org.elasticsearch.snapshots.SourceOnlySnapshotIT_3F6F96908F439A68-061/tempDir-002/custom/XVxgoOZJNO/0/T29u63acR4erreBcU8dteA/0/index/write.lock\r\n\tat org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:139) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.FilterDirectory.obtainLock(FilterDirectory.java:105) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.MockDirectoryWrapper.obtainLock(MockDirectoryWrapper.java:1046) ~[lucene-test-framework-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.FilterDirectory.obtainLock(FilterDirectory.java:105) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.apache.lucene.store.FilterDirectory.obtainLock(FilterDirectory.java:105) ~[lucene-core-8.0.0-snapshot-c78429a554.jar:8.0.0-snapshot-c78429a554 c78429a554d28611dacd90c388e6c34039b228d1 - romseygeek - 2018-12-04 10:17:44]\r\n\tat org.elasticsearch.index.store.Store.getMetadata(Store.java:291) ~[main/:?]\r\n\tat org.elasticsearch.index.shard.IndexShard.snapshotStoreMetadata(IndexShard.java:1143) ~[main/:?]\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.getStoreMetadataSnapshot(PeerRecoveryTargetService.java:300) [main/:?]\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.getStartRecoveryRequest(PeerRecoveryTargetService.java:321) [main/:?]\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.doRecovery(PeerRecoveryTargetService.java:182) [main/:?]\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.access$500(PeerRecoveryTargetService.java:82) [main/:?]\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService$RecoveryRunner.doRun(PeerRecoveryTargetService.java:633) [main/:?]\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:759) [main/:?]\r\n\tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [main/:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\r\n\tat java.lang.Thread.run(Thread.java:834) [?:?]\r\n```\r\n\r\nIf I remove one case of `randomBoolean()` call in the test (and set the value directly to what it is when random is called), the test passes reliably.\r\n\r\nI see the recovery from snapshot that opens a `ReadOnlyEngine`.\r\n\r\n```\r\njava.lang.Throwable: start snapshot recovery\r\n\tat org.elasticsearch.index.shard.IndexShard.startRecovery(IndexShard.java:2138)\r\n\tat org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:611)\r\n\tat org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:155)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.createShard(IndicesClusterStateService.java:560)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.createOrUpdateShards(IndicesClusterStateService.java:537)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.applyClusterState(IndicesClusterStateService.java:234)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.lambda$callClusterStateAppliers$6(ClusterApplierService.java:483)\r\n\tat java.base/java.lang.Iterable.forEach(Iterable.java:75)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.callClusterStateAppliers(ClusterApplierService.java:480)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.applyChanges(ClusterApplierService.java:467)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.runTask(ClusterApplierService.java:418)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService$UpdateTask.run(ClusterApplierService.java:162)\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:660)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:244)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:207)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\r\njava.lang.Throwable: Obtain\r\n\tat org.elasticsearch.index.engine.ReadOnlyEngine.<init>(ReadOnlyEngine.java:105)\r\n\tat org.elasticsearch.snapshots.SourceOnlySnapshotRepository.lambda$getEngineFactory$1(SourceOnlySnapshotRepository.java:149)\r\n\tat org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:2251)\r\n\tat org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:2233)\r\n\tat org.elasticsearch.index.shard.IndexShard.innerOpenEngineAndTranslog(IndexShard.java:1389)\r\n\tat org.elasticsearch.index.shard.IndexShard.openEngineAndRecoverFromTranslog(IndexShard.java:1346)\r\n\tat org.elasticsearch.index.shard.StoreRecovery.restore(StoreRecovery.java:474)\r\n\tat org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromRepository$5(StoreRecovery.java:279)\r\n\tat org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:302)\r\n\tat org.elasticsearch.index.shard.StoreRecovery.recoverFromRepository(StoreRecovery.java:277)\r\n\tat org.elasticsearch.index.shard.IndexShard.restoreFromRepository(IndexShard.java:1631)\r\n\tat org.elasticsearch.index.shard.IndexShard.lambda$startRecovery$7(IndexShard.java:2142)\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:660)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n```\r\n\r\nAnd the recovery from `PEER` after the settings update.\r\n\r\n```\r\n[2018-12-06T23:51:45,427][INFO ][o.e.c.m.MetaDataUpdateSettingsService] [node_s0] updating number_of_replicas to [1] for indices [test-idx]\r\n\r\njava.lang.Throwable: start Recovery\r\n\tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.startRecovery(PeerRecoveryTargetService.java:145)\r\n\tat org.elasticsearch.index.shard.IndexShard.startRecovery(IndexShard.java:2128)\r\n\tat org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:611)\r\n\tat org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:155)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.createShard(IndicesClusterStateService.java:560)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.createOrUpdateShards(IndicesClusterStateService.java:537)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.applyClusterState(IndicesClusterStateService.java:234)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.lambda$callClusterStateAppliers$6(ClusterApplierService.java:483)\r\n\tat java.base/java.lang.Iterable.forEach(Iterable.java:75)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.callClusterStateAppliers(ClusterApplierService.java:480)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.applyChanges(ClusterApplierService.java:467)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService.runTask(ClusterApplierService.java:418)\r\n\tat org.elasticsearch.cluster.service.ClusterApplierService$UpdateTask.run(ClusterApplierService.java:162)\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:660)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:244)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:207)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n```\r\n\r\nThe only different that I see is that the filesystem locks are obtained for different temp directories. In the failing case both are for directory 002. And in the passing case one is on directory 002 and the other is on directory 003.\r\n\r\n```\r\n.SourceOnlySnapshotIT_3F6F96908F439A68-064/tempDir-002/data/nodes/0/indices/9jsnmkG5SDGgHj_zHeVFiw/0/index\r\n.SourceOnlySnapshotIT_3F6F96908F439A68-064/tempDir-002/data/nodes/0/indices/9jsnmkG5SDGgHj_zHeVFiw/0/index\r\n\r\nvs\r\n\r\n.SourceOnlySnapshotIT_3F6F96908F439A68-064/tempDir-002/data/nodes/0/indices/9jsnmkG5SDGgHj_zHeVFiw/0/index\r\n.SourceOnlySnapshotIT_3F6F96908F439A68-064/tempDir-003/data/nodes/0/indices/9jsnmkG5SDGgHj_zHeVFiw/0/index\r\n```\r\n\r\nMy understanding of `ESTestCase` is that these different directories are for different nodes. So it seems like in the passing case, the two different recoveries for the same index are happening on different nodes. And in the failing case, the recoveries are happening on the same node, leading to a filesystem lock collision.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/445075232","html_url":"https://github.com/elastic/elasticsearch/issues/36330#issuecomment-445075232","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36330","id":445075232,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTA3NTIzMg==","user":{"login":"tbrooks8","id":862472,"node_id":"MDQ6VXNlcjg2MjQ3Mg==","avatar_url":"https://avatars3.githubusercontent.com/u/862472?v=4","gravatar_id":"","url":"https://api.github.com/users/tbrooks8","html_url":"https://github.com/tbrooks8","followers_url":"https://api.github.com/users/tbrooks8/followers","following_url":"https://api.github.com/users/tbrooks8/following{/other_user}","gists_url":"https://api.github.com/users/tbrooks8/gists{/gist_id}","starred_url":"https://api.github.com/users/tbrooks8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tbrooks8/subscriptions","organizations_url":"https://api.github.com/users/tbrooks8/orgs","repos_url":"https://api.github.com/users/tbrooks8/repos","events_url":"https://api.github.com/users/tbrooks8/events{/privacy}","received_events_url":"https://api.github.com/users/tbrooks8/received_events","type":"User","site_admin":false},"created_at":"2018-12-07T00:03:30Z","updated_at":"2018-12-07T00:03:30Z","author_association":"CONTRIBUTOR","body":"I actually was looking at the code wrong and the a single cluster shares one `tempDir`. So the different nodes is not the explanation.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/445076698","html_url":"https://github.com/elastic/elasticsearch/issues/36330#issuecomment-445076698","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36330","id":445076698,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTA3NjY5OA==","user":{"login":"tbrooks8","id":862472,"node_id":"MDQ6VXNlcjg2MjQ3Mg==","avatar_url":"https://avatars3.githubusercontent.com/u/862472?v=4","gravatar_id":"","url":"https://api.github.com/users/tbrooks8","html_url":"https://github.com/tbrooks8","followers_url":"https://api.github.com/users/tbrooks8/followers","following_url":"https://api.github.com/users/tbrooks8/following{/other_user}","gists_url":"https://api.github.com/users/tbrooks8/gists{/gist_id}","starred_url":"https://api.github.com/users/tbrooks8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tbrooks8/subscriptions","organizations_url":"https://api.github.com/users/tbrooks8/orgs","repos_url":"https://api.github.com/users/tbrooks8/repos","events_url":"https://api.github.com/users/tbrooks8/events{/privacy}","received_events_url":"https://api.github.com/users/tbrooks8/received_events","type":"User","site_admin":false},"created_at":"2018-12-07T00:10:54Z","updated_at":"2018-12-07T00:10:54Z","author_association":"CONTRIBUTOR","body":"Actually ðŸ™ƒ it looks like this specific test starts a data node with a different `path.home` directory (`003`). So it does look like the test fails depending on which node the peer recovery is initiated on.\r\n\r\nWhere we create the new node:\r\n```\r\n        logger.info(\"--> start a new data node\");\r\n        final Settings dataSettings = Settings.builder()\r\n            .put(Node.NODE_NAME_SETTING.getKey(), randomAlphaOfLength(5))\r\n            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()) // to get a new node id\r\n            .build();\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/445179666","html_url":"https://github.com/elastic/elasticsearch/issues/36330#issuecomment-445179666","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36330","id":445179666,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTE3OTY2Ng==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2018-12-07T09:50:50Z","updated_at":"2018-12-07T09:50:50Z","author_association":"CONTRIBUTOR","body":"I muted this test in master in 60289e7331951c7dcf475ec00deb0c147e0465be as it is making PR builds fail","performed_via_github_app":null}]