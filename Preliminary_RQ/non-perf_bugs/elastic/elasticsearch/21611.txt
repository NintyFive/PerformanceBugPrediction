{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611/events","html_url":"https://github.com/elastic/elasticsearch/issues/21611","id":189939534,"node_id":"MDU6SXNzdWUxODk5Mzk1MzQ=","number":21611,"title":"Run away search thread","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"labels":[{"id":144797810,"node_id":"MDU6TGFiZWwxNDQ3OTc4MTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Infra/Core","name":":Core/Infra/Core","color":"0e8a16","default":false,"description":"Core issues without another label"},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":18,"created_at":"2016-11-17T05:09:51Z","updated_at":"2016-11-28T09:41:08Z","closed_at":"2016-11-28T09:41:08Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\r\nGitHub is reserved for bug reports and feature requests. The best place\r\nto ask a general question is at the Elastic Discourse forums at\r\nhttps://discuss.elastic.co. If you are in fact posting a bug report or\r\na feature request, please include one and only one of the below blocks\r\nin your new issue. Note that whether you're filing a bug report or a\r\nfeature request, ensure that your submission is for an\r\n[OS that we support](https://www.elastic.co/support/matrix#show_os).\r\nBug reports on an OS that we do not support or feature requests\r\nspecific to an OS that we do not support will be closed.\r\n-->\r\n\r\n<!--\r\nIf you are filing a bug report, please remove the below feature\r\nrequest block and provide responses for all of the below items.\r\n-->\r\n\r\n**Elasticsearch version**:\r\n5.0.0\r\n**Plugins installed**: []\r\nNone\r\n**JVM version**:\r\n1.8.0_77\r\n**OS version**:\r\nCentOS release 6.4 (Final)\r\nLinux  2.6.32-431.29.2.el6.x86_64 #1 SMP Tue Sep 9 21:36:05 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\r\n**Description of the problem including expected versus actual behavior**:\r\nOn some data nodes with expensive query, the search threads could be active for hours even with search timeout set to 35 seconds.  During that period, disk IO peaks in reading and search thread pilled up in queue with hard time getting into being served by active threads.\r\n**Steps to reproduce**:\r\n This happens rather frequently on a daily basis when there are some expensive search loads hitting the cluster.  Back in v2.4.0 time , this never happened.\r\n\r\n**Provide logs (if relevant)**:\r\nDuring the occurence of the issue , I observed from our monitoring system the number of active search thread peaks to max limit even though the open search context dropped to a few ,  and search completed is hardly stepping forward. Tasks management api shows active query running for a very long time.\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20376817/0cc51eb0-acc5-11e6-8600-74d1651512ee.png)\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20376834/41b375ae-acc5-11e6-9db1-480b64c6465b.png)\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20376843/4cb88c50-acc5-11e6-80aa-7faf7adc5f14.png)\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20376859/65aab918-acc5-11e6-897b-d46237790ea3.png)\r\n\r\nCompared by similar stats weeks ago before upgrading to 5.0.0,  the number of search active threads anytime was almost in line with that of search open context.\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20376910/f0958788-acc5-11e6-816f-640a588df52c.png)\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20376914/faddd1f0-acc5-11e6-9d16-838ad1605589.png)\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20376928/0b330494-acc6-11e6-8d7b-49c78dd04c1c.png)\r\n\r\nIt looks to me search threads could be not freed after the search context had been terminated.\r\n","closed_by":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"performed_via_github_app":null}