{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/22013","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22013/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22013/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22013/events","html_url":"https://github.com/elastic/elasticsearch/issues/22013","id":193944929,"node_id":"MDU6SXNzdWUxOTM5NDQ5Mjk=","number":22013,"title":"Memory leak on Alias","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2016-12-07T02:53:48Z","updated_at":"2016-12-07T12:53:57Z","closed_at":"2016-12-07T12:53:57Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\r\nGitHub is reserved for bug reports and feature requests. The best place\r\nto ask a general question is at the Elastic Discourse forums at\r\nhttps://discuss.elastic.co. If you are in fact posting a bug report or\r\na feature request, please include one and only one of the below blocks\r\nin your new issue. Note that whether you're filing a bug report or a\r\nfeature request, ensure that your submission is for an\r\n[OS that we support](https://www.elastic.co/support/matrix#show_os).\r\nBug reports on an OS that we do not support or feature requests\r\nspecific to an OS that we do not support will be closed.\r\n-->\r\n\r\n<!--\r\nIf you are filing a bug report, please remove the below feature\r\nrequest block and provide responses for all of the below items.\r\n-->\r\n\r\n**Elasticsearch version**:\r\n5.0.0\r\n**Plugins installed**: []\r\nNone\r\n**JVM version**:\r\n1.8.0_77-b03\r\n**OS version**:\r\nCentOS release 6.4 (Final)\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\nOne of our data node's is suffering from high heap usage last night and old GC was not able to reclaim any heap space.  At the time, either bulk or queries were light and all thread pools were pretty idle. The node is one of the 120 data node cluster for logs analysis. Every night we have maintenance job deleting/force merging cold data and creating indices/aliases for the new day.\r\n\r\nThe node is configured with 31GB of heap and holds about 450 shards,  2k-2.5k segments.  For the past week, the segment count/memory remained constant and even dropping. Heap usage  had been crawling up until I restarted the node last night.  I looked at all memory related stats from our monitoring systems and could not find the culprit for increasing heap usage.\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20952656/f75f9b82-bc68-11e6-893d-1af726e02378.png)\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20952683/226484dc-bc69-11e6-83bb-a80861f2b339.png)\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20952949/0b243fa4-bc6b-11e6-9c73-2e563d9b3554.png)\r\n![image](https://cloud.githubusercontent.com/assets/10510416/20952698/3c2655f8-bc69-11e6-814f-81c1aa853543.png)\r\n\r\nBefore restarting the node, I took a heap dump and analyzed with MAT.  The huge number of `org.elasticsearch.cluster.metadata.AliasOrIndex$Alias` objects looks suspicious. They retained nearly 7GB of memory. \r\n![2016-12-06 17 58 44](https://cloud.githubusercontent.com/assets/10510416/20952752/8c5ff1fa-bc69-11e6-8679-22625d06eb32.png)\r\n![2016-12-06 17 58 10](https://cloud.githubusercontent.com/assets/10510416/20952753/8e30eab6-bc69-11e6-8265-37b6f2ea689c.png)\r\n\r\nWe do use alias intensively and there are 40k aliases in total across the whole cluster.  After the node was recovered, another heap dump was taken. This time the number of `org.elasticsearch.cluster.metadata.AliasOrIndex$Alias` objects dropped to 673,427 instances and retained only 16MB of memory.\r\n\r\nDoes this suggest memory leak on Alias metadata?\r\n\r\n<!--\r\nIf you are filing a feature request, please remove the above bug\r\nreport block and provide responses for all of the below items.\r\n-->","closed_by":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"performed_via_github_app":null}