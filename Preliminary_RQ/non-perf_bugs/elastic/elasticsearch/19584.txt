{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/19584","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19584/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19584/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19584/events","html_url":"https://github.com/elastic/elasticsearch/issues/19584","id":167417626,"node_id":"MDU6SXNzdWUxNjc0MTc2MjY=","number":19584,"title":"Elasticsearch continues to attempt writes to an index that has shards on disks that are full","user":{"login":"neuroticnetworks","id":3965137,"node_id":"MDQ6VXNlcjM5NjUxMzc=","avatar_url":"https://avatars0.githubusercontent.com/u/3965137?v=4","gravatar_id":"","url":"https://api.github.com/users/neuroticnetworks","html_url":"https://github.com/neuroticnetworks","followers_url":"https://api.github.com/users/neuroticnetworks/followers","following_url":"https://api.github.com/users/neuroticnetworks/following{/other_user}","gists_url":"https://api.github.com/users/neuroticnetworks/gists{/gist_id}","starred_url":"https://api.github.com/users/neuroticnetworks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/neuroticnetworks/subscriptions","organizations_url":"https://api.github.com/users/neuroticnetworks/orgs","repos_url":"https://api.github.com/users/neuroticnetworks/repos","events_url":"https://api.github.com/users/neuroticnetworks/events{/privacy}","received_events_url":"https://api.github.com/users/neuroticnetworks/received_events","type":"User","site_admin":false},"labels":[{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2016-07-25T17:11:39Z","updated_at":"2016-07-26T16:57:33Z","closed_at":"2016-07-26T15:42:47Z","author_association":"NONE","active_lock_reason":null,"body":"Using\n1. Elasticsearch: 2.3.1 \n2. JVM version: 1.8.0_60-b27\n3. OS version: Centos 7 \n\nWith \n\n```\n.\n.\n.\nnode:\n  data: true\n  master: false\n  name: esd02-data4\n  tier: hot\nos:\n  available_processors: 8\npath:\n  data:\n      - /storage/sdb1\n      - /storage/sdx1\n.\n.\n.\n```\n\nin `elasticsearch.yml` we have observed twice now where one of the disks (say, `/storage/sdx1`) will fill up to 100% while the other `/storage/sdb1` still has space. The error logs fill up with errors like:\n\n```\n[logstash-syslog-2016.07.21][[logstash-syslog-2016.07.21][1]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog.ckp -> /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog-7395039383191700052.tlog: No space left on device];\nCaused by: [logstash-syslog-2016.07.21][[logstash-syslog-2016.07.21][1]] EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog.ckp -> /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog-7395039383191700052.tlog: No space left on device];\nCaused by: java.nio.file.FileSystemException: /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog.ckp -> /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog-7395039383191700052.tlog: No space left on device\njava.nio.file.FileSystemException: /storage/sdb1/stage/nodes/0/indices/logstash-syslog-2016.07.21/_state/state-13787.st -> /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/_state/state-13787.st.tmp: No space left on device\n```\n\nIt's pretty obvious what's happening... `storage/sdx1` owns a shard of `logstash-syslog-2016.07.21` and ES is refusing to \"break\" the shard across multiple disk paths....\n\nbut it's also failing to recover from this on its own, or to stop sending data to the shard whose disk is full.  I would expect either (A) in the event of a full disk, ES would nominate the other shards to accept 100% of the writes to the index or (B) ES would loosen its requirement that shards are not allowed to span multiple write paths. \n","closed_by":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"performed_via_github_app":null}