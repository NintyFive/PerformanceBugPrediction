{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/33229","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33229/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33229/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33229/events","html_url":"https://github.com/elastic/elasticsearch/issues/33229","id":355046327,"node_id":"MDU6SXNzdWUzNTUwNDYzMjc=","number":33229,"title":"Huge number of translog files created during hot index recovery","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"labels":[{"id":152510590,"node_id":"MDU6TGFiZWwxNTI1MTA1OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Recovery","name":":Distributed/Recovery","color":"0e8a16","default":false,"description":"Anything around constructing a new shard, either from a local or a remote source."},{"id":929267538,"node_id":"MDU6TGFiZWw5MjkyNjc1Mzg=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/team-discuss","name":"team-discuss","color":"fbca04","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2018-08-29T08:11:41Z","updated_at":"2018-09-11T21:06:56Z","closed_at":"2018-09-09T17:19:53Z","author_association":"NONE","active_lock_reason":null,"body":"<!-- Bug report -->\r\n\r\n**Elasticsearch version** (`bin/elasticsearch --version`):\r\n6.4.0\r\n**Plugins installed**: []\r\n\r\n**JVM version** (`java -version`):\r\n1.8.0_152-b16\r\n\r\n**OS version** (`uname -a` if on a Unix-like system):\r\nLinux SVR13912HW2288 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\nWhen recovery took place for a hot index with high indexing rate,  both primary and replica shards kept generating new translog files until the the whole process done.  The number of files created could be thousands for some large shards.\r\n\r\n**Steps to reproduce**:\r\n 1.  Set replica for a hot index to 0 and back to 1 , to trigger the replica shard recovery\r\n 2.  During the file copy phase,  translog files on primary shards stopped rotating and new files are constantly created with each file about 70MB in size.\r\n 3.  After entering translog recovery phase, both primary shards and replica shards kept creating new translog files.\r\n\r\n**Provide logs (if relevant)**:\r\nBelow are some information I captured during the test:\r\n1.  Index translog related settings:\r\n```json\r\n\"translog\": {\r\n          \"flush_threshold_size\": \"1gb\",\r\n          \"sync_interval\": \"60s\",\r\n          \"retention\": {\r\n            \"size\": \"1gb\"\r\n          },\r\n          \"durability\": \"async\"\r\n        }\r\n```\r\n\r\n2. The index in recovery is about 25.7GB in size:\r\n```\r\nfncpayment-backmainclientlog-2018.08.29 0 r INITIALIZING                  xx.2.27.186 data_xx.2.27.186_A\r\nfncpayment-backmainclientlog-2018.08.29 0 p STARTED      186688837 25.7gb xx.2.27.185 data_xx.2.27.185_A\r\n```\r\n3.  Stats of translog recovery phase, with about 7.6 million entries to replay.\r\n```\r\nfncpayment-backmainclientlog-2018.08.29 0 42.5m peer translog xx.2.27.185 data_xx.2.27.185_A xx.2.27.186 data_xx.2.27.186_A n/a n/a 163 163 100.0% 163 24622074982 24622074982 100.0% 24622074982 7646229 3505729 45.8%\r\n```\r\n\r\n4.  Translog stats of primary / replica shards\r\n```json\r\n\"shards\": {\r\n        \"0\": [\r\n          {\r\n            \"routing\": {\r\n              \"state\": \"INITIALIZING\",\r\n              \"primary\": false,\r\n              \"node\": \"OoKC2Z7VTrer1TPygVrXYg\",\r\n              \"relocating_node\": null\r\n            },\r\n            \"translog\": {\r\n              \"operations\": 14391764,\r\n              \"size_in_bytes\": 15950728635,\r\n              \"uncommitted_operations\": 14391764,\r\n              \"uncommitted_size_in_bytes\": 15950728580,\r\n              \"earliest_last_modified_age\": 1560615\r\n            },\r\n            \"commit\": {\r\n              \"id\": \"CK4jTpYH5k7Pj2p7Xdp1sw==\",\r\n              \"generation\": 176,\r\n              \"user_data\": {\r\n                \"local_checkpoint\": \"167930856\",\r\n                \"max_unsafe_auto_id_timestamp\": \"1535525410521\",\r\n                \"translog_uuid\": \"MsN0q8i6T1Olglt-qctlKA\",\r\n                \"history_uuid\": \"A0kc4a8RTOCdkzJ0cDjUSA\",\r\n                \"translog_generation\": \"2\",\r\n                \"max_seq_no\": \"176162155\"\r\n              },\r\n              \"num_docs\": 168892968\r\n            },\r\n            \"seq_no\": {\r\n              \"max_seq_no\": 186410914,\r\n              \"local_checkpoint\": 167930856,\r\n              \"global_checkpoint\": -2\r\n            },\r\n            \"shard_path\": {\r\n              \"state_path\": \"/var/data/elasticsearch/A/nodes/0\",\r\n              \"data_path\": \"/var/data/elasticsearch/A/nodes/0\",\r\n              \"is_custom_data_path\": false\r\n            }\r\n          },\r\n          {\r\n            \"routing\": {\r\n              \"state\": \"STARTED\",\r\n              \"primary\": true,\r\n              \"node\": \"Ph8B5eKQTp-s0tLuPalNiA\",\r\n              \"relocating_node\": null\r\n            },\r\n            \"translog\": {\r\n              \"operations\": 18701434,\r\n              \"size_in_bytes\": 20726083030,\r\n              \"uncommitted_operations\": 919603,\r\n              \"uncommitted_size_in_bytes\": 1019279882,\r\n              \"earliest_last_modified_age\": 2620616\r\n            },\r\n            \"commit\": {\r\n              \"id\": \"PAv900puYwtErrt4/k8VFA==\",\r\n              \"generation\": 192,\r\n              \"user_data\": {\r\n                \"local_checkpoint\": \"185503832\",\r\n                \"max_unsafe_auto_id_timestamp\": \"1535500804172\",\r\n                \"translog_uuid\": \"pPDrqHf1QTOWzZyso-VPdA\",\r\n                \"history_uuid\": \"A0kc4a8RTOCdkzJ0cDjUSA\",\r\n                \"translog_generation\": \"2740\",\r\n                \"max_seq_no\": \"185503834\"\r\n              },\r\n              \"num_docs\": 185502733\r\n            },\r\n            \"seq_no\": {\r\n              \"max_seq_no\": 186423434,\r\n              \"local_checkpoint\": 186423434,\r\n              \"global_checkpoint\": 186410914\r\n            },\r\n            \"shard_path\": {\r\n              \"state_path\": \"/var/data/elasticsearch/A/nodes/0\",\r\n              \"data_path\": \"/var/data/elasticsearch/A/nodes/0\",\r\n              \"is_custom_data_path\": false\r\n            }\r\n          }\r\n        ]\r\n      }\r\n```\r\n5. Over 300 translog files already created during this phase and kept growing.\r\n6. Those translog files are not purged until the whole recovery process completes.\r\n\r\nIn some cases, when a node serving several shards with high write rate, was down and rejoined the cluster, the recovery process could took hours. Thus translog files created on the node could accumulated to tens of thousands.  Purging such amount of files following recovery done, some times blocked write for several minutes.   The effect could be the cluster having hard time apply state update and cluster state api not responding. ","closed_by":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"performed_via_github_app":null}