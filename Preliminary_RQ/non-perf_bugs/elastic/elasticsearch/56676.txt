{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/56676","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56676/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56676/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56676/events","html_url":"https://github.com/elastic/elasticsearch/issues/56676","id":617388454,"node_id":"MDU6SXNzdWU2MTczODg0NTQ=","number":56676,"title":"CharGroupTokenizerFactory should allow users to set the maximum character limit for a word","user":{"login":"ADBalici","id":9438684,"node_id":"MDQ6VXNlcjk0Mzg2ODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/9438684?v=4","gravatar_id":"","url":"https://api.github.com/users/ADBalici","html_url":"https://github.com/ADBalici","followers_url":"https://api.github.com/users/ADBalici/followers","following_url":"https://api.github.com/users/ADBalici/following{/other_user}","gists_url":"https://api.github.com/users/ADBalici/gists{/gist_id}","starred_url":"https://api.github.com/users/ADBalici/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ADBalici/subscriptions","organizations_url":"https://api.github.com/users/ADBalici/orgs","repos_url":"https://api.github.com/users/ADBalici/repos","events_url":"https://api.github.com/users/ADBalici/events{/privacy}","received_events_url":"https://api.github.com/users/ADBalici/received_events","type":"User","site_admin":false},"labels":[{"id":142001965,"node_id":"MDU6TGFiZWwxNDIwMDE5NjU=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Analysis","name":":Search/Analysis","color":"0e8a16","default":false,"description":"How text is split into tokens"},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null},{"id":110815527,"node_id":"MDU6TGFiZWwxMTA4MTU1Mjc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/help%20wanted","name":"help wanted","color":"207de5","default":true,"description":"adoptme"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2020-05-13T11:59:50Z","updated_at":"2020-05-20T12:16:01Z","closed_at":"2020-05-20T12:16:00Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Elasticsearch Version: 7.6.1\r\n\r\nIt appears that the only supported setting for the ```CharGroupTokinzer``` is ```tokenize_on_chars```. This is fine for most users as long as the resulting words (after the split) are less than 256 characters long. If longer, words will be truncated.\r\n\r\nThis behaviour is the cause of a default setting in ```org.apache.lucene.analysis.util.CharTokenizer```:\r\n\r\n```\r\npublic static final int DEFAULT_MAX_WORD_LEN = 255;\r\n```\r\n\r\nHowever, Lucene allows for overriding this default value, which is something that should be done here as well.\r\n","closed_by":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"performed_via_github_app":null}