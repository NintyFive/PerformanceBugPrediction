[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/271031330","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-271031330","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":271031330,"node_id":"MDEyOklzc3VlQ29tbWVudDI3MTAzMTMzMA==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2017-01-06T22:47:15Z","updated_at":"2017-01-06T22:47:15Z","author_association":"MEMBER","body":"> I'd expect to get the tokens: `is, thi, this, dej, deja, vu`\r\n\r\nSince you use the `icu_tokenizer` your text is being split into four tokens:\r\n\r\n``` json\r\n{\r\n  \"tokens\" : [ {\r\n    \"token\" : \"Is\",\r\n    \"start_offset\" : 0,\r\n    \"end_offset\" : 2,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 0\r\n  }, {\r\n    \"token\" : \"this\",\r\n    \"start_offset\" : 3,\r\n    \"end_offset\" : 7,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 1\r\n  }, {\r\n    \"token\" : \"deja\",\r\n    \"start_offset\" : 8,\r\n    \"end_offset\" : 12,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 2\r\n  }, {\r\n    \"token\" : \"vu\",\r\n    \"start_offset\" : 13,\r\n    \"end_offset\" : 15,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 3\r\n  } ]\r\n}\r\n```\r\n\r\nAnd then if you do the folding and `keyword_repeat`:\r\n\r\n``` json\r\n{\r\n  \"tokens\" : [ {\r\n    \"token\" : \"is\",\r\n    \"start_offset\" : 0,\r\n    \"end_offset\" : 2,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 0\r\n  }, {\r\n    \"token\" : \"is\",\r\n    \"start_offset\" : 0,\r\n    \"end_offset\" : 2,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 0\r\n  }, {\r\n    \"token\" : \"this\",\r\n    \"start_offset\" : 3,\r\n    \"end_offset\" : 7,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 1\r\n  }, {\r\n    \"token\" : \"this\",\r\n    \"start_offset\" : 3,\r\n    \"end_offset\" : 7,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 1\r\n  }, {\r\n    \"token\" : \"deja\",\r\n    \"start_offset\" : 8,\r\n    \"end_offset\" : 12,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 2\r\n  }, {\r\n    \"token\" : \"deja\",\r\n    \"start_offset\" : 8,\r\n    \"end_offset\" : 12,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 2\r\n  }, {\r\n    \"token\" : \"vu\",\r\n    \"start_offset\" : 13,\r\n    \"end_offset\" : 15,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 3\r\n  }, {\r\n    \"token\" : \"vu\",\r\n    \"start_offset\" : 13,\r\n    \"end_offset\" : 15,\r\n    \"type\" : \"<ALPHANUM>\",\r\n    \"position\" : 3\r\n  } ]\r\n}\r\n```\r\n\r\nIf you then try to do edge ngrams for the `is` and `vu` terms, they are below\r\nthe `min_gram` threshold of 3 in your configuration, so they are dropped.\r\n\r\nIf you want to keep the whitespace, perhaps inject a shingle token filter (with\r\ntwo shingles) in there so that `is this` becomes a token including the\r\nwhitespace, which you can then analyze with edgengrams to get `is `, `is t`, `is\r\nth`, `is this`.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/271345252","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-271345252","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":271345252,"node_id":"MDEyOklzc3VlQ29tbWVudDI3MTM0NTI1Mg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2017-01-09T17:16:32Z","updated_at":"2017-01-09T17:16:32Z","author_association":"CONTRIBUTOR","body":"I've run into this same issue before.  `keyword_repeat` only works for stemmers, but I wonder if this functionality should be extended to edge-ngrams.\r\n\r\n@mikemccand what do you think?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/271353034","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-271353034","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":271353034,"node_id":"MDEyOklzc3VlQ29tbWVudDI3MTM1MzAzNA==","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2017-01-09T17:46:18Z","updated_at":"2017-01-09T17:46:18Z","author_association":"CONTRIBUTOR","body":"FWIW, my current work around is to do always use the lang specific analysis field when I think I'm searching in a non-whitespace separated language (but I don't really trust my lang detection), and I use the lang specific field anytime the text is less than 3 chars, or if the trailing word is less than three chars (eg a search like \"math pi\" ).\r\n\r\nShingle tokens as a work around will still have the same problem of not letting me have sub 3 char tokens i think. I also suspect it would blow up the index size even more than including 1 and 2 char edgengrams would.\r\n\r\nBTW, if we change this, can it be easily back ported to 2.x ;)\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/295806208","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-295806208","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":295806208,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NTgwNjIwOA==","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2017-04-20T16:39:13Z","updated_at":"2017-04-20T16:39:13Z","author_association":"CONTRIBUTOR","body":"Heh, and now we've found a case where my workarounds don't work: \"Game of Thrones\". \r\n\r\nAny updates here? I guess I could just adjust to edgengrams starting from 1 char just seems likely to cause lots of inefficiencies.\r\n\r\nShingle tokens sounds interesting (and maybe improves relevancy) but will also significantly increase index size.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/295814147","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-295814147","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":295814147,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NTgxNDE0Nw==","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2017-04-20T16:59:08Z","updated_at":"2017-04-20T16:59:08Z","author_association":"CONTRIBUTOR","body":"Another idea (for anyone following along). I could have one edgegrams field per language and then specify a language analyzer that has stop words for that language. Would fix the worst cases, but still not fix something like \"pi\".\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374731989","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-374731989","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":374731989,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDczMTk4OQ==","user":{"login":"mayya-sharipova","id":5738841,"node_id":"MDQ6VXNlcjU3Mzg4NDE=","avatar_url":"https://avatars1.githubusercontent.com/u/5738841?v=4","gravatar_id":"","url":"https://api.github.com/users/mayya-sharipova","html_url":"https://github.com/mayya-sharipova","followers_url":"https://api.github.com/users/mayya-sharipova/followers","following_url":"https://api.github.com/users/mayya-sharipova/following{/other_user}","gists_url":"https://api.github.com/users/mayya-sharipova/gists{/gist_id}","starred_url":"https://api.github.com/users/mayya-sharipova/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mayya-sharipova/subscriptions","organizations_url":"https://api.github.com/users/mayya-sharipova/orgs","repos_url":"https://api.github.com/users/mayya-sharipova/repos","events_url":"https://api.github.com/users/mayya-sharipova/events{/privacy}","received_events_url":"https://api.github.com/users/mayya-sharipova/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T19:45:22Z","updated_at":"2018-03-20T19:45:22Z","author_association":"CONTRIBUTOR","body":"@gibrown  Can you please confirm what tokens do you expect when you index \"\"Is this déjà vu?\"\r\nAre you expecting ngrams (3-15) as well?\r\n\r\n\"Is t\"\r\n\"Is th\"\r\n...\r\nCan you index using edge ngram tokenizer? \r\nAnd if you need original tokens as well, can you use  another field for this?\r\n\r\n`keyword_repeat` is specifically designed to be followed by some stem filter.  It is not relevant for edge ngrams.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374732110","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-374732110","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":374732110,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDczMjExMA==","user":{"login":"mayya-sharipova","id":5738841,"node_id":"MDQ6VXNlcjU3Mzg4NDE=","avatar_url":"https://avatars1.githubusercontent.com/u/5738841?v=4","gravatar_id":"","url":"https://api.github.com/users/mayya-sharipova","html_url":"https://github.com/mayya-sharipova","followers_url":"https://api.github.com/users/mayya-sharipova/followers","following_url":"https://api.github.com/users/mayya-sharipova/following{/other_user}","gists_url":"https://api.github.com/users/mayya-sharipova/gists{/gist_id}","starred_url":"https://api.github.com/users/mayya-sharipova/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mayya-sharipova/subscriptions","organizations_url":"https://api.github.com/users/mayya-sharipova/orgs","repos_url":"https://api.github.com/users/mayya-sharipova/repos","events_url":"https://api.github.com/users/mayya-sharipova/events{/privacy}","received_events_url":"https://api.github.com/users/mayya-sharipova/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T19:45:43Z","updated_at":"2018-03-20T19:45:43Z","author_association":"CONTRIBUTOR","body":"cc @elastic/es-search-aggs","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374745063","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-374745063","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":374745063,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDc0NTA2Mw==","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T20:28:02Z","updated_at":"2018-03-20T20:28:02Z","author_association":"CONTRIBUTOR","body":"For edgengrams on \"Is this déjà vu?\" I would only expect the following tokens:\r\n\"is\",\"thi\",\"this\",\"dej\",\"deja\",\"vu\"\r\n\r\n\"is t\" and \"is th\" would not be in the index.\r\n\r\n>Can you index using edge ngram tokenizer?\r\n\r\nNo we are using the icu_tokenizer. We are doing indexing across all languages. Technically we should even be using special tokenization for Japanese, Korean, and Chinese so we can get the tokenization correct there.\r\n\r\nThanks for taking a look.\r\n\r\nOur workaround that we have deployed is to search both the edgengram field and an icu tokenized field that doesn't have any ngrams. We do this with a multi_match query that uses the cross_fields and AND as the operator. Makes for a more expensive query but it kinda works.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374745887","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-374745887","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":374745887,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDc0NTg4Nw==","user":{"login":"mayya-sharipova","id":5738841,"node_id":"MDQ6VXNlcjU3Mzg4NDE=","avatar_url":"https://avatars1.githubusercontent.com/u/5738841?v=4","gravatar_id":"","url":"https://api.github.com/users/mayya-sharipova","html_url":"https://github.com/mayya-sharipova","followers_url":"https://api.github.com/users/mayya-sharipova/followers","following_url":"https://api.github.com/users/mayya-sharipova/following{/other_user}","gists_url":"https://api.github.com/users/mayya-sharipova/gists{/gist_id}","starred_url":"https://api.github.com/users/mayya-sharipova/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mayya-sharipova/subscriptions","organizations_url":"https://api.github.com/users/mayya-sharipova/orgs","repos_url":"https://api.github.com/users/mayya-sharipova/repos","events_url":"https://api.github.com/users/mayya-sharipova/events{/privacy}","received_events_url":"https://api.github.com/users/mayya-sharipova/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T20:30:39Z","updated_at":"2018-03-20T20:30:47Z","author_association":"CONTRIBUTOR","body":"@gibrown If you found the workaround, would you mind if I close this issue?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374753002","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-374753002","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":374753002,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDc1MzAwMg==","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T20:54:23Z","updated_at":"2018-03-20T20:54:23Z","author_association":"CONTRIBUTOR","body":"I still think that some way to index edgengrams from X-Y plus also the original token would be a very worthwhile improvement. I would use it if it were available. I still think the keyword_repeat is the closest approximation. My workaround breaks if i am trying to do a phrase match. For instance: `\"is this dej\"`\r\n\r\nTechnically what I would love is a clearer language that lets me have multiple flows for extracting tokens:\r\n- extract the original token\r\n- extract a stemmed version of the token\r\n- extract the edgengrams for a token.\r\n\r\nThis lets me do an AND match on multiple tokens as well as a phrase match. Having them be in multiple fields has a number of drawbacks.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/390686004","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-390686004","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":390686004,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MDY4NjAwNA==","user":{"login":"romseygeek","id":1347065,"node_id":"MDQ6VXNlcjEzNDcwNjU=","avatar_url":"https://avatars0.githubusercontent.com/u/1347065?v=4","gravatar_id":"","url":"https://api.github.com/users/romseygeek","html_url":"https://github.com/romseygeek","followers_url":"https://api.github.com/users/romseygeek/followers","following_url":"https://api.github.com/users/romseygeek/following{/other_user}","gists_url":"https://api.github.com/users/romseygeek/gists{/gist_id}","starred_url":"https://api.github.com/users/romseygeek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/romseygeek/subscriptions","organizations_url":"https://api.github.com/users/romseygeek/orgs","repos_url":"https://api.github.com/users/romseygeek/repos","events_url":"https://api.github.com/users/romseygeek/events{/privacy}","received_events_url":"https://api.github.com/users/romseygeek/received_events","type":"User","site_admin":false},"created_at":"2018-05-21T15:18:18Z","updated_at":"2018-05-21T15:18:18Z","author_association":"CONTRIBUTOR","body":"I've been doing some work on making branches possible in TokenStreams (see https://issues.apache.org/jira/browse/LUCENE-8273).  If that were combined with a generalisation of KeywordRepeatFilter, we could build an analysis chain that looked something like:\r\n```\r\nKeywordRepeatFilter(none, stem, ngram) -> repeats each token three times with a different keyword set\r\nif (keyword == stem) then apply Stemmer\r\nif (keyword == ngram) then apply EdgeNGramFilter\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/394822527","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-394822527","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":394822527,"node_id":"MDEyOklzc3VlQ29tbWVudDM5NDgyMjUyNw==","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2018-06-05T18:58:39Z","updated_at":"2018-06-05T18:58:39Z","author_association":"CONTRIBUTOR","body":"@romseygeek I love the idea of being able to have multiple paths of processing tokens. This would help in a lot of cases I've seen I think.\r\n\r\nIt feels like the analysis syntax would need a bit more structure than it currently has to handle this sort of thing.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/395055521","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-395055521","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":395055521,"node_id":"MDEyOklzc3VlQ29tbWVudDM5NTA1NTUyMQ==","user":{"login":"nomoa","id":5939211,"node_id":"MDQ6VXNlcjU5MzkyMTE=","avatar_url":"https://avatars1.githubusercontent.com/u/5939211?v=4","gravatar_id":"","url":"https://api.github.com/users/nomoa","html_url":"https://github.com/nomoa","followers_url":"https://api.github.com/users/nomoa/followers","following_url":"https://api.github.com/users/nomoa/following{/other_user}","gists_url":"https://api.github.com/users/nomoa/gists{/gist_id}","starred_url":"https://api.github.com/users/nomoa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nomoa/subscriptions","organizations_url":"https://api.github.com/users/nomoa/orgs","repos_url":"https://api.github.com/users/nomoa/repos","events_url":"https://api.github.com/users/nomoa/events{/privacy}","received_events_url":"https://api.github.com/users/nomoa/received_events","type":"User","site_admin":false},"created_at":"2018-06-06T12:45:40Z","updated_at":"2018-06-06T12:45:40Z","author_association":"CONTRIBUTOR","body":"We had exactly the same issue, problem is that not all filters support the `keyword` attribute. We ended up adding a new Token filter in a [plugin we maintain](https://github.com/wikimedia/search-extra/blob/master/docs/preserve_original.md) to work around this limitation.\r\nIt would be great to to have such support upstream (either by making all filters aware the `keyword` attribute or by providing another way to really emit the original token).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/398686546","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-398686546","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":398686546,"node_id":"MDEyOklzc3VlQ29tbWVudDM5ODY4NjU0Ng==","user":{"login":"romseygeek","id":1347065,"node_id":"MDQ6VXNlcjEzNDcwNjU=","avatar_url":"https://avatars0.githubusercontent.com/u/1347065?v=4","gravatar_id":"","url":"https://api.github.com/users/romseygeek","html_url":"https://github.com/romseygeek","followers_url":"https://api.github.com/users/romseygeek/followers","following_url":"https://api.github.com/users/romseygeek/following{/other_user}","gists_url":"https://api.github.com/users/romseygeek/gists{/gist_id}","starred_url":"https://api.github.com/users/romseygeek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/romseygeek/subscriptions","organizations_url":"https://api.github.com/users/romseygeek/orgs","repos_url":"https://api.github.com/users/romseygeek/repos","events_url":"https://api.github.com/users/romseygeek/events{/privacy}","received_events_url":"https://api.github.com/users/romseygeek/received_events","type":"User","site_admin":false},"created_at":"2018-06-20T09:32:15Z","updated_at":"2018-06-20T09:32:15Z","author_association":"CONTRIBUTOR","body":"Added in #31208 ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/400110126","html_url":"https://github.com/elastic/elasticsearch/issues/22478#issuecomment-400110126","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22478","id":400110126,"node_id":"MDEyOklzc3VlQ29tbWVudDQwMDExMDEyNg==","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2018-06-25T22:01:45Z","updated_at":"2018-06-25T22:01:45Z","author_association":"CONTRIBUTOR","body":"Very excited that this is in 6.4. Thanks @romseygeek nice work.","performed_via_github_app":null}]