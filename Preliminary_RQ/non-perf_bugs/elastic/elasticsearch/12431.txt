{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/12431","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12431/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12431/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12431/events","html_url":"https://github.com/elastic/elasticsearch/issues/12431","id":96921851,"node_id":"MDU6SXNzdWU5NjkyMTg1MQ==","number":12431,"title":"Shard awareness leaves replica shards unassigned when racks are unevenly sized","user":{"login":"ppf2","id":7216393,"node_id":"MDQ6VXNlcjcyMTYzOTM=","avatar_url":"https://avatars0.githubusercontent.com/u/7216393?v=4","gravatar_id":"","url":"https://api.github.com/users/ppf2","html_url":"https://github.com/ppf2","followers_url":"https://api.github.com/users/ppf2/followers","following_url":"https://api.github.com/users/ppf2/following{/other_user}","gists_url":"https://api.github.com/users/ppf2/gists{/gist_id}","starred_url":"https://api.github.com/users/ppf2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ppf2/subscriptions","organizations_url":"https://api.github.com/users/ppf2/orgs","repos_url":"https://api.github.com/users/ppf2/repos","events_url":"https://api.github.com/users/ppf2/events{/privacy}","received_events_url":"https://api.github.com/users/ppf2/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"},{"id":23715,"node_id":"MDU6TGFiZWwyMzcxNQ==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Edocs","name":">docs","color":"db755e","default":false,"description":"General docs changes"},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":{"login":"debadair","id":362578,"node_id":"MDQ6VXNlcjM2MjU3OA==","avatar_url":"https://avatars1.githubusercontent.com/u/362578?v=4","gravatar_id":"","url":"https://api.github.com/users/debadair","html_url":"https://github.com/debadair","followers_url":"https://api.github.com/users/debadair/followers","following_url":"https://api.github.com/users/debadair/following{/other_user}","gists_url":"https://api.github.com/users/debadair/gists{/gist_id}","starred_url":"https://api.github.com/users/debadair/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/debadair/subscriptions","organizations_url":"https://api.github.com/users/debadair/orgs","repos_url":"https://api.github.com/users/debadair/repos","events_url":"https://api.github.com/users/debadair/events{/privacy}","received_events_url":"https://api.github.com/users/debadair/received_events","type":"User","site_admin":false},"assignees":[{"login":"debadair","id":362578,"node_id":"MDQ6VXNlcjM2MjU3OA==","avatar_url":"https://avatars1.githubusercontent.com/u/362578?v=4","gravatar_id":"","url":"https://api.github.com/users/debadair","html_url":"https://github.com/debadair","followers_url":"https://api.github.com/users/debadair/followers","following_url":"https://api.github.com/users/debadair/following{/other_user}","gists_url":"https://api.github.com/users/debadair/gists{/gist_id}","starred_url":"https://api.github.com/users/debadair/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/debadair/subscriptions","organizations_url":"https://api.github.com/users/debadair/orgs","repos_url":"https://api.github.com/users/debadair/repos","events_url":"https://api.github.com/users/debadair/events{/privacy}","received_events_url":"https://api.github.com/users/debadair/received_events","type":"User","site_admin":false}],"milestone":null,"comments":5,"created_at":"2015-07-23T22:55:52Z","updated_at":"2018-04-18T09:36:43Z","closed_at":"2018-04-18T09:36:42Z","author_association":"MEMBER","active_lock_reason":null,"body":"There are a few parts to this ticket:\n\n**Functionality**\nThere is this logic (https://github.com/elastic/elasticsearch/blob/c57951780e0132c50b723d78038ab73e10d176c5/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java#L222) which prevents the allocation of shards when the number of shards per unique node attribute used in the awareness (eg. rack_id) is exceeded (`if (currentNodeCount > (requiredCountPerAttribute + leftoverPerAttribute)) {`).\n\nIn this particular use case, the user has 69 nodes with an index that has 1 primary shard defined with 68 replicas (so 69 copies of the shard).  The expectation is that it will put 1 shard on each node.  The issue here is that it doesn't and it ends up leaving 2 copies unassigned.\n\nFor example, one of the rack_id's used in allocation awareness is associated to 5 different nodes in the cluster.  It ended up allocating 1 shard on 4 of the 5 nodes.  At this point, (currentNodeCount > (requiredCountPerAttribute + leftoverPerAttribute)), so it prevents the 5th node from getting a copy of this shard.  In this case averagePerAttribute is 2 (with 28 unique attributes) so requiredCountPerAttribute is 2.  totalLeftover is not 0, so leftoverPerAttribute is 1.  So 4 > 3 and it prevents the 5th node from getting a shard copy.\n\nThere is quite an uneven distribution of nodes per rack_id, some rack_ids in this setup has 1 node, others have up to 5 nodes in them.  The allocation decider logic is preventing all shards for that index from being allocated and leaving a few unassigned.\n\nFor this use case, the user is not that concerned about distribution of shards for this particular index across racks.  But awareness is currently enabled for the node/cluster and there does not appear to be a way to configure a specific index to be excluded from the awareness logic.\n\n**Documentation**\nNot much available on the above logic.\n\n**Logging/Debugging**\nThis may get a bit verbose so I am not sure about the feasibility of it.  For troubleshooting purposes, it will be helpful if we can write a trace line with a bit more details than just the current message in the reroute explain (\"explanation\": \"too many shards on nodes for attribute: [rack_id]\").  Like it will be useful if it will tell you the problem index, how many shards are allowed, and how many are on there currently, etc..  Doesn't have to be logging either, maybe add to the explanation string that is returned by reroute explain to give some more specific information.\n","closed_by":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"performed_via_github_app":null}