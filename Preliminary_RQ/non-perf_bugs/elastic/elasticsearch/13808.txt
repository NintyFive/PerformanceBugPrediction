{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/13808","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13808/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13808/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13808/events","html_url":"https://github.com/elastic/elasticsearch/issues/13808","id":108414646,"node_id":"MDU6SXNzdWUxMDg0MTQ2NDY=","number":13808,"title":"Elastic Search cluster not auto repairing (1.7.2)","user":{"login":"trikosuave","id":947008,"node_id":"MDQ6VXNlcjk0NzAwOA==","avatar_url":"https://avatars1.githubusercontent.com/u/947008?v=4","gravatar_id":"","url":"https://api.github.com/users/trikosuave","html_url":"https://github.com/trikosuave","followers_url":"https://api.github.com/users/trikosuave/followers","following_url":"https://api.github.com/users/trikosuave/following{/other_user}","gists_url":"https://api.github.com/users/trikosuave/gists{/gist_id}","starred_url":"https://api.github.com/users/trikosuave/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/trikosuave/subscriptions","organizations_url":"https://api.github.com/users/trikosuave/orgs","repos_url":"https://api.github.com/users/trikosuave/repos","events_url":"https://api.github.com/users/trikosuave/events{/privacy}","received_events_url":"https://api.github.com/users/trikosuave/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2015-09-25T22:00:00Z","updated_at":"2015-09-28T07:02:41Z","closed_at":"2015-09-27T09:55:30Z","author_association":"NONE","active_lock_reason":null,"body":" I have es 1.7.2 and have a cluster of 10 nodes. I wanted to scale down to ten, so I terminated one of the boxes and watched hte cluster status change from green to yellow. Before the upgrade to 1.7.2 from 1.4.5, it will automatically relocate all the shards.\nNow it sits with a bunch of unassigned_shards forever. I found a fix where I run the following:\n\ncurl -XPUT 'localhost:9200/_cluster/settings' -d '{\n    \"transient\" : {\n        \"cluster.routing.allocation.enable\" : \"all\"\n    }\n}'\n\nThis fixed my issue and my cluster went back to green. However, I need to run that command every time I teardown a node on my way down to 5. Before I never had to run that command and I could safely destroy one node, one by one, going from green to yellow back to green on its own. \n\nI would eventually like this to autoscale, but cannot do that if I need to enable allocation each time i drop a node. \n\nAny thoughts? \n\nThanks\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}