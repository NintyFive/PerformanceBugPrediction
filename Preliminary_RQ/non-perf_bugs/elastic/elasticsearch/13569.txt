{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/13569","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13569/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13569/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13569/events","html_url":"https://github.com/elastic/elasticsearch/issues/13569","id":106507385,"node_id":"MDU6SXNzdWUxMDY1MDczODU=","number":13569,"title":"ES failing with \"marking and sending shard failed due to [failed recovery]\"","user":{"login":"ashwgupt","id":8414341,"node_id":"MDQ6VXNlcjg0MTQzNDE=","avatar_url":"https://avatars2.githubusercontent.com/u/8414341?v=4","gravatar_id":"","url":"https://api.github.com/users/ashwgupt","html_url":"https://github.com/ashwgupt","followers_url":"https://api.github.com/users/ashwgupt/followers","following_url":"https://api.github.com/users/ashwgupt/following{/other_user}","gists_url":"https://api.github.com/users/ashwgupt/gists{/gist_id}","starred_url":"https://api.github.com/users/ashwgupt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ashwgupt/subscriptions","organizations_url":"https://api.github.com/users/ashwgupt/orgs","repos_url":"https://api.github.com/users/ashwgupt/repos","events_url":"https://api.github.com/users/ashwgupt/events{/privacy}","received_events_url":"https://api.github.com/users/ashwgupt/received_events","type":"User","site_admin":false},"labels":[{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2015-09-15T08:27:28Z","updated_at":"2016-01-28T17:30:27Z","closed_at":"2016-01-28T17:30:27Z","author_association":"NONE","active_lock_reason":null,"body":"We have a ES Cluster of 1.5.2 with 2 nodes in it. The cluster has been working smoothly for more than 4months until it went into some kind of Failed Shards errors and has stopped working.\n\nBelow is the huge number of errors reported by the cluster on both nodes, and stopping any sending (Logstash) or receiving application (Kibana here) from connecting to it.\n\nRecently it also started causing Memory exhaustion as well on one of the nodes, when I had to shut down our ES completely.\n\nCan anyone advise on the possible reason of the failure and any fix for it?\n\n**************\\* Exception in logs **********************\n[2015-09-15 00:02:52,692][WARN ][transport                ] [fil_middleware_clus01_node02] Received response for a request that has timed out, sent [30761ms] ago, timed out [760ms] ago, action [in\nternal:discovery/zen/fd/master_ping], node [[fil_middleware_clus01_node01][cYhvLzXwT92_0EsM6IEvDg][ukx06621.uk.fid-intl.com][inet[/10.60.172.71:9301]]{master=true}], id [893623]\n[2015-09-15 00:02:52,726][WARN ][indices.cluster          ] [fil_middleware_clus01_node02] [[.mw_kibana02][0]] marking and sending shard failed due to [failed recovery]\norg.elasticsearch.indices.recovery.RecoveryFailedException: [.mw_kibana02][0]: Recovery failed from [fil_middleware_clus01_node01][cYhvLzXwT92_0EsM6IEvDg][ukx06621.uk.fid-intl.com][inet[/10.60.172\n.71:9301]]{master=true} into [fil_middleware_clus01_node02][DVKSAI0dTyWk3u_nkcaVoA][ukx06622.uk.fid-intl.com][inet[/10.60.172.72:9302]]{master=true}\n        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:274)\n        at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:69)\n        at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:550)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.transport.RemoteTransportException: [fil_middleware_clus01_node01][inet[/10.60.172.71:9301]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [.mw_kibana02][0] Phase[1] Execution failed\n        at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:842)\n        at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:699)\n        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)\n        at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [.mw_kibana02][0] Failed to transfer [0] files with total size of [0b]\n        at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:413)\n        at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:837)\n        ... 10 more\nCaused by: java.io.IOException: directory '/elkmwpoc/ES/data/fil_middleware_cluster01/nodes/0/indices/.mw_kibana02/0/index' exists and is a directory, but cannot be listed: list() returned null\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:226)\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:237)\n        at org.elasticsearch.index.store.fs.DefaultFsDirectoryService$1.listAll(DefaultFsDirectoryService.java:57)\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:532)\n        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:528)\n        at org.elasticsearch.index.store.Store.getMetadata(Store.java:219)\n        at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:159)\n        ... 11 more\n[2015-09-15 00:02:52,743][WARN ][indices.cluster          ] [fil_middleware_clus01_node02] [[logstash-ctm-stats-2015.04.29][4]] marking and sending shard failed due to [failed recovery]\norg.elasticsearch.indices.recovery.RecoveryFailedException: [logstash-ctm-stats-2015.04.29][4]: Recovery failed from [fil_middleware_clus01_node01][cYhvLzXwT92_0EsM6IEvDg][ukx06621.uk.fid-intl.com\n][inet[/10.60.172.71:9301]]{master=true} into [fil_middleware_clus01_node02][DVKSAI0dTyWk3u_nkcaVoA][ukx06622.uk.fid-intl.com][inet[/10.60.172.72:9302]]{master=true}\n        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:274)\n        at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:69)\n        at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:550)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [logstash-ctm-stats-2015.04.29][4] Phase[1] Execution failed\n        at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:842)\n        at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:699)\n        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)\n        at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [logstash-ctm-stats-2015.04.29][4] Failed to transfer [0] files with total size of [0b]\n        at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:413)\n        at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:837)\n        ... 10 more\nCaused by: java.io.IOException: directory '/elkmwpoc/ES/data/fil_middleware_cluster01/nodes/0/indices/logstash-ctm-stats-2015.04.29/4/index' exists and is a directory, but cannot be listed: list()\n returned null\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:226)\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:237)\n        at org.elasticsearch.index.store.fs.DefaultFsDirectoryService$1.listAll(DefaultFsDirectoryService.java:57)\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:532)\n        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:528)\n        at org.elasticsearch.index.store.Store.getMetadata(Store.java:219)\n        at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:159)\n        ... 11 more\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}