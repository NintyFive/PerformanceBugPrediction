[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/128749831","html_url":"https://github.com/elastic/elasticsearch/issues/12726#issuecomment-128749831","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12726","id":128749831,"node_id":"MDEyOklzc3VlQ29tbWVudDEyODc0OTgzMQ==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2015-08-07T16:14:09Z","updated_at":"2015-08-07T16:38:06Z","author_association":"MEMBER","body":"Hi @gavelino, thanks for stopping by!  \n\nYour preprint generated some interesting discussion internally when it first came out.  Knowledge sharing is something that we take seriously;  it's vital to the health of Elasticsearch that knowledge be spread around and not concentrated in a small handful of individuals.  \n\nEspecially since we are a distributed company, information sharing is something we spend a lot of time talking about and brainstorming new ways to help.  We can't simply walk over to our colleague's desk and ask a question, when they live across the ocean and in a different timezone :)\n\nTo summarize our discussions though, I don't think the calculated \"truck factor\" represents our project very well, for a variety of hard and soft reasons.  I realize that a lot of my complaints are due to the fact the study was automated, which necessarily limits what data you can collect.\n\nApologies for the long post, I come from an academic background and I miss reviewing papers :)\n\n### Answers to your questions\n\nI'll get these out of the way first, before diving into various complaints of the methodology.\n\n> (a) Do you agree that the listed developers are the main developers of Elasticsearch?\n> \n> (b) Do you agree that Elasticsearch will be in trouble if the listed developers leave the project (e.g., if they win in the lottery, to be less morbid)?\n\nTo be honest, I think Shay is on the verge of no longer being a leading driver of code changes (sorry Shay!).  He obviously has huge input on major roadmap planning, and definitely lends his expertise to tricky refactoring or customer issues, but day-to-day code details have long since moved out of his hands.\n\nIMO, the Core Contributors are: Simon, Uri, Martijn, Boaz, Adrien, Luca, Ryan, Robert, Igor, Lee, Mike.  I might be missing some, but that's off the top of my head.  If we lost them, _then_ I'd personally start to worry :)\n\n> c) Does Elasticsearch have some characteristics that would attenuate the loss of the listed developers (e.g., detailed documentation)?\n\nWe do:\n- LGTM culture, where PRs require at least one review before merging... and preferably more.  We encourage junior devs (myself included) to review PRs, because it exposes them to new parts of the code\n- Internal team groups, which help spread knowledge about various components, solicit reviews, get opinions/help, etc\n- A newly created \"Blackbelt sessions\" project, where individual engineers field an hour-long Q&A about a specific component of Elasticsearch, so that everyone (devs, solution architects, support, etc) can dig into the deep technical details about one sub-system.  Like most of our meetings, these are recorded and stored online for later viewing for any who missed it, or joins the company at a later date.\n- A mentor program that pairs new hires with an existing engineer, to help facilitate the initial onboarding process\n- Various instruments to get help: email, chat, video conferencing, an video room which people loiter in and can offer quick help, wiki, documentation in the ES repo itself\n- Extensive unit and integration tests, which can provide a lot of insight into what a specific piece of code is supposed to do, the edge cases it encounters, etc.\n\n### General Methodology Complaints\n\nThe major assumption of the paper seems to equate first authorship as the main indicator of _knowledge_.  While you would expect these to be correlated, I think the model used is too coarse to accurately reflect real development processes.\n- The DOA model you are using from Murphy-Hill _et-al_ is lacking the very component that made the Murphy-Hill paper interesting: the Degree-of-interest (DOI) factor.  To quote the paper: \n\n> [Existing approaches] ignore knowledge that is gained by a developer interacting with the code [...]\n\nand (emphasis mine):\n\n> This study also found that other factors, such as authorship of code, should be used to **augment** DOI when attempting to gauge a developer's knowledge of the code\n\nDOA should be used to _augment_ DOI, not stand alone as the entire metric.  Without the DOI component, the Murphy-Hill model is no better than existing systems which, as described by the original author, are not particularly good\n- The weights on the original DOK model were determined via multiple linear regression.  Regression min-maxes the value of weights to fit the data points.  Obtained regression lines are no longer valid if you remove one of the weights entirely, such as removing the DOI component.  You would need to rerun the regression to find new weights.  Your DOA model is effectively modeling is this:\n\n```\nDOK = 3.293 + 1.098*FA + 0.164*DL - 0.321*ln(1+AC) + 0.19*ln(1+0)\n                                                           ^\n                                                           |\n                         This component is basically zero  |\n```\n\nE.g. your DOA model is basically saying the Degree-of-interest for each developer is zero, which obviously would heavily skew the original Murphy-Hill model.  It simply is not legitimate to use the DOA component in isolation.\n- I'm not a big fan of the original DOK model anyway.  It was \"trained\" with an incredibly small sample size, and only validated against two datasets.  It is very difficult to know if the model generalizes to many different development environments and projects.  Ask any engineer and they will have their own opinion about project structure, design patterns, modularity, etc.  And ask any data-scientist, and they'll tell you that small sample size almost guarantees overfitting your model which will lead to terrible predictions\n- Indeed, the original DOK model only predicts file \"ownership\" 55% of the time.  Murphy-Hill attribute this low rate to \"assignments [which] were sometimes guesses\" and that after seeing the final results \"they realized their assignments were likely wrong\".  IMO this is a worrying sign that test data was so unreliable that opinions change after seeing the model results (which is also coincidentally a _faux pas_ in machine learning... you cannot revise your model based on the accuracy of the validation set).  So it begs the question...can training data be trusted when the data is \"likely wrong\" to begin with?  \n- Interestingly, the 6 developers could only assign 64% of the code to being \"owned\" by a single developer.  Which seems to imply that a third of the codebase is not easily \"assigned\" to a single person, and therefore must be more collaborative (or perhaps abandoned).  You could argue these places are probably the most significant and important pieces of the code, since not a single person is responsible for it.  It also raises the question if asking who \"owns\" a physical file is really a useful metric?  \n- The original DOK model had issues with files older than the 3-month evaluation window, which meant they lacked first-authorship information. If I had to guess, this would have skewed the regression.\n- It's unclear to me if your pre-processing accounted for packages or files that had simply been moved.  E.g. a package or set of packages may be reorganized internally but remain untouched code-wise.  This gives an entirely new \"first authorship\" to the set of files and removes the old history of committers.\n- Finally, seeing your results regarding Homebrew seems to indicate to me that the algorithm weights first authorship too heavily.  By simply including user-contributed recipes, the Homebrew metric is heavily skewed.  It is almost 100% likely that none of these external recipe authors have the knowledge or skills to contribute to meaningful development of core Homebrew code, yet your algorithm awards them a very high score.  This is not a knock against Homebrew, but rather an indicator that your algorithm might be a bit too coarse.\n\n### Elasticsearch-specific\n- As noted in the original Murphy-Hill paper, this approach ignores largely static code, such as APIs which by necessity do not change often.  Elasticsearch has a large amount of code that is effectively RESTful API boilerplate which does not change often.  I would add to this other boilerplate such as interfaces, base classes, unit tests, etc.  This code does not change often (sometimes never).  Much of this code is also very easily understood, e.g. an interface is self-descriptive.\n- Your DOA model seems to place too high of an emphasis on first authorship, which negatively penalizes projects like Elasticsearch where a single author created the bulk of the original boilerplate and architectural framework.  Or projects like Cassandra which were developed internally and then open-sourced by a small handful of individuals.\n- There doesn't seem to be a notion of TF-over-time, delta-TF or \"momentum\".  For example, Shay does indeed have a large number of commits and first-authorships as the founder.  But if you look at his commit history, it is obvious that his role in day-to-day coding has diminished _substantially_.\n  ![screen shot 2015-08-07 at 11 02 48 am](https://cloud.githubusercontent.com/assets/1224228/9138769/e5bf584a-3cf3-11e5-83a8-ffa5a69c59a0.png)\n\nEven ignoring my previous complaints over methodology, if you were to re-analyze Elasticsearch starting around 2013 (the time period when Elastic was founded as a company and developers were hired to work on ES full time), it is fairly obvious that a substantial amount of work is being done by authors who are not Shay or Simon:\n\n![screen shot 2015-08-07 at 11 04 01 am](https://cloud.githubusercontent.com/assets/1224228/9138803/26b80a0e-3cf4-11e5-9d01-d536199819e5.png)\n- Elasticsearch is in an interesting position where it is effectively a distributed wrapper around the Lucene OSS library.  A good chunk of knowledge is wrapped up in knowing how Lucene works, how it interfaces with Elasticsearch, etc.  To that end, we have 8+ Lucene developers who split their time between Elasticsearch and Lucene.  The application of this deep knowledge is not easily seen in commit charts in ES, since these low-level details tend to be surgical rather than sweeping refactoring.  E.g. when Mike joined, his work on the the ES/Lucene interface code had drastic improvements to performance and stability, but are a relatively modest amount of actual LoC changes.  And a fair portion of the work is in Lucene itself, not Elasticsearch, but is no less critical to ES for it.  To implement these changes he still needed relatively deep knowledge of the involved components, despite being first-author on almost none of the code itself.\n\n### Conclusion\n\nI hope my complaints come across as constructive, and not combative.  We really do take knowledge sharing seriously, and your paper sparked some more discussion about how we can improve.  But I think there are some very large flaws in the methodology that prevent it from being an accurate forecast of a project's trajectory.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/130302551","html_url":"https://github.com/elastic/elasticsearch/issues/12726#issuecomment-130302551","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12726","id":130302551,"node_id":"MDEyOklzc3VlQ29tbWVudDEzMDMwMjU1MQ==","user":{"login":"gavelino","id":666210,"node_id":"MDQ6VXNlcjY2NjIxMA==","avatar_url":"https://avatars1.githubusercontent.com/u/666210?v=4","gravatar_id":"","url":"https://api.github.com/users/gavelino","html_url":"https://github.com/gavelino","followers_url":"https://api.github.com/users/gavelino/followers","following_url":"https://api.github.com/users/gavelino/following{/other_user}","gists_url":"https://api.github.com/users/gavelino/gists{/gist_id}","starred_url":"https://api.github.com/users/gavelino/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gavelino/subscriptions","organizations_url":"https://api.github.com/users/gavelino/orgs","repos_url":"https://api.github.com/users/gavelino/repos","events_url":"https://api.github.com/users/gavelino/events{/privacy}","received_events_url":"https://api.github.com/users/gavelino/received_events","type":"User","site_admin":false},"created_at":"2015-08-12T13:31:27Z","updated_at":"2015-08-12T13:31:27Z","author_association":"NONE","body":"Hi, polyfractal,\n\nThank you for your comments. We really appreciate the feedback. Our research is under development and the answers we are receiving for this survey will help to better interpret the results and improve our approach. Below, I tried to answer some of yours complains and questions.\n\n> IMO, the Core Contributors are: Simon, Uri, Martijn, Boaz, Adrien, Luca, Ryan, Robert, Igor, Lee, Mike. I might be missing some, but that's off the top of my head. If we lost them, then I'd personally start to worry :)\n\nAs you can see in list below, most of the core contributors elected by you are in the top-10 list generated by our approach. However, we just have listed the developers that represent the TF calculated.\n\nTop-10: \n• Shay Banon  - 51%\n• Simon Willnauer - 19%\n• Martijn van Groningen - 7%\n• uboness: - 6%\n• Luca Cavanna - 6%\n• Adrien Grand - 6%\n• Igor Motov: - 5%\n• Alexander Reelsen - 3%\n• Britta Weber - 3%\n• Lee Hinman - 3%\n\n> E.g. your DOA model is basically saying the Degree-of-interest for each developer is zero, which obviously would heavily skew the original Murphy-Hill model. It simply is not legitimate to use the DOA component in isolation.\n\nWe understand that the Degree-of-Interest is a importante part of the Degree of Authoship proposed by Murhy-Hill, but in the extended version of their work (http://dl.acm.org/citation.cfm?id=2600788.2512207) they comment that remotion of the DOI componente has a small impact on the weights of DOA, quoted below. As our aim is to automatically calculate the authorship for a large number of projects we had to focus only in the informations able to be recovered from CVS.\n\n“Furthermore, applying linear regression to a model without DOI as a variable, results in weighting factors for FA, DL,and AC that are very close (on average, 4% change) to the ones from the model using DOI.”\n\n> It's unclear to me if your pre-processing accounted for packages or files that had simply been moved. E.g. a package or set of packages may be reorganized internally but remain untouched code-wise. This gives an entirely new \"first authorship\" to the set of files and removes the old history of committers.\n\nIn order to avoid the problem that you pointed, we use the git-log functionality to identify commits that perform renames. When this happens we merge the development history of the old and new files. That way, they are not counted as a new \"first authorship\".\n\n> Finally, seeing your results regarding Homebrew seems to indicate to me that the algorithm weights first authorship too heavily. By simply including user-contributed recipes, the Homebrew metric is heavily skewed. It is almost 100% likely that none of these external recipe authors have the knowledge or skills to contribute to meaningful development of core Homebrew code, yet your algorithm awards them a very high score. This is not a knock against Homebrew, but rather an indicator that your algorithm might be a bit too coarse.\n\nHomebrew currently supports thousands of formulas, which are typically implemented by the package’s developers or users. For this reason, the system has one of the largest base of contributors on GitHub (almost 5K contributors, on July, 2015). However, if we do not consider the files in Library/Formula, HomeBrew’s Truck factor is just two. In some cases the knowledgement of particularies of the Project is necessary to filter the file history to be studied.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/135157293","html_url":"https://github.com/elastic/elasticsearch/issues/12726#issuecomment-135157293","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12726","id":135157293,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNTE1NzI5Mw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2015-08-26T20:12:43Z","updated_at":"2015-08-26T20:12:43Z","author_association":"CONTRIBUTOR","body":"Thank you for bringing this up. Even though your analysis is probably right that we need to better share knowledge about this code base, I think things tend to get better over time. I'm closing this ticket but feel free to bring it up again on the forums if you want to discuss it more.\n","performed_via_github_app":null}]