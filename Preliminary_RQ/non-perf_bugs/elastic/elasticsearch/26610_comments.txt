[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329091370","html_url":"https://github.com/elastic/elasticsearch/issues/26610#issuecomment-329091370","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26610","id":329091370,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTA5MTM3MA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T08:07:19Z","updated_at":"2017-09-13T08:07:19Z","author_association":"CONTRIBUTOR","body":"- Is this something you're newly experiencing with 5.5.1 but worked fine in earlier ES versions?\r\n- how many total shards you have?\r\n- what's the size of your cluster state?\r\n- are you experiencing the same issues when the `prometheus-exporter` plugin is not installed?\r\n- Can you provide a heap dump that's taken on OOM?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329094482","html_url":"https://github.com/elastic/elasticsearch/issues/26610#issuecomment-329094482","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26610","id":329094482,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTA5NDQ4Mg==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T08:20:36Z","updated_at":"2017-09-13T08:20:36Z","author_association":"MEMBER","body":"8,000 shards across 20 data nodes is quite excessive, you should consider reducing the number of shards per index as it is likely you are adversely affecting the performance of your cluster with this. Also what was the reason for moving form weekly to hourly indices? What is the average size on disk for your shards with hourly indices? Each shard has an overhead both on the node that it is allocated to and on the master node (in the form of entries in the cluster state). With the kinds of numbers you are talking about here you may well be getting memory problems due to the shear size of you cluster state, especially if you indices have a reasonably large number of fields. I would consider going back to daily or weekly indices here. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329108541","html_url":"https://github.com/elastic/elasticsearch/issues/26610#issuecomment-329108541","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26610","id":329108541,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTEwODU0MQ==","user":{"login":"lruslan","id":1275141,"node_id":"MDQ6VXNlcjEyNzUxNDE=","avatar_url":"https://avatars1.githubusercontent.com/u/1275141?v=4","gravatar_id":"","url":"https://api.github.com/users/lruslan","html_url":"https://github.com/lruslan","followers_url":"https://api.github.com/users/lruslan/followers","following_url":"https://api.github.com/users/lruslan/following{/other_user}","gists_url":"https://api.github.com/users/lruslan/gists{/gist_id}","starred_url":"https://api.github.com/users/lruslan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lruslan/subscriptions","organizations_url":"https://api.github.com/users/lruslan/orgs","repos_url":"https://api.github.com/users/lruslan/repos","events_url":"https://api.github.com/users/lruslan/events{/privacy}","received_events_url":"https://api.github.com/users/lruslan/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T09:16:13Z","updated_at":"2017-09-13T09:16:13Z","author_association":"NONE","body":"@ywelsch \r\n\r\n- Is this something you're newly experiencing with 5.5.1 but worked fine in earlier ES versions?\r\nNo. Before we were mostly using 1.7.* with considerably smaller amount of data nodes/indexes. And it's only in 5.5.1 we changed cluster setup to use 20 datanodes  and hourly index.\r\n- how many total shards you have?\r\n9087 shards ( 31 nodes in cluster, 20 data nodes, 490 indexes, 20 shards per index, hourly index) \r\n- what's the size of your cluster state?\r\n64MB\r\n- are you experiencing the same issues when the prometheus-exporter plugin is not installed?\r\nyes we experienced same memory growth issues before the  prometheus-exporter plugin, I actually installed prometheus-exporter on master nodes to start collecting jvm stats\r\n- Can you provide a heap dump that's taken on OOM?\r\nFor now I can provide dump from the jvm with memory usage close to OOM (8G of heap used out of 12G)\r\nhttps://drive.google.com/a/travix.com/file/d/0B40JaBxFEd3ET3FMRDhmY0FzbkE/view?usp=sharing\r\n\r\n I will try to get another taken during OOM will let you know when I have it.\r\n\r\nThank you","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329116310","html_url":"https://github.com/elastic/elasticsearch/issues/26610#issuecomment-329116310","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26610","id":329116310,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTExNjMxMA==","user":{"login":"lruslan","id":1275141,"node_id":"MDQ6VXNlcjEyNzUxNDE=","avatar_url":"https://avatars1.githubusercontent.com/u/1275141?v=4","gravatar_id":"","url":"https://api.github.com/users/lruslan","html_url":"https://github.com/lruslan","followers_url":"https://api.github.com/users/lruslan/followers","following_url":"https://api.github.com/users/lruslan/following{/other_user}","gists_url":"https://api.github.com/users/lruslan/gists{/gist_id}","starred_url":"https://api.github.com/users/lruslan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lruslan/subscriptions","organizations_url":"https://api.github.com/users/lruslan/orgs","repos_url":"https://api.github.com/users/lruslan/repos","events_url":"https://api.github.com/users/lruslan/events{/privacy}","received_events_url":"https://api.github.com/users/lruslan/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T09:46:00Z","updated_at":"2017-09-13T09:46:00Z","author_association":"NONE","body":"@colings86 thanks for the suggestions,\r\n- it is likely you are adversely affecting the performance of your cluster with this\r\nWith current setup our performance is actually very good (I tried number of different setups) it's only masters expose this memory issue (btw CPU usage on all the servers is really low). I think we may even go up to 30G of RAM on masters, maybe memory usage will settle somewhere close to 20G  (would be useful to know if there is a formula to estimate how much memory would be needed by masters assuming we have this many shards with this many fields). My initial assumption was - masters are not doing any particular heavy lifting and should be fine with relatively low resources assigned.  Initially I tried with the smaller amount of data nodes/shards which resulted in slower searches larger timeouts  and our systems being under utilised - we are running on dedicated hardware servers with 256G of RAM and 48 cores. Now I have 4 instances of elasticsearch per hardware server running in docker and with 20 shards I see huge improvement on a search/indexing side.\r\n- what was the reason for moving form weekly to hourly indices?\r\nwe write almost 1.2-1.4TB of data daily to our logstash indexes, with daily index and 20 data nodes we will have approximately 60GB-80Gb per shard (first we tried daily index but it proved too slow to recover/relocate quickly in case of hardware issues, curator index merging on 60G takes too long, also with daily index we are not so flexible when need adjust mappings/change type of filed during the day - we need to edit index template and wait another day/index to start using changed template, with hourly index it's way too faster)\r\nSo far it seems hourly index is our best bet. What I wanted to do next is reduce amount of shards from 20 to 10 but then enable replication (to have at least one replica) which will bring us to the same amount of shards 20 ( 10 index + 10 replica)\r\n- What is the average size on disk for your shards with hourly indices?\r\n3.7Gb ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329117814","html_url":"https://github.com/elastic/elasticsearch/issues/26610#issuecomment-329117814","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26610","id":329117814,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTExNzgxNA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T09:51:45Z","updated_at":"2017-09-13T10:04:06Z","author_association":"CONTRIBUTOR","body":"> For now I can provide dump from the jvm with memory usage close to OOM (8G of heap used out of 12G)\r\nhttps://drive.google.com/a/travix.com/file/d/0B40JaBxFEd3ET3FMRDhmY0FzbkE/view?usp=sharing\r\n\r\nI can't access the heap dump. Can you give permission to ** redacted **?\r\n\r\nEDIT: works now, thanks","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329172776","html_url":"https://github.com/elastic/elasticsearch/issues/26610#issuecomment-329172776","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26610","id":329172776,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTE3Mjc3Ng==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T13:46:37Z","updated_at":"2017-09-13T13:46:37Z","author_association":"CONTRIBUTOR","body":"The heapdump contains mostly garbage (waiting to be collected), 400MB out of 6,7GB are actually reachable objects. Only a heapdump on OOM can explain what's going on here.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329180722","html_url":"https://github.com/elastic/elasticsearch/issues/26610#issuecomment-329180722","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26610","id":329180722,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTE4MDcyMg==","user":{"login":"lruslan","id":1275141,"node_id":"MDQ6VXNlcjEyNzUxNDE=","avatar_url":"https://avatars1.githubusercontent.com/u/1275141?v=4","gravatar_id":"","url":"https://api.github.com/users/lruslan","html_url":"https://github.com/lruslan","followers_url":"https://api.github.com/users/lruslan/followers","following_url":"https://api.github.com/users/lruslan/following{/other_user}","gists_url":"https://api.github.com/users/lruslan/gists{/gist_id}","starred_url":"https://api.github.com/users/lruslan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lruslan/subscriptions","organizations_url":"https://api.github.com/users/lruslan/orgs","repos_url":"https://api.github.com/users/lruslan/repos","events_url":"https://api.github.com/users/lruslan/events{/privacy}","received_events_url":"https://api.github.com/users/lruslan/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T14:12:50Z","updated_at":"2017-09-13T14:12:50Z","author_association":"NONE","body":"@ywelsch thanks. I've enabled HeapDumpOnOutOfMemoryError and gc logging , will let you know once I have a heapdump on OOM. \r\nI also can see heap garbage being properly collected once in a while with 10G heap limit, jvm allows heap grow up to  7G and then collection happens. It looks I rushed to post an issue which is not real problem.\r\n<img width=\"1239\" alt=\"screen shot 2017-09-13 at 15 59 39\" src=\"https://user-images.githubusercontent.com/1275141/30381744-92cd487e-989d-11e7-87fb-a70e6911dd89.png\">\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329182976","html_url":"https://github.com/elastic/elasticsearch/issues/26610#issuecomment-329182976","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26610","id":329182976,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTE4Mjk3Ng==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T14:20:14Z","updated_at":"2017-09-13T14:20:14Z","author_association":"CONTRIBUTOR","body":"ok, I'll close this issue for now. Feel free to reopen once there is new information.","performed_via_github_app":null}]