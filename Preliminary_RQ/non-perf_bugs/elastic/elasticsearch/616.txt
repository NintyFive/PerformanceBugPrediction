{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/616","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/616/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/616/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/616/events","html_url":"https://github.com/elastic/elasticsearch/issues/616","id":515336,"node_id":"MDU6SXNzdWU1MTUzMzY=","number":616,"title":"Recovery is too aggressive","user":{"login":"mrflip","id":6128,"node_id":"MDQ6VXNlcjYxMjg=","avatar_url":"https://avatars0.githubusercontent.com/u/6128?v=4","gravatar_id":"","url":"https://api.github.com/users/mrflip","html_url":"https://github.com/mrflip","followers_url":"https://api.github.com/users/mrflip/followers","following_url":"https://api.github.com/users/mrflip/following{/other_user}","gists_url":"https://api.github.com/users/mrflip/gists{/gist_id}","starred_url":"https://api.github.com/users/mrflip/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mrflip/subscriptions","organizations_url":"https://api.github.com/users/mrflip/orgs","repos_url":"https://api.github.com/users/mrflip/repos","events_url":"https://api.github.com/users/mrflip/events{/privacy}","received_events_url":"https://api.github.com/users/mrflip/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2011-01-09T05:49:45Z","updated_at":"2011-09-23T14:27:29Z","closed_at":"2011-09-23T14:27:29Z","author_association":"NONE","active_lock_reason":null,"body":"When a cluster is under heavy load, the recovery mechanism can drive it into failure.\n\nIf a node becomes so overloaded it falls off the network, the other nodes begin immediately replicating its data.  This increases _their_ load, increasing the chance some other node fails.\n\nFurthermore, when nodes wake up they make foolish choices about whether to release their shards.  We frequently see a node with say 18 shards flush most of them and start recovering a different set of shards back to itself, even though none of the shards in question have received writes. This imposes a major additional cluster load.\n\nFinally, if the master node fails the cluster seems to become confused about which shards are primary, leading to data loss.\n\nWe're using the s3 gateway but have seen similar problems when using local\n\nProposed:\n- A setting to delay recovery after failure detection. This will damp out the oscillation caused by an overloaded node falling down.\n- Improve the shard distribution algorithm so that nodes don't see as much churn when restarted\n- Don't be as aggressive about deleting data: retain shards unless disk space demands or they become out of date\n","closed_by":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"performed_via_github_app":null}