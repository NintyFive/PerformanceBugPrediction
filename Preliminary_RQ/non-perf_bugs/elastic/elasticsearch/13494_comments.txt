[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/139396908","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-139396908","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":139396908,"node_id":"MDEyOklzc3VlQ29tbWVudDEzOTM5NjkwOA==","user":{"login":"costin","id":76245,"node_id":"MDQ6VXNlcjc2MjQ1","avatar_url":"https://avatars3.githubusercontent.com/u/76245?v=4","gravatar_id":"","url":"https://api.github.com/users/costin","html_url":"https://github.com/costin","followers_url":"https://api.github.com/users/costin/followers","following_url":"https://api.github.com/users/costin/following{/other_user}","gists_url":"https://api.github.com/users/costin/gists{/gist_id}","starred_url":"https://api.github.com/users/costin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/costin/subscriptions","organizations_url":"https://api.github.com/users/costin/orgs","repos_url":"https://api.github.com/users/costin/repos","events_url":"https://api.github.com/users/costin/events{/privacy}","received_events_url":"https://api.github.com/users/costin/received_events","type":"User","site_admin":false},"created_at":"2015-09-10T22:10:17Z","updated_at":"2015-09-10T22:10:17Z","author_association":"MEMBER","body":"@boaz @imotov please add your thoughts to the issue\n@clintongormley I'm going to close #9483 in favour of this issue; hopefully it won't send the issue in the back of the queue, planning wise :)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/141124174","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-141124174","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":141124174,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MTEyNDE3NA==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2015-09-17T15:34:25Z","updated_at":"2015-09-17T15:34:25Z","author_association":"MEMBER","body":"@costin - just to clarify. I don't really see much sense in creating more slices than we have shards in the index. So, if a client asks for 10 slices and index has only 5 shards, it wouldn't make much sense to return more than 5 slices. On the other side, if the client asks for 2 slices, but we have 5 shards running on 3 different nodes, there is really no good split either. So, maybe client shouldn't really ask for the number of chunks but instead have parallelism mode: `nodes` or `shards` which will cause elasticsearch to give one scroll_id per shard or one scroll_id per node. What do you think?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/144726259","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-144726259","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":144726259,"node_id":"MDEyOklzc3VlQ29tbWVudDE0NDcyNjI1OQ==","user":{"login":"petchema","id":1211431,"node_id":"MDQ6VXNlcjEyMTE0MzE=","avatar_url":"https://avatars2.githubusercontent.com/u/1211431?v=4","gravatar_id":"","url":"https://api.github.com/users/petchema","html_url":"https://github.com/petchema","followers_url":"https://api.github.com/users/petchema/followers","following_url":"https://api.github.com/users/petchema/following{/other_user}","gists_url":"https://api.github.com/users/petchema/gists{/gist_id}","starred_url":"https://api.github.com/users/petchema/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/petchema/subscriptions","organizations_url":"https://api.github.com/users/petchema/orgs","repos_url":"https://api.github.com/users/petchema/repos","events_url":"https://api.github.com/users/petchema/events{/privacy}","received_events_url":"https://api.github.com/users/petchema/received_events","type":"User","site_admin":false},"created_at":"2015-10-01T13:29:26Z","updated_at":"2015-10-01T13:55:19Z","author_association":"NONE","body":"We had this issue with Elasticsearch-hadoop, the map/reduce client is a relatively (compared to our ES cluster) large Hadoop cluster, and since ES-hadoop can only start one mapper per shard we had to create indices with several hundred shards (more than we usually do) so it doesn't limit the mapping parallelism. That requires some planning, and also make those indices less efficient for other operations, like indexing or standard search queries.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/157084814","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-157084814","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":157084814,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NzA4NDgxNA==","user":{"login":"apatrida","id":182340,"node_id":"MDQ6VXNlcjE4MjM0MA==","avatar_url":"https://avatars3.githubusercontent.com/u/182340?v=4","gravatar_id":"","url":"https://api.github.com/users/apatrida","html_url":"https://github.com/apatrida","followers_url":"https://api.github.com/users/apatrida/followers","following_url":"https://api.github.com/users/apatrida/following{/other_user}","gists_url":"https://api.github.com/users/apatrida/gists{/gist_id}","starred_url":"https://api.github.com/users/apatrida/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apatrida/subscriptions","organizations_url":"https://api.github.com/users/apatrida/orgs","repos_url":"https://api.github.com/users/apatrida/repos","events_url":"https://api.github.com/users/apatrida/events{/privacy}","received_events_url":"https://api.github.com/users/apatrida/received_events","type":"User","site_admin":false},"created_at":"2015-11-16T16:15:55Z","updated_at":"2015-11-16T16:15:55Z","author_association":"CONTRIBUTOR","body":"@imotov it does make sense to divide a shard into slices if you can read one slice from a primary, one slice from a replica, another from yet another replica, etc.  \n\nbut definitely having a plan that allows parallel reading and dividing the load across shards, nodes, machines, the rack or racks.  And then yes, need something that is a per node ID so you can read individually.\n\nI think a lot of us have done this in the same way.  Take an alias, convert to list of indexes, look at the shard plan, solve the plan for evenly dividing work across the highest number of nodes that have data, query from each node using shard/node hints, either locally (if our extraction is on the node or in a plugin) or remotely ensure more parallelism.\n\nThis is also something that might cause the desire for I/O, CPU or memory throttling for these queries because you start to have larger batch reads at the same time you don't want to crush the cluster.  And similar circuit breakers (throttles are likely better).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/158097140","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-158097140","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":158097140,"node_id":"MDEyOklzc3VlQ29tbWVudDE1ODA5NzE0MA==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2015-11-19T15:49:26Z","updated_at":"2015-11-19T15:49:26Z","author_association":"MEMBER","body":"@apatrida what I meant is there is no good practical way to \"slice\" a primary shard and a replica shard into the same slices. You can do things like scanning all records and selecting some records based on hash of id, but it's not very practical and it's something that you can do on the client side already.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/210052797","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-210052797","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":210052797,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMDA1Mjc5Nw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-04-14T17:07:38Z","updated_at":"2016-04-14T17:08:39Z","author_association":"CONTRIBUTOR","body":"Now that search requests are parsed on the coordinating node, we can add support for breaking a single scroll request down into multiple scrolls, which can be pulled in parallel.  The way it could work is as follows:\n\nThis request starts a search against all indices starting with `my_index` and requests 10 scroll IDs in response:\n\n```\nGET my_index*/_search?scroll=1m&scroll_ids=10\n```\n\nElasticsearch finds all matching shards and determines that there are 8 shards involved, which means we need two more scroll_ids than we have shards.  This means that it needs to split two shards into two.\n\nIt sorts the shards by number of docs in each and chooses to split the two largest shards.  The field stats API returns the min/max UID which we split into two ranges, and apply those ranges as a filter on those shards, eg:\n\n```\n{ \"range: { \"_uid\": { \"gte\": \"a000000\", \"lt\": \"m000000\" }}}\n```\n\nThe response returns an array of scroll_ids with the total hits, but without returning the first tranche of hits:\n\n```\n{\n  \"scroll_ids\": [ {\n    \"scroll_id\": \"cXVlcnlUaGVuRmV0Y2g7NTs3NzpWMzZBQTJXb1JEMlFNcGZkZTFMQ3FnOzc2OlYzNkFBMldvUkQyUU1wZmRlMUxDcWc7Nzg6VjM2QUEyV29SRDJRTXBmZGUxTENxZzs3OTpWMzZBQTJXb1JEMlFNcGZkZTFMQ3FnOzgwOlYzNkFBMldvUkQyUU1wZmRlMUxDcWc7MDs=\",\n   \"hits\": {\n    \"total\": 10000,\n    \"hits\": []\n  },\n  ...\n]}\n```\n\nWe assume UIDs are well distributed but that may not be the case. There is no guarantee that each scroll ID will return a similar number of results.  \n\nNOTE: If we have more shards than we requested, we just return one scroll id per shard.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/210377762","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-210377762","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":210377762,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMDM3Nzc2Mg==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-04-15T09:19:27Z","updated_at":"2016-04-15T09:19:27Z","author_association":"MEMBER","body":"imo, and @costin can confirm, but I believe that reading shards in parallel is good enough and we don't need the complexity of sub-shard split. I think we can start with figuring that part out (and if people ask for 10 readers and we only have 8 shards, we'll give them two empty readers).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/210397010","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-210397010","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":210397010,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMDM5NzAxMA==","user":{"login":"costin","id":76245,"node_id":"MDQ6VXNlcjc2MjQ1","avatar_url":"https://avatars3.githubusercontent.com/u/76245?v=4","gravatar_id":"","url":"https://api.github.com/users/costin","html_url":"https://github.com/costin","followers_url":"https://api.github.com/users/costin/followers","following_url":"https://api.github.com/users/costin/following{/other_user}","gists_url":"https://api.github.com/users/costin/gists{/gist_id}","starred_url":"https://api.github.com/users/costin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/costin/subscriptions","organizations_url":"https://api.github.com/users/costin/orgs","repos_url":"https://api.github.com/users/costin/repos","events_url":"https://api.github.com/users/costin/events{/privacy}","received_events_url":"https://api.github.com/users/costin/received_events","type":"User","site_admin":false},"created_at":"2016-04-15T09:59:04Z","updated_at":"2016-04-15T09:59:04Z","author_association":"MEMBER","body":"The sub-shard splitting is real and already a problem. This occurs for example in Spark where the reader tries to load the data in memory but it can only access a max of 2 GB (during shuffling, related to `ByteBuffers` and `Integer.MAX_VALUE`).\nThe solution in Spark is to increase the number of workers so the memory is spread in chunks of 2GBs so many times a reader will have way more partitions than shards.\nUnless the shard is actually split, this problem still remains.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/212039222","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-212039222","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":212039222,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjAzOTIyMg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-04-19T17:50:23Z","updated_at":"2016-04-19T17:50:23Z","author_association":"CONTRIBUTOR","body":"@costin and @imotov just had a chat about this and came up with a better idea:\n\nReturn one scroll ID per shard.  Each scroll ID can be used in parallel processes, which means that requests to the same search context on each shard should be executed serially to avoid overlaps. Once no more hits are returned, scrolling is done.\n\nThis API could also return info about which node hosts each shard, which can be used by spark/hadoop to choose local workers.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/216189146","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-216189146","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":216189146,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNjE4OTE0Ng==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-05-02T10:17:51Z","updated_at":"2016-05-02T10:17:51Z","author_association":"CONTRIBUTOR","body":"> Return one scroll ID per shard. Each scroll ID can be used in parallel processes, which means that requests to the same search context on each shard should be executed serially to avoid overlaps. Once no more hits are returned, scrolling is done.\n\nI personally think a solution to this problem should have some more properties than serialization. I think we need to make sure we don't depend on the state of a shard to ensure we can recover from failures without starting all over again. For instance if one worker needs to restart we should be able to only resume that one worker not all others. I also think that all state should be on the client except of the information we are already maintaining ie. the point-in-time snapshot (index reader). For this to work, the user should be in-charge of partitioning the document space. For instance a user with N consumers must specify the partition and the number of consumers when the scroll is initialized ie. user smust provide:\n- number for workers\n- docs per batch \n\nFor every request we allow to provide the worker ID (0 - N) and we return only the slice of the data for the given worker ID. Even further we can allow to provide a document offset of some sort to resume (that would be nice). Implementation wise we can just fork a search context per worker and return all context IDs with the scroll ID once per worker. \n\nthat way we can keep everything with the same concurrency patterns and we can consume documents in parallel if the user wants to.\n\nI hope this makes sense?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/216778564","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-216778564","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":216778564,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNjc3ODU2NA==","user":{"login":"costin","id":76245,"node_id":"MDQ6VXNlcjc2MjQ1","avatar_url":"https://avatars3.githubusercontent.com/u/76245?v=4","gravatar_id":"","url":"https://api.github.com/users/costin","html_url":"https://github.com/costin","followers_url":"https://api.github.com/users/costin/followers","following_url":"https://api.github.com/users/costin/following{/other_user}","gists_url":"https://api.github.com/users/costin/gists{/gist_id}","starred_url":"https://api.github.com/users/costin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/costin/subscriptions","organizations_url":"https://api.github.com/users/costin/orgs","repos_url":"https://api.github.com/users/costin/repos","events_url":"https://api.github.com/users/costin/events{/privacy}","received_events_url":"https://api.github.com/users/costin/received_events","type":"User","site_admin":false},"created_at":"2016-05-04T08:15:30Z","updated_at":"2016-05-04T08:15:30Z","author_association":"MEMBER","body":"Danke for the in-depth reply!\n\n> we need to make sure we don't depend on the state of a shard to ensure we can recover from failures without starting all over again\n\nAgreed. This is already a problem (if a shard fails, one cannot _resume_ the scroll/read) so to not make this feature heavier, during the discussion we ignored this aspect.\nHaving this functionality built-in would be of course great since cluster changes would not break clients performing reading.\n\n>  Even further we can allow to provide a document offset of some sort to resume (that would be nice).\n\nThat would be indeed nice. There are two scenarios here and it would be great if we could support both:\n- the reading task gets restarted but without an offset (it got lost) can simply ask ES for the most recent offset in that _scroll_ and continue. This might imply that the last request might have been lost by the consuming task but that's not ES concern.\n- the reading task gets restarted and has an offset. This might point back in time to replay some data that was already sent to the client.\n\nWdyt?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/216782402","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-216782402","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":216782402,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNjc4MjQwMg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-05-04T08:25:52Z","updated_at":"2016-05-04T08:25:52Z","author_association":"CONTRIBUTOR","body":"Resuming a scroll request could only be done reliably if:\n- the index is quiescent (ie no further writes)\n- the scroll request is sorted in a deterministic order\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/216782931","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-216782931","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":216782931,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNjc4MjkzMQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-05-04T08:27:31Z","updated_at":"2016-05-04T08:27:31Z","author_association":"CONTRIBUTOR","body":"> the reading task gets restarted but without an offset (it got lost) can simply ask ES for the most recent offset in that scroll and continue. This might imply that the last request might have been lost by the consuming task but that's not ES concern.\n\nI think for simplicity we can only allow this for the `restart from where you are` which means basically just reusing the scroll ID. For this to work we have to ensure that the search context is not closed until the worker is restarted which is 5 min by default.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/216839998","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-216839998","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":216839998,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNjgzOTk5OA==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2016-05-04T12:02:31Z","updated_at":"2016-05-04T12:02:31Z","author_association":"MEMBER","body":"> For every request we allow to provide the worker ID (0 - N) and we return only the slice of the data for the given worker ID.\n\nThere are some points we need to discuss before going further on this.\nFirst of all if we allow parallel reads on the same shard we'll need to ensure that the searcher is the same among the parallel readers. For instance in the hadoop case I assume that each worker/mapper is independent,  they start a scroll in the beginning of each map and we have no guarantee that the searcher is the same but it's not a problem because each scroll is independent (they'll see different documents from different shards). With parallel readers we need to ensure that the searcher is the same, the only way to achieve this is to create the scroll context at the beginning of the job and we also need to ensure that those contexts won't go away during the timeline of the job. Assuming that the job could take hours I don't see how we could set the ttl on the context beforehand.\nNow what can we do if a replica is stopped during the job ? In the current implementation it's not a problem, we can start a new scroll on another replica. With parallel readers it's impossible, we still need to ensure that we read from the same replica/searcher which means that if a replica used in a parallel scroll is not reachable anymore the whole job should fail.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/216852866","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-216852866","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":216852866,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNjg1Mjg2Ng==","user":{"login":"costin","id":76245,"node_id":"MDQ6VXNlcjc2MjQ1","avatar_url":"https://avatars3.githubusercontent.com/u/76245?v=4","gravatar_id":"","url":"https://api.github.com/users/costin","html_url":"https://github.com/costin","followers_url":"https://api.github.com/users/costin/followers","following_url":"https://api.github.com/users/costin/following{/other_user}","gists_url":"https://api.github.com/users/costin/gists{/gist_id}","starred_url":"https://api.github.com/users/costin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/costin/subscriptions","organizations_url":"https://api.github.com/users/costin/orgs","repos_url":"https://api.github.com/users/costin/repos","events_url":"https://api.github.com/users/costin/events{/privacy}","received_events_url":"https://api.github.com/users/costin/received_events","type":"User","site_admin":false},"created_at":"2016-05-04T12:50:25Z","updated_at":"2016-05-04T12:50:25Z","author_association":"MEMBER","body":"Not sure whether it helps but in the Hadoop/Spark case the workers do not communicate with each other. At the start of the job, there is some light configuration (in ES-Hadoop that performs a `search_shards` and each task is instructed to connect against one) but that's about it. The configuration is very small and easily transferable. I mention this since `context` sounds heavyweight and not something that can be easily be serialized.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/217054402","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-217054402","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":217054402,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNzA1NDQwMg==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2016-05-05T01:25:34Z","updated_at":"2016-05-05T01:26:20Z","author_association":"MEMBER","body":"#### New proposal based on Simon’s idea:\n\nTo slice the data we rely on _id or on a field provided by the user \nAPI wise it’s really simple and concise: \n\n```\nGET _search?scroll=1m\n{\n    \"slice\": {\n        \"field\": \"_id\",\n        \"id\": 0,\n        \"max\": 2\n    },\n    \"query\": {\n        \"match_all\": {}\n    }\n}\n```\n\nIf the field is a string we use the hash of the string to perform the slicing, if it’s a number we use the value. OTB _uid is used from the fielddata. It’s safe we are sure that the content of the field is constant over time which is the guarantee to make the slicing consistent and it’s unique per index so each slice should have the same number of documents even if the number of slices is big. Though _id is not the most efficient way to do it especially when the number of slices is small.  To address that we can simply advise in the documentation to use an auto-generated field populated with the hash of the id and docvalues enabled. It can be even more efficient to populate a field with a random number generated when the document is created (and that is never updated). This random number should be in the range [0, max_splits_per_query] to maximize the compression in the docvalues.  \n\nThe main benefit I see with this approach is that each slice can be independent, they don’t need to use the same searcher/replica. \n\nIMO in the Hadoop/Yarn/Spark case we should slice the data based on the desired number of documents per task/map and default to something like 10,000. With Spark and even with Hadoop it’s difficult to handle shards with millions of documents that produces long running tasks. So instead of having one task per shard we could have one task per slice_id and each slice_id would hit every shard but get only the slice they are responsible for. So the only states that a split has are the split_id, the number of splits and the query. This approach uses the entire cluster in a nice and fair way as, each slice_id can theoretically use a different replica of the same shard. The drawback is that we’ll need to handle jobs with thousands of map/task but I don’t think it’s a problem because most of the map/reduce systems allow to cap the number of maps/tasks that are allowed to run concurrently. In fact this number should be derived from the number of nodes available in the cluster.\n\nAs an high level API it can be used for parallel scrolls of the same query (parallel readers) but also as a way to resume a long running task. For instance the reindex could divide the input data in multiple slices and then perform each slice sequentially. Then to resume a reindex task we would just need to restart after the last successfully processed slice.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/217107545","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-217107545","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":217107545,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNzEwNzU0NQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-05-05T09:06:08Z","updated_at":"2016-05-05T09:06:08Z","author_association":"CONTRIBUTOR","body":"I love the proposal.  Wondering if we should add support for `from` to sorted scroll requests?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/217368934","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-217368934","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":217368934,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNzM2ODkzNA==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2016-05-06T07:20:22Z","updated_at":"2016-05-06T07:20:22Z","author_association":"MEMBER","body":"Thanks @clintongormley. I think it could be dangerous to use `from`in a sorted scroll request. The sort might be consistent but if the sort field is updatable we might miss documents ? Moreover I think that replaying the whole scroll request with the slice id is enough. For the Hadoop case having the slice feature would mean that we can create splits based on the desired number of documents for each. This number should be set from the time it takes to process all the documents. Tasks that last 30s to 1 minute should be the goal and in that case replaying the failing task should not be a problem (vs replaying from where we fail in the task). What do you think ?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/217388721","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-217388721","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":217388721,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNzM4ODcyMQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-05-06T08:54:26Z","updated_at":"2016-05-06T08:54:26Z","author_association":"CONTRIBUTOR","body":"Sounds good to me\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/217428806","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-217428806","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":217428806,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNzQyODgwNg==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-05-06T12:40:57Z","updated_at":"2016-05-06T12:40:57Z","author_association":"CONTRIBUTOR","body":"I like it @jimferenczi lets start with this and get it going!!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/220443666","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-220443666","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":220443666,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMDQ0MzY2Ng==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2016-05-19T20:32:36Z","updated_at":"2016-05-19T20:32:36Z","author_association":"MEMBER","body":"I did some testing of the `slice` feature. The following table shows the results for a full scroll of an index of 1M random documents on 1 shard (no replica) with different scroll configurations. The configuration contains the number of slices in the scroll, the number of documents per slice, the size of each request within the slice and the number of threads. The \"elapsed time\" is the total time for the scroll to complete in seconds. For instance the first line is for a scroll with 1 slice (current behavior), 1M documents in that slice, 10,000 documents per scroll request and 1 thread. The elapsed time is 23s. \nThe slicing is done on the _uid (only when the number of slices is greater than 1) and the query cache is cleared after each run. I've kept the worst run out of 10 for each configuration:\n\n| Number of slice | Number of documents per slice | Fetch size | Number of threads | Elapsed time |\n| :-: | :-: | :-: | :-: | :-: |\n| 1 | 1000000 | 10000 | 1 | **23s** |\n| 1 | 1000000 | 1000 | 1 | 25s |\n| 1 | 1000000 | 100 | 1 | 40s |\n| 10 | 100000 | 10000 | 4 | **8s** |\n| 10 | 100000 | 10000 | 3 | 10s |\n| 10 | 100000 | 10000 | 2 | 12s |\n| 10 | 100000 | 10000 | 1 | 25s |\n| 10 | 100000 | 1000 | 4 | **8s** |\n| 10 | 100000 | 1000 | 3 | 10s |\n| 10 | 100000 | 1000 | 2 | 13s |\n| 10 | 100000 | 1000 | 1 | 27s |\n| 10 | 100000 | 100 | 4 | 11s |\n| 10 | 100000 | 100 | 3 | 15s |\n| 10 | 100000 | 100 | 2 | 19s |\n| 10 | 100000 | 100 | 1 | 38s |\n| 100 | 10000 | 10000 | 4 | 11s |\n| 100 | 10000 | 10000 | 3 | 14s |\n| 100 | 10000 | 10000 | 2 | 21s |\n| 100 | 10000 | 10000 | 1 | 43s |\n| 100 | 10000 | 1000 | 4 | 12s |\n| 100 | 10000 | 1000 | 3 | 15s |\n| 100 | 10000 | 1000 | 2 | 23s |\n| 100 | 10000 | 1000 | 1 | 46s |\n| 100 | 10000 | 100 | 4 | 13s |\n| 100 | 10000 | 100 | 3 | 17s |\n| 100 | 10000 | 100 | 2 | 26s |\n| 100 | 10000 | 100 | 1 | 54s |\n| 1000 | 1000 | 1000 | 4 | 44s |\n| 1000 | 1000 | 1000 | 3 | 57s |\n| 1000 | 1000 | 1000 | 2 | 85s |\n| 1000 | 1000 | 1000 | 1 | 175s |\n| 1000 | 1000 | 100 | 4 | 54s |\n| 1000 | 1000 | 100 | 3 | 71s |\n| 1000 | 1000 | 100 | 2 | 106s |\n| 1000 | 1000 | 100 | 1 | 212s |\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/230432410","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-230432410","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":230432410,"node_id":"MDEyOklzc3VlQ29tbWVudDIzMDQzMjQxMA==","user":{"login":"garyelephant","id":2291859,"node_id":"MDQ6VXNlcjIyOTE4NTk=","avatar_url":"https://avatars3.githubusercontent.com/u/2291859?v=4","gravatar_id":"","url":"https://api.github.com/users/garyelephant","html_url":"https://github.com/garyelephant","followers_url":"https://api.github.com/users/garyelephant/followers","following_url":"https://api.github.com/users/garyelephant/following{/other_user}","gists_url":"https://api.github.com/users/garyelephant/gists{/gist_id}","starred_url":"https://api.github.com/users/garyelephant/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/garyelephant/subscriptions","organizations_url":"https://api.github.com/users/garyelephant/orgs","repos_url":"https://api.github.com/users/garyelephant/repos","events_url":"https://api.github.com/users/garyelephant/events{/privacy}","received_events_url":"https://api.github.com/users/garyelephant/received_events","type":"User","site_admin":false},"created_at":"2016-07-05T09:37:20Z","updated_at":"2016-07-05T09:37:20Z","author_association":"NONE","body":"I'd love this feature for Spark SQL on Elasticsearch case.For now, the bottleneck I see in my environment is that scroll is too slow with one shard per task.This has serious concurrent problem. I'm looking forward for it.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/354398499","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-354398499","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":354398499,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NDM5ODQ5OQ==","user":{"login":"karansinghkjs346","id":4413645,"node_id":"MDQ6VXNlcjQ0MTM2NDU=","avatar_url":"https://avatars3.githubusercontent.com/u/4413645?v=4","gravatar_id":"","url":"https://api.github.com/users/karansinghkjs346","html_url":"https://github.com/karansinghkjs346","followers_url":"https://api.github.com/users/karansinghkjs346/followers","following_url":"https://api.github.com/users/karansinghkjs346/following{/other_user}","gists_url":"https://api.github.com/users/karansinghkjs346/gists{/gist_id}","starred_url":"https://api.github.com/users/karansinghkjs346/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/karansinghkjs346/subscriptions","organizations_url":"https://api.github.com/users/karansinghkjs346/orgs","repos_url":"https://api.github.com/users/karansinghkjs346/repos","events_url":"https://api.github.com/users/karansinghkjs346/events{/privacy}","received_events_url":"https://api.github.com/users/karansinghkjs346/received_events","type":"User","site_admin":false},"created_at":"2017-12-29T05:04:28Z","updated_at":"2017-12-29T05:04:28Z","author_association":"NONE","body":"@costin - \r\nI am using Spark Streaming Job which  is dumping data into ES , When a parallel batch job is running and reading data from ES then Streaming job is getting into queue , Why ES is not able to handle the load  ?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/354412148","html_url":"https://github.com/elastic/elasticsearch/issues/13494#issuecomment-354412148","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13494","id":354412148,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NDQxMjE0OA==","user":{"login":"costin","id":76245,"node_id":"MDQ6VXNlcjc2MjQ1","avatar_url":"https://avatars3.githubusercontent.com/u/76245?v=4","gravatar_id":"","url":"https://api.github.com/users/costin","html_url":"https://github.com/costin","followers_url":"https://api.github.com/users/costin/followers","following_url":"https://api.github.com/users/costin/following{/other_user}","gists_url":"https://api.github.com/users/costin/gists{/gist_id}","starred_url":"https://api.github.com/users/costin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/costin/subscriptions","organizations_url":"https://api.github.com/users/costin/orgs","repos_url":"https://api.github.com/users/costin/repos","events_url":"https://api.github.com/users/costin/events{/privacy}","received_events_url":"https://api.github.com/users/costin/received_events","type":"User","site_admin":false},"created_at":"2017-12-29T07:57:55Z","updated_at":"2017-12-29T07:57:55Z","author_association":"MEMBER","body":"@karansinghkjs346\r\nFor questions, please use the dedicated [forum](https://discuss.elastic.co/c/elasticsearch-and-hadoop) not the issue tracker.\r\n\r\nThanks,","performed_via_github_app":null}]