{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/53736","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/53736/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/53736/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/53736/events","html_url":"https://github.com/elastic/elasticsearch/issues/53736","id":583799623,"node_id":"MDU6SXNzdWU1ODM3OTk2MjM=","number":53736,"title":"Single unbounded date_range document triggers circuit breaker","user":{"login":"spinscale","id":667544,"node_id":"MDQ6VXNlcjY2NzU0NA==","avatar_url":"https://avatars2.githubusercontent.com/u/667544?v=4","gravatar_id":"","url":"https://api.github.com/users/spinscale","html_url":"https://github.com/spinscale","followers_url":"https://api.github.com/users/spinscale/followers","following_url":"https://api.github.com/users/spinscale/following{/other_user}","gists_url":"https://api.github.com/users/spinscale/gists{/gist_id}","starred_url":"https://api.github.com/users/spinscale/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spinscale/subscriptions","organizations_url":"https://api.github.com/users/spinscale/orgs","repos_url":"https://api.github.com/users/spinscale/repos","events_url":"https://api.github.com/users/spinscale/events{/privacy}","received_events_url":"https://api.github.com/users/spinscale/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2020-03-18T15:12:06Z","updated_at":"2020-03-18T16:22:10Z","closed_at":"2020-03-18T16:22:10Z","author_association":"MEMBER","active_lock_reason":null,"body":"**Elasticsearch version** (`bin/elasticsearch --version`): 7.6.1\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\n**Steps to reproduce**:\r\n\r\nPlease include a *minimal* but *complete* recreation of the problem, including\r\n(e.g.) index creation, mappings, settings, query etc.  The easier you make for\r\nus to reproduce it, the more likely that somebody will take the time to look at it.\r\n\r\n```\r\nDELETE test\r\n\r\nPUT test \r\n{\r\n\t\"mappings\": {\r\n\t\t\"properties\": {\r\n\t\t\t\"dateRange\": { \"type\": \"date_range\" }\r\n\t\t}\r\n\t}\r\n}\r\n\r\nPUT test/_doc/1\r\n{\r\n\t\"dateRange\": {\r\n\t\t\"gte\": \"2020-03-01\"\r\n\t}\r\n}\r\n\r\nGET test/_search\r\n{\r\n\t\"aggs\": {\r\n\t\t\"test\": {\r\n\t\t\t\"date_histogram\": {\r\n\t\t\t\t\"field\": \"dateRange\",\r\n\t\t\t\t\"interval\" : \"day\"\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\ntriggers the circuitbreaker like this\r\n\r\n```\r\n{\r\n  \"error\" : {\r\n    \"root_cause\" : [\r\n      {\r\n        \"type\" : \"circuit_breaking_exception\",\r\n        \"reason\" : \"[request] Data too large, data for [<reused_arrays>] would be [805344256/768mb], which is larger than the limit of [622775500/593.9mb]\",\r\n        \"bytes_wanted\" : 805344256,\r\n        \"bytes_limit\" : 622775500,\r\n        \"durability\" : \"TRANSIENT\"\r\n      }\r\n    ],\r\n    \"type\" : \"search_phase_execution_exception\",\r\n    \"reason\" : \"all shards failed\",\r\n    \"phase\" : \"query\",\r\n    \"grouped\" : true,\r\n    \"failed_shards\" : [\r\n      {\r\n        \"shard\" : 0,\r\n        \"index\" : \"test\",\r\n        \"node\" : \"I8l1uS_GSKO8LfCDlV69OQ\",\r\n        \"reason\" : {\r\n          \"type\" : \"circuit_breaking_exception\",\r\n          \"reason\" : \"[request] Data too large, data for [<reused_arrays>] would be [805344256/768mb], which is larger than the limit of [622775500/593.9mb]\",\r\n          \"bytes_wanted\" : 805344256,\r\n          \"bytes_limit\" : 622775500,\r\n          \"durability\" : \"TRANSIENT\"\r\n        }\r\n      }\r\n    ]\r\n  },\r\n  \"status\" : 429\r\n}\r\n\r\n```\r\n\r\nThe above snippet triggers the circuitbreaker after a few seconds (which is good!). But just having a single document with an unbounded upper range in your dataset will make any aggregations on date range fields impossible and slow down your system. Maybe we want to exit earlier in that case?\r\n","closed_by":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"performed_via_github_app":null}