[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/243093912","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-243093912","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":243093912,"node_id":"MDEyOklzc3VlQ29tbWVudDI0MzA5MzkxMg==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-08-29T10:57:27Z","updated_at":"2016-08-29T10:57:27Z","author_association":"CONTRIBUTOR","body":"> Current implementation can guarantee - correct me if Im wrong - a zero percentage of errors due the implementation.\n\nCorrect. For the linear counting part (ie. when before reaching the `precision_threshold`), which seems to be what you are interested in, see section 3 of http://dblab.kaist.ac.kr/Prof/pdf/Whang1990%28linear%29.pdf. It cannot guarantee a maximum error but it can give a standard error. You can use 2^25 for `m` in the formulas since the linear counting phase uses 25 bits of each hash value.\n\n>  The idea here is give to the user, up his responsibility and his own risk, a new implementation based on a determinist algorithm - such as a hash implementation with collision support or a bitmap with the ordinals of this field- to get always a precision number of the cardinality. Regarding low-cardinality fields with a none high impact in memory.\n\nIt feels like something that can already be done today by using a `terms` aggregation. Say that you think the field has 50 unique elements or less, you could run a `terms` aggregation with both `size` and `shard_size` set to 51. Then on the client side you would count the number of returned buckets. If less than or equal to 50, this gives you the exact cardinality of the field. If 51, then you know the field has 51 unique values or more.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/243097258","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-243097258","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":243097258,"node_id":"MDEyOklzc3VlQ29tbWVudDI0MzA5NzI1OA==","user":{"login":"pfreixes","id":1226121,"node_id":"MDQ6VXNlcjEyMjYxMjE=","avatar_url":"https://avatars0.githubusercontent.com/u/1226121?v=4","gravatar_id":"","url":"https://api.github.com/users/pfreixes","html_url":"https://github.com/pfreixes","followers_url":"https://api.github.com/users/pfreixes/followers","following_url":"https://api.github.com/users/pfreixes/following{/other_user}","gists_url":"https://api.github.com/users/pfreixes/gists{/gist_id}","starred_url":"https://api.github.com/users/pfreixes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pfreixes/subscriptions","organizations_url":"https://api.github.com/users/pfreixes/orgs","repos_url":"https://api.github.com/users/pfreixes/repos","events_url":"https://api.github.com/users/pfreixes/events{/privacy}","received_events_url":"https://api.github.com/users/pfreixes/received_events","type":"User","site_admin":false},"created_at":"2016-08-29T11:17:51Z","updated_at":"2016-08-29T11:17:51Z","author_association":"NONE","body":"@jpountz as a workaround it can work for really low-cardinality sets and where it does not rise network issues, even though I was thinking more in low-cardinality sets with 10K or even bigger. \n\nThe contras of the resources consumed for this sort of size vs the pros of get an exact accuracy value are IMHO out of discussion.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/243104533","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-243104533","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":243104533,"node_id":"MDEyOklzc3VlQ29tbWVudDI0MzEwNDUzMw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-08-29T12:00:20Z","updated_at":"2016-08-29T12:00:20Z","author_association":"CONTRIBUTOR","body":"> The contras of the resources consumed for this sort of size vs the pros of get an exact accuracy value are IMHO out of discussion.\n\nI disagree, to me it is exactly what this is about. I don't want to expose features that do not scale.\n\nI only see two ways to get accurate cardinalities, either in an offline way where values are streamed to a different node based on a hash of the value, and then sorted and counted before summing up the counts of all involved nodes (very high hanging fruit) or by optimizing the case that the field that the aggregation is computed on was used at index time for routing as suggested in #16433.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/243131994","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-243131994","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":243131994,"node_id":"MDEyOklzc3VlQ29tbWVudDI0MzEzMTk5NA==","user":{"login":"pfreixes","id":1226121,"node_id":"MDQ6VXNlcjEyMjYxMjE=","avatar_url":"https://avatars0.githubusercontent.com/u/1226121?v=4","gravatar_id":"","url":"https://api.github.com/users/pfreixes","html_url":"https://github.com/pfreixes","followers_url":"https://api.github.com/users/pfreixes/followers","following_url":"https://api.github.com/users/pfreixes/following{/other_user}","gists_url":"https://api.github.com/users/pfreixes/gists{/gist_id}","starred_url":"https://api.github.com/users/pfreixes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pfreixes/subscriptions","organizations_url":"https://api.github.com/users/pfreixes/orgs","repos_url":"https://api.github.com/users/pfreixes/repos","events_url":"https://api.github.com/users/pfreixes/events{/privacy}","received_events_url":"https://api.github.com/users/pfreixes/received_events","type":"User","site_admin":false},"created_at":"2016-08-29T13:59:58Z","updated_at":"2016-08-29T13:59:58Z","author_association":"NONE","body":"Correct me if Im wrong, just Im landing with the ElasticSearch internals. Your concerns are about introduce features that wont scale horizontally, something that I've already seen in other POSTs. I agree, and its a a sort of rule that it wouldn't be broken.\n\nThe global ordinals are a shared structure across the cluster with information about the fields values, as a first impressions looks like it might be used to implement that feature. The results got at each shard should be compatible, and an \"easy\" intersection would be enough to get the final cardinality.\n\nIf this way is reasonable, the footprint in terms of memory used by a bitmap, having it the occurrences of the ordinals in a specific query/aggregation for low-cardinality fields should be handleable.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/244404185","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-244404185","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":244404185,"node_id":"MDEyOklzc3VlQ29tbWVudDI0NDQwNDE4NQ==","user":{"login":"pfreixes","id":1226121,"node_id":"MDQ6VXNlcjEyMjYxMjE=","avatar_url":"https://avatars0.githubusercontent.com/u/1226121?v=4","gravatar_id":"","url":"https://api.github.com/users/pfreixes","html_url":"https://github.com/pfreixes","followers_url":"https://api.github.com/users/pfreixes/followers","following_url":"https://api.github.com/users/pfreixes/following{/other_user}","gists_url":"https://api.github.com/users/pfreixes/gists{/gist_id}","starred_url":"https://api.github.com/users/pfreixes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pfreixes/subscriptions","organizations_url":"https://api.github.com/users/pfreixes/orgs","repos_url":"https://api.github.com/users/pfreixes/repos","events_url":"https://api.github.com/users/pfreixes/events{/privacy}","received_events_url":"https://api.github.com/users/pfreixes/received_events","type":"User","site_admin":false},"created_at":"2016-09-02T15:16:10Z","updated_at":"2016-09-02T15:16:10Z","author_association":"NONE","body":"I have just realized that the page about cardinality has a graphic with the relative error [1], I guess using a specific random corpus, where you can do the maths. In my environment a 2K field cardinality with a 10K threshold wouldn't get issues at all, only in exceptional cases.\n\nEven though, IMHO There is room to implement a deterministic algorithm for low-cardinality fields instead of a discrete algorithm based on probabilistic and with a logarithmic behavior that penalizes low-cardinality sets.\n\nIf it was related with the global ordinals, and due to the needless use of hashing functions the performance should be better than the one used by HyperLogLog++. In terms of size and taking into account the scope of this feature, low-cardinality fields, it might acceptable. Even though hardcode limits could be implemented, having a limit such as the HyperLogLog++ it must afford field cardinalities till 300K different values. \n\n[1] https://www.elastic.co/guide/en/elasticsearch/reference/current/images/cardinality_error.png\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/248366869","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-248366869","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":248366869,"node_id":"MDEyOklzc3VlQ29tbWVudDI0ODM2Njg2OQ==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2016-09-20T17:09:32Z","updated_at":"2016-09-20T17:09:32Z","author_association":"MEMBER","body":"> The global ordinals are a shared structure across the cluster with information about the fields values, as a first impressions looks like it might be used to implement that feature. The results got at each shard should be compatible, and an \"easy\" intersection would be enough to get the final cardinality.\n\nUnfortunately, this isn't true.  Each shard constructs it's own set of global ordinals, so \"foo\" may be mapped to ID#1 on one shard, but ID#100 on another shard.  No easy fix here I'm afraid :(\n\nAlso, intersecting global ordinals would only work in the simple case where there isn't any filtering.  Once you start filtering, you can no longer run simple intersections, as multi-valued fields may share terms.  \n\nE.g. if you filter out documents having `title: foo`, you won't know from just the ordinals that a document may have: `{ \"title\": [\"foo\", \"bar\"] }`.  In this situation, you'd mistakenly include \"bar\" as a distinct value when in reality it was filtered out.  The only way to find this is to run the actual filtering and collect the remaining documents into a Set (or sketch like HLL)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/248395862","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-248395862","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":248395862,"node_id":"MDEyOklzc3VlQ29tbWVudDI0ODM5NTg2Mg==","user":{"login":"pfreixes","id":1226121,"node_id":"MDQ6VXNlcjEyMjYxMjE=","avatar_url":"https://avatars0.githubusercontent.com/u/1226121?v=4","gravatar_id":"","url":"https://api.github.com/users/pfreixes","html_url":"https://github.com/pfreixes","followers_url":"https://api.github.com/users/pfreixes/followers","following_url":"https://api.github.com/users/pfreixes/following{/other_user}","gists_url":"https://api.github.com/users/pfreixes/gists{/gist_id}","starred_url":"https://api.github.com/users/pfreixes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pfreixes/subscriptions","organizations_url":"https://api.github.com/users/pfreixes/orgs","repos_url":"https://api.github.com/users/pfreixes/repos","events_url":"https://api.github.com/users/pfreixes/events{/privacy}","received_events_url":"https://api.github.com/users/pfreixes/received_events","type":"User","site_admin":false},"created_at":"2016-09-20T18:48:13Z","updated_at":"2016-09-20T18:48:13Z","author_association":"NONE","body":"Let's for the moment leave aside the big issue with the global ordinals as structure only placed per shard.\n\nAbout the example that you exposed, I thought that the cardinality of a field is the number of unique values of a set of documents filtered or not. If the document `{ \"title\": [\"foo\", \"bar\"] }` becomes part of the set of the documents that will be used as a source for the cardinality aggregation once we run it the `title` value will be used as a source for the algorithm. In that case, the implementation is not important. Im missing something ?\n\nIn another level but related to that, there is the multi-value field. Which will be the inputs for HLL u others implementations?  In an ideal world will be two inputs `foo` and `bar` that will compute two different values, in that case the implementation is not important. Im missing something ?\n\nGetting back to the ordinals, let's reduce the range of places where it can be solved to get a deterministic. In my case, the field type is an integer and the domain range is `[0, 100000]`. Map it as a bitmap without optimization will get rough 125K. Therefore an ad-hoc implementation for integer fields is still plausible IMHO. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/248564128","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-248564128","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":248564128,"node_id":"MDEyOklzc3VlQ29tbWVudDI0ODU2NDEyOA==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2016-09-21T09:50:47Z","updated_at":"2016-09-21T09:50:47Z","author_association":"MEMBER","body":"> About the example that you exposed, I thought that the cardinality of a field is the number of unique values of a set of documents filtered or not.\n\nThat is indeed the definition for the cardinality of the entire field, but you can use the cardinality agg in more scenarios:\n\nGiven these two documents:\n\n``` json\nPUT /test/test/1\n{\n  \"tags\": [\"foo\"],\n  \"status\" : \"red\"\n}\n\nPUT /test/test/2\n{\n  \"tags\": [\"bar\"],\n  \"status\" : \"green\"\n}\n```\n\n``` json\nGET /test/test/_search\n{\n  \"query\": {\n    \"match\": {\n      \"tags\": \"foo\"\n    }\n  },\n  \"aggs\": {\n    \"cardinality\": {\n      \"field\": \"status\"\n    }\n  }\n}\n```\n\nThat aggregation only runs on documents who have the tag `\"foo\"`, and finds the cardinality of the `status` field.  In this case, the cardinality of `status` is 1 (\"red\"), while the overall, non-filtered cardinality of `status` is 2 (\"green\", \"red\").\n\nThat's just a simple example, but applies to many scenarios.  The cardinality agg is often used as a leaf calculation for many bucketing aggs: histograms, date histo, terms, etc.  Those all require filtering/partitioning in a way that the cardinality inside the bucket is different from the overall field's cardinality.\n\n> Which will be the inputs for HLL u others implementations? \n\nThe current implementation offers both values to the HLL sketch (e.g. both are hashed and sketched) as they are independent, distinct values.\n\n> In my case, the field type is an integer and the domain range is [0, 100000]. Map it as a bitmap without optimization will get rough 125K.\n\nThe problem is that the feature needs to be general-case, and not everyone can guarantee small bounds like your data.  What would happen if someone used this proposed data structure\\* on data which had a much greater range?  We don't know cardinality until the agg is run (otherwise we'd just return it), and so there is no way to guarantee the bounds ahead of time.  So the only option is to throw some kind of exception mid-aggregation, or allow the node to OOM.  Neither are good solutions\n\n*_assuming the technical bits could be ironed out, which I don't think are possible with ordinals for the previously mentioned reasons_\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/248615097","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-248615097","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":248615097,"node_id":"MDEyOklzc3VlQ29tbWVudDI0ODYxNTA5Nw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-09-21T13:39:23Z","updated_at":"2016-09-21T13:39:23Z","author_association":"CONTRIBUTOR","body":"> The problem is that the feature needs to be general-case, and not everyone can guarantee small bounds like your data. \n\nOr the user puts the cardinality agg under a high cardinality terms agg\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/248815951","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-248815951","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":248815951,"node_id":"MDEyOklzc3VlQ29tbWVudDI0ODgxNTk1MQ==","user":{"login":"pfreixes","id":1226121,"node_id":"MDQ6VXNlcjEyMjYxMjE=","avatar_url":"https://avatars0.githubusercontent.com/u/1226121?v=4","gravatar_id":"","url":"https://api.github.com/users/pfreixes","html_url":"https://github.com/pfreixes","followers_url":"https://api.github.com/users/pfreixes/followers","following_url":"https://api.github.com/users/pfreixes/following{/other_user}","gists_url":"https://api.github.com/users/pfreixes/gists{/gist_id}","starred_url":"https://api.github.com/users/pfreixes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pfreixes/subscriptions","organizations_url":"https://api.github.com/users/pfreixes/orgs","repos_url":"https://api.github.com/users/pfreixes/repos","events_url":"https://api.github.com/users/pfreixes/events{/privacy}","received_events_url":"https://api.github.com/users/pfreixes/received_events","type":"User","site_admin":false},"created_at":"2016-09-22T05:27:38Z","updated_at":"2016-09-22T05:27:38Z","author_association":"NONE","body":"@polyfractal there is no misunderstanding with your explanation, maybe I've explained wrong. The aggregation as I tried to say is performed against the number of the documents belonging to the set documents gathered in an aggregation level. With all of the filters applied until there. Placed as a leaf usually. The implementation of the cardinality algorithm is not important.\n\nGetting back to the ad-hoc case. You are right. The only way to deal with that will be moving from the bitmap to the HLL transparently for the user. Let's guess that it is still an option.\n\nA new option for the `cardinality` agg might be aggregated, something like `try_bitmap`, that will be `False` per default. If the user sets up this field as `True`,  ES will apply the bitmap strategy together with the HLL. The size limit of the bitmap might be hard coded to the same limit that the HLL has - `40k * 8` ?. If some value overflows the bitmap only the HLL will be used. For those queries that are composed by N results from the N nodes, if one node does not send the bitmap the merge will use only the HLL result. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/248856872","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-248856872","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":248856872,"node_id":"MDEyOklzc3VlQ29tbWVudDI0ODg1Njg3Mg==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2016-09-22T09:46:44Z","updated_at":"2016-09-22T09:46:44Z","author_association":"MEMBER","body":"What you're describing is basically how the cardinality agg already works for fields that have ordinals:  https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregator.java#L96-L101\n\nE.g. If a field has ordinals, and the size of those ordinals won't be drastically larger than the HLL sketch, collect the ordinals directly.  Otherwise, use the HLL sketch (which will first use the linear-counting mode before switching to the approximate, sketching mode).\n\nNote: Ordinals only exist for string fields, so this whole conversation is moot for numerics. :)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/249146478","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-249146478","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":249146478,"node_id":"MDEyOklzc3VlQ29tbWVudDI0OTE0NjQ3OA==","user":{"login":"pfreixes","id":1226121,"node_id":"MDQ6VXNlcjEyMjYxMjE=","avatar_url":"https://avatars0.githubusercontent.com/u/1226121?v=4","gravatar_id":"","url":"https://api.github.com/users/pfreixes","html_url":"https://github.com/pfreixes","followers_url":"https://api.github.com/users/pfreixes/followers","following_url":"https://api.github.com/users/pfreixes/following{/other_user}","gists_url":"https://api.github.com/users/pfreixes/gists{/gist_id}","starred_url":"https://api.github.com/users/pfreixes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pfreixes/subscriptions","organizations_url":"https://api.github.com/users/pfreixes/orgs","repos_url":"https://api.github.com/users/pfreixes/repos","events_url":"https://api.github.com/users/pfreixes/events{/privacy}","received_events_url":"https://api.github.com/users/pfreixes/received_events","type":"User","site_admin":false},"created_at":"2016-09-23T09:33:10Z","updated_at":"2016-09-23T09:33:10Z","author_association":"NONE","body":"@polyfractal  Amazing I haven't seen it, does it mean that If I would convert the integer field as a string and configuring it as an ordinal ... will I have a deterministic behaviour ? Which should be the maximum  cardinality of the ordinals to assert the previous sentence ?\n\nAs I can see the `precission_threshold` is still related to, then the value of the maximum cardinality relates also to the `precission_threshold`. Which cardinality can I expect as maximum with a 40k `precision_threshold`? \n\nThe use of many shards and the merge of the different results gathered from them is an issue ?  If not, in our case that we are routing all data, that will be used for a specific query, to the same shard will help us ?  \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/288103376","html_url":"https://github.com/elastic/elasticsearch/issues/20178#issuecomment-288103376","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20178","id":288103376,"node_id":"MDEyOklzc3VlQ29tbWVudDI4ODEwMzM3Ng==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2017-03-21T14:52:23Z","updated_at":"2017-03-21T14:52:23Z","author_association":"MEMBER","body":"Sorry for delay, this fell off my radar.  First, disclaimer that we don't recommend basing data-design decisions off internal implementation details, as those are always subject to change.\r\n\r\nUsing a string vs number won't change the behavior of approximation.  The difference between the OrdinalCollector and the DirectCollector is just when values are hashed and offered to the HLL sketch; OrdinalCollector caches the ordinals during collection and offers them to HLL at the end, while Direct offers them during collection.\r\n\r\nI'm unsure why this distinction is made, but it's likely for performance and reducing temporary memory overhead.  But they both end up in the HLL structure.\r\n\r\nI'm going to defer the rest of the discussion to this forum post (https://discuss.elastic.co/t/default-precision-of-cardinality-aggregation/51665/8) which seems to have picked up steam again and covers much of the same material.","performed_via_github_app":null}]