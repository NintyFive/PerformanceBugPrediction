{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/8805","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8805/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8805/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8805/events","html_url":"https://github.com/elastic/elasticsearch/issues/8805","id":51212688,"node_id":"MDU6SXNzdWU1MTIxMjY4OA==","number":8805,"title":"After upgrade from 1.0.* to 1.4.1, checksum check fails after restart causing cluster to go red after yellow state","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":15,"created_at":"2014-12-07T09:29:25Z","updated_at":"2015-11-21T22:11:50Z","closed_at":"2015-11-21T22:11:50Z","author_association":"NONE","active_lock_reason":null,"body":"We did upgrade our cluster from 1.0.\\* to 1.4.1.\n\nAfter the upgrade, we indeed had 3 shards correctly identified as broken (checksum check failed), which we fixed. (we fixed the index and saw that it had errors). Before we had to restart, the cluster state was nearly completely green.\n\nThen we had to restart our cluster again:\n- The cluster state turned yellow (all primaries allocated)\n- Then it turned red again, caused by the allocation of the non primary shards for some indexes.\n- The checksum check failed on about 1/50th of our shards which we indexed data to:\n  We ran checkindex on the shards on disk and they were not corrupt. An hardware error is also very unlikely since these servers only have 2-3 shards on them, so there must have been many hardware errors which is unlikely.\n- We deleted the checksum and the marker file and ES loaded the shards again automatically.\n\nCould it be related to the merging of old and new segments? (we didn't observer this on shards where we didn't index to)? At the moment we delete the checksum and the marker file? What should we do?\n\nMaster log:\n[2014-12-07 00:03:59,192][WARN ][cluster.action.shard     ] [master] [index1][6] received shard failed for [index1][6], node[QPUH7WcyT3SSBuYjCvKHaQ], [P], s[STARTED], indexUUID [vxjN24PlRROou8Y-W6ObPw], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index1][6] Failed to transfer [86] files with total size of [80.3gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=xzqmes actual=a2zr8o resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@40ea4a1e)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#11]{New I/O worker #28}}\n[2014-12-07 00:03:59,265][WARN ][cluster.action.shard     ] [master] [index2][8] received shard failed for [index2][8], node[n-kMHaf-QH2LTjncGjmkLw], [P], s[STARTED], indexUUID [VU0RN4QtRo2Ciae8b6oT7w], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index2][8] Failed to transfer [110] files with total size of [82.1gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=wtmawb actual=85psa3 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@1a87889d)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#3]{New I/O worker #20}}\n[2014-12-07 00:03:59,660][WARN ][cluster.action.shard     ] [master] [index3][7] received shard failed for [index3][7], node[pHKQxOBYTuqReDWDStP6JQ], [P], s[STARTED], indexUUID [v_5dSwWwQQ-ylb000A9s5Q], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index3][7] Failed to transfer [113] files with total size of [82.7gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=1ut1u4d actual=dsmbzp resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@2e3275f)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#10]{New I/O worker #27}}\n[2014-12-07 00:03:59,822][WARN ][cluster.action.shard     ] [master] [index4][7] received shard failed for [index4][7], node[Petlv8BJTXeAldR66ar_RQ], [P], s[STARTED], indexUUID [EVdW2JJLSwmhCQcQ9zWiuQ], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index4][7] Failed to transfer [133] files with total size of [81.8gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=er2pdw actual=1a0wwft resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@77e5ccc1)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#3]{New I/O worker #20}}\n[2014-12-07 00:03:59,839][WARN ][cluster.action.shard     ] [master] [index5][1] received shard failed for [index5][1], node[iCaUmle9SZOeK5z_VqAwwQ], [P], s[STARTED], indexUUID [_SdOrcFJSj6I8jI3Rxus0Q], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index5][1] Failed to transfer [136] files with total size of [81.8gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=1s2u9d3 actual=cggxwd resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@1d4e624)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#6]{New I/O worker #23}}\n[2014-12-07 00:03:59,975][WARN ][cluster.action.shard     ] [master] [index6][0] received shard failed for [index6][0], node[t5ieNHyPScOzEew0Rd0EcA], [P], s[STARTED], indexUUID [2lk2p8AKQSSxFF_iLffPUA], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index6][0] Failed to transfer [120] files with total size of [82gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=mgcerl actual=14hf0lf resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@661abd91)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#15]{New I/O worker #32}}\n[2014-12-07 00:04:00,251][WARN ][cluster.action.shard     ] [master] [index7][8] received shard failed for [index7][8], node[GwY2MBlwRHWbKYOgoAqBiA], [P], s[STARTED], indexUUID [bYPu5KhiTYqumxrzRh7OZg], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index7][8] Failed to transfer [176] files with total size of [77.6gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=1xnasey actual=pmztvp resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@1e3484f)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#12]{New I/O worker #29}}\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}