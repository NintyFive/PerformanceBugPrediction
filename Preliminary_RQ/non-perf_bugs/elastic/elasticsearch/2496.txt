{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/2496","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2496/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2496/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2496/events","html_url":"https://github.com/elastic/elasticsearch/issues/2496","id":9382337,"node_id":"MDU6SXNzdWU5MzgyMzM3","number":2496,"title":"Shards deleted from index (and from disk) on cluster restart","user":{"login":"jgagnon1","id":568924,"node_id":"MDQ6VXNlcjU2ODkyNA==","avatar_url":"https://avatars0.githubusercontent.com/u/568924?v=4","gravatar_id":"","url":"https://api.github.com/users/jgagnon1","html_url":"https://github.com/jgagnon1","followers_url":"https://api.github.com/users/jgagnon1/followers","following_url":"https://api.github.com/users/jgagnon1/following{/other_user}","gists_url":"https://api.github.com/users/jgagnon1/gists{/gist_id}","starred_url":"https://api.github.com/users/jgagnon1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jgagnon1/subscriptions","organizations_url":"https://api.github.com/users/jgagnon1/orgs","repos_url":"https://api.github.com/users/jgagnon1/repos","events_url":"https://api.github.com/users/jgagnon1/events{/privacy}","received_events_url":"https://api.github.com/users/jgagnon1/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2012-12-18T21:25:36Z","updated_at":"2013-02-27T15:38:52Z","closed_at":"2013-02-27T15:38:52Z","author_association":"NONE","active_lock_reason":null,"body":"I just had an issue with ES 0.20.0.RC1 that resulted on an cluster with 40 shards missing out of 250 on my production ready index. Note that the shards are not only missing in ElasticSearch **but also on the filesystem**. I have 51 nodes on my cluster.\n\nThis is the second time similar event happens to me, so I decided to file a bug on that.\n\n I will describe the timelime event with logs snipped that explain what I think happenned.\n\n14:12:00 - Cluster is on green state everything is fine -> sending shutdown via API\n14:13:02 - First cluster restart. Restarting nodes 10 per 10 (I use tmux so I do it almost simultaneously)\n\nNote: After the restart there are 8 nodes missing from the cluster.\n\nLogs from one of the server that is NOT on the cluster .\n\n```\n[2012-12-18 14:13:17,942][WARN ][discovery.zen.ping.unicast] [es1b] failed to send ping to [[#zen_unicast_3#][inet[es12b.cx.wajam/10.1.\n16.154:9300]]]\norg.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[es12b.cx.wajam/10.1.16.154:9300]][discovery/zen/unicast] request_\nid [0] timed out after [3750ms]\n        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:342)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:722)\n[2012-12-18 14:13:17,948][WARN ][discovery.zen.ping.unicast] [es1b] failed to send ping to [[#zen_unicast_9#][inet[es18b.cx.wajam/10.1.\n16.160:9300]]]\norg.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[es18b.cx.wajam/10.1.16.160:9300]][discovery/zen/unicast] request_\nid [9] timed out after [3750ms]\n        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:342)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:722)\n[2012-12-18 14:13:17,971][WARN ][discovery.zen.ping.unicast] [es1b] failed to send ping to [[#zen_unicast_28#][inet[es35b.cx.wajam/10.1\n.16.55:9300]]]\n```\n\n14:13:13-14:14:00 - Restarting the missing node on the cluster one by one (8 of them)\n\n14:14:08 - NullPointerException on Master and showing unassigned shards (not the one that are missing ?)\n\nLogs from master :\n\n```\njava.lang.NullPointerException\n        at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:75)\n        at org.elasticsearch.gateway.local.LocalGatewayAllocator.allocateUnassigned(LocalGatewayAllocator.java:198)\n        at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:70)\n        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:188)\n        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:138)\n        at org.elasticsearch.cluster.routing.RoutingService$1.execute(RoutingService.java:135)\n        at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:223)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:722)\n```\n\nLogs from one of the node that have missing shards\n\n```\n[2012-12-18 14:14:10,041][WARN ][discovery.zen            ] [es1b] received a cluster state from [[es22b][4FgfHy7vTOOgBZBPodjW2A][inet[/10.1.16.102:9300]]{master=true}] and not part of the cluster, should not happen\n[2012-12-18 14:14:10,190][DEBUG][action.admin.indices.status] [es1b] [wajam][119], node[aGZztLJ0TOWe5qMaTvDsBg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.admin.indices.status.IndicesStatusRequest@19661482]\norg.elasticsearch.transport.RemoteTransportException: [es5b][inet[/10.1.13.135:9300]][indices/status/s]\nCaused by: org.elasticsearch.indices.IndexMissingException: [wajam] missing\n        at org.elasticsearch.indices.InternalIndicesService.indexServiceSafe(InternalIndicesService.java:244)\n        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:152)\n        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:59)\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:398)\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:384)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:722)\n[2012-12-18 14:14:10,199][DEBUG][action.admin.cluster.node.stats] [es1b] failed to execute on node [AaYgqtpFStuwxI6fMlJs9w]\norg.elasticsearch.transport.RemoteTransportException: [es8b][inet[/10.1.13.138:9300]][cluster/nodes/stats/n]\nCaused by: java.lang.NullPointerException\n        at org.elasticsearch.action.support.nodes.NodeOperationResponse.writeTo(NodeOperationResponse.java:66)\n        at org.elasticsearch.action.admin.cluster.node.stats.NodeStats.writeTo(NodeStats.java:290)\n        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:91)\n        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:67)\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:276)\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:267)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:722)\n\n```\n\nThe last part is repeated for each missing shards.\n\n14:16:00 - At this point I have a red cluster with all the nodes but 40 shards missing. (51, 210). Looking into the filesystem, the shards folders are not there anymore. I'm wondering what's happening, everything has happened kind of fast.... \n","closed_by":{"login":"jgagnon1","id":568924,"node_id":"MDQ6VXNlcjU2ODkyNA==","avatar_url":"https://avatars0.githubusercontent.com/u/568924?v=4","gravatar_id":"","url":"https://api.github.com/users/jgagnon1","html_url":"https://github.com/jgagnon1","followers_url":"https://api.github.com/users/jgagnon1/followers","following_url":"https://api.github.com/users/jgagnon1/following{/other_user}","gists_url":"https://api.github.com/users/jgagnon1/gists{/gist_id}","starred_url":"https://api.github.com/users/jgagnon1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jgagnon1/subscriptions","organizations_url":"https://api.github.com/users/jgagnon1/orgs","repos_url":"https://api.github.com/users/jgagnon1/repos","events_url":"https://api.github.com/users/jgagnon1/events{/privacy}","received_events_url":"https://api.github.com/users/jgagnon1/received_events","type":"User","site_admin":false},"performed_via_github_app":null}