{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/16708","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16708/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16708/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16708/events","html_url":"https://github.com/elastic/elasticsearch/issues/16708","id":134413365,"node_id":"MDU6SXNzdWUxMzQ0MTMzNjU=","number":16708,"title":"Cluster Unable to Assign Shards, Marvel or otherwise.","user":{"login":"zukeru","id":8093421,"node_id":"MDQ6VXNlcjgwOTM0MjE=","avatar_url":"https://avatars2.githubusercontent.com/u/8093421?v=4","gravatar_id":"","url":"https://api.github.com/users/zukeru","html_url":"https://github.com/zukeru","followers_url":"https://api.github.com/users/zukeru/followers","following_url":"https://api.github.com/users/zukeru/following{/other_user}","gists_url":"https://api.github.com/users/zukeru/gists{/gist_id}","starred_url":"https://api.github.com/users/zukeru/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zukeru/subscriptions","organizations_url":"https://api.github.com/users/zukeru/orgs","repos_url":"https://api.github.com/users/zukeru/repos","events_url":"https://api.github.com/users/zukeru/events{/privacy}","received_events_url":"https://api.github.com/users/zukeru/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2016-02-17T22:01:09Z","updated_at":"2018-02-14T14:05:34Z","closed_at":"2016-05-09T12:40:58Z","author_association":"NONE","active_lock_reason":null,"body":"Hello, I'm trying to start a new cluster of elasticsearch, but I can't seem to get the shards to allocate correctly. I upgrade to the latest marvel, and elasticsearch 2.2.0 and the cluster won't register the marvel shards.  I cant figure out why it wont register. I can't even manually register because it tells me the shard is disabled.\n\nI then created a custom index with a few shards and the shards remain unassigned as well.\n\n```\ncurl -XPUT http://localhost:9200/test -d '\n{\n   \"settings\" : {\n      \"number_of_shards\" : 3,\n      \"number_of_replicas\" : 1\n   }\n}\n\n'\n```\n\n In the logs I get the following error:\n\n```\n[2016-02-17 20:04:52,458][ERROR][marvel.agent             ] [i-11a6decb] background thread had an uncaught exception\nElasticsearchException[failed to flush exporter bulks]\n    at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:104)\n    at org.elasticsearch.marvel.agent.exporter.ExportBulk.close(ExportBulk.java:53)\n    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:201)\n    at java.lang.Thread.run(Thread.java:745)\n    Suppressed: ElasticsearchException[failed to flush [default_local] exporter bulk]; nested: ElasticsearchException[failure in bulk execution:\n[0]: index [.marvel-es-2016.02.17], type [node_stats], id [AVLw1O4Ctq-FZ8CmFK_-], message [UnavailableShardsException[[.marvel-es-2016.02.17][0] primary shard is not active Timeout: [1m], request: [shard bulk {[.marvel-es-2016.02.17][0]}]]]];\n        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:106)\n        ... 3 more\n    Caused by: ElasticsearchException[failure in bulk execution:\n[0]: index [.marvel-es-2016.02.17], type [node_stats], id [AVLw1O4Ctq-FZ8CmFK_-], message [UnavailableShardsException[[.marvel-es-2016.02.17][0] primary shard is not active Timeout: [1m], request: [shard bulk {[.marvel-es-2016.02.17][0]}]]]]\n        at org.elasticsearch.marvel.agent.exporter.local.LocalBulk.flush(LocalBulk.java:114)\n        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:101)\n        ... 3 more\n\n```\n\nThen when I try to run a re-route I get:\n\n```\nKenzans-MacBook-Pro-39:~ grantzukel$ curl -XPOST http://localhost:9200/_cluster/reroute?pretty -d '{ \"commands\" : [ { \"allocate\" : { \"index\" : \".marvel-es-data\", \"shard\" : 0, \"node\" :\"i-e098e03a\" } } ] }' \n{\n  \"error\" : {\n    \"root_cause\" : [ {\n      \"type\" : \"remote_transport_exception\",\n      \"reason\" : \"[i-169f4cce][10.194.35.20:9300][cluster:admin/reroute]\"\n    } ],\n    \"type\" : \"illegal_argument_exception\",\n    \"reason\" : \"[allocate] trying to allocate a primary shard [.marvel-es-data][0], which is disabled\"\n  },\n  \"status\" : 400\n}\n\n```\n\nHere is my elasticsearch config where i enable rebalance and rerouting and primaries to true.\n\n```\n\nmy settings:\n\n---\n\ncluster.name: infra_elastic_cluster_3\n\nindex.number_of_shards: 3\nindex.store.throttle.type: none\n\naction.auto_create_index: true\n\nindex.number_of_replicas: 1\nindex.requests.cache.enable: true\nindex.search.slowlog.threshold.query.warn: 10s\nindex.search.slowlog.threshold.query.info: 5s\nindex.search.slowlog.threshold.query.debug: 2s\nindex.search.slowlog.threshold.query.trace: 500ms\nindex.search.slowlog.threshold.fetch.warn: 1s\nindex.search.slowlog.threshold.fetch.info: 800ms\nindex.search.slowlog.threshold.fetch.debug: 500ms\nindex.search.slowlog.threshold.fetch.trace: 200ms\n\nindex.refresh_interval: 1\n\ncloud:\n    aws:\n      region: us-west-2\n    node:\n      auto_attributes: true\n\ndiscovery:\n    type: ec2\n    ec2:\n      groups: infra_elastic_cluster_3\n      any_group: false\n      ping_timeout: 60s\n    zen:\n      minimum_master_nodes: 2\n\nnode:\n  data: true\n  master: false\n  name: i-29fd85f3\n\nhttp:\n  max_content_length: 1000mb\n  cors.allow-origin: \"/.*/\"\n  cors.enabled: true\n\nbootstrap.mlockall: true\n\nscript.inline: on \nscript.indexed: on \n\ntr.logging.maxlength: 500000\n\nindices.memory.index_buffer_size: 30%\nindices.store.throttle.max_bytes_per_sec: 1000mb\nindices.store.throttle.type: Merge\nindices.fielddata.cache.size:  40%\n\nthreadpool.bulk.type: fixed\nthreadpool.bulk.size: 100\nthreadpool.bulk.queue_size: 10000\n\nnetwork.host: _eth0_\n\nquery.bool.max_clause_count: 10240\n\ncluster.routing.allocation.enable: all\ncluster.routing.allocation.disable_new_allocation: false\ncluster.routing.allocation.disable_allocation: false\n\ncluster.routing.allocation.allow_primary: true\ncluster.routing.allocation.allow_rebalance: always\n\n```\n\nTrace log output\n\n```\n2016-02-17 21:57:03,686][TRACE][action.bulk              ] [i-29fd85f3] primary shard [[.marvel-es-2016.02.17][0]] is not yet active, scheduling a retry: action [indices:data/write/bulk[s]], request [shard bulk {[.marvel-es-2016.02.17][0]}], cluster state version [50]\n[2016-02-17 21:57:03,686][TRACE][action.bulk              ] [i-29fd85f3] observer: sampled state rejected by predicate (version [50], status [APPLIED]). adding listener to ClusterService\n[2016-02-17 21:57:03,686][TRACE][action.bulk              ] [i-29fd85f3] observer: postAdded - predicate rejected state (version [50], status [APPLIED])\n[2016-02-17 21:57:43,104][DEBUG][org.apache.http.impl.conn.PoolingClientConnectionManager] Closing connections idle longer than 60 SECONDS\n[2016-02-17 21:57:43,104][DEBUG][com.amazonaws.internal.SdkSSLSocket] shutting down output of ec2.us-west-2.amazonaws.com/205.251.235.5:443\n[2016-02-17 21:57:43,105][DEBUG][com.amazonaws.internal.SdkSSLSocket] closing ec2.us-west-2.amazonaws.com/205.251.235.5:443\n[2016-02-17 21:57:43,106][DEBUG][org.apache.http.impl.conn.DefaultClientConnection] Connection 0.0.0.0:38289<->205.251.235.5:443 closed\n[2016-02-17 21:58:03,687][TRACE][action.bulk              ] [i-29fd85f3] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2016-02-17 21:58:03,687][TRACE][action.bulk              ] [i-29fd85f3] primary shard [[.marvel-es-2016.02.17][0]] is not yet active, scheduling a retry: action [indices:data/write/bulk[s]], request [shard bulk {[.marvel-es-2016.02.17][0]}], cluster state version [50]\n[2016-02-17 21:58:03,687][TRACE][action.bulk              ] [i-29fd85f3] operation failed. action [indices:data/write/bulk[s]], request [shard bulk {[.marvel-es-2016.02.17][0]}]\nUnavailableShardsException[[.marvel-es-2016.02.17][0] primary shard is not active Timeout: [1m], request: [shard bulk {[.marvel-es-2016.02.17][0]}]]\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.retryBecauseUnavailable(TransportReplicationAction.java:555)\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:431)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2.onTimeout(TransportReplicationAction.java:520)\n    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:239)\n    at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:794)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n[2016-02-17 21:58:03,687][ERROR][marvel.agent             ] [i-29fd85f3] background thread had an uncaught exception\nElasticsearchException[failed to flush exporter bulks]\n    at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:104)\n    at org.elasticsearch.marvel.agent.exporter.ExportBulk.close(ExportBulk.java:53)\n    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:201)\n    at java.lang.Thread.run(Thread.java:745)\n    Suppressed: ElasticsearchException[failed to flush [default_local] exporter bulk]; nested: ElasticsearchException[failure in bulk execution:\n[0]: index [.marvel-es-2016.02.17], type [node_stats], id [AVLxPI5GwpZSvKDdhqNh], message [UnavailableShardsException[[.marvel-es-2016.02.17][0] primary shard is not active Timeout: [1m], request: [shard bulk {[.marvel-es-2016.02.17][0]}]]]];\n        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:106)\n        ... 3 more\n    Caused by: ElasticsearchException[failure in bulk execution:\n[0]: index [.marvel-es-2016.02.17], type [node_stats], id [AVLxPI5GwpZSvKDdhqNh], message [UnavailableShardsException[[.marvel-es-2016.02.17][0] primary shard is not active Timeout: [1m], request: [shard bulk {[.marvel-es-2016.02.17][0]}]]]]\n        at org.elasticsearch.marvel.agent.exporter.local.LocalBulk.flush(LocalBulk.java:114)\n        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:101)\n        ... 3 more\n[2016-02-17 21:58:13,693][TRACE][action.bulk              ] [i-29fd85f3] primary shard [[.marvel-es-2016.02.17][0]] is not yet active, scheduling a retry: action [indices:data/write/bulk[s]], request [shard bulk {[.marvel-es-2016.02.17][0]}], cluster state version [50]\n[2016-02-17 21:58:13,693][TRACE][action.bulk              ] [i-29fd85f3] observer: sampled state rejected by predicate (version [50], status [APPLIED]). adding listener to ClusterService\n[2016-02-17 21:58:13,694][TRACE][action.bulk              ] [i-29fd85f3] observer: postAdded - predicate rejected state (version [50], status [APPLIED])\n\n\n```\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}