[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106804842","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106804842","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":106804842,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjgwNDg0Mg==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2015-05-29T13:40:34Z","updated_at":"2015-05-29T13:40:34Z","author_association":"CONTRIBUTOR","body":"> In other words ES says I have 1.115.857 documents in my index, but my export contains 1.049.191 documents.\n\nHow are you measuring the number of documents in the index? Just `curl localhost:9200/index/_count`?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106807750","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106807750","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":106807750,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjgwNzc1MA==","user":{"login":"developmentstudio","id":5364555,"node_id":"MDQ6VXNlcjUzNjQ1NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/5364555?v=4","gravatar_id":"","url":"https://api.github.com/users/developmentstudio","html_url":"https://github.com/developmentstudio","followers_url":"https://api.github.com/users/developmentstudio/followers","following_url":"https://api.github.com/users/developmentstudio/following{/other_user}","gists_url":"https://api.github.com/users/developmentstudio/gists{/gist_id}","starred_url":"https://api.github.com/users/developmentstudio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/developmentstudio/subscriptions","organizations_url":"https://api.github.com/users/developmentstudio/orgs","repos_url":"https://api.github.com/users/developmentstudio/repos","events_url":"https://api.github.com/users/developmentstudio/events{/privacy}","received_events_url":"https://api.github.com/users/developmentstudio/received_events","type":"User","site_admin":false},"created_at":"2015-05-29T13:50:39Z","updated_at":"2015-05-29T13:51:43Z","author_association":"NONE","body":"Hi @nik9000,\n\nStarting the scroll/scan query with the query below return a count.\n\n```\nGET /20141016v3small/accesslog/_search?search_type=scan&scroll=10m\n```\n\nResults in 1115857 documents.\n\n```\n{\n   \"_scroll_id\": \"c2Nhbjs1OzMxMjpQdFFGVFhwTVJ2YXl5T2tjZkNYMUpROzMxNDpQdFFGVFhwTVJ2YXl5T2tjZkNYMUpROzMxNTpQdFFGVFhwTVJ2YXl5T2tjZkNYMUpROzMxMzpQdFFGVFhwTVJ2YXl5T2tjZkNYMUpROzMxMTpQdFFGVFhwTVJ2YXl5T2tjZkNYMUpROzE7dG90YWxfaGl0czoxMTE1ODU3Ow==\",\n   \"took\": 6,\n   \"timed_out\": false,\n   \"_shards\": {\n      \"total\": 5,\n      \"successful\": 5,\n      \"failed\": 0\n   },\n   \"hits\": {\n      \"total\": 1115857,\n      \"max_score\": 0,\n      \"hits\": []\n   }\n}\n```\n\nI write every document id to an MySQL database table and simple check with \n\n```\nSELECT COUNT(*) FROM table\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106816975","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106816975","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":106816975,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjgxNjk3NQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-05-29T14:07:42Z","updated_at":"2015-05-29T14:07:42Z","author_association":"CONTRIBUTOR","body":"I wonder if you have documents with the same `_id` and different type names, which are getting overwritten when you index into mysql?  Or if you have used routing on some documents so that you have duplicate copies of the same document on different shards?\n\nInstead of using mysql, why not just store these IDs in an in-memory hash and print out any clashes that you get?  You can maintain a simple count of all docs retrieved at the same time.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106821865","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106821865","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":106821865,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjgyMTg2NQ==","user":{"login":"developmentstudio","id":5364555,"node_id":"MDQ6VXNlcjUzNjQ1NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/5364555?v=4","gravatar_id":"","url":"https://api.github.com/users/developmentstudio","html_url":"https://github.com/developmentstudio","followers_url":"https://api.github.com/users/developmentstudio/followers","following_url":"https://api.github.com/users/developmentstudio/following{/other_user}","gists_url":"https://api.github.com/users/developmentstudio/gists{/gist_id}","starred_url":"https://api.github.com/users/developmentstudio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/developmentstudio/subscriptions","organizations_url":"https://api.github.com/users/developmentstudio/orgs","repos_url":"https://api.github.com/users/developmentstudio/repos","events_url":"https://api.github.com/users/developmentstudio/events{/privacy}","received_events_url":"https://api.github.com/users/developmentstudio/received_events","type":"User","site_admin":false},"created_at":"2015-05-29T14:17:32Z","updated_at":"2015-05-29T14:18:27Z","author_association":"NONE","body":"@clintongormley I am testing/prototyping some stuff where I, in the end, need a MySQL database to store some result sets for further analysis. But indeed checking the id's can be in-memory. I will check that.\n\nWhat I do is just indexing via the bulk end-point in a fresh ES installation without changing the default configurations (except HEAP_SIZE). All the `_id` are generated by ES so I can assume all are unique? Correct me if I am wrong. \n\n```\nhttp://127.0.0.1:9200/20141016v3small/accesslog/_bulk\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106823215","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106823215","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":106823215,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjgyMzIxNQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-05-29T14:19:41Z","updated_at":"2015-05-29T14:19:41Z","author_association":"CONTRIBUTOR","body":"that should be the case, yes.  it's still worth doing the test i propose - it would be interesting to know if there is indeed an issue with generated IDs (or if perhaps inserts are being lost in mysql?).  Also, what version of ES are you using?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106830345","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106830345","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":106830345,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjgzMDM0NQ==","user":{"login":"developmentstudio","id":5364555,"node_id":"MDQ6VXNlcjUzNjQ1NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/5364555?v=4","gravatar_id":"","url":"https://api.github.com/users/developmentstudio","html_url":"https://github.com/developmentstudio","followers_url":"https://api.github.com/users/developmentstudio/followers","following_url":"https://api.github.com/users/developmentstudio/following{/other_user}","gists_url":"https://api.github.com/users/developmentstudio/gists{/gist_id}","starred_url":"https://api.github.com/users/developmentstudio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/developmentstudio/subscriptions","organizations_url":"https://api.github.com/users/developmentstudio/orgs","repos_url":"https://api.github.com/users/developmentstudio/repos","events_url":"https://api.github.com/users/developmentstudio/events{/privacy}","received_events_url":"https://api.github.com/users/developmentstudio/received_events","type":"User","site_admin":false},"created_at":"2015-05-29T14:39:34Z","updated_at":"2015-05-29T14:39:34Z","author_association":"NONE","body":"I will do some test on duplicate _id's later this evening and will let you know the results. I'm using ES version 1.5.1.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106923383","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106923383","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":106923383,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjkyMzM4Mw==","user":{"login":"developmentstudio","id":5364555,"node_id":"MDQ6VXNlcjUzNjQ1NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/5364555?v=4","gravatar_id":"","url":"https://api.github.com/users/developmentstudio","html_url":"https://github.com/developmentstudio","followers_url":"https://api.github.com/users/developmentstudio/followers","following_url":"https://api.github.com/users/developmentstudio/following{/other_user}","gists_url":"https://api.github.com/users/developmentstudio/gists{/gist_id}","starred_url":"https://api.github.com/users/developmentstudio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/developmentstudio/subscriptions","organizations_url":"https://api.github.com/users/developmentstudio/orgs","repos_url":"https://api.github.com/users/developmentstudio/repos","events_url":"https://api.github.com/users/developmentstudio/events{/privacy}","received_events_url":"https://api.github.com/users/developmentstudio/received_events","type":"User","site_admin":false},"created_at":"2015-05-29T20:13:23Z","updated_at":"2015-05-30T12:05:25Z","author_association":"NONE","body":"I've written a small PHP script to check the total amount of documents given by ES in the initial scroll response vs the document count to exactly see how much documents are not retrieved. See below.\n\n``` php\n<?php\n$query = '{\n    \"size\": 1000,\n    \"filter\": {\n        \"bool\": {\n            \"must\": []\n        }\n    },   \n}';\n\n$search = new ScrollSearch($query);\n\nclass ScrollSearch\n{\n    private $scrollId = null;\n    private $totalDocuments = null;\n    private $totalResultDocuments = 0;\n\n    private $shards = array(\n        'total' => 0,\n        'successful' => 0,\n        'failed' => 0\n    );\n\n    public function __construct($query) \n    {\n        $this->initScrollScan($query);\n\n        while ($this->scrollId != false) {\n            $this->getDocuments();\n        }\n\n        $this->printInfo();\n        echo \"COMPLETED\" . PHP_EOL;\n    }\n\n    private function initScrollScan()\n    {\n        $ch = curl_init(\"http://127.0.0.1:9200/20141016v3/accesslog/_search?scroll=10m&search_type=scan\");\n        curl_setopt( $ch, CURLOPT_POSTFIELDS, $data);\n        curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));\n        curl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );\n        $response = curl_exec($ch);\n        $response = json_decode($response);\n\n        $this->scrollId = $response->{'_scroll_id'};\n        $this->totalDocuments = $response->{'hits'}->{'total'};\n\n        curl_close($ch);\n    }\n\n    private function getDocuments()\n    {\n        $ch = curl_init(\"http://127.0.0.1:9200/_search/scroll?scroll=10m\");\n        curl_setopt( $ch, CURLOPT_POSTFIELDS, $this->scrollId);\n        curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));\n        curl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );\n        $json = curl_exec($ch);\n        $response = json_decode(utf8_encode($json));\n        $countDocuments = count($response->{'hits'}->{'hits'});\n        $info = curl_getinfo($ch);\n\n        $this->shards['total'] += $response->{'_shards'}->{'total'};\n        $this->shards['successful'] += $response->{'_shards'}->{'successful'};\n        $this->shards['failed'] += $response->{'_shards'}->{'failed'};\n\n        if ($info['http_code'] != 200) {\n            echo \"Request goes wrong: \" . $info['http_code'] . '. Retry.' .PHP_EOL;\n            getDocuments();\n        }\n\n        if ($countDocuments > 0) {\n            $this->scrollId = $response->{'_scroll_id'};\n            $this->totalResultDocuments += $countDocuments;\n            $this->printInfo();\n        } else {\n            echo 'NO MORE RESULTS!' . PHP_EOL;\n            $this->scrollId = false;\n        }\n\n        curl_close($ch);\n    }\n\n    private function printInfo() {\n        echo ($this->totalResultDocuments - $this->totalDocuments) . \" missing documents. \" . $this->shards['total'] . \" shards, \" . $this->shards['failed'] . \" failed and \" . $this->shards['successful'] . \" successful\" . PHP_EOL;\n    }\n\n}\n```\n\nIndex contains: 1.115.857 documents which resulted in:\n\n``` bash\n...\n...\n...\n-57333 missing documents\n-57323 missing documents\n-57318 missing documents\nNO MORE RESULTS!\n-57318 missing documents\n```\n\nI did not check if their are duplicate id's, but just summed the amount of document return in each query.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106930944","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106930944","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":106930944,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjkzMDk0NA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-05-29T20:49:13Z","updated_at":"2015-05-29T20:49:13Z","author_association":"CONTRIBUTOR","body":"Hi @developmentstudio \n\nI put together a small test.  I indexed 1.115.857 docs with auto-generated IDs, then I retrieved them with scrolling and got back exactly 1.115.857 docs with 1.115.857 unique IDs.\n\nI think you're going to need to give us more information - perhaps dig a bit deeper into what is and isn't being returned. Perhaps you need to check the `_shards` element of each response, to see whether you are having shard failures, perhaps because of long GCs.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106935722","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106935722","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":106935722,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjkzNTcyMg==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2015-05-29T21:02:02Z","updated_at":"2015-05-29T21:02:02Z","author_association":"CONTRIBUTOR","body":"I'm curious - does it always spit out the same number? What does _count\nspit out?\n\nWould you mind showing how you do the bulk indexing?\n\nOn Fri, May 29, 2015 at 4:13 PM, Kevin van Cleef notifications@github.com\nwrote:\n\n> I've written a small PHP script to check the amount of document given by\n> ES in the initial scroll response vs the document count to exactly see how\n> much documents are not retrieved. See below.\n> \n> <?php$query = '{    \"size\": 1000,    \"filter\": {        \"bool\": {            \"must\": []        }    },   }';$search = new ScrollSearch($query);class ScrollSearch{    private $scrollId = null;    private $totalDocuments = null;    private $totalResultDocuments = 0;    public function __construct($query)     {        $this->initScrollScan($query);        while ($this->scrollId != false) {            $this->getDocuments();        }        $this->printMissingDocumentCount();        echo \"COMPLETED\" . PHP_EOL;    }    private function initScrollScan()    {        $ch = curl_init(\"http://127.0.0.1:9200/20141016v3/accesslog/_search?scroll=10m&search_type=scan\");        curl_setopt( $ch, CURLOPT_POSTFIELDS, $data);        curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));        curl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );        $response = curl_exec($ch);        $response = json_decode($response);        $this->scrollId = $response->{'_scroll_id'};        $this->totalDocuments = $response->{'hits'}->{'total'};        curl_close($ch);    }    private function getDocuments()    {        $ch = curl_init(\"http://127.0.0.1:9200/_search/scroll?scroll=10m\");        curl_setopt( $ch, CURLOPT_POSTFIELDS, $this->scrollId);        curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));        curl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );        $json = curl_exec($ch);        $response = json_decode(utf8_encode($json));        $countDocuments = count($response->{'hits'}->{'hits'});        $info = curl_getinfo($ch);        if ($info['http_code'] != 200) {            echo \"Request goes wrong: \" . $info['http_code'] . '. Retry.' .PHP_EOL;            getDocuments();        }        if ($countDocuments > 0) {            $this->scrollId = $response->{'_scroll_id'};            $this->totalResultDocuments += $countDocuments;            $this->printMissingDocumentCount();        } else {            echo 'NO MORE RESULTS!' . PHP_EOL;            $this->scrollId = false;        }        curl_close($ch);    }    private function printMissingDocumentCount() {        echo ($this->totalResultDocuments - $this->totalDocuments) . \" missing documents\" . PHP_EOL;    }}\n> \n> Index contains: 1.115.857 documents which resulted in:\n> \n> ...\n> ...\n> ...\n> -57333 missing documents\n> -57323 missing documents\n> -57318 missing documents\n> NO MORE RESULTS!\n> -57318 missing documents\n> \n> I did not check if their are duplicate id's, but just summed the amount of\n> document return in each query.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/elastic/elasticsearch/issues/11419#issuecomment-106923383\n> .\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107035786","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107035786","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107035786,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzAzNTc4Ng==","user":{"login":"developmentstudio","id":5364555,"node_id":"MDQ6VXNlcjUzNjQ1NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/5364555?v=4","gravatar_id":"","url":"https://api.github.com/users/developmentstudio","html_url":"https://github.com/developmentstudio","followers_url":"https://api.github.com/users/developmentstudio/followers","following_url":"https://api.github.com/users/developmentstudio/following{/other_user}","gists_url":"https://api.github.com/users/developmentstudio/gists{/gist_id}","starred_url":"https://api.github.com/users/developmentstudio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/developmentstudio/subscriptions","organizations_url":"https://api.github.com/users/developmentstudio/orgs","repos_url":"https://api.github.com/users/developmentstudio/repos","events_url":"https://api.github.com/users/developmentstudio/events{/privacy}","received_events_url":"https://api.github.com/users/developmentstudio/received_events","type":"User","site_admin":false},"created_at":"2015-05-30T12:32:38Z","updated_at":"2015-05-31T08:43:57Z","author_association":"NONE","body":"@clintongormley Thanks for helping. I've changed the test code above so it also checks how many shards are successful and how many failed which resulted in `-57318 missing documents. 105857 shards, 1 failed and 105856 successful`. One missing shard doesn't explains all the missing the documents, right? Below the logfile after running the scroll query. This is related to the failing shard if I'm right.\n\n```\n[2015-05-30 13:48:53,034][DEBUG][action.search.type       ] [Robbie Robertson] [666] Failed to execute query phase\norg.elasticsearch.search.fetch.FetchPhaseExecutionException: [20141016v3][0]: query[ConstantScore(cache(_type:accesslog))],from[165860],size[10]: Fetch Failed [Failed to fetch doc id [165863]]\n    at org.elasticsearch.search.fetch.FetchPhase.loadStoredFields(FetchPhase.java:409)\n    at org.elasticsearch.search.fetch.FetchPhase.createSearchHit(FetchPhase.java:217)\n    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:182)\n    at org.elasticsearch.search.SearchService.executeScan(SearchService.java:261)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.index.CorruptIndexException: Corrupted: lengths mismatch: 17381 > 17380 (resource=NIOFSIndexInput(path=\"/var/lib/elasticsearch/experiment-kevin-van-cleef/nodes/0/indices/20141016v3/0/index/_hn.fdt\"))\n    at org.apache.lucene.codecs.compressing.CompressionMode$4.decompress(CompressionMode.java:137)\n    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.visitDocument(CompressingStoredFieldsReader.java:354)\n    at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:335)\n    at org.elasticsearch.search.fetch.FetchPhase.loadStoredFields(FetchPhase.java:407)\n    ... 9 more\n\n```\n\nWhat do you mean with long GCs? \nHow can I best approach diff the result set retrieved by the scroll-scan query and the documents in the index?\n\n@nik9000 I've reindexed the dataset multiple times for testing. At the current index the missing amount of documents remains the same. (Missing: 57318) At the initial index where I started the post with I missed 66.666 documents. It looks like the amount of missing documents is constant for the created index. \n\n`GET /20141016v3/accesslog/_count` results in the same amount of documents namely 1.115.857, see below.\n\n```\n{\n   \"count\": 1115857,\n   \"_shards\": {\n      \"total\": 5,\n      \"successful\": 5,\n      \"failed\": 0\n   }\n}\n```\n\nThe script used for indexing.\n\n``` php\n<?php\n$file_handle = fopen(\"access.json.log-20141016\", \"r\");\n\n$batch = array();\n$max_number_of_batches = 130000;\nwhile (!feof($file_handle)) {\n    $line = fgets($file_handle);\n    $line = preg_replace_callback(\"(\\\\\\\\x([0-9a-f]{2}))i\", function($a) { return chr(hexdec($a[1])); }, $line);\n\n    if (strlen($line) > 0 ) {\n        $batch[] = \"{\\\"create\\\" : {}} \\n\" . $line;  \n    }\n\n    if (count($batch) == 10) {\n        $result = bulkIndex($batch);\n\n        $result = json_decode($result);\n        if ($result->errors) {\n            var_dump($result);\n        }\n\n        $batch = array();\n\n        --$max_number_of_batches;\n        echo $max_number_of_batches . PHP_EOL;\n        if ($max_number_of_batches == 0) {\n            break;\n        }\n    }\n}\nbulkIndex($batch);\nfclose($file_handle);\n\nfunction bulkIndex($batch) {\n    $data = implode(\"\\n\", $batch) . \"\\n\";\n\n    $ch = curl_init(\"http://127.0.0.1:9200/20141016v3/accesslog/_bulk\");\n    curl_setopt( $ch, CURLOPT_POSTFIELDS, $data);\n    curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));\n    curl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );\n    $result = curl_exec($ch);\n    return $result;\n    curl_close($ch);\n}\n```\n\nMapping used for the index.\n\n```\nPUT /20141016v3\n{\n    \"index\" : {\n        \"analysis\" : {\n            \"analyzer\" : {\n                \"default\" : {\n                    \"type\" : \"keyword\"\n                }\n            }\n        }\n    }\n}\n```\n\nExample document structure.\n\n``` json\n{\n    \"request\": {\n        \"timestamp\": \"2014-10-15T03:42:51+02:00\",\n        \"method\": \"GET\",\n        \"cookies\": {},\n        \"url\": {\n            \"host\": \"www.domein.nl\",\n            \"uri\": \"/product/142855/product-name.html\",\n            \"query\": {}\n        },\n        \"protocol\": \"HTTP/1.1\",\n        \"headers\": {\n            \"x-forwarded-host\": \"www.domein.nl\",\n            \"x-forwarded-for\": \"111.11.11.1, 111.111.111.11, 111.111.111.111\",\n            \"connection\": \"Keep-Alive\",\n            \"accept\": \"*/*\",\n            \"cache-control\": \"no-cache\",\n            \"host\": \"www.domein.nl\",\n            \"x-forwarded-server\": \"www.domein.nl\",\n            \"sitespect\": \"1-1249\",\n            \"pragma\": \"no-cache\",\n            \"user-agent\": \"msnbot/2.0b (+http://search.msn.com/msnbot.htm)\"\n        },\n        \"remoteAddress\": \"111.11.11.1\"\n    },\n    \"response\": {\n        \"status\": 200,\n        \"bytesSent\": 163552,\n        \"processingTime\": 0.687,\n        \"headers\": {\n            \"pragma\": \"no-cache\",\n            \"expires\": \"Thu, 19 Nov 1981 08:52:00 GMT\",\n            \"content-type\": \"text/html; charset=iso-8859-1\",\n            \"set-cookie\": \"PHPSESSID=8llnj7b5vo4lrv1ehjeo09rre5; path=/\",\n            \"x-frame-options\": \"SAMEORIGIN\",\n            \"cache-control\": \"no-store, no-cache, must-revalidate, post-check=0, pre-check=0\"\n        }\n    }\n}\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107068038","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107068038","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107068038,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzA2ODAzOA==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-05-30T17:30:13Z","updated_at":"2015-05-30T17:30:13Z","author_association":"CONTRIBUTOR","body":"that exception looks suspicious. Is there any way we can get a copy of the shard for inspection? What is the output of $JAVA_HOME/bin/java -version ?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107074488","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107074488","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107074488,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzA3NDQ4OA==","user":{"login":"developmentstudio","id":5364555,"node_id":"MDQ6VXNlcjUzNjQ1NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/5364555?v=4","gravatar_id":"","url":"https://api.github.com/users/developmentstudio","html_url":"https://github.com/developmentstudio","followers_url":"https://api.github.com/users/developmentstudio/followers","following_url":"https://api.github.com/users/developmentstudio/following{/other_user}","gists_url":"https://api.github.com/users/developmentstudio/gists{/gist_id}","starred_url":"https://api.github.com/users/developmentstudio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/developmentstudio/subscriptions","organizations_url":"https://api.github.com/users/developmentstudio/orgs","repos_url":"https://api.github.com/users/developmentstudio/repos","events_url":"https://api.github.com/users/developmentstudio/events{/privacy}","received_events_url":"https://api.github.com/users/developmentstudio/received_events","type":"User","site_admin":false},"created_at":"2015-05-30T18:35:43Z","updated_at":"2015-05-30T18:35:43Z","author_association":"NONE","body":"@rmuir I have to ask my security manager if sharing the dataset is allowed as it contains sensitive information.\n\n```\n$ java -version\nopenjdk version \"1.8.0_45\"\nOpenJDK Runtime Environment (build 1.8.0_45-b13)\nOpenJDK 64-Bit Server VM (build 25.45-b02, mixed mode)\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107162033","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107162033","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107162033,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE2MjAzMw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T11:59:13Z","updated_at":"2015-05-31T11:59:13Z","author_association":"CONTRIBUTOR","body":"@developmentstudio one possibility is that you have a hardware issue.  Would it be possible to reindex your data on different disks and try these tests again?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107163396","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107163396","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107163396,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE2MzM5Ng==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T12:08:25Z","updated_at":"2015-05-31T12:08:25Z","author_association":"CONTRIBUTOR","body":"> One missing shard doesn't explains all the missing the documents, right? Below the logfile after running the scroll query.\n\nI bet what is happening is that, after the exception, no more results are being returned from that shard. But the main question is: what is the source of this corruption?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107173533","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107173533","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107173533,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE3MzUzMw==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T12:56:40Z","updated_at":"2015-05-31T12:56:40Z","author_association":"CONTRIBUTOR","body":"Can you send the output of checkindex against the shard?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107174044","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107174044","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107174044,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE3NDA0NA==","user":{"login":"developmentstudio","id":5364555,"node_id":"MDQ6VXNlcjUzNjQ1NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/5364555?v=4","gravatar_id":"","url":"https://api.github.com/users/developmentstudio","html_url":"https://github.com/developmentstudio","followers_url":"https://api.github.com/users/developmentstudio/followers","following_url":"https://api.github.com/users/developmentstudio/following{/other_user}","gists_url":"https://api.github.com/users/developmentstudio/gists{/gist_id}","starred_url":"https://api.github.com/users/developmentstudio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/developmentstudio/subscriptions","organizations_url":"https://api.github.com/users/developmentstudio/orgs","repos_url":"https://api.github.com/users/developmentstudio/repos","events_url":"https://api.github.com/users/developmentstudio/events{/privacy}","received_events_url":"https://api.github.com/users/developmentstudio/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T13:00:29Z","updated_at":"2015-05-31T13:00:29Z","author_association":"NONE","body":"@rmuir What do you mean exactly with output of checkindex?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107176964","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107176964","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107176964,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE3Njk2NA==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T13:19:08Z","updated_at":"2015-05-31T13:19:08Z","author_association":"CONTRIBUTOR","body":"Lucene has a commandline tool called CheckIndex which will do verification of the index and print out any problems. It can maybe tell us a little bit more about what happened: if you are ultimately able to share the index, it would be the first thing I would run. \n\nBut if you do not mind, you can run it yourself. There are some instructions here that look good:\n\nhttps://www.found.no/foundation/dive-into-elasticsearch-storage/#fixing-problematic-shards\n\nBut one note about those instructions, _PLEASE DO NOT_ pass the -fix option. We just want to run diagnostics here.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107177099","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107177099","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107177099,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE3NzA5OQ==","user":{"login":"developmentstudio","id":5364555,"node_id":"MDQ6VXNlcjUzNjQ1NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/5364555?v=4","gravatar_id":"","url":"https://api.github.com/users/developmentstudio","html_url":"https://github.com/developmentstudio","followers_url":"https://api.github.com/users/developmentstudio/followers","following_url":"https://api.github.com/users/developmentstudio/following{/other_user}","gists_url":"https://api.github.com/users/developmentstudio/gists{/gist_id}","starred_url":"https://api.github.com/users/developmentstudio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/developmentstudio/subscriptions","organizations_url":"https://api.github.com/users/developmentstudio/orgs","repos_url":"https://api.github.com/users/developmentstudio/repos","events_url":"https://api.github.com/users/developmentstudio/events{/privacy}","received_events_url":"https://api.github.com/users/developmentstudio/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T13:20:02Z","updated_at":"2015-05-31T13:20:02Z","author_association":"NONE","body":"@clintongormley I am running ES in a Vagrant box with CentOS6.6, 2 cpu's and 6 GB of RAM of which 3 are for the HEAP.  Host is a MacBook Pro Retina which quad core i7 and 16GB RAM. Do you think there is something wrong in this setup which can explain the issue?\n\n> I bet what is happening is that, after the exception, no more results are being returned from that shard. But the main question is: what is the source of this corruption?\n\nI think you are right. I've echo'ed the amount of shards returned in each response. After the first time the shard failed the total shards returned is lowered to 4 instead of 5.\n\n```\n-1079057 missing documents. 3680 shards, 0 failed and 3680 successful\nShards: 5\n-1079007 missing documents. 3685 shards, 0 failed and 3685 successful\nShards: 5\n-1078967 missing documents. 3690 shards, 1 failed and 3689 successful\nShards: 4\n-1078927 missing documents. 3694 shards, 1 failed and 3693 successful\nShards: 4\n-1078887 missing documents. 3698 shards, 1 failed and 3697 successful\n```\n\nLast night and this morning I have reindexed the data-set some more times. The amount of missing documents is different for each index. Which goes from 0 to above 200.000 missing documents, but the amount of missing documents resulted from the test stays the same for each index. This makes me think something goes wrong at the moment of indexing.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107180813","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107180813","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107180813,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE4MDgxMw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T13:35:37Z","updated_at":"2015-05-31T13:35:37Z","author_association":"CONTRIBUTOR","body":"> Do you think there is something wrong in this setup which can explain the issue?\n\nThere's always a possibility that there is an actual hardware (eg disk) failure.  Just trying to exclude that.  \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107182938","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107182938","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107182938,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE4MjkzOA==","user":{"login":"developmentstudio","id":5364555,"node_id":"MDQ6VXNlcjUzNjQ1NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/5364555?v=4","gravatar_id":"","url":"https://api.github.com/users/developmentstudio","html_url":"https://github.com/developmentstudio","followers_url":"https://api.github.com/users/developmentstudio/followers","following_url":"https://api.github.com/users/developmentstudio/following{/other_user}","gists_url":"https://api.github.com/users/developmentstudio/gists{/gist_id}","starred_url":"https://api.github.com/users/developmentstudio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/developmentstudio/subscriptions","organizations_url":"https://api.github.com/users/developmentstudio/orgs","repos_url":"https://api.github.com/users/developmentstudio/repos","events_url":"https://api.github.com/users/developmentstudio/events{/privacy}","received_events_url":"https://api.github.com/users/developmentstudio/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T13:42:19Z","updated_at":"2015-05-31T14:32:41Z","author_association":"NONE","body":"@rmuir @clintongormley See below the result of CheckIndex. Hardware is possibly the reason. I will check another machine to run these tests and let you know. (will be tomorrow) Thanks!\n\n```\nelasticsearch]$ java -cp lib/elasticsearch-*.jar:lib/*:lib/sigar/* -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex $SHARD_PATH\n\nOpening index @ /var/lib/elasticsearch/experiment-kevin-van-cleef/nodes/0/indices/20141016v5/0/index\n\nSegments file=segments_2 numSegments=19 version=4.10.4 format= userData={translog_id=1433018704706}\n  1 of 19: name=_73 docCount=36477\n    version=4.10.4\n    codec=Lucene410\n    compound=false\n    numFiles=12\n    size (MB)=75.554\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433019039094}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....FAILED\n    WARNING: fixIndex() would remove reference to this segment; full exception:\norg.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=dc15b066 actual=cc0d3842 (resource=BufferedChecksumIndexInput(MMapIndexInput(path=\"/var/lib/elasticsearch/experiment-kevin-van-cleef/nodes/0/indices/20141016v5/0/index/_73.fdt\")))\n    at org.apache.lucene.codecs.CodecUtil.checkFooter(CodecUtil.java:211)\n    at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:268)\n    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.checkIntegrity(CompressingStoredFieldsReader.java:535)\n    at org.apache.lucene.index.SegmentReader.checkIntegrity(SegmentReader.java:624)\n    at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:590)\n    at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:2096)\n\n  2 of 19: name=_sp docCount=64676\n    version=4.10.4\n    codec=Lucene410\n    compound=false\n    numFiles=12\n    size (MB)=176.853\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433019758952}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [1736 fields]\n    test: field norms.........OK [1699 fields]\n    test: terms, freq, prox...OK [543408 terms; 5466327 terms/docs pairs; 1700019 tokens]\n    test: stored fields.......OK [129352 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  3 of 19: name=_kn docCount=43922\n    version=4.10.4\n    codec=Lucene410\n    compound=false\n    numFiles=12\n    size (MB)=103.51\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433019490621}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [1832 fields]\n    test: field norms.........OK [1762 fields]\n    test: terms, freq, prox...OK [324060 terms; 3596507 terms/docs pairs; 1095198 tokens]\n    test: stored fields.......OK [87844 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  4 of 19: name=_vh docCount=15790\n    version=4.10.4\n    codec=Lucene410\n    compound=false\n    numFiles=12\n    size (MB)=57.05\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433019852048}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [791 fields]\n    test: field norms.........OK [776 fields]\n    test: terms, freq, prox...OK [179732 terms; 1430534 terms/docs pairs; 462434 tokens]\n    test: stored fields.......OK [31580 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  5 of 19: name=_yk docCount=14347\n    version=4.10.4\n    codec=Lucene410\n    compound=false\n    numFiles=12\n    size (MB)=55.812\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433019954193}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [743 fields]\n    test: field norms.........OK [728 fields]\n    test: terms, freq, prox...OK [169755 terms; 1333122 terms/docs pairs; 436246 tokens]\n    test: stored fields.......OK [28694 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  6 of 19: name=_11c docCount=13144\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=50.831\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020047341}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [753 fields]\n    test: field norms.........OK [739 fields]\n    test: terms, freq, prox...OK [155725 terms; 1218242 terms/docs pairs; 398232 tokens]\n    test: stored fields.......OK [26288 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  7 of 19: name=_13u docCount=13457\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=47.191\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020130429}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [747 fields]\n    test: field norms.........OK [733 fields]\n    test: terms, freq, prox...OK [149465 terms; 1207508 terms/docs pairs; 388684 tokens]\n    test: stored fields.......OK [26914 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  8 of 19: name=_162 docCount=12123\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=31.108\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020204484}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [614 fields]\n    test: field norms.........OK [599 fields]\n    test: terms, freq, prox...OK [112993 terms; 994561 terms/docs pairs; 305514 tokens]\n    test: stored fields.......OK [24246 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  9 of 19: name=_16c docCount=1636\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=2.946\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020213488}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [231 fields]\n    test: field norms.........OK [221 fields]\n    test: terms, freq, prox...OK [16298 terms; 123755 terms/docs pairs; 36203 tokens]\n    test: stored fields.......OK [3272 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  10 of 19: name=_16m docCount=1540\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=2.533\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020222504}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [216 fields]\n    test: field norms.........OK [205 fields]\n    test: terms, freq, prox...OK [14237 terms; 116351 terms/docs pairs; 33909 tokens]\n    test: stored fields.......OK [3080 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  11 of 19: name=_16w docCount=1713\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=2.678\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020231512}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [201 fields]\n    test: field norms.........OK [191 fields]\n    test: terms, freq, prox...OK [15570 terms; 127546 terms/docs pairs; 36869 tokens]\n    test: stored fields.......OK [3426 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  12 of 19: name=_176 docCount=1681\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=2.608\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020241517}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [233 fields]\n    test: field norms.........OK [222 fields]\n    test: terms, freq, prox...OK [15259 terms; 124763 terms/docs pairs; 35983 tokens]\n    test: stored fields.......OK [3362 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  13 of 19: name=_17g docCount=1695\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=2.617\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020250526}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [202 fields]\n    test: field norms.........OK [188 fields]\n    test: terms, freq, prox...OK [15204 terms; 126487 terms/docs pairs; 36605 tokens]\n    test: stored fields.......OK [3390 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  14 of 19: name=_166 docCount=206\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=0.507\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020209005}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [111 fields]\n    test: field norms.........OK [102 fields]\n    test: terms, freq, prox...OK [2836 terms; 15893 terms/docs pairs; 4717 tokens]\n    test: stored fields.......OK [412 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  15 of 19: name=_17f docCount=210\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=0.318\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020250756}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [94 fields]\n    test: field norms.........OK [82 fields]\n    test: terms, freq, prox...OK [2537 terms; 15574 terms/docs pairs; 4473 tokens]\n    test: stored fields.......OK [420 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  16 of 19: name=_17h docCount=164\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=0.289\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020251771}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [91 fields]\n    test: field norms.........OK [81 fields]\n    test: terms, freq, prox...OK [1949 terms; 12846 terms/docs pairs; 3806 tokens]\n    test: stored fields.......OK [328 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  17 of 19: name=_17i docCount=196\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=0.316\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020252790}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [99 fields]\n    test: field norms.........OK [89 fields]\n    test: terms, freq, prox...OK [2246 terms; 14785 terms/docs pairs; 4283 tokens]\n    test: stored fields.......OK [392 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  18 of 19: name=_17j docCount=168\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=0.288\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020253810}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [98 fields]\n    test: field norms.........OK [89 fields]\n    test: terms, freq, prox...OK [2192 terms; 12480 terms/docs pairs; 3605 tokens]\n    test: stored fields.......OK [336 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\n  19 of 19: name=_17k docCount=82\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=0.151\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_45, lucene.version=4.10.4, os.arch=amd64, source=flush, os.version=2.6.32-504.3.3.el6.x86_64, timestamp=1433020254838}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [76 fields]\n    test: field norms.........OK [67 fields]\n    test: terms, freq, prox...OK [1143 terms; 6104 terms/docs pairs; 1761 tokens]\n    test: stored fields.......OK [164 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\nWARNING: 1 broken segments (containing 36477 documents) detected\nWARNING: would write new segments file, and 36477 documents would be lost, if -fix were specified\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107186962","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107186962","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107186962,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE4Njk2Mg==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T13:49:04Z","updated_at":"2015-05-31T13:49:04Z","author_association":"CONTRIBUTOR","body":"Thanks very much for running this, I am relieved it is not a lucene issue :)\n\nIn this case bits in the .fdt file are getting flipped somehow...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107389109","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107389109","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107389109,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzM4OTEwOQ==","user":{"login":"developmentstudio","id":5364555,"node_id":"MDQ6VXNlcjUzNjQ1NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/5364555?v=4","gravatar_id":"","url":"https://api.github.com/users/developmentstudio","html_url":"https://github.com/developmentstudio","followers_url":"https://api.github.com/users/developmentstudio/followers","following_url":"https://api.github.com/users/developmentstudio/following{/other_user}","gists_url":"https://api.github.com/users/developmentstudio/gists{/gist_id}","starred_url":"https://api.github.com/users/developmentstudio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/developmentstudio/subscriptions","organizations_url":"https://api.github.com/users/developmentstudio/orgs","repos_url":"https://api.github.com/users/developmentstudio/repos","events_url":"https://api.github.com/users/developmentstudio/events{/privacy}","received_events_url":"https://api.github.com/users/developmentstudio/received_events","type":"User","site_admin":false},"created_at":"2015-06-01T10:15:43Z","updated_at":"2015-06-01T10:15:43Z","author_association":"NONE","body":"Just indexed and executed the tests twice on another machine without any problems. Looking for hardware failures on the machine used above with OS X's Disk Utility found some issues and I have repaired them via Disk Repair. After disk repair reindexed and executed the tests two more times, no problems occurred anymore. Thus so far, hardware failure was the reason for the issue. \n\nMany thanks to everybody for the help!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107390075","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-107390075","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":107390075,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzM5MDA3NQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-06-01T10:22:51Z","updated_at":"2015-06-01T10:22:51Z","author_association":"CONTRIBUTOR","body":"thanks for coming back with this information! I will close the issue for now feel free to come back if you have further issues.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/132187270","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-132187270","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":132187270,"node_id":"MDEyOklzc3VlQ29tbWVudDEzMjE4NzI3MA==","user":{"login":"lexand","id":927515,"node_id":"MDQ6VXNlcjkyNzUxNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/927515?v=4","gravatar_id":"","url":"https://api.github.com/users/lexand","html_url":"https://github.com/lexand","followers_url":"https://api.github.com/users/lexand/followers","following_url":"https://api.github.com/users/lexand/following{/other_user}","gists_url":"https://api.github.com/users/lexand/gists{/gist_id}","starred_url":"https://api.github.com/users/lexand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lexand/subscriptions","organizations_url":"https://api.github.com/users/lexand/orgs","repos_url":"https://api.github.com/users/lexand/repos","events_url":"https://api.github.com/users/lexand/events{/privacy}","received_events_url":"https://api.github.com/users/lexand/received_events","type":"User","site_admin":false},"created_at":"2015-08-18T12:03:50Z","updated_at":"2015-08-18T12:03:50Z","author_association":"NONE","body":"Salutations\nhave the same issue but cannot determine why\n\nnewly created index\ninsert 50 random documents (one text field per each doc) with autogenerated _id\n\nSearch without scroll\n\n``` PHP\n        $res = $client->search(\n            [\n                'index'  => $index,\n                'type'   => $type,\n                'size'   => 100,\n                'body'   => []\n            ]);\n        print_r(['total'    => $res['hits']['total'],\n                 'returned' => count($res['hits']['hits']),\n                 '_shards'  => $res['_shards']]);\n```\n\n```\nArray\n(\n    [total] => 50\n    [returned] => 50\n    [_shards] => Array\n        (\n            [total] => 3\n            [successful] => 3\n            [failed] => 0\n        )\n)\n```\n\nBut search with scroll returns next\n\n``` PHP\n        $res = $client->search(\n            [\n                'index'  => $index,\n                'type'   => $type,\n                'scroll' => '1m',\n                'size'   => 10, // per shards, so I expect 30 results maximum. Where are they ?\n                'body'   => []\n            ]);\n                $scrollId = $res['_scroll_id'];\n        print_r(...);\n\n                $res = $client->scroll(['scroll_id' => $scrollId]);\n        print_r(...);\n                $res = $client->scroll(['scroll_id' => $scrollId]);\n        print_r(...);\n                $res = $client->scroll(['scroll_id' => $scrollId]);\n        print_r(...);\n```\n\n```\nArray\n(\n    [total] => 50\n    [returned] => 10\n    [_shards] => Array\n        (\n            [total] => 3\n            [successful] => 3\n            [failed] => 0\n        )\n\n)\nscroll 1\nArray\n(\n    [total] => 50\n    [returned] => 10\n    [_shards] => Array\n        (\n            [total] => 3\n            [successful] => 3\n            [failed] => 0\n        )\n\n)\nscroll 2\nArray\n(\n    [total] => 29\n    [returned] => 10\n    [_shards] => Array\n        (\n            [total] => 3\n            [successful] => 2\n            [failed] => 1\n            [failures] => Array\n                (\n                    [0] => Array\n                        (\n                            [status] => 404\n                            [reason] => SearchContextMissingException[No search context found for id [22]]\n                        )\n\n                )\n\n        )\n\n)\nscroll 3\nArray\n(\n    [total] => 2\n    [returned] => 2\n    [_shards] => Array\n        (\n            [total] => 3\n            [successful] => 1\n            [failed] => 2\n            [failures] => Array\n                (\n                    [0] => Array\n                        (\n                            [status] => 404\n                            [reason] => SearchContextMissingException[No search context found for id [22]]\n                        )\n\n                    [1] => Array\n                        (\n                            [status] => 404\n                            [reason] => SearchContextMissingException[No search context found for id [23]]\n                        )\n\n                )\n\n        )\n)\n```\n\nAnd totally 32 documents are returned\n\n```\nOpening index @ /var/lib/elasticsearch/elasticsearch/nodes/0/indices/test/0/index/\n\nSegments file=segments_3 numSegments=1 version=4.10.4 format= userData={sync_id=AU9AS-k-fF4_530OBpQo, translog_id=1439890621685}\n  1 of 1: name=_0 docCount=21\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=0.024\n    diagnostics = {timestamp=1439890622956, os=Linux, os.version=4.1.4-100.fc21.x86_64, source=flush, lucene.version=4.10.4, os.arch=amd64, java.version=1.7.0_76, java.vendor=Oracle Corporation}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [7 fields]\n    test: field norms.........OK [2 fields]\n    test: terms, freq, prox...OK [168 terms; 2078 terms/docs pairs; 2222 tokens]\n    test: stored fields.......OK [42 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\nNo problems were detected with this index.\n\nOpening index @ /var/lib/elasticsearch/elasticsearch/nodes/0/indices/test/1/index/\n\nSegments file=segments_3 numSegments=1 version=4.10.4 format= userData={sync_id=AU9AS-k_fF4_530OBpQp, translog_id=1439890621705}\n  1 of 1: name=_0 docCount=27\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=0.026\n    diagnostics = {timestamp=1439890622962, os=Linux, os.version=4.1.4-100.fc21.x86_64, source=flush, lucene.version=4.10.4, os.arch=amd64, java.version=1.7.0_76, java.vendor=Oracle Corporation}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [7 fields]\n    test: field norms.........OK [2 fields]\n    test: terms, freq, prox...OK [168 terms; 2398 terms/docs pairs; 2530 tokens]\n    test: stored fields.......OK [54 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\nNo problems were detected with this index.\n\nOpening index @ /var/lib/elasticsearch/elasticsearch/nodes/0/indices/test/2/index/\n\nSegments file=segments_3 numSegments=1 version=4.10.4 format= userData={sync_id=AU9AS-k_fF4_530OBpQq, translog_id=1439890621710}\n  1 of 1: name=_0 docCount=2\n    version=4.10.4\n    codec=Lucene410\n    compound=true\n    numFiles=3\n    size (MB)=0.007\n    diagnostics = {timestamp=1439890622865, os=Linux, os.version=4.1.4-100.fc21.x86_64, source=flush, lucene.version=4.10.4, os.arch=amd64, java.version=1.7.0_76, java.vendor=Oracle Corporation}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [7 fields]\n    test: field norms.........OK [2 fields]\n    test: terms, freq, prox...OK [143 terms; 254 terms/docs pairs; 278 tokens]\n    test: stored fields.......OK [4 total field count; avg 2 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\n\nNo problems were detected with this index.\n```\n\nES Version\n\n``` JSON\n{\n  \"status\" : 200,\n  \"name\" : \"Temugin\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"1.7.1\",\n    \"build_hash\" : \"b88f43fc40b0bcd7f173a1f9ee2e97816de80b19\",\n    \"build_timestamp\" : \"2015-07-29T09:54:16Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n```\n\nIndex settings\n\n``` JSON\n{\n  \"test\" : {\n    \"aliases\" : { },\n    \"mappings\" : {\n      \"test\" : {\n        \"properties\" : {\n          \"text\" : {\n            \"type\" : \"string\"\n          }\n        }\n      }\n    },\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1439890619858\",\n        \"uuid\" : \"0OgM42BBSVGjcd_z4mTKxw\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"3\",\n        \"version\" : {\n          \"created\" : \"1070199\"\n        }\n      }\n    },\n    \"warmers\" : { }\n  }\n}\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/132189908","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-132189908","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":132189908,"node_id":"MDEyOklzc3VlQ29tbWVudDEzMjE4OTkwOA==","user":{"login":"lexand","id":927515,"node_id":"MDQ6VXNlcjkyNzUxNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/927515?v=4","gravatar_id":"","url":"https://api.github.com/users/lexand","html_url":"https://github.com/lexand","followers_url":"https://api.github.com/users/lexand/followers","following_url":"https://api.github.com/users/lexand/following{/other_user}","gists_url":"https://api.github.com/users/lexand/gists{/gist_id}","starred_url":"https://api.github.com/users/lexand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lexand/subscriptions","organizations_url":"https://api.github.com/users/lexand/orgs","repos_url":"https://api.github.com/users/lexand/repos","events_url":"https://api.github.com/users/lexand/events{/privacy}","received_events_url":"https://api.github.com/users/lexand/received_events","type":"User","site_admin":false},"created_at":"2015-08-18T12:20:24Z","updated_at":"2015-08-18T12:20:24Z","author_association":"NONE","body":"hmm\n\nbut when I requested scroll with  'scroll' => '1m'\n\n``` PHP\n$res = $client->scroll(['scroll' => '1m', 'scroll_id' => $scrollId]);\n```\n\ni got all 50 documents\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/132257993","html_url":"https://github.com/elastic/elasticsearch/issues/11419#issuecomment-132257993","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11419","id":132257993,"node_id":"MDEyOklzc3VlQ29tbWVudDEzMjI1Nzk5Mw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-08-18T15:56:25Z","updated_at":"2015-08-18T15:56:25Z","author_association":"CONTRIBUTOR","body":"@lexand you are not using the new scroll ID from the previous scroll response.  Please ask questions like these on the forum instead: http://discuss.elastic.co/\n","performed_via_github_app":null}]