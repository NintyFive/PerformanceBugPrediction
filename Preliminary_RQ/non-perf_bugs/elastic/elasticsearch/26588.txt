{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/26588","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26588/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26588/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26588/events","html_url":"https://github.com/elastic/elasticsearch/issues/26588","id":256841790,"node_id":"MDU6SXNzdWUyNTY4NDE3OTA=","number":26588,"title":"Potential memory/cache problem in 5.5.2","user":{"login":"jodylapp","id":16481806,"node_id":"MDQ6VXNlcjE2NDgxODA2","avatar_url":"https://avatars3.githubusercontent.com/u/16481806?v=4","gravatar_id":"","url":"https://api.github.com/users/jodylapp","html_url":"https://github.com/jodylapp","followers_url":"https://api.github.com/users/jodylapp/followers","following_url":"https://api.github.com/users/jodylapp/following{/other_user}","gists_url":"https://api.github.com/users/jodylapp/gists{/gist_id}","starred_url":"https://api.github.com/users/jodylapp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jodylapp/subscriptions","organizations_url":"https://api.github.com/users/jodylapp/orgs","repos_url":"https://api.github.com/users/jodylapp/repos","events_url":"https://api.github.com/users/jodylapp/events{/privacy}","received_events_url":"https://api.github.com/users/jodylapp/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2017-09-11T21:04:28Z","updated_at":"2017-09-11T23:10:09Z","closed_at":"2017-09-11T23:10:09Z","author_association":"NONE","active_lock_reason":null,"body":"<!-- Bug report -->\r\n\r\n**Elasticsearch version** (`bin/elasticsearch --version`): 5.5.2\r\n\r\n**Plugins installed**: [x-pack (basic license)]\r\n\r\n**JVM version** (`java -version`): \r\n\r\n**OS version** (`uname -a` if on a Unix-like system): CentOS 7\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\nDuring periods of ingest using the bulk API (500 docs per request with a max of 5 concurrent requests) we're seeing the memory usage on the 10 ES nodes in our cluster increase slowly, but continually, until the nodes eventually OOM.\r\n\r\n**Steps to reproduce**:\r\n\r\nUnfortunately, I don't have the ability to provide configuration/mappings/etc. due to our elasticsearch instance being on a closed network.  I will do my best to provide as much detail as possible that might be relevant to tracking down this issue.\r\n\r\nDuring these periods of ingest, regular GCs are occurring, but the trough after GC is completed is always higher than before.   Basically, the typical pattern you'd see from a memory leak.   These OOMs occurred consistently and I was able to reproduce several times through multiple cluster restarts.  In trying to track this down today, I let our process run until the nodes hit around 90% memory used and then I stopped the ingest process, basically short circuiting it right before the OOMs.   The memory on all of the nodes flatlined at 90% and didn't decrease, despite no ingest/query activity taking place.\r\n\r\nThe heap on the 10 ES nodes are set to 12gb and we're heavily favoring 'keyword' types for all but a couple of string fields.  There are no aggs or anything going against analyzed fields and our overall fielddata is very low on all the nodes.   In looking at 'fielddata', 'query_cache' and 'request_cache' stats, there is a total of a couple of gb per node -- nowhere near the 12gb configured for the jvm.  I also did a force 'flush' to make sure the translog wasn't somehow going beyond the default 512mb.  That had no effect.\r\n\r\nI suspected there was still some cache somewhere that was causing this, so I used the 'clear cache' API to see if I could free up the ~90% memory that was still being consumed by all of the nodes.  First, I tried _only_ clearing the cache against the indices we created.  That had no effect on the memory usage.  Next I tried clearing using ```/.*/_cache/clear``` (matching against the ES .monitoring*, .tasks, etc.).  That too had no effect.  Finally, I issued a clear cache against all indices using ```/*/_cache/clear```. All of the nodes immediately dropped their memory usage to ~10%.\r\n\r\nAre there any internal elasticsearch indices that are not exposed externally (via ```_cat/indices```, for example) that could be caching without regard to the configured cache limits (all using the defaults)?  We're using x-pack, but not any of the ML or anything else like that (though xpack.ml.enabled was true initially).   Alternatively, are there any other internal cache mechanisms that get cleared when a 'clear all' is submitted?\r\n\r\nAny help on this is appreciated.   Thanks.","closed_by":{"login":"jodylapp","id":16481806,"node_id":"MDQ6VXNlcjE2NDgxODA2","avatar_url":"https://avatars3.githubusercontent.com/u/16481806?v=4","gravatar_id":"","url":"https://api.github.com/users/jodylapp","html_url":"https://github.com/jodylapp","followers_url":"https://api.github.com/users/jodylapp/followers","following_url":"https://api.github.com/users/jodylapp/following{/other_user}","gists_url":"https://api.github.com/users/jodylapp/gists{/gist_id}","starred_url":"https://api.github.com/users/jodylapp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jodylapp/subscriptions","organizations_url":"https://api.github.com/users/jodylapp/orgs","repos_url":"https://api.github.com/users/jodylapp/repos","events_url":"https://api.github.com/users/jodylapp/events{/privacy}","received_events_url":"https://api.github.com/users/jodylapp/received_events","type":"User","site_admin":false},"performed_via_github_app":null}