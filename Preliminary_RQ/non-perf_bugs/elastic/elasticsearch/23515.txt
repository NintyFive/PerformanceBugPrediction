{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/23515","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23515/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23515/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23515/events","html_url":"https://github.com/elastic/elasticsearch/issues/23515","id":212933780,"node_id":"MDU6SXNzdWUyMTI5MzM3ODA=","number":23515,"title":"Setting xpack.security.enabled=false does not stop the master from trying to authenticate with tribe node","user":{"login":"dforste","id":5168898,"node_id":"MDQ6VXNlcjUxNjg4OTg=","avatar_url":"https://avatars2.githubusercontent.com/u/5168898?v=4","gravatar_id":"","url":"https://api.github.com/users/dforste","html_url":"https://github.com/dforste","followers_url":"https://api.github.com/users/dforste/followers","following_url":"https://api.github.com/users/dforste/following{/other_user}","gists_url":"https://api.github.com/users/dforste/gists{/gist_id}","starred_url":"https://api.github.com/users/dforste/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dforste/subscriptions","organizations_url":"https://api.github.com/users/dforste/orgs","repos_url":"https://api.github.com/users/dforste/repos","events_url":"https://api.github.com/users/dforste/events{/privacy}","received_events_url":"https://api.github.com/users/dforste/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2017-03-09T04:40:39Z","updated_at":"2017-03-10T06:35:43Z","closed_at":"2017-03-09T19:32:39Z","author_association":"NONE","active_lock_reason":null,"body":"When running elasticsearch in docker per the documentation with xpack.security.enabled=false tribe nodes connect to the master and are immediately disconnected. \r\n\r\n\r\n\r\n\r\n<!--\r\nIf you are filing a bug report, please remove the below feature\r\nrequest block and provide responses for all of the below items.\r\n-->\r\n\r\n**Elasticsearch version**: \r\n Docker image: docker.elastic.co/elasticsearch/elasticsearch:5.2.2\r\n**Plugins installed**: [x-pack]\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\nExpected tribe node to be able to connect to master. \r\n\r\n**Steps to reproduce**:\r\n 1. This should be the minimal docker-compose.yaml to reproduce: \r\n```\r\nversion: '2'\r\nservices:\r\n  master:\r\n    image: docker.elastic.co/elasticsearch/elasticsearch:5.2.2\r\n    environment:\r\n      - cluster.name=logs.es\r\n      - bootstrap.memory_lock=true\r\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\r\n      - node.data=false\r\n      - node.master=true\r\n      - network.publish_host=172.20.5.193\r\n      - node.name=hostname-master\r\n      - http.compression=true\r\n      - http.port=9200\r\n      - transport.tcp.port=9300\r\n      - transport.bind_host=_eth0_\r\n      - xpack.security.enabled=false\r\n      - xpack.watcher.enabled=false\r\n      - discovery.zen.ping.unicast.hosts=172.20.5.193:9300\r\n    volumes:\r\n    - /app/data/elk/master/data:/usr/share/elasticsearch/data\r\n    - /app/data/elk/master/logs:/usr/share/elasticsearch/logs\r\n    ports:\r\n      - \"9200:9200\"\r\n      - \"9300:9300\"\r\n    ulimits:\r\n      memlock:\r\n        soft: -1\r\n        hard: -1\r\n      nofile:\r\n        soft: 65536\r\n        hard: 65536\r\n    cap_add:\r\n      - IPC_LOCK\r\n\r\n  tribe:\r\n    image: docker.elastic.co/elasticsearch/elasticsearch:5.2.2\r\n    environment:\r\n      - cluster.name=logs.es\r\n      - bootstrap.memory_lock=true\r\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\r\n      - network.publish_host=172.20.5.193\r\n      - node.name=hostname-tribe\r\n      - http.compression=true\r\n      - http.port=9201\r\n      - transport.tcp.port=9301\r\n      - transport.bind_host=_eth0_\r\n      - tribe.cluster1.cluster.name=logs.es\r\n      - tribe.cluster1.discovery.zen.ping.unicast.hosts=172.20.5.193:9300\r\n      - tribe.cluster1.transport.tcp.port=9302\r\n      - xpack.security.enabled=false\r\n      - xpack.watcher.enabled=false\r\n    volumes:\r\n      - /app/data/elk/tribe/data:/usr/share/elasticsearch/data\r\n      - /app/data/elk/tribe/logs:/usr/share/elasticsearch/logs\r\n    ports:\r\n      - \"9201:9201\"\r\n      - \"9301:9301\"\r\n      - \"9302:9302\"\r\n    ulimits:\r\n      memlock:\r\n        soft: -1\r\n        hard: -1\r\n      nofile:\r\n        soft: 65536\r\n        hard: 65536\r\n    cap_add:\r\n      - IPC_LOCK\r\n```\r\n\r\n**Provide logs (if relevant)**:\r\nLogs from the master showing the issue: \r\n```[2017-03-09T01:54:52,906][INFO ][o.e.c.s.ClusterService   ] [hostname-master] removed {{hostname-tribe/cluster1}{1ME8L_ejRwmC1ySEt6aZLw}{XJGFTz1uSk-hwHN9kxSvDg}{172.20.5.193}{172.20.5.193:9302},}, reason: zen-disco-node-failed({hostname-tribe/cluster1}{1ME8L_ejRwmC1ySEt6aZLw}{XJGFTz1uSk-hwHN9kxSvDg}{172.20.5.193}{172.20.5.193:9302}), reason(failed to ping, tried [3] times, each with maximum [30s] timeout)[{hostname-tribe/cluster1}{1ME8L_ejRwmC1ySEt6aZLw}{XJGFTz1uSk-hwHN9kxSvDg}{172.20.5.193}{172.20.5.193:9302} failed to ping, tried [3] times, each with maximum [30s] timeout]\r\n[2017-03-09T01:54:52,941][INFO ][o.e.c.s.ClusterService   ] [hostname-master] removed {{hostname-tribe/cluster1}{3qO9HNDCT3mFj2dnnuuDsg}{JLFzoCPmS8KjrSi1Skxq0w}{172.20.5.193}{172.20.5.193:9302},}, reason: zen-disco-node-failed({hostname-tribe/cluster1}{3qO9HNDCT3mFj2dnnuuDsg}{JLFzoCPmS8KjrSi1Skxq0w}{172.20.5.193}{172.20.5.193:9302}), reason(failed to ping, tried [3] times, each with maximum [30s] timeout)[{hostname-tribe/cluster1}{3qO9HNDCT3mFj2dnnuuDsg}{JLFzoCPmS8KjrSi1Skxq0w}{172.20.5.193}{172.20.5.193:9302} failed to ping, tried [3] times, each with maximum [30s] timeout], zen-disco-node-failed({hostname-tribe/cluster1}{1ME8L_ejRwmC1ySEt6aZLw}{XJGFTz1uSk-hwHN9kxSvDg}{172.20.5.195}{172.20.5.193:9302}), reason(failed to ping, tried [3] times, each with maximum [30s] timeout)[{hostname-tribe/cluster1}{1ME8L_ejRwmC1ySEt6aZLw}{XJGFTz1uSk-hwHN9kxSvDg}{172.20.5.193}{172.20.5.193:9302} failed to ping, tried [3] times, each with maximum [30s] timeout]\r\n[2017-03-09T01:54:52,946][WARN ][o.e.a.a.c.n.i.TransportNodesInfoAction] [hostname-master] not accumulating exceptions, excluding exception from response\r\norg.elasticsearch.action.FailedNodeException: Failed node [3qO9HNDCT3mFj2dnnuuDsg]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.onFailure(TransportNodesAction.java:247) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$300(TransportNodesAction.java:160) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction$1.handleException(TransportNodesAction.java:219) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1024) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.transport.TcpTransport.lambda$handleException$17(TcpTransport.java:1411) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.common.util.concurrent.EsExecutors$1.execute(EsExecutors.java:109) [elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.transport.TcpTransport.handleException(TcpTransport.java:1409) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.transport.TcpTransport.handlerResponseError(TcpTransport.java:1401) [elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1345) [elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:74) [transport-netty4-5.2.2.jar:5.2.2]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:341) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293) [netty-codec-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:280) [netty-codec-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:396) [netty-codec-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248) [netty-codec-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:341) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:642) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:527) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:481) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:441) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.7.Final.jar:4.1.7.Final]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_92-internal]\r\nCaused by: org.elasticsearch.transport.RemoteTransportException: [hostname-tribe/cluster1][172.17.0.4:9302][cluster:monitor/nodes/info[n]]\r\nCaused by: org.elasticsearch.ElasticsearchSecurityException: failed to authenticate user [elastic]\r\n        at org.elasticsearch.xpack.security.support.Exceptions.authenticationError(Exceptions.java:39) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.lambda$null$2(ReservedRealm.java:95) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.lambda$doAuthenticate$3(ReservedRealm.java:99) ~[?:?]\r\n        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:56) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.getUserInfo(ReservedRealm.java:193) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.doAuthenticate(ReservedRealm.java:78) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.doAuthenticateAndCache(CachingUsernamePasswordRealm.java:139) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:106) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:92) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$9(AuthenticationService.java:253) ~[?:?]\r\n        at org.elasticsearch.xpack.common.IteratingActionListener.run(IteratingActionListener.java:58) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeToken(AuthenticationService.java:272) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$extractToken$6(AuthenticationService.java:228) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.extractToken(AuthenticationService.java:236) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$0(AuthenticationService.java:184) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$2(AuthenticationService.java:201) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:213) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:180) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:142) ~[?:?]\r\n        at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:114) ~[?:?]\r\n        at org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.inbound(ServerTransportFilter.java:142) ~[?:?]\r\n        at org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:296) ~[?:?]\r\n        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.transport.TcpTransport$RequestHandler.doRun(TcpTransport.java:1488) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:596) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.2.2.jar:5.2.2]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_92-internal]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_92-internal]\r\n        ... 1 more\r\n```\r\n\r\n","closed_by":{"login":"dforste","id":5168898,"node_id":"MDQ6VXNlcjUxNjg4OTg=","avatar_url":"https://avatars2.githubusercontent.com/u/5168898?v=4","gravatar_id":"","url":"https://api.github.com/users/dforste","html_url":"https://github.com/dforste","followers_url":"https://api.github.com/users/dforste/followers","following_url":"https://api.github.com/users/dforste/following{/other_user}","gists_url":"https://api.github.com/users/dforste/gists{/gist_id}","starred_url":"https://api.github.com/users/dforste/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dforste/subscriptions","organizations_url":"https://api.github.com/users/dforste/orgs","repos_url":"https://api.github.com/users/dforste/repos","events_url":"https://api.github.com/users/dforste/events{/privacy}","received_events_url":"https://api.github.com/users/dforste/received_events","type":"User","site_admin":false},"performed_via_github_app":null}