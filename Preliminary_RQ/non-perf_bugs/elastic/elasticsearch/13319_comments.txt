[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/142113357","html_url":"https://github.com/elastic/elasticsearch/issues/13319#issuecomment-142113357","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13319","id":142113357,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MjExMzM1Nw==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2015-09-21T21:27:58Z","updated_at":"2015-09-21T21:27:58Z","author_association":"CONTRIBUTOR","body":"I theorize that the trouble was [an assert in IndexShard](https://github.com/elastic/elasticsearch/commit/f216d92d19cf2f12fbc460c7aef66f38998ef9fd#diff-3d9da880b4a90ac1e9545d4039d5c4e6L257) and I added the assert back. Then I ran a random test, `CommonTermsQueryParserTests`, and it took 30 seconds for it to fail. It timed out waiting for the cluster to go green.\n\n@rmuir , is that what you were seeing?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/142116975","html_url":"https://github.com/elastic/elasticsearch/issues/13319#issuecomment-142116975","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13319","id":142116975,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MjExNjk3NQ==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-09-21T21:38:42Z","updated_at":"2015-09-21T21:38:42Z","author_association":"CONTRIBUTOR","body":"yeah thats it. and you see that timed out failure versus the assert. so when those asserts go in \"remove this on upgrade to lucene X\", they do not have the desired impact.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/142122223","html_url":"https://github.com/elastic/elasticsearch/issues/13319#issuecomment-142122223","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13319","id":142122223,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MjEyMjIyMw==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2015-09-21T21:58:46Z","updated_at":"2015-09-21T21:58:46Z","author_association":"CONTRIBUTOR","body":"Part of the problem is that we've set `es.logger.level` to `ERROR` but the error is being logged at `WARN`:\n\n```\n  1> [2015-09-21 17:56:04,262][WARN ][indices.cluster          ] [node_s_0] [[test-index][0]] marking and sending shard failed due to [failed to create shard]\n  1> [test-index][[test-index][0]] ElasticsearchException[failed to create shard]; nested: AssertionError;\n  1>    at org.elasticsearch.index.IndexService.createShard(IndexService.java:376)\n  1>    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:643)\n  1>    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:543)\n  1>    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:178)\n  1>    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:496)\n  1>    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\n  1>    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\n  1>    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n  1>    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n  1>    at java.lang.Thread.run(Thread.java:745)\n  1> Caused by: java.lang.AssertionError\n  1>    at org.elasticsearch.index.shard.IndexShard.<init>(IndexShard.java:250)\n  1>    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  1>    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n  1>    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  1>    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n  1>    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)\n  1>    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\n  1>    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:110)\n  1>    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)\n  1>    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)\n  1>    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)\n  1>    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)\n  1>    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)\n  1>    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)\n  1>    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)\n  1>    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)\n  1>    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)\n  1>    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)\n  1>    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)\n  1>    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:162)\n  1>    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:55)\n  1>    at org.elasticsearch.index.IndexService.createShard(IndexService.java:374)\n  1>    ... 9 more\n```\n\nIs there a good reason not to set `es.logger.level` to `WARN`?\n\nThe other issue I see is that we wait 30 seconds before giving up on the cluster ever getting into the desired state.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/142129026","html_url":"https://github.com/elastic/elasticsearch/issues/13319#issuecomment-142129026","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13319","id":142129026,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MjEyOTAyNg==","user":{"login":"rjernst","id":289412,"node_id":"MDQ6VXNlcjI4OTQxMg==","avatar_url":"https://avatars3.githubusercontent.com/u/289412?v=4","gravatar_id":"","url":"https://api.github.com/users/rjernst","html_url":"https://github.com/rjernst","followers_url":"https://api.github.com/users/rjernst/followers","following_url":"https://api.github.com/users/rjernst/following{/other_user}","gists_url":"https://api.github.com/users/rjernst/gists{/gist_id}","starred_url":"https://api.github.com/users/rjernst/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rjernst/subscriptions","organizations_url":"https://api.github.com/users/rjernst/orgs","repos_url":"https://api.github.com/users/rjernst/repos","events_url":"https://api.github.com/users/rjernst/events{/privacy}","received_events_url":"https://api.github.com/users/rjernst/received_events","type":"User","site_admin":false},"created_at":"2015-09-21T22:37:50Z","updated_at":"2015-09-21T22:37:50Z","author_association":"MEMBER","body":"I think the problem is one of delayed action. AFAIK, \"creating an index\" starts by just putting something in the cluster state that says \"there is a new uninitialized index\" and returning. Then the master goes off and does allocation and such, and fails. In lieu of fixing the real issue (synchronous index creation), it seems like there the cluster health with waitFor request should fail once it is impossible to reach that state. So eg the index creation fails, it should return with a failure right away, not continue waiting for the timeout.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/142130584","html_url":"https://github.com/elastic/elasticsearch/issues/13319#issuecomment-142130584","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13319","id":142130584,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MjEzMDU4NA==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2015-09-21T22:48:12Z","updated_at":"2015-09-21T22:48:12Z","author_association":"CONTRIBUTOR","body":"> I think the problem is one of delayed action. AFAIK, \"creating an index\" starts by just putting something in the cluster state that says \"there is a new uninitialized index\" and returning. Then the master goes off and does allocation and such, and fails. In lieu of fixing the real issue (synchronous index creation), it seems like there the cluster health with waitFor request should fail once it is impossible to reach that state. So eg the index creation fails, it should return with a failure right away, not continue waiting for the timeout.\n\nI believe your assessment is correct. I started debugging around this afternoon looking for something to put a watch on.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/142207830","html_url":"https://github.com/elastic/elasticsearch/issues/13319#issuecomment-142207830","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13319","id":142207830,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MjIwNzgzMA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-09-22T08:06:57Z","updated_at":"2015-09-22T08:06:57Z","author_association":"MEMBER","body":"> Then the master goes off and does allocation and such, and fails.\n\nSmall correction - the master doesn't fail to allocate (typically), but rather the node that receives the cluster state and goes and allocates the shard fails in creating it and notifies the master about it. This is where the asynchronicity  comes from.\n\n> it seems like there the cluster health with waitFor request should fail once it is impossible to reach that state\n\nThis is trickier than it seems - how do you define that it is impossible to reach a state in  30s? maybe there is a network hick up that caused a node to drop off the cluster and it will be back? Maybe , like in our tests, we are busy forming a cluster and the nodes will join in 10 seconds?\n\nThe problem with this issue is we currently don't keep track of all the places we tried to create a shard and it failed. The master sees the shard fails message from a node, and tries another. If that fails it may go back to the first one. If you only have one node, the master will first clean the shard by sending the node a state where the shard is not assigned to it and then will try again. We should find a place to keep track of this and be able to say \"we tried all options so far, so no point in trying again\".  Perhaps the new unassigned info can be extended to keep \"tried and failed\" node list, which will be cleared upon successful allocation (because unassigned info goes away) and will be used to stop this infinite loop.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/142293179","html_url":"https://github.com/elastic/elasticsearch/issues/13319#issuecomment-142293179","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13319","id":142293179,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MjI5MzE3OQ==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2015-09-22T13:43:26Z","updated_at":"2015-09-22T13:43:26Z","author_association":"CONTRIBUTOR","body":"> The problem with this issue is we currently don't keep track of all the places we tried to create a shard and it failed. The master sees the shard fails message from a node, and tries another. If that fails it may go back to the first one. If you only have one node, the master will first clean the shard by sending the node a state where the shard is not assigned to it and then will try again. We should find a place to keep track of this and be able to say \"we tried all options so far, so no point in trying again\". Perhaps the new unassigned info can be extended to keep \"tried and failed\" node list, which will be cleared upon successful allocation (because unassigned info goes away) and will be used to stop this infinite loop.\n\nAt that point you could indeed \"give up\" on things - so long as you knew that you weren't going to add new nodes to the cluster which would give it more chances. But something like this seems like a long and windy journey. A journey with greatness at the end, for sure.\n\nMinimally I think we should make sure that the actual failure is logged, even if we can't timeout eagerly. Could we set `es.logger.level` to `WARN` or even `INFO` for the tests? I liked the old days better, when tests were noisy and we used randomized testing to keep them quiet if they passed but they spit out a kajillion lines of useful logs if they failed.\n\nWhen we timeout waiting for the cluster to go green can we attach the last hand full of errors as \"potential causes\"?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/143646403","html_url":"https://github.com/elastic/elasticsearch/issues/13319#issuecomment-143646403","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13319","id":143646403,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MzY0NjQwMw==","user":{"login":"rjernst","id":289412,"node_id":"MDQ6VXNlcjI4OTQxMg==","avatar_url":"https://avatars3.githubusercontent.com/u/289412?v=4","gravatar_id":"","url":"https://api.github.com/users/rjernst","html_url":"https://github.com/rjernst","followers_url":"https://api.github.com/users/rjernst/followers","following_url":"https://api.github.com/users/rjernst/following{/other_user}","gists_url":"https://api.github.com/users/rjernst/gists{/gist_id}","starred_url":"https://api.github.com/users/rjernst/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rjernst/subscriptions","organizations_url":"https://api.github.com/users/rjernst/orgs","repos_url":"https://api.github.com/users/rjernst/repos","events_url":"https://api.github.com/users/rjernst/events{/privacy}","received_events_url":"https://api.github.com/users/rjernst/received_events","type":"User","site_admin":false},"created_at":"2015-09-28T05:58:25Z","updated_at":"2015-09-28T05:58:25Z","author_association":"MEMBER","body":"IMO definitely not `INFO`. We moved away from that because it is way too verbose when running tests. When tests fail, you want to see the failure. I would be ok with `WARN`. I have the gradle branch set to this, because I found there are some errors sometimes emitted as a warning (not sure why...we should fix that too but I've already forgot what the error was...).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/155716770","html_url":"https://github.com/elastic/elasticsearch/issues/13319#issuecomment-155716770","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13319","id":155716770,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NTcxNjc3MA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-11-11T09:39:22Z","updated_at":"2015-11-11T09:39:22Z","author_association":"CONTRIBUTOR","body":"I added only the assertion back in master and ran `IndexShardTests.java` with this output:\n\n```\n[2015-11-11 10:36:26,734][INFO ][org.elasticsearch.index.shard] --> idxPath: [/private/var/folders/qj/rsr2js6n275f3r88r1z5bbgw0000gn/T/org.elasticsearch.index.shard.IndexShardTests_A92392023D5F82E0-001/OibDuggcVx]\n[2015-11-11 10:36:26,940][INFO ][org.elasticsearch.cluster.metadata] [node_s_0] [test] creating index, cause [api], templates [], shards [1]/[0], mappings []\n[2015-11-11 10:36:27,042][WARN ][org.elasticsearch.indices.cluster] [node_s_0] [[test][0]] marking and sending shard failed due to [failed to create shard]\njava.lang.AssertionError\n    at org.elasticsearch.index.shard.IndexShard.<init>(IndexShard.java:264)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:279)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:627)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:527)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:184)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:517)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-11-11 10:36:27,121][WARN ][org.elasticsearch.cluster.action.shard] [node_s_0] [test][0] received shard failed for [test][0], node[tTPyckqMRDaf8clvc9SzOA], [P], v[1], s[INITIALIZING], a[id=OlU9n_QSS9OXBvk_TgNzeA], unassigned_info[[reason=INDEX_CREATED], at[2015-11-11T09:36:26.943Z]], indexUUID [6qDGq_IkToCsYtCvoKMWUg], message [failed to create shard], failure [AssertionError[null]]\njava.lang.AssertionError\n    at org.elasticsearch.index.shard.IndexShard.<init>(IndexShard.java:264)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:279)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:627)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:527)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:184)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:517)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\n\nyou will now see this in master once per shard but after 30 sec the test times out and all is well. This is purely caused by guice which is gone in master. I am closing this\n","performed_via_github_app":null}]