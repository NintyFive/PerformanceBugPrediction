[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/296968406","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-296968406","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":296968406,"node_id":"MDEyOklzc3VlQ29tbWVudDI5Njk2ODQwNg==","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T09:10:52Z","updated_at":"2017-04-25T09:10:52Z","author_association":"MEMBER","body":"I don't think that crashing the node is the best approach when there is no space left on device. The node can still serve read requests and if indices are removed/relocated (or in this case old log files are removed) then there should be sufficient space to handle write requests.\r\n\r\nIn case of log files filling up, maybe ES should try to prevent this by capping the size of the log file, for example like is done for deprecation logging (with log4j's SizeBasedTriggeringPolicy)?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/296970406","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-296970406","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":296970406,"node_id":"MDEyOklzc3VlQ29tbWVudDI5Njk3MDQwNg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T09:18:54Z","updated_at":"2017-04-25T09:18:54Z","author_association":"CONTRIBUTOR","body":"Also, a disk might fill up because of a big merge.  As soon as that fails, disk space could be freed up again allowing the node to continue working.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/296981426","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-296981426","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":296981426,"node_id":"MDEyOklzc3VlQ29tbWVudDI5Njk4MTQyNg==","user":{"login":"khionu","id":11195266,"node_id":"MDQ6VXNlcjExMTk1MjY2","avatar_url":"https://avatars2.githubusercontent.com/u/11195266?v=4","gravatar_id":"","url":"https://api.github.com/users/khionu","html_url":"https://github.com/khionu","followers_url":"https://api.github.com/users/khionu/followers","following_url":"https://api.github.com/users/khionu/following{/other_user}","gists_url":"https://api.github.com/users/khionu/gists{/gist_id}","starred_url":"https://api.github.com/users/khionu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/khionu/subscriptions","organizations_url":"https://api.github.com/users/khionu/orgs","repos_url":"https://api.github.com/users/khionu/repos","events_url":"https://api.github.com/users/khionu/events{/privacy}","received_events_url":"https://api.github.com/users/khionu/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T09:58:18Z","updated_at":"2017-04-25T09:58:18Z","author_association":"NONE","body":"In that case, it might be best to put a hold on the operations that are unable to complete.\r\n\r\nA queue, that will retry at a much lower rate, until operations are successful, would appease the concerns of being available for read and waiting for disk to be freed, without causing undue spam. For an exception that requires manual intervention, a retry rate of once a minute would be acceptable, in my opinion.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297006232","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297006232","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297006232,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzAwNjIzMg==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T11:55:18Z","updated_at":"2017-04-25T11:56:01Z","author_association":"CONTRIBUTOR","body":"I agree killing the node might not be the right thing here from a 10k feet view while I can see it as a viable solution for some use - cases. Out of the box I'd like us to rather detect that we are close to full disk and then simply stop write operations like merging and indexing. Yet, this still has issues since if we have a replica on the node we'd reject a write which would bring the replica out of sync in that moment. Disk-threshold allocation deciders should help here moving stuff away from the node but essentially killing it and leave the cluster would be the right thing to do here IMO. If we wanna do that we need to somehow add corresponding handlers in many places or we start with adding it into the engine and allow folks to opt out of it? I generally think we should kill nodes more-often in disaster situations instead of just sitting there and wait for more disaster to happen.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297061924","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297061924","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297061924,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzA2MTkyNA==","user":{"login":"nullpixel","id":6704113,"node_id":"MDQ6VXNlcjY3MDQxMTM=","avatar_url":"https://avatars2.githubusercontent.com/u/6704113?v=4","gravatar_id":"","url":"https://api.github.com/users/nullpixel","html_url":"https://github.com/nullpixel","followers_url":"https://api.github.com/users/nullpixel/followers","following_url":"https://api.github.com/users/nullpixel/following{/other_user}","gists_url":"https://api.github.com/users/nullpixel/gists{/gist_id}","starred_url":"https://api.github.com/users/nullpixel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nullpixel/subscriptions","organizations_url":"https://api.github.com/users/nullpixel/orgs","repos_url":"https://api.github.com/users/nullpixel/repos","events_url":"https://api.github.com/users/nullpixel/events{/privacy}","received_events_url":"https://api.github.com/users/nullpixel/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T15:11:10Z","updated_at":"2017-04-25T15:11:10Z","author_association":"NONE","body":"Yeah, so you cannot write to the disk if it's full, but spamming the logs isn't the way to go.\r\n\r\nCould do a \"read only\" style mode where it just says it went into read only because of no disk space. there's no need to spam the logs; even with a cap","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297062911","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297062911","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297062911,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzA2MjkxMQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T15:14:11Z","updated_at":"2017-04-25T15:14:11Z","author_association":"MEMBER","body":"> Yeah, so you cannot write to the disk if it's full, but spamming the logs isn't the way to go.\r\n\r\nThis is not necessarily true, the logs could (and should!) be on a separate mount, they could have log rotation applied to them, etc.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297063850","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297063850","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297063850,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzA2Mzg1MA==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T15:16:42Z","updated_at":"2017-04-25T15:16:42Z","author_association":"MEMBER","body":"> I generally think we should kill nodes more-often in disaster situations instead of just sitting there and wait for more disaster to happen.\r\n\r\nI agree but I'm unsure if disk-full qualifies as such a disaster situation since it's possible to recover.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297066439","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297066439","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297066439,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzA2NjQzOQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T15:23:29Z","updated_at":"2017-04-25T15:24:46Z","author_association":"MEMBER","body":"> Why not make it leave a **single message** in the logs, then just **crash**? As the disk is full, the database cannot run anyway, so rather than spamming logs, just adapt a simpler approach.\r\n\r\nIn a concurrent server application, there are likely many disk operations in flight, expecting a single message is not realistic. As others have mentioned, this situation is not completely fatal, and can be recovered from so crashing on disk-full should not be a first option.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297113487","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297113487","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297113487,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzExMzQ4Nw==","user":{"login":"khionu","id":11195266,"node_id":"MDQ6VXNlcjExMTk1MjY2","avatar_url":"https://avatars2.githubusercontent.com/u/11195266?v=4","gravatar_id":"","url":"https://api.github.com/users/khionu","html_url":"https://github.com/khionu","followers_url":"https://api.github.com/users/khionu/followers","following_url":"https://api.github.com/users/khionu/following{/other_user}","gists_url":"https://api.github.com/users/khionu/gists{/gist_id}","starred_url":"https://api.github.com/users/khionu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/khionu/subscriptions","organizations_url":"https://api.github.com/users/khionu/orgs","repos_url":"https://api.github.com/users/khionu/repos","events_url":"https://api.github.com/users/khionu/events{/privacy}","received_events_url":"https://api.github.com/users/khionu/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T17:57:27Z","updated_at":"2017-04-25T17:57:27Z","author_association":"NONE","body":"One option is to provide a few behaviors for the host to pick from. Fatal crash, start refusing writes, or something else.\r\n\r\nSomething else to be considered is maybe preventative? It should be reasonable to check the space remaining daily, and if it goes below, say, 5%, log a severe warning.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297633330","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297633330","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297633330,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzYzMzMzMA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-04-27T07:23:53Z","updated_at":"2017-04-27T07:23:53Z","author_association":"CONTRIBUTOR","body":"@khionu just out of curiosity did you look into [this](https://www.elastic.co/guide/en/elasticsearch/reference/current/disk-allocator.html) ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297634215","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297634215","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297634215,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzYzNDIxNQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-04-27T07:28:33Z","updated_at":"2017-04-27T07:28:33Z","author_association":"CONTRIBUTOR","body":"> I agree but I'm unsure if disk-full qualifies as such a disaster situation since it's possible to recover.\r\n\r\nthe question is what the recovery path is? Most likely we need to relocate shards but shouldn't the disk-threshold decider have taken care of this already? Once we are like at 99% there is not much we can do but failing? it's likely the most healthy option here, it tells the cluster to heal itself by allocating shards on other nodes, it notifies the users since a node died. The log message might be clear and we can refuse to start up until we have at lest 5% disk back? I kind of like this option the more I think about it.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297730440","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297730440","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297730440,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzczMDQ0MA==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-04-27T14:30:03Z","updated_at":"2017-04-27T14:30:03Z","author_association":"MEMBER","body":"@s1monw You're starting to convince me. Here's another thought has occurred to me: if we keep the node alive, I don't think there's a lot that we should or can do (without a lot of jumping through hoops) about the disk-full log messages, so those are going to keep pumping out. If those log messages are being sent to a remote monitoring cluster, the disks on nodes on the remote monitoring cluster could be overwhelmed too and now you have two problems (remote denial of service on the monitoring cluster). This is an argument for dying.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297734410","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297734410","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297734410,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzczNDQxMA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-04-27T14:43:32Z","updated_at":"2017-04-27T14:43:32Z","author_association":"CONTRIBUTOR","body":"I'd like to hear what @rjernst and @nik9000 are thinking about this","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297739367","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297739367","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297739367,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzczOTM2Nw==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-04-27T14:59:22Z","updated_at":"2017-04-27T14:59:22Z","author_association":"MEMBER","body":"Another thing to consider with respect to dying is that operators are going to have their nodes set to auto-restart (we encourage this because of dying with dignity). If we fail startup when the disk is full, as we should if we proceed with dying when the disk is full, we will end up in an infinite retry loop that will also be spamming logs and we haven't solved anything. I discussed this concern with @s1monw and we discussed the idea of a marker file to track the number of retries and simply not start at all if that marker file is present and the count exceeds some amount. At this point, manual intervention is required but it already is for disk full anyway.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297749976","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-297749976","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":297749976,"node_id":"MDEyOklzc3VlQ29tbWVudDI5Nzc0OTk3Ng==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-04-27T15:31:14Z","updated_at":"2017-04-27T15:31:14Z","author_association":"CONTRIBUTOR","body":"> I'd like to hear what @rjernst and @nik9000 are thinking about this\r\n\r\nLots of things.\r\n\r\n1. What if you only have a single copy of the shard and the node kills itself because it ran out of space? I mean having only a single copy is asking for trouble, but the problem still stands with multiple copies. It is just less common.\r\n2. If a primary fails to write to a replica because it ran out of space then it'll fail the replica, freeing up the space that the replica was taking up pretty quick. I could see this being a transient problem if you are running really close to the edge. Shooting the node seems drastic in these situations because it'll have disk to work with soon.\r\n3. Why don't we roll the log files based on size? If we did that at least we'd limit the amount of disk we consume in this case. We'd still chew through IOPS but still.\r\n4. I feel like we should do more indoctrination about running separate mount points for the data directory and everything else. It is fairly uncommon these days to separate `/var/log` and `/` but I've never run Elasticsearch in production without `/var/lib/elasticsearch` being on a separate volume.\r\n5. I feel like if disk is full it'd be nice if the node stuck around so other nodes could recover from it.\r\n6. I'm fairly concerned that killing the node will make the problem worse for the rest of the cluster which will have to recover the copies of the shard somewhere else which will fill the disk space of other nodes. At least, it'll push them as close to the edge as the disk allocation decider allows them to go.\r\n\r\nI'm sure I'll think of more things. I don't really like the idea of shooting the node if it runs out of disk space. I just have a gut feeling about it more than all the stuff I wrote.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298115762","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-298115762","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":298115762,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODExNTc2Mg==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-04-28T21:55:40Z","updated_at":"2017-04-28T21:55:40Z","author_association":"CONTRIBUTOR","body":"> What if you only have a single copy of the shard and the node kills itself because it ran out of space? I mean having only a single copy is asking for trouble, but the problem still stands with multiple copies. It is just less common.\r\n\r\nthis one is interesting. if you have only one copy it will be unavailable until the node has enough disk to recover. But you won't loose data, it's still there. If you have a copy the cluster will try to allocate it elsewhere and we slowly heal the cluster or the node comes back quickly with more disk space. I think dropping out is a good option IMO?\r\n\r\n> What if you only have a single copy of the shard and the node kills itself because it ran out of space? I mean having only a single copy is asking for trouble, but the problem still stands with multiple copies. It is just less common.\r\n\r\nthe same goes for OOM or any other fatal/non-recoverable error. the question is do we treat disk full as non-recoverable. IMO yes, we won't recover from it, the cluster will be in trouble anyhow.\r\n\r\n> If a primary fails to write to a replica because it ran out of space then it'll fail the replica, freeing up the space that the replica was taking up pretty quick. I could see this being a transient problem if you are running really close to the edge. Shooting the node seems drastic in these situations because it'll have disk to work with soon.\r\n\r\nthis is not true - we keep the data on disk until we allocated the replica on another node. this can take very long\r\n\r\n> Why don't we roll the log files based on size? If we did that at least we'd limit the amount of disk we consume in this case. We'd still chew through IOPS but still.\r\n\r\nthis is a different problem which I agree we should tackle but it's unrelated to out of disk IMO\r\n\r\n> I feel like if disk is full it'd be nice if the node stuck around so other nodes could recover from it.\r\n\r\nwe don't have this option unless we switch all indices allocated on it read-only? that is pretty drastic and very error prone","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298361847","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-298361847","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":298361847,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODM2MTg0Nw==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-05-01T16:01:59Z","updated_at":"2017-05-01T16:01:59Z","author_association":"CONTRIBUTOR","body":"> this is not true - we keep the data on disk until we allocated the replica on another node. this can take very long\r\n\r\nRight. I take this point back.\r\n\r\n\r\nPersonally I don't think nodes should kill themselves but I'm aware that is asking too much. There are unrecoverable things OOMs and other bugs. We work hard to prevent these and they break things in subtle ways when they hit.\r\n\r\nIf running out of disk is truly as unrecoverable as a Java OOM then we should kill the node but we need to open intensive efforts to make sure that it doesn't happen. Like we did with the circuit breakers. The disk allocation stuff doesn't look like it is enough.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298649195","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-298649195","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":298649195,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODY0OTE5NQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-05-02T14:18:44Z","updated_at":"2017-05-02T14:18:44Z","author_association":"CONTRIBUTOR","body":"> If running out of disk is truly as unrecoverable as a Java OOM then we should kill the node but we need to open intensive efforts to make sure that it doesn't happen. Like we did with the circuit breakers. The disk allocation stuff doesn't look like it is enough.\r\n\r\nagreed we should think and try harder to make it less likely to get to this point. One thought I was playing with was to tell the primary that the replica has not enough space left and if so we can reject subsequent writes. Such and information can be transported back to the primary with the replica write responses. Once the primary is in such a state it will stay there until the replica tells it's primary it's in a good shape again so we can continue indexing. I really think we should push back to the user is stuff like this happens and we need to give the nodes some time to move shards away which is sometimes not possible due to allocation deciders or no space on other nodes. In such a case we can only reject writes.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298649657","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-298649657","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":298649657,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODY0OTY1Nw==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-05-02T14:20:17Z","updated_at":"2017-05-02T14:20:17Z","author_association":"CONTRIBUTOR","body":"> agreed we should think and try harder to make it less likely to get to this point. One thought I was playing with was to tell the primary that the replica has not enough space left and if so we can reject subsequent writes. Such and information can be transported back to the primary with the replica write responses. Once the primary is in such a state it will stay there until the replica tells it's primary it's in a good shape again so we can continue indexing. I really think we should push back to the user is stuff like this happens and we need to give the nodes some time to move shards away which is sometimes not possible due to allocation deciders or no space on other nodes. In such a case we can only reject writes.\r\n\r\n+1","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298654940","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-298654940","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":298654940,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODY1NDk0MA==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-05-02T14:38:20Z","updated_at":"2017-05-02T14:38:20Z","author_association":"CONTRIBUTOR","body":"@s1monw, if there is space between when we start moving shards off of a node and when we reject writes then this can be a backstop. I guess also we'd want the primary to have this behavior too, right?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298656310","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-298656310","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":298656310,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODY1NjMxMA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-05-02T14:42:58Z","updated_at":"2017-05-02T14:42:58Z","author_association":"CONTRIBUTOR","body":"@nik9000 yes so we should accept all writes on replicas all the time we just need to prevent the primary to send them. So yes, the primary should also have such a flag","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298659023","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-298659023","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":298659023,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODY1OTAyMw==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-05-02T14:51:58Z","updated_at":"2017-05-02T14:51:58Z","author_association":"CONTRIBUTOR","body":"Ah! Now I get it.\r\n\r\nFor those of you like me that don't get it at first: the replication model in Elasticsearch dictates that if a primary has accepted a write but the replica rejects it then the replica must fail. Once failed the replica has to recover from the primary which is a fairly heavy operation. So replicas must do their best not to fail. In this context that means that the replica should absorb the write even though it is running out of space but it should tell the primary to reject further writes. We can tuck the \"help I'm running out of space\" flag into the write response.\r\n\r\nSo we have to set the disk space percent quite a bit before we get full because it is super-asynchronous.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298660431","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-298660431","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":298660431,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODY2MDQzMQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-05-02T14:56:33Z","updated_at":"2017-05-02T14:56:33Z","author_association":"CONTRIBUTOR","body":"@nik9000 yes that is what I meant... thanks for explaining it in other words again.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298880570","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-298880570","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":298880570,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODg4MDU3MA==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-05-03T10:59:35Z","updated_at":"2017-05-03T10:59:35Z","author_association":"MEMBER","body":"A concern I have: consider a homogenous cluster with well-sharded data (these are not unreasonable assumptions). If one node is running low on disk space, then they are all running low on disk space. Killing the first node to run out of disk space will lead to recoveries on the other nodes in the cluster exacerbating their low disk issues. Shooting a node can lead to a cluster-wide outage.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/299137012","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-299137012","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":299137012,"node_id":"MDEyOklzc3VlQ29tbWVudDI5OTEzNzAxMg==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-05-04T09:28:02Z","updated_at":"2017-05-04T09:28:02Z","author_association":"CONTRIBUTOR","body":"> A concern I have: consider a homogenous cluster with well-sharded data (these are not unreasonable assumptions). If one node is running low on disk space, then they are all running low on disk space. Killing the first node to run out of disk space will lead to recoveries on the other nodes in the cluster exacerbating their low disk issues. Shooting a node can lead to a cluster-wide outage.\r\n\r\nwe spoke about this yesterday in a meeting but I want to add my response here anyway for completeness. I think in such a situation the watermarks will protect us since if a node is already high on disk usage we will not allocate shards on it. We also have a good notion of how big shards are for relocation so we can make good decisions here. That is not absolutely water tight but I think we are ok along those lines.\r\n\r\nWe also spoke about a possible solution to the problem of continuing indexing when a node is under pressure disk space wise. The plan to tackle this issue is to introduce a new kind of index level cluster block that will be set automatically on all indices that have at least one shard on a node that is above the _flood_stage_ (which is another setting that is set to `95%` disk utilization by default). The master is currently monitoring disk usage of all nodes with a refresh interval of 30 seconds. The new cluster block will prevent indexing / updating but only for operations with `Engine.Operation.Origin.PRIMARY` such that we never get in the way of replica requests etc. The user will still be able to delete indices to make room on tight nodes with this cluster block but elasticsearch will not make any effort to move away from the block. This has to happen based on user actions ie. the user must remove the block from the indices. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/299195437","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-299195437","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":299195437,"node_id":"MDEyOklzc3VlQ29tbWVudDI5OTE5NTQzNw==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-05-04T14:07:34Z","updated_at":"2017-05-04T14:07:34Z","author_association":"CONTRIBUTOR","body":"Just giving @bleskes a ping here since he might be interested in this as well...","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/300170160","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-300170160","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":300170160,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMDE3MDE2MA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2017-05-09T13:49:21Z","updated_at":"2017-05-09T13:49:21Z","author_association":"MEMBER","body":"Thx @s1monw . Indeed interesting.\r\n\r\n1) Reading the ticket I kept going back and forth between killing the node and trying to deal with it while staying alive. I get the argument for killing the node as it is not functioning well. On the other side - killing a node is a very drastic operation from a cluster perspective - it's not likely to come back with a minute  (like is the case with OOM) and this means that the cluster will start recovering all shards that used to be on it to other nodes. So if you have 500GB of data on the node and one active shard, causing disk to fill up, or if you had a logging issue (and its not on a different mount), now we start copying all those 500GB around.\r\n2) I like the idea of throttling indexing when indexing overloads shard moving - i.e., the master is already trying to move shards off the troubled node but it takes time, if someone is indexing at the fills up the disk faster than we're moving data, we should slow them down.\r\n3) In terms of throttling - did we consider using the current throttling mechanism we already have - i.e., when the memory controller locks indexing to a single thread? will tying that to free disk space be enough of throttling with the advantage that it's fully local?\r\n3) I would like to understand better where the out of disk came from - if it is a merge, will stopping indexing actually help? Also, it seems that a disk full issue only affect shards that are actively writing. Maybe instead of killing the node, we should kill the shard that can't live on it - i.e., fail it? I looked a bit at the code and it seems we currently treat this as a document failure (please do tell me I'm wrong - Lucene is complicated ;)). ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/300770461","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-300770461","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":300770461,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMDc3MDQ2MQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-05-11T12:11:31Z","updated_at":"2017-05-11T12:11:31Z","author_association":"CONTRIBUTOR","body":"> I like the idea of throttling indexing when indexing overloads shard moving - i.e., the master is already trying to move shards off the troubled node but it takes time, if someone is indexing at the fills up the disk faster than we're moving data, we should slow them down.\r\n\r\nI am convinced we should reject all writes and not throttle once we cross a certain line. Folks can raise that bar if they feel confident but lets not continue writing.\r\n\r\n> I would like to understand better where the out of disk came from - if it is a merge, will stopping indexing actually help?\r\n\r\nwe are trying to not even get to this point. We try to prevent adding more data once we crossed the `flood_stage` which will then hopefully prevent running out of disk. If, lets say the last doc we index before we cross the line is triggering a merge and that merge causes an out of disk exception we fail that shard immediately. That will just causes this one shard to fail here and we give back the space we used for the merge target immediately so I think we are fine here. We also spoke about disabling merges on read-only indices (ie. when the shard goes inactive, which can be a side-effect of marking them as read-only) \r\n\r\n> Also, it seems that a disk full issue only affect shards that are actively writing. Maybe instead of killing the node, we should kill the shard that can't live on it - i.e., fail it? I looked a bit at the code and it seems we currently treat this as a document failure (please do tell me I'm wrong - Lucene is complicated ;)).\r\n\r\nmy summary of our chats steps away from failing the node... we try to not even get to the point and make indices that are allocated on the node that crosses the `flodd_stage` as `read_only` so I think it's fine?!\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/301128837","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-301128837","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":301128837,"node_id":"MDEyOklzc3VlQ29tbWVudDMwMTEyODgzNw==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2017-05-12T16:51:12Z","updated_at":"2017-05-12T16:51:12Z","author_association":"MEMBER","body":"> I am convinced we should reject all writes and not throttle once we cross a certain line. \r\n\r\nI thought about this more and I agree. It's a simpler solution than slowing things down.\r\n\r\n> my summary of our chats steps away from failing the node... \r\n\r\nOk. Good. Then there is no need offer alternatives :)\r\n\r\n> The new cluster block will prevent indexing / updating but only for operations with Engine.Operation.Origin.PRIMARY such that we never get in the way of replica requests etc.\r\n\r\nBy adding an index level block which exclude writes, we block write operations on the [reroute phase](https://github.com/elastic/elasticsearch/blob/e804817ce4ed5c969b38ff428c5ca9d373c0b855/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java#L659), even before they go into the replication phase. Everything that's beyond this point will be processed correctly on both replicas and primaries. I think we're good here.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/313310851","html_url":"https://github.com/elastic/elasticsearch/issues/24299#issuecomment-313310851","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24299","id":313310851,"node_id":"MDEyOklzc3VlQ29tbWVudDMxMzMxMDg1MQ==","user":{"login":"nullpixel","id":6704113,"node_id":"MDQ6VXNlcjY3MDQxMTM=","avatar_url":"https://avatars2.githubusercontent.com/u/6704113?v=4","gravatar_id":"","url":"https://api.github.com/users/nullpixel","html_url":"https://github.com/nullpixel","followers_url":"https://api.github.com/users/nullpixel/followers","following_url":"https://api.github.com/users/nullpixel/following{/other_user}","gists_url":"https://api.github.com/users/nullpixel/gists{/gist_id}","starred_url":"https://api.github.com/users/nullpixel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nullpixel/subscriptions","organizations_url":"https://api.github.com/users/nullpixel/orgs","repos_url":"https://api.github.com/users/nullpixel/repos","events_url":"https://api.github.com/users/nullpixel/events{/privacy}","received_events_url":"https://api.github.com/users/nullpixel/received_events","type":"User","site_admin":false},"created_at":"2017-07-06T06:47:07Z","updated_at":"2017-07-06T06:47:07Z","author_association":"NONE","body":"Woo!","performed_via_github_app":null}]