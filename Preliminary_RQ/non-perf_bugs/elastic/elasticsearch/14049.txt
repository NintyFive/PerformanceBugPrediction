{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/14049","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14049/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14049/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14049/events","html_url":"https://github.com/elastic/elasticsearch/issues/14049","id":110689321,"node_id":"MDU6SXNzdWUxMTA2ODkzMjE=","number":14049,"title":"Ingest Node - enrich data as it gets in via pipelines","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"labels":[{"id":268963484,"node_id":"MDU6TGFiZWwyNjg5NjM0ODQ=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Features/Ingest","name":":Core/Features/Ingest","color":"0e8a16","default":false,"description":"Execution or management of Ingest Pipelines"},{"id":158399402,"node_id":"MDU6TGFiZWwxNTgzOTk0MDI=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/Meta","name":"Meta","color":"e11d21","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":14,"created_at":"2015-10-09T16:08:38Z","updated_at":"2017-10-04T18:18:05Z","closed_at":"2016-03-04T15:52:39Z","author_association":"MEMBER","active_lock_reason":null,"body":"[[currently open issues](https://github.com/elastic/elasticsearch/pulls?q=is%3Aopen+is%3Apr+label%3A%22%3Aingest%22)]\r\n\r\nrelated issues from other projects:\r\n- Beats: https://github.com/elastic/beats/issues/805, Merged! Hurray!\r\n- Kibana: https://github.com/elastic/kibana/issues/5974\r\n\r\nThere are many use-cases where it is important to enrich incoming data. This enrichment may be something simple like using a regular expression to extract metadata from an existing field, or something more advanced like a geoip lookup or language identification. The filter stage of the [Logstash processing pipeline](https://www.elastic.co/guide/en/logstash/current/pipeline.html#_filters) provides great examples of the ways in which data is often enriched. Node ingest implements a new type of ES node, which performs this enrichment prior to indexing.\r\n\r\nNode ingest is a pure Java implementation of the filters in logstash, integrated with Elasticsearch. It works by wrapping the bulk/index APIs, executing a pipeline that is composed of multiple processors to enrich the documents. A processor is just a component that can modify incoming documents (the source before ES turns it into a document). A pipeline is a list of processors grouped under an unique id. If node ingest is enabled then the index and bulk apis can reroute the the request with documents through a pipeline.\r\n\r\nThe ingest plugin runs on dedicated client nodes and after bulk and index requests have been enriched these index and bulk request continue their way into the cluster.\r\n\r\nNode ingest will be a plugin in the elasticsearch project, implementing 2 main aspects:\r\n\r\nThe first is a pure Java implementation for Pipeline, Processor, as well as initial processor implementation of grok, geoip, kv/mutate, date. This java implementation can then be reused in others places, such as logstash itself, reindex API, and so on. In the first version of the ingest plugin the processor implementations can reside in the ingest plugin, but the framework and processor implementations shouldnâ€™t rely on any ES specific code, so that later on it can be moved to an isolated library.\r\n\r\nThe second part is the integration with Elasticsearch. This includes interception of the bulk/index APIs, management APIs (stats and so on in future phase), storage and live reload of the configuration, supporting multiple \"live\" pipelines, and simulation of pipeline execution.\r\n\r\nThe goal of the ingest plugin is to make data enrichment easier and it will not replace logstash at all. The ingest plugin should make data enrichment in most of the cases easier when events are only stored in Elasticsearch. For example when only file beat is used to ship logs, a logstash instance will no longer be required. In cases where events are stored in multiple outputs a Logstash installation is required. Also at some point Logstash will reuse the pipeline/processor framework, so the end goal is that both Elasticsearch and Logstash will benefit from the ingest initiative.\r\n\r\nDevelopment happens in a feature branch: https://github.com/elastic/elasticsearch/tree/feature/ingest\r\n\r\nCurrent node ingest tasks:\r\n- [x] Hook into the index and bulk APIs. If the ingest plugin is enabled a `pipeline_id` parameter is available to select what pipeline should be used to preprocess the documents before the index/bulk APIs get executed. #13941\r\n- [x] Manage pipeline configuration. Pipelines are stored as a document in an index. Each node ingest node will have the pipelines in memory around to be used when needed. A background process makes sure that an ingest node will eventually get the modifications. #13941\r\n- [x] The pipeline document enrichment shouldn't be non blocking and happen via a dedicated thread pool. #13990\r\n- [x] Add first version of CRUD pipeline APIs. #14047\r\n- [x] Data substructure manipulation. Processors should be able to introduce new nested fields with no pre-existing parent structure. For example, It should be possible to create a new field \"location.lat\" without \"location\" existing. #14250\r\n- [x] Add grok processor. #14132\r\n- [x] Add geoip processor. #14208\r\n- [x] Add date processor. #14184 \r\n- [x] Add kv/mutate processor. #14253 \r\n- [x] Strict configuration validation #14552\r\n- [x] geoip processor output fields should be configurable #14582\r\n- [x] Add simulate API. This allows pipelines to be tested out before actually being used. This api accepts a pipeline definition and actual documents and the output is the transformed documents and optionally showing how each document gets modified after each processor. #14572 \r\n- [x] Do not fail whole bulk request if any pipeline for a single document fails #14888\r\n- [x] Split mutate processor into separate processors for each function (e.g. update, remove, etc) #14938\r\n- [x] Add support for setting nested fields in document #14250\r\n- [x] Data and metadata manipulation.  #14644\r\n- [x] Processors and factors should throw exception on the interface.\r\n- [x] Throw exception when grok expression does not match #15132 \r\n- [x] Add ability to provide custom patterns within Grok Processor config definition #15167 \r\n- [x] Reduce number of fields operated on by processors to just one. #15133 \r\n- [x] Support for ingest transient metadata #15036\r\n- [x] on failure pipeline handler #14548 / #15565 (tal)\r\n- [x] append processor #14324 #15577\r\n- [x] Ingest nodes should update pipelines in a sync manner #14998 / #15203 (mvg)\r\n- [x] support for templating in any processor that sets a field value #14990 #15415 (mvg)\r\n- [x] Add support for ingest node boolean flag to enable/disable ingest at the node level. If a node with `node.ingest` set to false receives an ingest request, it should explicitly fail. (mvg) #15610\r\n- [x] Figure out if geoip2 library can be used without suppressing jvm access checks. (mvg) https://github.com/maxmind/GeoIP2-java/pull/52\r\n- [x] Ingest forks a thread for each bulk item in a bulk request. Instead ingest should use one thread to process an entire bulk request. (mvg) #15593 \r\n- [x] Ingest should only try the load the pipelines if the `.ingest` index has been started. (mvg) #15203 \r\n- [x]  Cut over to a non-guice structure. We should at most bind one class to an instance directly rather thatn use the dep-injection framework that we have to un-do once we get rid of juice. #15203 (mvg)\r\n- [x] Change `pipeline_id`param name to `pipeline`. #15618 \r\n- [x] Add index template for the .ingest index, which should be installed by default. #15001 #15631\r\n- [x] Move ingest infrastructure to core\r\n- [x] Move processors with no dependencies to core\r\n- [x] Make grok a module as it has external deps, but it will be installed by default\r\n- [x] Make geoip a plugin which needs to be installed manually\r\n- [x] If node ingest has been disabled then it should redirect a bulk/index request that has the ingest parameter to a node that has ingest enabled.\r\n- [x] Add more descriptive error messages to pipeline factory exceptions (tal) #16010\r\n- [x] In addition to the isolated processor unit tests we should also test combination of processors. #15247\r\n- [x] Benchmark ingest plugin. #14425 (mvg&tal) \r\n- [x] Move DedotProcessor into its own plugin #16322 \r\n- [x] Add processor `tag`s to `on_failure` metadata #16202 \r\n- [x] Documentation before release #16009\r\n\r\npossible v2 tasks:\r\n- Make it possible to let the ingest plugin know what pipeline to use via index settings / index template.\r\n- configuration management\r\n- _simulate w/ verbose should show document diffs between processor results #14698 \r\n- Composite pipelines. It would be convenient to pre-define certain pipelines that process specific things that can be re-used for other documents. For example, there may be a pipeline that processes date + geoip, and these operate on fields that are common to other documents that may require further processing.\r\n- Add ability to show stats in _simulate response to show resource usage and execution times of pipelines/processors\r\n- A pipeline should be able to choose what (custom) thread it uses. Some pipelines just do some simple modifications to the incoming documents while other may reach out to external systems to enrich the incoming documents. #14616 \r\n- Grok discover  api #15041\r\n- Add notion of transactions for processors which mutate multiple fields within a document. This will allow `on_failure` processors to receive a document with a pre-failed-processor state (ref: https://github.com/elastic/elasticsearch/issues/14548#issuecomment-161799133)\r\n- Add compare processor. #14647\r\n- Add json processor, which converts a json string into json.\r\n- Add kv processor.\r\n","closed_by":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"performed_via_github_app":null}