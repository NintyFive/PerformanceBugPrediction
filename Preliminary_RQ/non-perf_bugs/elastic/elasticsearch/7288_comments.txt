[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53240430","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53240430","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53240430,"node_id":"MDEyOklzc3VlQ29tbWVudDUzMjQwNDMw","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2014-08-25T08:40:54Z","updated_at":"2014-08-25T08:40:54Z","author_association":"MEMBER","body":"@gibrown this sounds useful, but I think using the term \"recovery\" has too much baggage for this. Maybe naming it something like \"existing\" would be better suited (not 100% sure about the name).\n\nSo maybe \"existing_and_new\" would allow existing replicas to be recovered and brand new shards to be allocated, but not recovering replicas if a node went down. Does this sound like what you're after?\n\nThis does sound good, it sounds similar to (but not the same as) another issue (which I can't find right now) that has a \"recover_after_time\" setting for shard recovery, so you could say \"wait 5 minutes after losing a replica before trying to recover it to another machine\".\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53242776","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53242776","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53242776,"node_id":"MDEyOklzc3VlQ29tbWVudDUzMjQyNzc2","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-08-25T09:09:20Z","updated_at":"2014-08-25T09:09:20Z","author_association":"MEMBER","body":"@gibrown It seems you havea  deployment where a single datacenter is not capable of hosting more then one copy of the data. I wonder if force awareness  ( http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-cluster.html#forced-awareness ) , where you use the datacenter as  the attribute, in combination with `gateway.expected_nodes` for full cluster restart. This will make sure only one replica will be assigned to each datacenter, but if a node goes down within a data center another node in the same DC will pick up the missing replica. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53289445","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53289445","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53289445,"node_id":"MDEyOklzc3VlQ29tbWVudDUzMjg5NDQ1","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2014-08-25T16:41:32Z","updated_at":"2014-08-25T16:41:32Z","author_association":"CONTRIBUTOR","body":"@dakrone \"existing_and_new\" sounds much better, and exactly like what I'm after. I thought about suggesting a timeout to control this. I think its hard to decide what I'd set the time to though. If nodes aren't coming back up after some period of time, there is probably a reason that our ops team is well aware of. Having the system suddenly try to correct itself is more likely to exacerbate the problem IMO.\n\n@bleskes We do use a DC awareness attribute, but also have a secondary attribute (\"parity\" which indicates which is tied to the network router in the rack) so we can have multiple replicas in one DC. In practice we see the replicas getting reallocated on another node in the same DC (presumably due to the DC attribute being higher priority). Ideally though, I don't want the replica moved at all. If a DC is having problems, moving TBs of data around does not help.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53618688","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53618688","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53618688,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjE4Njg4","user":{"login":"sax","id":77073,"node_id":"MDQ6VXNlcjc3MDcz","avatar_url":"https://avatars2.githubusercontent.com/u/77073?v=4","gravatar_id":"","url":"https://api.github.com/users/sax","html_url":"https://github.com/sax","followers_url":"https://api.github.com/users/sax/followers","following_url":"https://api.github.com/users/sax/following{/other_user}","gists_url":"https://api.github.com/users/sax/gists{/gist_id}","starred_url":"https://api.github.com/users/sax/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sax/subscriptions","organizations_url":"https://api.github.com/users/sax/orgs","repos_url":"https://api.github.com/users/sax/repos","events_url":"https://api.github.com/users/sax/events{/privacy}","received_events_url":"https://api.github.com/users/sax/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T18:29:14Z","updated_at":"2014-08-27T18:29:14Z","author_association":"NONE","body":"@gibrown we have a much smaller cluster than you, but are running into similar issues.\n\nHave you tried setting `index.routing.allocation.total_shards_per_node`? We're thinking that if we set this to the exact number of shards expected per node in a healthy state (with all nodes available), then when nodes go down shards will not be reallocated. If multiple nodes become unavailable, this may leave the cluster in a red state; we are already catching this in our client applications and disabling associated features for users.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53621824","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53621824","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53621824,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjIxODI0","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T18:50:29Z","updated_at":"2014-08-27T18:50:29Z","author_association":"CONTRIBUTOR","body":"I think there are some things we can do that aren't as aggressive as not allowing reallocation on nodes that don't already have some of the index.  That might still be a good idea but it isn't one I'm likely to use.\n\nWhen a node goes down all the shards that it hosted are shifted to \"unassigned\" state.  Right now when the allocation algorithm for unassigned shards tries to assign them as quickly as possible.  It makes some effort to balance them while it is allocating them but if the most balanced node is throttled then it'll just assign the shard to the next most balanced node.  This causes it to assign all the shards super fast (good) but it can make the cluster quite unbalanced (bad).\n\nMaybe instead we need a way to be more leisurely and balanced when assigning shards.  So we only assign the shard to the node that would be most balanced and if that node is throttled then we just give up on assigning that shard for now and come back to it when the node isn't throttled.  I don't think you want to do this if there aren't any replicas live, but if you already have a single replica and you are just looking to add another then maybe its ok.\n\nI think this would have prevented your massive shuffling problem.  I think.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53622503","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53622503","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53622503,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjIyNTAz","user":{"login":"sax","id":77073,"node_id":"MDQ6VXNlcjc3MDcz","avatar_url":"https://avatars2.githubusercontent.com/u/77073?v=4","gravatar_id":"","url":"https://api.github.com/users/sax","html_url":"https://github.com/sax","followers_url":"https://api.github.com/users/sax/followers","following_url":"https://api.github.com/users/sax/following{/other_user}","gists_url":"https://api.github.com/users/sax/gists{/gist_id}","starred_url":"https://api.github.com/users/sax/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sax/subscriptions","organizations_url":"https://api.github.com/users/sax/orgs","repos_url":"https://api.github.com/users/sax/repos","events_url":"https://api.github.com/users/sax/events{/privacy}","received_events_url":"https://api.github.com/users/sax/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T18:55:05Z","updated_at":"2014-08-27T18:55:05Z","author_association":"NONE","body":"@gibrown we just tried this and it appears to do what we expected. When we restarted an individual node, the shards on that node became unassigned. When the node rejoined the cluster, those shards were allocated back to it, and it was able to recover quickly from the shards already on disk (and in the disk cache).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53625310","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53625310","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53625310,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjI1MzEw","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T19:14:26Z","updated_at":"2014-08-27T19:14:26Z","author_association":"CONTRIBUTOR","body":"@sax that's a good workaround that I hadn't considered. \n\nI don't think it works for my use case because we use index templates to auto create new indices, and on one of our clusters we are fairly constantly adding new indices (and hence shards).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53628032","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53628032","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53628032,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjI4MDMy","user":{"login":"sax","id":77073,"node_id":"MDQ6VXNlcjc3MDcz","avatar_url":"https://avatars2.githubusercontent.com/u/77073?v=4","gravatar_id":"","url":"https://api.github.com/users/sax","html_url":"https://github.com/sax","followers_url":"https://api.github.com/users/sax/followers","following_url":"https://api.github.com/users/sax/following{/other_user}","gists_url":"https://api.github.com/users/sax/gists{/gist_id}","starred_url":"https://api.github.com/users/sax/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sax/subscriptions","organizations_url":"https://api.github.com/users/sax/orgs","repos_url":"https://api.github.com/users/sax/repos","events_url":"https://api.github.com/users/sax/events{/privacy}","received_events_url":"https://api.github.com/users/sax/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T19:35:42Z","updated_at":"2014-08-27T19:35:42Z","author_association":"NONE","body":"I'll double check, but I thought it was set per index.\n\nSent from my iPhone\n\nOn Aug 27, 2014, at 12:14 PM, Greg Ichneumon Brown notifications@github.com\nwrote:\n\n@sax https://github.com/sax that's a good workaround that I hadn't\nconsidered.\n\nI don't think it works for my use case because we use index templates to\nauto create new indices, and on one of our clusters we are fairly\nconstantly adding new indices (and hence shards).\n\n—\nReply to this email directly or view it on GitHub\nhttps://github.com/elasticsearch/elasticsearch/issues/7288#issuecomment-53625310\n.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53628368","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53628368","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53628368,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjI4MzY4","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T19:37:59Z","updated_at":"2014-08-27T19:37:59Z","author_association":"CONTRIBUTOR","body":"@nik9000 a smarter allocation algorithm would help for some use cases, but I think would also make the dynamics more complex and confusing. I'd prefer the system to be more predictable.\n\nFor our use case reusing the 2TB of data that is already on the disks of each node is almost always the fastest way to get our replicas back. Moving shards around will always be MUCH slower. Even if it takes hours to resolve a hardware problem, moving shards is often an event that takes 24-36 hours to complete. I'd rather intentionally decide to move that much data around.\n\nIn those cases where we do choose to move the data around though, I agree that paying more attention to the overall balance of shards would help.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53630711","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53630711","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53630711,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjMwNzEx","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T19:54:32Z","updated_at":"2014-08-27T19:54:32Z","author_association":"CONTRIBUTOR","body":"@sax oh you're totally right.\n\nStill wouldn't be my preferred way to manage this, but does seem to be a good workaround.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53632129","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53632129","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53632129,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjMyMTI5","user":{"login":"sax","id":77073,"node_id":"MDQ6VXNlcjc3MDcz","avatar_url":"https://avatars2.githubusercontent.com/u/77073?v=4","gravatar_id":"","url":"https://api.github.com/users/sax","html_url":"https://github.com/sax","followers_url":"https://api.github.com/users/sax/followers","following_url":"https://api.github.com/users/sax/following{/other_user}","gists_url":"https://api.github.com/users/sax/gists{/gist_id}","starred_url":"https://api.github.com/users/sax/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sax/subscriptions","organizations_url":"https://api.github.com/users/sax/orgs","repos_url":"https://api.github.com/users/sax/repos","events_url":"https://api.github.com/users/sax/events{/privacy}","received_events_url":"https://api.github.com/users/sax/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T20:03:33Z","updated_at":"2014-08-27T20:03:33Z","author_association":"NONE","body":"@gibrown agreed. I think we have a host of other tuning issues, but at least this will simplify the way we restart nodes until there's a better option.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53633460","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53633460","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53633460,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjMzNDYw","user":{"login":"sax","id":77073,"node_id":"MDQ6VXNlcjc3MDcz","avatar_url":"https://avatars2.githubusercontent.com/u/77073?v=4","gravatar_id":"","url":"https://api.github.com/users/sax","html_url":"https://github.com/sax","followers_url":"https://api.github.com/users/sax/followers","following_url":"https://api.github.com/users/sax/following{/other_user}","gists_url":"https://api.github.com/users/sax/gists{/gist_id}","starred_url":"https://api.github.com/users/sax/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sax/subscriptions","organizations_url":"https://api.github.com/users/sax/orgs","repos_url":"https://api.github.com/users/sax/repos","events_url":"https://api.github.com/users/sax/events{/privacy}","received_events_url":"https://api.github.com/users/sax/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T20:13:14Z","updated_at":"2014-08-27T20:13:14Z","author_association":"NONE","body":"@nik9000 I think there are two competing priorities for us. One is the ability to do a rolling restart of a cluster as quickly as possible. The other is to tune recovery such that it does not cause the cluster to become unavailable. Since we're struggling mightily with the latter, being able to solve the former in a very simple fashion is nice.\n\nIdeally there would be another way of configuring recovery timeouts, where you could tell the cluster to allow shards to remain unassigned for some time period. If a node rejoins the cluster, then expected_nodes would be met and the missing shards could be reassigned back (where they could be loaded from disk). After some timeout, however, the unassigned shards would be allocated to different nodes for redundancy.\n\nPardon if this is what is described above, my brain is spinning a bit trying to keep all the bits in.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53634207","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53634207","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53634207,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjM0MjA3","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T20:18:50Z","updated_at":"2014-08-27T20:18:50Z","author_association":"CONTRIBUTOR","body":"@sax - what I was describing was more a way to prevent the cluster from becoming super unbalanced when it comes back up - a timeout would help, I think.\n\nMaybe something like this:\n- If there aren't any replicas online then try to assign the shard as fast as possible just like how it works now.\n- If there are replicas online then wait some specified timeout hoping the node will come back.  If it comes back then restore wherever would be most efficient/balanced.  Hopefully that node is the one that just came back.  If the node is already throttled then wait for it to become unthrottled.\n\nI think it'd make make a single node going down less exciting.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53634503","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53634503","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53634503,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjM0NTAz","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2014-08-27T20:21:04Z","updated_at":"2014-08-27T20:21:04Z","author_association":"CONTRIBUTOR","body":"It'd change rolling restarts too.  I'm not sure if it'd help them or hurt them.  Ultimately I think full cluster restarts are a lost cause until we have a way to restore from the master shard's translog even if the files differ.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53688227","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-53688227","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":53688227,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNjg4MjI3","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2014-08-28T08:38:32Z","updated_at":"2014-08-28T08:38:32Z","author_association":"MEMBER","body":"> Ultimately I think full cluster restarts are a lost cause until we have a way to restore from the master shard's translog even if the files differ.\n\nYes, unfortunately this is something that will require sequence numbers, which we're working on, but until then a super-fast recovery after full restart (without pre-optimizing and changing replica settings) is not possible.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/54712221","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-54712221","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":54712221,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzEyMjIx","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-09-06T13:25:28Z","updated_at":"2014-09-06T13:25:28Z","author_association":"CONTRIBUTOR","body":"Stalled by #6069\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/54922290","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-54922290","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":54922290,"node_id":"MDEyOklzc3VlQ29tbWVudDU0OTIyMjkw","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2014-09-09T04:15:02Z","updated_at":"2014-09-09T04:15:02Z","author_association":"CONTRIBUTOR","body":"@clintongormley wasn't @dakrone's comment about only full cluster restarts requiring sequence numbers? I would think for implementing 'existing_and_new' as an additional option for the cluster.routing.allocation.enable would be independent of the translog.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/54964287","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-54964287","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":54964287,"node_id":"MDEyOklzc3VlQ29tbWVudDU0OTY0Mjg3","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-09-09T12:56:11Z","updated_at":"2014-09-09T12:56:11Z","author_association":"CONTRIBUTOR","body":"@gibrown No, even a node restart will benefit from sequence numbers.  Imagine that you have a primary on one node and a replica on the other.  You keep indexing documents, refreshes happen at different times on primary and replica, so the segments diverge.  Now the node with the replica disappears temporarily (eg network disconnect or node restart). \n\nTo ensure that the primary is in sync with the replica, we can compare segments and copy over all of the segments that the primary has and the replica doesn't.  But these segments have diverged over time, so this can end up being a lot of data.  With sequence numbers the situation is different.  As long as the last sequence number that the replica knows about is still in the transaction log of the primary, we can just replay the translog from that point on.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/55004623","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-55004623","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":55004623,"node_id":"MDEyOklzc3VlQ29tbWVudDU1MDA0NjIz","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2014-09-09T17:22:16Z","updated_at":"2014-09-09T17:22:16Z","author_association":"CONTRIBUTOR","body":"Right, I see how they can help restart times in lots of scenarios, and I'm all for it and agree sequence numbers would be a huge improvement. But I don't think they address the original problem of large shards being moved from a node that is temporarily down but then returns from production.\n\nIf shard A is on node 1, and node 1 goes down due to a hardware failure, then shard A is going to get reallocated on another node that has no data and all the data needs to be copied over the network. I would much rather have the reduced redundancy until our ops team resolves the hardware failure and node 1 comes back up than have TBs of data moving around the cluster exacerbating the problems that already exist in the cluster.\n\nI would think this problem is orthogonal to sequence numbers, but its quite likely I'm misunderstanding something.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/60229747","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-60229747","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":60229747,"node_id":"MDEyOklzc3VlQ29tbWVudDYwMjI5NzQ3","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2014-10-23T12:13:38Z","updated_at":"2014-10-23T12:13:38Z","author_association":"CONTRIBUTOR","body":"just as a side note I just pushed #8190 to master which might help here as well @gibrown \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/131523316","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-131523316","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":131523316,"node_id":"MDEyOklzc3VlQ29tbWVudDEzMTUyMzMxNg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-08-16T10:44:32Z","updated_at":"2015-08-16T10:44:32Z","author_association":"CONTRIBUTOR","body":"This is now closed by #11438, #12421, and #11417\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/131847694","html_url":"https://github.com/elastic/elasticsearch/issues/7288#issuecomment-131847694","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","id":131847694,"node_id":"MDEyOklzc3VlQ29tbWVudDEzMTg0NzY5NA==","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"created_at":"2015-08-17T14:46:54Z","updated_at":"2015-08-17T14:46:54Z","author_association":"CONTRIBUTOR","body":"@clintongormley thanks to you and everyone else for these improvements.\n","performed_via_github_app":null}]