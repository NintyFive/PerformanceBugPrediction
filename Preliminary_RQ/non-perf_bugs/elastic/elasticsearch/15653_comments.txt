[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/170381950","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-170381950","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":170381950,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MDM4MTk1MA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-01-10T18:57:10Z","updated_at":"2016-01-10T18:57:10Z","author_association":"CONTRIBUTOR","body":"@imotov could you take a look at this please?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/170585114","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-170585114","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":170585114,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MDU4NTExNA==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2016-01-11T15:20:24Z","updated_at":"2016-01-11T15:20:24Z","author_association":"MEMBER","body":"@iostat thank you for the analysis of the issue. I agree, that it would be a valuable feature that would help some users who lost a part of their index due to shard corruptions. I also agree that RestoreService is already more complicated than we would like it to be. The good news is that a lot of complexity in RestoreService comes from managing of the restore lifecycle and we are currently working on extracting some of this lifecycle management logic into a separate TaskManagement service. So, my hope is that when this work is complete, it would be possible to split RestoreService into more manageable pieces. Until then I would definitely not recommend to use this task as the first foray into Elasticsearch because it touches some of the most complex parts of Elasticsearch such as shard allocation and recover/restore processes. \n\nThe placement of the new logic that you described is one issue. Even a bigger issue, in my opinion, is that metadata in a snapshot might not be compatible with metadata of an existing index. That means that it might or might not be OK to restore a few shards. When we restore all shards (even in case of a partial restore) we can wipe out old metadata and replace it with metadata from a snapshot without thinking about it because it's guaranteed that the data in the existing index will be completely gone by the end of the process. When we restore only some shards, it's much trickier. What would you do if during restore you discover that settings and/or mappings of the new index are different. In the case of mappings, we have the mapping merging logic that we could reuse to a large degree. In the case of settings - there is no such mechanism. So, we would need to make a determination about what to do. \n\nSome settings are ok to merge. For example, if an index in a snapshot has refresh_interval set to 10 sec and the corresponding index in the cluster has refresh_interval of 1 sec and everything else is the same, it's perfectly fine to partially restore the index. The same goes for the number_of_replicas setting. However, if the number_of_shards setting is different we definitely shouldn't restore. It gets even more complicated with analysis settings. For example, different settings for the same analyzer can break the restored index, but if we are just adding a new analyzer, it's perfectly fine to merge the settings. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/170746936","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-170746936","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":170746936,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MDc0NjkzNg==","user":{"login":"iostat","id":2927187,"node_id":"MDQ6VXNlcjI5MjcxODc=","avatar_url":"https://avatars2.githubusercontent.com/u/2927187?v=4","gravatar_id":"","url":"https://api.github.com/users/iostat","html_url":"https://github.com/iostat","followers_url":"https://api.github.com/users/iostat/followers","following_url":"https://api.github.com/users/iostat/following{/other_user}","gists_url":"https://api.github.com/users/iostat/gists{/gist_id}","starred_url":"https://api.github.com/users/iostat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iostat/subscriptions","organizations_url":"https://api.github.com/users/iostat/orgs","repos_url":"https://api.github.com/users/iostat/repos","events_url":"https://api.github.com/users/iostat/events{/privacy}","received_events_url":"https://api.github.com/users/iostat/received_events","type":"User","site_admin":false},"created_at":"2016-01-12T01:00:14Z","updated_at":"2016-01-12T01:00:14Z","author_association":"NONE","body":"@imotov: Thanks for the reply! I definitely agree that it's a behemoth of a\ntask especially for someone like me getting their feet wet with the\ncodebase, but I thought it was least worth starting a discussion for.\n\nOn the point of mismatched settings: I think it's fair to say that if the\nnumber of shards in the snapshot doesn't match the number of shards in the\nindex, you shouldn't be able to restore that snapshot period. As far as I'm\naware there's no way to reshard an index without creating a new one anyway,\nso in that case the snapshot being restored from is effectively from a\ncompletely different index and shouldn't be restored from in the first\nplace. Likewise for settings such as the routing hash function, analyzer\nsettings, etc. You could apply this to the extreme and even say if the\ndestination index was created after snapshot was taken, then to reject the\nrestore request completely.\n\nI don't really see this feature as an absolute replacement for the existing\nrestore functionality, it should really be available as a convenience if\nanything, for instance in the case that 1) regular backups are being taken\nand 2) out of nowhere a shard got corrupted for both primary and replica\nand 3) no \"major\" settings changes occurred since the snapshot was taken.\n\nAs far as the TaskManagement service, any way to track the progress of\nthat? I'd love to revisit this if/when that's implemented, but if it's a\nlong ways off (next major release or something like that) perhaps it's\nworth writing this feature as a separate module and then merging its\nfunctionality with the original RestoreService when that's all refactored.\n\nOn Monday, January 11, 2016, Igor Motov notifications@github.com wrote:\n\n> @iostat https://github.com/iostat thank you for the analysis of the\n> issue. I agree, that it would be a valuable feature that would help some\n> users who lost a part of their index due to shard corruptions. I also agree\n> that RestoreService is already more complicated than we would like it to\n> be. The good news is that a lot of complexity in RestoreService comes from\n> managing of the restore lifecycle and we are currently working on\n> extracting some of this lifecycle management logic into a separate\n> TaskManagement service. So, my hope is that when this work is complete, it\n> would be possible to split RestoreService into more manageable pieces.\n> Until then I would definitely not recommend to use this task as the first\n> foray into Elasticsearch because it touches some of the most complex parts\n> of Elasticsearch such as shard allocation and recover/restore processes.\n> \n> The placement of the new logic that you described is one issue. Even a\n> bigger issue, in my opinion, is that metadata in a snapshot might not be\n> compatible with metadata of an existing index. That means that it might or\n> might not be OK to restore a few shards. When we restore all shards (even\n> in case of a partial restore) we can wipe out old metadata and replace it\n> with metadata from a snapshot without thinking about it because it's\n> guaranteed that the data in the existing index will be completely gone by\n> the end of the process. When we restore only some shards, it's much\n> trickier. What would you do if during restore you discover that settings\n> and/or mappings of the new index are different. In the case of mappings, we\n> have the mapping merging logic that we could reuse to a large degree. In\n> the case of settings - there is no such mechanism. So, we would need to\n> make a determination about what to do.\n> \n> Some settings are ok to merge. For example, if an index in a snapshot has\n> refresh_interval set to 10 sec and the corresponding index in the cluster\n> has refresh_interval of 1 sec and everything else is the same, it's\n> perfectly fine to partially restore the index. The same goes for the\n> number_of_replicas setting. However, if the number_of_shards setting is\n> different we definitely shouldn't restore. It gets even more complicated\n> with analysis settings. For example, different settings for the same\n> analyzer can break the restored index, but if we are just adding a new\n> analyzer, it's perfectly fine to merge the settings.\n> \n> â€”\n> Reply to this email directly or view it on GitHub\n> https://github.com/elastic/elasticsearch/issues/15653#issuecomment-170585114\n> .\n\n## \n\n## \n\nSent from a web browser, why is it a trend to tell people what you used to\nsend an email?\n\nIlya Ostrovskiy\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/170757133","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-170757133","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":170757133,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MDc1NzEzMw==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2016-01-12T01:56:22Z","updated_at":"2016-01-12T01:56:22Z","author_association":"MEMBER","body":"> You could apply this to the extreme and even say if the destination index was created after snapshot was taken, then to reject the restore request completely.  \n\nThe requirement of a snapshot being created after creation of the restored index is irrelevant to success or failure of the partial restore. I think what you are trying to say is we could reject the restore request completely if the index that we restore is not the same as the index that was snapshotted. Yes, we could make this a requirement, but even then there is plenty of scenarios for a snapshot and an index to diverge. So, I don't think it's practical to make the determination about possibility of restore by simply looking at historical origins of indices.\n\n> I don't really see this feature as an absolute replacement for the existing restore functionality, it should really be available as a convenience if anything.\n\nYes, and this is exactly why I wouldn't want to rush this feature in. \n\n> ... no \"major\" settings changes occurred since the snapshot was taken.  \n\nCurrently, it's hard to determine which settings change is \"major\" and which one is \"minor\". \n\n> As far as the TaskManagement service, any way to track the progress of that?\n\nYou can keep track of the task management development progress on the task management meta issue #15117.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/278777043","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-278777043","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":278777043,"node_id":"MDEyOklzc3VlQ29tbWVudDI3ODc3NzA0Mw==","user":{"login":"c4urself","id":391030,"node_id":"MDQ6VXNlcjM5MTAzMA==","avatar_url":"https://avatars3.githubusercontent.com/u/391030?v=4","gravatar_id":"","url":"https://api.github.com/users/c4urself","html_url":"https://github.com/c4urself","followers_url":"https://api.github.com/users/c4urself/followers","following_url":"https://api.github.com/users/c4urself/following{/other_user}","gists_url":"https://api.github.com/users/c4urself/gists{/gist_id}","starred_url":"https://api.github.com/users/c4urself/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/c4urself/subscriptions","organizations_url":"https://api.github.com/users/c4urself/orgs","repos_url":"https://api.github.com/users/c4urself/repos","events_url":"https://api.github.com/users/c4urself/events{/privacy}","received_events_url":"https://api.github.com/users/c4urself/received_events","type":"User","site_admin":false},"created_at":"2017-02-09T21:20:06Z","updated_at":"2017-02-09T21:20:06Z","author_association":"CONTRIBUTOR","body":"FWIW I would love this feature and found this issue while trying to cope with an unexpected outage where two nodes in different racks were lost and 13 out of 128 shards went missing. \r\n\r\nBecause we had a snapshot from the day before we took the following steps (note: this may not be the best way to do this) to restore only a small subset of the shards:\r\n\r\n* started the snapshot restore process to a new restore index within the same cluster\r\n* disabled shard allocation for the restore index\r\n* used the Cluster Reroute api to manually allocate the 13 shards we wanted to recover (allow_primary: true)\r\n* disabled shard allocation throughout the cluster and stopped Elasticsearch on the node we were trying to recover on\r\n* copied the directory containing the shard from the restore index to the origin index (removed `write.lock`, and ensured that the `_state/state-*.st` file contained the same UUID by copying it from an existing shard\r\n* restarted elasticsearch on that node and re-enabled shard allocation to get replicas of the restored shards\r\n* rinse/repeat for other nodes/shards\r\n\r\nHuge caveat, it worked for us on 2.4.x and YMMV but it seemed relatively painless (though time intensive) and much faster than restoring the full snapshot and then reindexing missing writes.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/375332056","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-375332056","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":375332056,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NTMzMjA1Ng==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-03-22T14:45:41Z","updated_at":"2018-03-22T14:45:41Z","author_association":"CONTRIBUTOR","body":"This feature request is an interesting idea but since its opening we have not seen enough feedback that it is a feature we should pursue. We prefer to close this issue as a clear indication that we are not going to work on this at this time. We are always open to reconsidering this in the future based on compelling feedback; despite this issue being closed please feel free to leave feedback on the proposal (including +1s).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/378064014","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-378064014","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":378064014,"node_id":"MDEyOklzc3VlQ29tbWVudDM3ODA2NDAxNA==","user":{"login":"taintedkernel","id":352735,"node_id":"MDQ6VXNlcjM1MjczNQ==","avatar_url":"https://avatars2.githubusercontent.com/u/352735?v=4","gravatar_id":"","url":"https://api.github.com/users/taintedkernel","html_url":"https://github.com/taintedkernel","followers_url":"https://api.github.com/users/taintedkernel/followers","following_url":"https://api.github.com/users/taintedkernel/following{/other_user}","gists_url":"https://api.github.com/users/taintedkernel/gists{/gist_id}","starred_url":"https://api.github.com/users/taintedkernel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/taintedkernel/subscriptions","organizations_url":"https://api.github.com/users/taintedkernel/orgs","repos_url":"https://api.github.com/users/taintedkernel/repos","events_url":"https://api.github.com/users/taintedkernel/events{/privacy}","received_events_url":"https://api.github.com/users/taintedkernel/received_events","type":"User","site_admin":false},"created_at":"2018-04-02T22:17:41Z","updated_at":"2018-04-02T22:19:58Z","author_association":"NONE","body":"I would like this feature as well, it seems like the task management service is finished for 6.0.0.  I'm aware the complexities mentioned, but for clusters holding large amounts of data it could potentially be very useful.  Thanks!","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/500595387","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-500595387","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":500595387,"node_id":"MDEyOklzc3VlQ29tbWVudDUwMDU5NTM4Nw==","user":{"login":"Sukesh-Alluri","id":1489930,"node_id":"MDQ6VXNlcjE0ODk5MzA=","avatar_url":"https://avatars3.githubusercontent.com/u/1489930?v=4","gravatar_id":"","url":"https://api.github.com/users/Sukesh-Alluri","html_url":"https://github.com/Sukesh-Alluri","followers_url":"https://api.github.com/users/Sukesh-Alluri/followers","following_url":"https://api.github.com/users/Sukesh-Alluri/following{/other_user}","gists_url":"https://api.github.com/users/Sukesh-Alluri/gists{/gist_id}","starred_url":"https://api.github.com/users/Sukesh-Alluri/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Sukesh-Alluri/subscriptions","organizations_url":"https://api.github.com/users/Sukesh-Alluri/orgs","repos_url":"https://api.github.com/users/Sukesh-Alluri/repos","events_url":"https://api.github.com/users/Sukesh-Alluri/events{/privacy}","received_events_url":"https://api.github.com/users/Sukesh-Alluri/received_events","type":"User","site_admin":false},"created_at":"2019-06-10T21:07:59Z","updated_at":"2019-06-10T21:07:59Z","author_association":"NONE","body":"+1","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/566939937","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-566939937","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":566939937,"node_id":"MDEyOklzc3VlQ29tbWVudDU2NjkzOTkzNw==","user":{"login":"serjgamover","id":3994178,"node_id":"MDQ6VXNlcjM5OTQxNzg=","avatar_url":"https://avatars0.githubusercontent.com/u/3994178?v=4","gravatar_id":"","url":"https://api.github.com/users/serjgamover","html_url":"https://github.com/serjgamover","followers_url":"https://api.github.com/users/serjgamover/followers","following_url":"https://api.github.com/users/serjgamover/following{/other_user}","gists_url":"https://api.github.com/users/serjgamover/gists{/gist_id}","starred_url":"https://api.github.com/users/serjgamover/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/serjgamover/subscriptions","organizations_url":"https://api.github.com/users/serjgamover/orgs","repos_url":"https://api.github.com/users/serjgamover/repos","events_url":"https://api.github.com/users/serjgamover/events{/privacy}","received_events_url":"https://api.github.com/users/serjgamover/received_events","type":"User","site_admin":false},"created_at":"2019-12-18T08:59:33Z","updated_at":"2020-02-24T07:32:42Z","author_association":"NONE","body":"We have similar case - a database with 50+ terrabytes of data.\r\nFor indices that do not have replication the backup recovery process will take a lot of time. Partial recovery just for broken shards will speed this process a lot.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/590268830","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-590268830","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":590268830,"node_id":"MDEyOklzc3VlQ29tbWVudDU5MDI2ODgzMA==","user":{"login":"ksa31","id":397815,"node_id":"MDQ6VXNlcjM5NzgxNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/397815?v=4","gravatar_id":"","url":"https://api.github.com/users/ksa31","html_url":"https://github.com/ksa31","followers_url":"https://api.github.com/users/ksa31/followers","following_url":"https://api.github.com/users/ksa31/following{/other_user}","gists_url":"https://api.github.com/users/ksa31/gists{/gist_id}","starred_url":"https://api.github.com/users/ksa31/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ksa31/subscriptions","organizations_url":"https://api.github.com/users/ksa31/orgs","repos_url":"https://api.github.com/users/ksa31/repos","events_url":"https://api.github.com/users/ksa31/events{/privacy}","received_events_url":"https://api.github.com/users/ksa31/received_events","type":"User","site_admin":false},"created_at":"2020-02-24T11:04:51Z","updated_at":"2020-02-24T11:04:51Z","author_association":"NONE","body":"+1","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/670284687","html_url":"https://github.com/elastic/elasticsearch/issues/15653#issuecomment-670284687","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15653","id":670284687,"node_id":"MDEyOklzc3VlQ29tbWVudDY3MDI4NDY4Nw==","user":{"login":"linhao1990","id":4331757,"node_id":"MDQ6VXNlcjQzMzE3NTc=","avatar_url":"https://avatars0.githubusercontent.com/u/4331757?v=4","gravatar_id":"","url":"https://api.github.com/users/linhao1990","html_url":"https://github.com/linhao1990","followers_url":"https://api.github.com/users/linhao1990/followers","following_url":"https://api.github.com/users/linhao1990/following{/other_user}","gists_url":"https://api.github.com/users/linhao1990/gists{/gist_id}","starred_url":"https://api.github.com/users/linhao1990/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/linhao1990/subscriptions","organizations_url":"https://api.github.com/users/linhao1990/orgs","repos_url":"https://api.github.com/users/linhao1990/repos","events_url":"https://api.github.com/users/linhao1990/events{/privacy}","received_events_url":"https://api.github.com/users/linhao1990/received_events","type":"User","site_admin":false},"created_at":"2020-08-07T02:22:29Z","updated_at":"2020-08-07T02:22:29Z","author_association":"NONE","body":"+1","performed_via_github_app":null}]