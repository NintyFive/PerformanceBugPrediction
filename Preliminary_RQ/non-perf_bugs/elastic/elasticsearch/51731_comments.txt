[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/580658290","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-580658290","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":580658290,"node_id":"MDEyOklzc3VlQ29tbWVudDU4MDY1ODI5MA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2020-01-31T09:35:15Z","updated_at":"2020-01-31T09:35:15Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-analytics-geo (:Analytics/Aggregations)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/580704942","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-580704942","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":580704942,"node_id":"MDEyOklzc3VlQ29tbWVudDU4MDcwNDk0Mg==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2020-01-31T11:56:24Z","updated_at":"2020-01-31T11:56:24Z","author_association":"MEMBER","body":"I agree that #46751 should change the perception of the `search.max_buckets` setting but I wonder if we should rather increase the default limit or apply this setting only during the final reduction. IMO this setting is a good protection for users because it forces to think of solutions that don't require to return thousands of buckets even if the size doesn't exceed the memory. This is similar in spirit to the max window size we have for top hits that redirects users to scrolls or search_after if they want to deeply paginate. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/581477509","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-581477509","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":581477509,"node_id":"MDEyOklzc3VlQ29tbWVudDU4MTQ3NzUwOQ==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2020-02-03T15:46:29Z","updated_at":"2020-02-03T15:46:29Z","author_association":"MEMBER","body":"Been thinking about this a bit.  I agree that the setting is still helpful from a \"soft limit\" standpoint, in that it helps prevent abusive aggs even if they would technically execute without breaking anything.  It helps prevent users from adding `size: MAX_INT` to all their terms aggs by default because they just want all the results, and instead reason about the best way to accomplish the goal (increase the limit?  Different agg structure? Use composite?  etc)\r\n\r\nIt also offers a convenient place to actually limit agg response size, so that clients aren't expected to handle unreasonably large responses.\r\n\r\nNow that the memory-safety implications are `max_buckets` are diminished, I wonder if we need to change the semantics a bit though. There are a few problem areas with how it works today:\r\n\r\n- Even if the soft-limit is increased, it can be confusing to users when the setting trips despite the final bucket count being under the limit (https://github.com/elastic/elasticsearch/issues/51731).  If a user sets a terms agg to `size: 1000` they may be confused when the threshold trips because 20 shards report back `1000*shard_size` terms each and we abort during an incremental reduction\r\n- Some aggs like `rare_terms` generate more buckets on the shard and prune those during reductions, so the final bucket count might be very small but fail due to the soft-limit.  I imagine future clustering aggs will be similar.\r\n- Some operations implicitly generate many, many buckets.  Geospatial bucketing (tile grid, etc), x/y scatter charts with histograms, 3d heatmaps, etc necessarily needs a lot of buckets, and even a high soft-limit could be problematic.  We could potentially solve these with specialized aggs (`scatter_chart`) that operate more like metrics than buckets, but the issue is still generally a problem.\r\n\r\nI'm not sure what to do about these problem cases, or if we _should_ do anything.  I guess I see a few options:\r\n\r\n1. Deprecate `search.max_buckets`.  Problem solved :)  Although we do lose a potentially useful soft limit\r\n2. Raise `max_buckets` to a large threshold, makes many of the common issues disappear.  Anyone hitting the previously mentioned corner cases still have to work around them or increase limit\r\n3. Allow `max_buckets` to be configured on a per-request basis, so a user can opt out of the limit for something like `rare_terms` or geospatial bucketing.  We didn't want this before because the setting was tied to memory implications, but perhaps we can loosen that restriction now?\r\n4. Only count buckets at the end of final reduction and rely on the recent breaker changes to catch memory issues.  This would help with cases like `terms` and `rare_terms` avoid false-positives, but would not help the geospatial case.\r\n\r\nOr some combination of the above :)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/589692094","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-589692094","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":589692094,"node_id":"MDEyOklzc3VlQ29tbWVudDU4OTY5MjA5NA==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2020-02-21T15:05:41Z","updated_at":"2020-02-21T15:05:41Z","author_association":"MEMBER","body":"Ruminated on this a bit more, leaning towards the dual approach of:\r\n\r\n1. Increase the limit to something larger, 50,000?\r\n2. Only count buckets during final reduction, after all buckets have been merged.  This means it is _solely_ used to limit response size, and avoids all the confusing side-effects (like tripping because `terms` might have `k` buckets that must be merged before you can get to the requested `n` buckets)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/589699661","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-589699661","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":589699661,"node_id":"MDEyOklzc3VlQ29tbWVudDU4OTY5OTY2MQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2020-02-21T15:23:27Z","updated_at":"2020-02-21T15:23:27Z","author_association":"CONTRIBUTOR","body":"This sounds like a plan to me.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/589967077","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-589967077","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":589967077,"node_id":"MDEyOklzc3VlQ29tbWVudDU4OTk2NzA3Nw==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2020-02-22T15:33:31Z","updated_at":"2020-02-22T15:33:31Z","author_association":"MEMBER","body":"It seems that we have a plan here, is it okay to remove the [discuss](https://github.com/elastic/elasticsearch/labels/discuss) label or do we think that broader input would be helpful?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/589972824","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-589972824","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":589972824,"node_id":"MDEyOklzc3VlQ29tbWVudDU4OTk3MjgyNA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2020-02-22T16:31:14Z","updated_at":"2020-02-22T16:31:14Z","author_association":"CONTRIBUTOR","body":"I think it is, I just removed it.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/590581272","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-590581272","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":590581272,"node_id":"MDEyOklzc3VlQ29tbWVudDU5MDU4MTI3Mg==","user":{"login":"nickpeihl","id":1638483,"node_id":"MDQ6VXNlcjE2Mzg0ODM=","avatar_url":"https://avatars2.githubusercontent.com/u/1638483?v=4","gravatar_id":"","url":"https://api.github.com/users/nickpeihl","html_url":"https://github.com/nickpeihl","followers_url":"https://api.github.com/users/nickpeihl/followers","following_url":"https://api.github.com/users/nickpeihl/following{/other_user}","gists_url":"https://api.github.com/users/nickpeihl/gists{/gist_id}","starred_url":"https://api.github.com/users/nickpeihl/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nickpeihl/subscriptions","organizations_url":"https://api.github.com/users/nickpeihl/orgs","repos_url":"https://api.github.com/users/nickpeihl/repos","events_url":"https://api.github.com/users/nickpeihl/events{/privacy}","received_events_url":"https://api.github.com/users/nickpeihl/received_events","type":"User","site_admin":false},"created_at":"2020-02-24T22:27:50Z","updated_at":"2020-02-24T22:27:50Z","author_association":"CONTRIBUTOR","body":"If we do set a new limit for `search.max_buckets`, I think it would be useful for the Elastic Maps application to set it to at least 65,536. We have a [POC in Kibana for constructing vector tiles from documents and geo_tile grid aggregations](https://github.com/elastic/kibana/pull/57248). The size of the vector tiles we generate is a multiple of 64 pixels by 64 pixels and usually 256x256. A sufficiently large geo_tile grid precision could generate a bucket for each pixel in the 256x256 tile. So if we are setting `search.max_buckets` to an arbitrary number, perhaps we can consider at least 65,536?\r\n\r\n@thomasneirynck ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/590875706","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-590875706","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":590875706,"node_id":"MDEyOklzc3VlQ29tbWVudDU5MDg3NTcwNg==","user":{"login":"thomasneirynck","id":1833023,"node_id":"MDQ6VXNlcjE4MzMwMjM=","avatar_url":"https://avatars2.githubusercontent.com/u/1833023?v=4","gravatar_id":"","url":"https://api.github.com/users/thomasneirynck","html_url":"https://github.com/thomasneirynck","followers_url":"https://api.github.com/users/thomasneirynck/followers","following_url":"https://api.github.com/users/thomasneirynck/following{/other_user}","gists_url":"https://api.github.com/users/thomasneirynck/gists{/gist_id}","starred_url":"https://api.github.com/users/thomasneirynck/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/thomasneirynck/subscriptions","organizations_url":"https://api.github.com/users/thomasneirynck/orgs","repos_url":"https://api.github.com/users/thomasneirynck/repos","events_url":"https://api.github.com/users/thomasneirynck/events{/privacy}","received_events_url":"https://api.github.com/users/thomasneirynck/received_events","type":"User","site_admin":false},"created_at":"2020-02-25T13:49:14Z","updated_at":"2020-02-25T13:49:14Z","author_association":"NONE","body":"+1 on @nickpeihl's suggestion. If `search.max_buckets` is an arbitrary limit, a limit which is a square of a `2 ^ n` makes a lot of sense in the context of mapping and tiling (especially wrt the use of `geotile_grid`.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/590892246","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-590892246","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":590892246,"node_id":"MDEyOklzc3VlQ29tbWVudDU5MDg5MjI0Ng==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2020-02-25T14:26:00Z","updated_at":"2020-02-25T14:26:00Z","author_association":"MEMBER","body":":+1: seems reasonable to me.  \r\n\r\nRelated note, I'll bring this up in our team meeting because we probably want to talk through if we can increase the limit in a 7.x minor, or if that would count as a \"break\".  E.g. if a client _is_ using that to limit responses sizes, it could potentially break clients by now receiving a very large response, which they weren't expecting.\r\n\r\nIf we decide it's a breaking change, a potential plan could be:\r\n\r\n1. Increase the limit to a large power of 2 in `8.0`, leave at 10k for `7.x`\r\n2. Only count buckets during final reduction\r\n3. Add a per-request setting that allows clients to override in 7.x.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/591538979","html_url":"https://github.com/elastic/elasticsearch/issues/51731#issuecomment-591538979","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/51731","id":591538979,"node_id":"MDEyOklzc3VlQ29tbWVudDU5MTUzODk3OQ==","user":{"login":"thomasneirynck","id":1833023,"node_id":"MDQ6VXNlcjE4MzMwMjM=","avatar_url":"https://avatars2.githubusercontent.com/u/1833023?v=4","gravatar_id":"","url":"https://api.github.com/users/thomasneirynck","html_url":"https://github.com/thomasneirynck","followers_url":"https://api.github.com/users/thomasneirynck/followers","following_url":"https://api.github.com/users/thomasneirynck/following{/other_user}","gists_url":"https://api.github.com/users/thomasneirynck/gists{/gist_id}","starred_url":"https://api.github.com/users/thomasneirynck/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/thomasneirynck/subscriptions","organizations_url":"https://api.github.com/users/thomasneirynck/orgs","repos_url":"https://api.github.com/users/thomasneirynck/repos","events_url":"https://api.github.com/users/thomasneirynck/events{/privacy}","received_events_url":"https://api.github.com/users/thomasneirynck/received_events","type":"User","site_admin":false},"created_at":"2020-02-26T17:13:06Z","updated_at":"2020-02-26T17:13:06Z","author_association":"NONE","body":"wrt (3) \r\n\r\nBeing able to override the limit on a per-request setting would be useful for Maps in the 7.x time-frame.\r\n\r\nThere are two ongoing efforts on the Maps-side in 7.x that really could use this:\r\n\r\n- adding top-terms as a sub-agg (https://github.com/elastic/kibana/pull/57875): to support this in Maps, Maps pages through results of a composite-agg to run `terms` sub-aggs under either a `geotile_grid` or `terms` agg. That way, requests are not hitting the limit. This PR merged for 7.7 because we felt the functionality was valuable to introduce. The absence of top-terms was also blocking another 7.7 feature (https://github.com/elastic/kibana/pull/57879). Being able to override the max-bucket setting will allow us to remove this duplicate code-path. Removing the use of the composite-agg code-path altogether will really simplify Maps, since they are not using for any other metric (sum, count, avg, ... calculations)).\r\n- vector tiling (https://github.com/elastic/kibana/issues/58519): we're hoping to support this sometime in the 7.x timeframe. We are looking for an ability to support fine gridding on a per-tile basis. After some experimentation (https://github.com/elastic/kibana/pull/57879), we would likely end up somewhere between 64x64 and 256x256 geotile_grid cells per tile (hence https://github.com/elastic/elasticsearch/issues/51731#issuecomment-590581272).\r\n\r\n\r\ncc @alexfrancoeur @nreese @nickpeihl ","performed_via_github_app":null}]