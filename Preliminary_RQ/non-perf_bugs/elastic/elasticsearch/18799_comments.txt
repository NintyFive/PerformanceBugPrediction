[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225656657","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225656657","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225656657,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTY1NjY1Nw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-06-13T17:49:28Z","updated_at":"2016-06-13T17:49:28Z","author_association":"CONTRIBUTOR","body":"@ywelsch could you take a look at this please\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225828390","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225828390","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225828390,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTgyODM5MA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T09:30:21Z","updated_at":"2016-06-14T09:30:21Z","author_association":"CONTRIBUTOR","body":"@SweetNSourPavement thanks for your interest in testing the ES 5.0 alpha releases and reporting back to us. Let me give some explanations for the observations you've made with ES. Setting `minimum_master_nodes` to a wrong value will indeed lead to data loss (I will later in this post explain why). We've taken a number of steps to make users aware of this:\n- The default configuration has a section that talks about \"split brain\" and the setting `minimum_master_nodes`, pointing to our documentation.\n- Since v5.0.0, we enforce that `minimum_master_nodes` is explicitly set in production mode (#17288)\n- Since v5.0.0, we log warnings when we detect that `minimum_master_nodes` is set to less than a quorum of master-eligible nodes in the cluster (#15625)\n- We are also discussing stricter rules to enforce that `minimum_master_nodes` is set to a quorum of master-eligible nodes (#18573).\n\nThe reason data loss occurs in case of split brains has to do with how we replicate data in ES. Data replication in ES is based on the primary/backup model. This means that there is a single shard copy that is the source of truth (called `the primary`), replicating it's contents to other shard copies. In case of the primary failing, one of the secondary copies is chosen as the new primary. Recovery on cluster restart is simple. The master choses a non-stale copy of the data as primary. The other shard copies simply resync their data from the new primary. As you might have noticed in this description, ES does not do quorum-based recoveries (it does not look at multiple shard copies to determine what data should be part of the new primary). The advantage of primary/backup replication is that this step is not required, making it a simple yet powerful model that also works in scenarios where users want to have only 2 copies of the data.\n\nLet me explain now what happened in the specific scenario which you outlined above: node 1 (when it was shut down) thinks that all three (empty at that point) shard copies of the data are non-stale. After node 1 left, both node 2 and node 3 know that only their shard copies are good (and the copy of node 1 is stale). When restarting the cluster, node 1 gets up first, however, and elects itself as master (before node 2 and 3 are up). This can happen because `minimum_master_nodes` is wrongly set to 1. Node 1 thinks that itâ€™s own copy of the data is good and makes it the primary shard. Node 2 and node 3 then join the cluster and resync their copies of the data from the primary, effectively removing the indexed document.\n\nThe other scenarios you describe can be explained in the same way and follow directly from the data replication model we use. If you want me to clarify on some points, I'm happy to explain further.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225878638","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225878638","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225878638,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTg3ODYzOA==","user":{"login":"SweetNSourPavement","id":19834423,"node_id":"MDQ6VXNlcjE5ODM0NDIz","avatar_url":"https://avatars1.githubusercontent.com/u/19834423?v=4","gravatar_id":"","url":"https://api.github.com/users/SweetNSourPavement","html_url":"https://github.com/SweetNSourPavement","followers_url":"https://api.github.com/users/SweetNSourPavement/followers","following_url":"https://api.github.com/users/SweetNSourPavement/following{/other_user}","gists_url":"https://api.github.com/users/SweetNSourPavement/gists{/gist_id}","starred_url":"https://api.github.com/users/SweetNSourPavement/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SweetNSourPavement/subscriptions","organizations_url":"https://api.github.com/users/SweetNSourPavement/orgs","repos_url":"https://api.github.com/users/SweetNSourPavement/repos","events_url":"https://api.github.com/users/SweetNSourPavement/events{/privacy}","received_events_url":"https://api.github.com/users/SweetNSourPavement/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T13:21:25Z","updated_at":"2016-06-14T13:21:25Z","author_association":"NONE","body":"Thank you for the explanation. My theories, which you now confirmed, led me to poke around replica quorums, and after realizing that this data loss scenario kinda works like intended I probed on.\n\nThe main issue I try to raise here is the inability of es5 to recover an index, even in a seemingly healthy enough state. The script `progress`, when run with parameter `red` configures the cluster correctly, I think:\n- has 3 master nodes\n- all masters stay up all the time, so they have the ability to witness all\n- `minimum_master_nodes` is 2\n- has 3 data nodes\n- 2 data nodes are running when queries are made\n\nand yet, the index state remains red.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225885643","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225885643","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225885643,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTg4NTY0Mw==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T13:46:37Z","updated_at":"2016-06-14T13:46:37Z","author_association":"CONTRIBUTOR","body":"The index state remaining red is expected behavior in this case. When the data nodes shut down one after the other, only the last one will be seen as having a non-stale copy of the data. The reason is that the master nodes can only witness nodes leaving the cluster but not data being written to the shards. Assume for example that after the first two data nodes have left the cluster a document is indexed (and acknowledged) into the shard on the third node just before that one is shut down. To prevent data loss, the master nodes can only resurrect the shard on the third node as primary.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225909199","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225909199","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225909199,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTkwOTE5OQ==","user":{"login":"SweetNSourPavement","id":19834423,"node_id":"MDQ6VXNlcjE5ODM0NDIz","avatar_url":"https://avatars1.githubusercontent.com/u/19834423?v=4","gravatar_id":"","url":"https://api.github.com/users/SweetNSourPavement","html_url":"https://github.com/SweetNSourPavement","followers_url":"https://api.github.com/users/SweetNSourPavement/followers","following_url":"https://api.github.com/users/SweetNSourPavement/following{/other_user}","gists_url":"https://api.github.com/users/SweetNSourPavement/gists{/gist_id}","starred_url":"https://api.github.com/users/SweetNSourPavement/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SweetNSourPavement/subscriptions","organizations_url":"https://api.github.com/users/SweetNSourPavement/orgs","repos_url":"https://api.github.com/users/SweetNSourPavement/repos","events_url":"https://api.github.com/users/SweetNSourPavement/events{/privacy}","received_events_url":"https://api.github.com/users/SweetNSourPavement/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T14:58:13Z","updated_at":"2016-06-14T14:58:13Z","author_association":"NONE","body":"I'm confused now. The expected behaviour is this:\n- There's no guarantee that an index remains available (in the long run) if nodes stop and start?\n- If an index remains unavailable, the best solution is to stop and start nodes until the correct start order is hit upon?\n\nWhat am I missing? Availability seems not that high when an _expected_ single hard failure can cause unbounded unavailability, and human intervention is required.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225914189","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225914189","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225914189,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTkxNDE4OQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T15:13:24Z","updated_at":"2016-06-14T15:13:24Z","author_association":"CONTRIBUTOR","body":"A single hard failure does not cause unbounded unavailability. In your scenario, there was a moment where 2 out of the 3 data nodes were unavailable (when restarted). This means that only the data on the third node was non-stale. Now by shutting down the third node and starting only the first two nodes this single remaining \"good\" copy became unavailable to the cluster...\n\nThe scenario with 3 data nodes and 3 shard copies supports two nodes to fail and the data to still be available.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225916162","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225916162","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225916162,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTkxNjE2Mg==","user":{"login":"SweetNSourPavement","id":19834423,"node_id":"MDQ6VXNlcjE5ODM0NDIz","avatar_url":"https://avatars1.githubusercontent.com/u/19834423?v=4","gravatar_id":"","url":"https://api.github.com/users/SweetNSourPavement","html_url":"https://github.com/SweetNSourPavement","followers_url":"https://api.github.com/users/SweetNSourPavement/followers","following_url":"https://api.github.com/users/SweetNSourPavement/following{/other_user}","gists_url":"https://api.github.com/users/SweetNSourPavement/gists{/gist_id}","starred_url":"https://api.github.com/users/SweetNSourPavement/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SweetNSourPavement/subscriptions","organizations_url":"https://api.github.com/users/SweetNSourPavement/orgs","repos_url":"https://api.github.com/users/SweetNSourPavement/repos","events_url":"https://api.github.com/users/SweetNSourPavement/events{/privacy}","received_events_url":"https://api.github.com/users/SweetNSourPavement/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T15:19:26Z","updated_at":"2016-06-14T15:19:26Z","author_association":"NONE","body":"Only 1 of the data nodes is stopped, the other 2 are up and running, so the write should be a quorum write.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225917462","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225917462","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225917462,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTkxNzQ2Mg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T15:23:29Z","updated_at":"2016-06-14T15:23:29Z","author_association":"CONTRIBUTOR","body":"@SweetNSourPavement That would be the case if all 3 nodes were running then one left.  However, you are recovering from scratch and the node holding the freshest shard copy has not yet appeared, which means that you might lose data unless you wait for it.\n\nIf that node is never coming back, there is an API which allows you to choose one of the stale shard copies as the new primary and to recover anyway, but you run the risk of losing data.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225918819","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225918819","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225918819,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTkxODgxOQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T15:27:32Z","updated_at":"2016-06-14T15:27:32Z","author_association":"CONTRIBUTOR","body":"> Only 1 of the data nodes is stopped, the other 2 are up and running, so the write should be a quorum write.\n\nThe two remaining data nodes are not stopped in perfect synchrony. There is a short moment in time where a write request can sneak in. Write requests do not have to be quorum, that is configurable on a per-request basis.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225921008","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225921008","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225921008,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTkyMTAwOA==","user":{"login":"SweetNSourPavement","id":19834423,"node_id":"MDQ6VXNlcjE5ODM0NDIz","avatar_url":"https://avatars1.githubusercontent.com/u/19834423?v=4","gravatar_id":"","url":"https://api.github.com/users/SweetNSourPavement","html_url":"https://github.com/SweetNSourPavement","followers_url":"https://api.github.com/users/SweetNSourPavement/followers","following_url":"https://api.github.com/users/SweetNSourPavement/following{/other_user}","gists_url":"https://api.github.com/users/SweetNSourPavement/gists{/gist_id}","starred_url":"https://api.github.com/users/SweetNSourPavement/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SweetNSourPavement/subscriptions","organizations_url":"https://api.github.com/users/SweetNSourPavement/orgs","repos_url":"https://api.github.com/users/SweetNSourPavement/repos","events_url":"https://api.github.com/users/SweetNSourPavement/events{/privacy}","received_events_url":"https://api.github.com/users/SweetNSourPavement/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T15:34:28Z","updated_at":"2016-06-14T15:34:28Z","author_association":"NONE","body":"So, I try to sum it up, maybe I will be clearer.\n\nHere's what the script does:\n- starts 3 master nodes\n- starts 3 data node (in the script they are referenced as 4, 5 and 6)\n- creates the index\n- stops one data node (say 4)\n- makes one insert\n- stops the remaining data nodes (5 and 6)\n- starts 2 data nodes (4 and 5)\n\nand I expect the insert to be a quorum write, after that some unavailability while the nodes cycle through their business, and after that an index which can be read.\n\nThe way the nodes are chosen influence the final outcome, red or yellow index state.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225931275","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225931275","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225931275,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTkzMTI3NQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T16:06:46Z","updated_at":"2016-06-14T16:06:46Z","author_association":"CONTRIBUTOR","body":"I'll reiterate what I said above. There is a moment in time in your scenario where only one data node is up (namely when you stop both node 5 and 6, which does not happen in perfect synchrony). The master node observes this and marks the shard on this last node as the only one to have non-stale data, thereby preventing data loss. Although no additional writes have happened in your scenario while stopping the data nodes 5 and 6, that's something the master node is unaware of (it does not participate in data replication).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225936339","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225936339","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225936339,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTkzNjMzOQ==","user":{"login":"SweetNSourPavement","id":19834423,"node_id":"MDQ6VXNlcjE5ODM0NDIz","avatar_url":"https://avatars1.githubusercontent.com/u/19834423?v=4","gravatar_id":"","url":"https://api.github.com/users/SweetNSourPavement","html_url":"https://github.com/SweetNSourPavement","followers_url":"https://api.github.com/users/SweetNSourPavement/followers","following_url":"https://api.github.com/users/SweetNSourPavement/following{/other_user}","gists_url":"https://api.github.com/users/SweetNSourPavement/gists{/gist_id}","starred_url":"https://api.github.com/users/SweetNSourPavement/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SweetNSourPavement/subscriptions","organizations_url":"https://api.github.com/users/SweetNSourPavement/orgs","repos_url":"https://api.github.com/users/SweetNSourPavement/repos","events_url":"https://api.github.com/users/SweetNSourPavement/events{/privacy}","received_events_url":"https://api.github.com/users/SweetNSourPavement/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T16:23:44Z","updated_at":"2016-06-14T16:23:44Z","author_association":"NONE","body":"That's maybe true, I've not read anything substantial on es replication yet, but doesn't help anybody with a stuck index. Power cycles and crashes are normal. Is my test misconfigured? What can I do to have some real higher availability?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225951286","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-225951286","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":225951286,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTk1MTI4Ng==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T17:15:54Z","updated_at":"2016-06-14T17:15:54Z","author_association":"CONTRIBUTOR","body":"> Power cycles and crashes are normal. Is my test misconfigured? What can I do to have some real higher availability?\n\nWith a power cycle, you expect all nodes to rejoin, in which case your index will go green.  With a running cluster where one or two nodes crash, the freshest primary is still online, in which case you can still index with `write_consistency=1` (which is one more node you can afford to lose than with a quorum).  If you do a full cluster restart and don't bring up all the nodes, then you may have lost data.  In this case a human needs to say \"i'd rather lose data than keep on waiting\".\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226467815","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-226467815","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":226467815,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjQ2NzgxNQ==","user":{"login":"SweetNSourPavement","id":19834423,"node_id":"MDQ6VXNlcjE5ODM0NDIz","avatar_url":"https://avatars1.githubusercontent.com/u/19834423?v=4","gravatar_id":"","url":"https://api.github.com/users/SweetNSourPavement","html_url":"https://github.com/SweetNSourPavement","followers_url":"https://api.github.com/users/SweetNSourPavement/followers","following_url":"https://api.github.com/users/SweetNSourPavement/following{/other_user}","gists_url":"https://api.github.com/users/SweetNSourPavement/gists{/gist_id}","starred_url":"https://api.github.com/users/SweetNSourPavement/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SweetNSourPavement/subscriptions","organizations_url":"https://api.github.com/users/SweetNSourPavement/orgs","repos_url":"https://api.github.com/users/SweetNSourPavement/repos","events_url":"https://api.github.com/users/SweetNSourPavement/events{/privacy}","received_events_url":"https://api.github.com/users/SweetNSourPavement/received_events","type":"User","site_admin":false},"created_at":"2016-06-16T12:14:42Z","updated_at":"2016-06-16T12:14:42Z","author_association":"NONE","body":"Do you plan improving on this?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226473599","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-226473599","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":226473599,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjQ3MzU5OQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-06-16T12:42:16Z","updated_at":"2016-06-16T12:42:16Z","author_association":"CONTRIBUTOR","body":"> Do you plan improving on this?\n\nHow would you suggest doing so?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226475652","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-226475652","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":226475652,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjQ3NTY1Mg==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-06-16T12:51:13Z","updated_at":"2016-06-16T12:51:13Z","author_association":"CONTRIBUTOR","body":"Yes, we're currently discussing a solution that involves not marking a shard copy as stale on node shutdown as long as none of the other active shards have received acknowledged writes. I've opened an issue for this here (#18919) so you can track progress on that.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226866096","html_url":"https://github.com/elastic/elasticsearch/issues/18799#issuecomment-226866096","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18799","id":226866096,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjg2NjA5Ng==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2016-06-17T19:53:18Z","updated_at":"2016-06-17T19:53:18Z","author_association":"MEMBER","body":"Closing in favor of #18919\n","performed_via_github_app":null}]