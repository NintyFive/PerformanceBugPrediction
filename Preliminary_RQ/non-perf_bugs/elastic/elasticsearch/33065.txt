{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/33065","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33065/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33065/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33065/events","html_url":"https://github.com/elastic/elasticsearch/issues/33065","id":353034545,"node_id":"MDU6SXNzdWUzNTMwMzQ1NDU=","number":33065,"title":"[Rollup] Managing index lifecycle","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"labels":[{"id":912843355,"node_id":"MDU6TGFiZWw5MTI4NDMzNTU=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Analytics/Rollup","name":":Analytics/Rollup","color":"0e8a16","default":false,"description":"Turn fine-grained time-based data into coarser-grained data"},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2018-08-22T16:37:52Z","updated_at":"2019-10-14T18:03:03Z","closed_at":"2019-10-14T18:03:03Z","author_association":"MEMBER","active_lock_reason":null,"body":"Today, a Rollup job will store its results in a single rollup index.  There is currently no provision for handling jobs that generate such a large volume that they need multiple indices to scale even the rollup data.\r\n\r\nThere are a couple routes we can take... it's not clear to me what the best is.  Current Rollup limitations make it tricky too.\r\n\r\n## Wait for ILM\r\nEasiest option... wait for ILM (#29823) to be merged and then revisit this conversation.  Integrating with ILM somehow will likely provide a better experience instead of baking smaller parts into Rollup.\r\n\r\n## Support external Rollover\r\nRollup doesn't play nicely with Rollover today because we try to create the destination rollup index (and if it exists, update the metadata).  So if the user points their config at a Rollover alias, we throw an exception.\r\n\r\nWe could allow Rollup to point at aliases, which I think would let the user manually Rollover indices.  There are some tricky bits to this though.  Because Rollup uses deterministic IDs for disaster recovery after a checkpoint, the user would have to make sure a checkpoint has been fully committed before rolling over:\r\n\r\n1. Stop the job, wait for it to checkpoint and finish\r\n2. Rollover the index\r\n3. Re-enable the job\r\n\r\nIt's not terrible, but not super user-friendly either.\r\n\r\n## Internally support Rollover\r\nWe could instead implement the Rollover functionality in Rollup.  It'd be essentially the same thing, same procedure, just handled by Rollup.  Probably as another config option, and we just check the Rollover criteria when checkpointing or something.\r\n\r\n## Destination date math/patterns\r\nWe could implement something like:\r\n\r\n```\r\n\"index_pattern\": \"logstash-*\"\r\n\"rollup_index\": \"logstash-%{+YYYY.MM.dd}\",\r\n```\r\nWhich would dynamically create destination indices according to the timestamp of the rollup document.  Unlike Rollover, we don't have to worry about backtracking and replaying documents because docs will deterministically land in their destination index too.\r\n\r\nThis does complicate job creation a little bit, since indices are generated on-demand instead of up-front.  Meaning we'd need to find a way to enrich those indices with metadata after it is generated dynamically\r\n\r\n# Big issue related to all approaches\r\nThe major problem with all of these approaches is that Rollup doesn't allow more than one rollup index in a RollupSearch API.  This was mainly to limit complexity internally rather than a hard limit.  And I think the restriction is less important now that `missing_bucket` is implemented.\r\n\r\nI think we could loosen this restriction as long as all indices involved in the search share the exact same set of rollup jobs, that way we don't have to worry about weird mixed-job incompatibilities.","closed_by":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"performed_via_github_app":null}