[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133320","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133320","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133320,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMyMA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-03T16:54:55Z","updated_at":"2018-04-25T01:36:07Z","author_association":"COLLABORATOR","body":"*Original comment by @droberts195:*\n\nThere is another problem with ML in Cloud, and potentially both could be solved with the same solution.  The other problem is as follows:\r\n\r\nThe cluster restore sequence that Cloud uses is:\r\n\r\n1. Restore indices used by internal services (Security, Watcher, ML, Monitoring, etc.)\r\n2. Restore global cluster state\r\n3. Restore all other indices\r\n\r\nThe reason this has problems with ML in the mix is that step 2 restores persistent tasks and once these are restored ML will start doing stuff.\r\n\r\nML is currently the only GA user of persistent tasks, but I imagine rollups will have very similar problems in 6.3.\r\n\r\nIf the restore sequence could be changed to this then it would solve the problem with persistent tasks:\r\n\r\n1. Set a transient cluster setting that tells ML not to allocate any persistent tasks, say `xpack.ml.enable_persistent_task_allocations: false`\r\n2. Restore indices used by internal services (Security, Watcher, ML, Monitoring, etc.)\r\n3. Restore global cluster state\r\n4. Restore all other indices\r\n5. Unset the transient cluster setting that tells ML not to allocate any persistent tasks - `xpack.ml.enable_persistent_task_allocations: true`\r\n\r\nThis works because transient cluster settings do _not_ get changed by the `include_global_state` option in a snapshot restore.\r\n\r\nSo, once we have implemented such a setting (it doesn't exist today) then it can also use it to solve the problem this issue was originally opened for.  The logic would be:\r\n\r\n1. Cloud clusters are initially created with no ML nodes and `xpack.ml.enable_persistent_task_allocations: false`\r\n2. User creates ML job\r\n3. User opens ML job - this would normally be an error with no ML nodes, but `xpack.ml.enable_persistent_task_allocations: false` means it's not\r\n4. Cloud notices that an ML job exists, adds an ML node to the cluster and, once that's ready, changes the setting of `xpack.ml.enable_persistent_task_allocations` to `true`\r\n5. ML job gets allocated to the ML node and starts doing stuff\r\n\r\nThis approach means a single new setting can solve two different problems we currently have with Cloud deployments.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133321","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133321","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133321,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMyMQ==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-12T11:19:16Z","updated_at":"2018-04-25T01:36:07Z","author_association":"COLLABORATOR","body":"*Original comment by @droberts195:*\n\nWe need to decide whether this should be a boolean setting or if it would make more sense to mimic the semantics of `cluster.routing.allocation.enable` as closely as possible.\r\n\r\nSimilarly for the naming; `xpack.ml.allocation.enable` would be closer to `cluster.routing.allocation.enable` than the name `xpack.ml.enable_persistent_task_allocations` that was suggested in the comment above.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133322","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133322","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133322,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMyMg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-12T15:10:00Z","updated_at":"2018-04-25T01:36:07Z","author_association":"COLLABORATOR","body":"*Original comment by @bleskes:*\n\n@droberts195 can we sync on this one? I think we should implement this on the PersistentTaskService level - something like `cluster.tasks.allocations.enable: _none_` or `cluster.task.allocations.enable: *` . I also think we should think ahead a bit and come up with a vision to how task allocation filtering by attribute would work to more sure we don't paint ourselves into a corner when introducing this setting and have to suffer that BWC implications.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133323","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133323","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133323,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMyMw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-13T11:11:38Z","updated_at":"2018-04-25T01:36:07Z","author_association":"COLLABORATOR","body":"*Original comment by @droberts195:*\n\nI talked with @bleskes about implementing this via new settings for persistent tasks.\r\n\r\nIt seems that much of the functionality that gives users control over shard allocation is also relevant to persistent task allocation:\r\n\r\n* Completely disabling persistent task allocation for the whole cluster\r\n* Disabling persistent task allocation for nodes with certain attributes\r\n* Disabling persistent task allocation for specific types of persistent tasks (e.g. only ML jobs, rollups, reindex, etc.)\r\n\r\nIn the 6.3 timeframe we could implement simple all or nothing:\r\n\r\n```\r\nPUT _cluster/settings\r\n{\r\n    \"transient\" : {\r\n        \"cluster.persistent_tasks.allocation.enable\" : \"all\"\r\n    }\r\n}\r\n```\r\n\r\n```\r\nPUT _cluster/settings\r\n{\r\n    \"transient\" : {\r\n        \"cluster.persistent_tasks.allocation.enable\" : \"none\"\r\n    }\r\n}\r\n```\r\n\r\nIn future this could be extended to enabling allocation of specific task names:\r\n\r\n```\r\nPUT _cluster/settings\r\n{\r\n    \"transient\" : {\r\n        \"cluster.persistent_tasks.allocation.enable\" : \"xpack/rollup/job\"\r\n    }\r\n}\r\n```\r\n\r\n```\r\nPUT _cluster/settings\r\n{\r\n    \"transient\" : {\r\n        \"cluster.persistent_tasks.allocation.enable\" : \"xpack/ml/job,xpack/ml/datafeed\"\r\n    }\r\n}\r\n```\r\n\r\nAs a further enhancement wildcarding could be added:\r\n\r\n```\r\nPUT _cluster/settings\r\n{\r\n    \"transient\" : {\r\n        \"cluster.persistent_tasks.allocation.enable\" : \"xpack/ml/*\"\r\n    }\r\n}\r\n```\r\n\r\nShard allocation by node attribute is controlled using index settings.  Persistent tasks do not have equivalent per-task settings.  For persistent task allocation by node attribute, it would be possible to add an optional sub-section to the existing config objects that would be made available to the persistent tasks framework via a new `getSettings()` method of the `PersistentTaskParams` interface.\r\n\r\nFor example:\r\n\r\n```\r\nPUT _xpack/ml/anomaly_detector/my_job\r\n{\r\n    .\r\n    .\r\n    .\r\n    \"allocation\" : {\r\n        \"include\" : \"fast_cpu,medium_cpu\",\r\n        \"require\" : \"big_memory\",\r\n        \"exclude\" : \"something_else\"\r\n    }\r\n}\r\n```\r\n\r\nThe same could be added to rollup configuration.  Then these attributes could be included in the default assignment decision (which is currently the least loaded data node).  For tasks that have more complex allocation criteria, such as ML jobs, the attributes would be taken into account in addition to the other criteria.  The `PersistentTasksExecutor` would have a method to encapsulate the allocation filtering by attribute.\r\n\r\nThe alternative would be to introduce a whole new endpoint for managing the generic settings that apply to all persistent tasks.  However, although this would lead to more consistency between persistent task types, it would make it impossible to atomically set the allocation filters at the point of creating the persistent task.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133324","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133324","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133324,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMyNA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-13T11:36:06Z","updated_at":"2018-04-25T01:36:07Z","author_association":"COLLABORATOR","body":"*Original comment by @bleskes:*\n\nThanks @droberts195 . This is inline with what I suggested in this ticket but I'm a bit on the fence as it is inconsistent with the cluster level settings for shard allocations. There we don't have `_all` or `_none` but have `all` and `none` (among other things). The main reason is that we never really needed to fully prevent allocation of a specific index and we also have more values there. Concretely:\r\n\r\n```\r\nPUT _cluster/settings {\r\n\t\"persistent\": {\r\n\t  \"cluster.tasks.allocation.enable\": \"none/all\"\r\n\t}\r\n}`\r\n```\r\n\r\nI tend to say we should go with consistency (i.e., `none` and not `_none`). We managed to get so far without people need to disable allocation on an index level. I wonder if we'll ever need it for persistent tasks.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133325","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133325","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133325,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMyNQ==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-13T14:12:53Z","updated_at":"2018-04-25T01:36:07Z","author_association":"COLLABORATOR","body":"*Original comment by @droberts195:*\n\n@bleskes it sounds like `all` and `none` without underscores are better for consistency with indices.  I edited my comment above to reflect this to make it easier for others to review.\r\n\r\n@imotov @martijnvg @polyfractal @jasontedor do you have any comments on the proposal in LINK REDACTED","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133326","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133326","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133326,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMyNg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-13T15:10:49Z","updated_at":"2018-04-25T01:36:08Z","author_association":"COLLABORATOR","body":"*Original comment by @bleskes:*\n\n/cc @tlrx ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133328","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133328","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133328,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMyOA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-14T07:01:50Z","updated_at":"2018-04-25T01:36:08Z","author_association":"COLLABORATOR","body":"*Original comment by @martijnvg:*\n\n@droberts195 The proposal as is now looks good to me. Like @bleskes I don't think we will need to more fine grained control in the future.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133329","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133329","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133329,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMyOQ==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-14T15:13:59Z","updated_at":"2018-04-25T01:36:08Z","author_association":"COLLABORATOR","body":"*Original comment by @polyfractal:*\n\n++ to the short-term `all`/`none` plan as outlined.\r\n\r\nI'm on the fence about the finer grained control.  Could be useful, or maybe not used at all.  Not sure.  No strong opinion from me :)\r\n\r\nAnd ++ to any future allocation control going on the configuration object of each individual persistent task.  We'll have to keep them consistent manually, but I think that's preferable to the complexity (both code- and user-complexity) of management by some unified endpoint.  Makes more sense to configure those details when you configure the Datafeed/Rollup/whatever, instead of after creation with a central endpoint","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133330","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133330","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133330,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMzMA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-15T14:57:11Z","updated_at":"2018-04-25T01:36:08Z","author_association":"COLLABORATOR","body":"*Original comment by @bleskes:*\n\n> And ++ to any future allocation control going on the configuration object of each individual persistent task. We'll have to keep them consistent manually, but I think that's preferable to the complexity (both code- and user-complexity) of management by some unified endpoint. Makes more sense to configure those details when you configure the Datafeed/Rollup/whatever, instead of after creation with a central endpoint\r\n\r\nI guess we'll have to see how it goes. I guess at the first step we'll see to control all jobs of the same type. Later on we may need finer per job control. The later was already described. For the former I mentioned a wildcard approach (colliding with `all`/`none` and hence the need for `_all`) but it can be easy as well if we stopped using `/` in job names to namespace but rather `.`. For example:\r\n\r\n```\r\nPUT _cluster/settings {\r\n\t\"persistent\": {\r\n\t  \"cluster.persistent_tasks.xpack.ml.allocation.enable\": \"none/all\"\r\n\t}\r\n}`\r\n```\r\n\r\nand replacing `xpack.ml` with any other task type.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133332","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133332","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133332,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMzMg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-15T15:06:43Z","updated_at":"2018-04-25T01:36:08Z","author_association":"COLLABORATOR","body":"*Original comment by @polyfractal:*\n\nSorry, I wasn't clear in my comment.  In that last portion, I was referring to _where_ jobs are allocated, similar to shard allocation filtering.  E.g. in @droberts195 proposal it was this syntax:\r\n\r\n```\r\n    \"allocation\" : {\r\n        \"include\" : \"fast_cpu,medium_cpu\",\r\n        \"require\" : \"big_memory\",\r\n        \"exclude\" : \"something_else\"\r\n    }\r\n```\r\n\r\nWhich I think makes sense to put on the individual job configuration objects (if or when we decide to add this sort of functionality), rather than some kind of global endpoint to control job location after the job is configured.\r\n\r\n\r\nI think the control of _when_ jobs are allocated is fine to leave at `enable: all/none` and done across all jobs (as currently proposed), no need to control individual jobs imo.\r\n\r\nSorry for the confusion :)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133333","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133333","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133333,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMzMw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-15T15:11:22Z","updated_at":"2018-04-25T01:36:08Z","author_association":"COLLABORATOR","body":"*Original comment by @bleskes:*\n\nhehe, I think I wasn't clear too in my example. Part of what I mean is that I think we'll first see a need to say \"allocated all ml jobs to this nodes, all reindex to those\". We may want to do it per job but also maybe per type.  This would look like:\r\n\r\n```\r\nPUT _cluster/settings {\r\n\t\"persistent\": {\r\n\t  \"cluster.persistent_tasks.xpack.ml.allocation.include.box\": \"big\"\r\n\t}\r\n}`\r\n```\r\n\r\nAnyway, time will tell. I feel that the direction we're heading is good and will give us what we need.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133334","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133334","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133334,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMzNA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T02:29:46Z","updated_at":"2018-04-25T01:36:09Z","author_association":"COLLABORATOR","body":"*Original comment by @suyograo:*\n\n@droberts195 is there an issue for enabling/disabling the persistent tasks -- I'd like to track it from the Cloud side. Or are we using this issue for adding the enhancement?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133336","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133336","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133336,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMzNg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T05:20:15Z","updated_at":"2018-04-25T01:36:09Z","author_association":"COLLABORATOR","body":"*Original comment by @suyograo:*\n\n@droberts195 @sophiec20 also, will this be available in 6.3? ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133337","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133337","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133337,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMzNw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T07:48:41Z","updated_at":"2018-04-25T01:36:09Z","author_association":"COLLABORATOR","body":"*Original comment by @bleskes:*\n\n@suyograo here's the relevant PR on the ES side: https://github.com/elastic/elasticsearch/pull/29137 . It will be part of 6.3","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384133338","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-384133338","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":384133338,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDEzMzMzOA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T09:09:49Z","updated_at":"2018-04-25T01:36:09Z","author_association":"COLLABORATOR","body":"*Original comment by @droberts195:*\n\n@suyograo I added some notes to your cloud issue.  One more thing to clarify is that _this_ issue has meandered from its original motivation (lazy ML nodes) - all the recent comments are related to generically preventing allocation of persistent tasks.  As I understand it, lazy ML nodes will not be supported in Cloud for a while, so the problem of clusters that don't have any ML nodes is being handled better in 6.3 by the UI changes detailed in LINK REDACTED.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/425393223","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-425393223","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":425393223,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNTM5MzIyMw==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2018-09-28T10:26:04Z","updated_at":"2018-09-28T10:26:04Z","author_association":"CONTRIBUTOR","body":"The original aim of this issue is being revived: having a setting to allow ML jobs to be opened even when there are no ML nodes in the cluster.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/429821794","html_url":"https://github.com/elastic/elasticsearch/issues/29991#issuecomment-429821794","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29991","id":429821794,"node_id":"MDEyOklzc3VlQ29tbWVudDQyOTgyMTc5NA==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2018-10-15T11:51:53Z","updated_at":"2018-10-15T11:51:53Z","author_association":"CONTRIBUTOR","body":"We should try to generalise to the possible future case of being able to add more than one ML node to the cluster.  For the immediate use case, if someone tries to open a job and there is already 1 ML node in the cluster but it's full we'd want to fail that request immediately.  But it's not hard to imagine a scenario where someone would be allowed to add up to 3 ML nodes to the cluster on demand, so jobs that won't fit on existing ML nodes should be permitted to open with 2 or fewer ML nodes in the cluster, but not with 3 or more.\r\n\r\nThat would imply a setting like `xpack.ml.max_lazy_ml_nodes`, with default `0`, near-term Cloud use case `1` and `3` in the example above.\r\n\r\nI think the piece of the code that needs to use this new setting is: https://github.com/elastic/elasticsearch/blob/51eca14288ec76a6533dca5ed5dedd5cffedf33b/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportOpenJobAction.java#L711-L717\r\n\r\nThe tests could go in `TooManyJobsIT`, which already tests scenarios around adding new ML nodes to a cluster, and should be extendible to the case of lazy nodes.  (Or if that's hard for some reason, create a new integration test in the same package that does similar things with different cluster settings.)","performed_via_github_app":null}]