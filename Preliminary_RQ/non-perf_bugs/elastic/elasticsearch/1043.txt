{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/1043","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1043/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1043/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1043/events","html_url":"https://github.com/elastic/elasticsearch/issues/1043","id":1075342,"node_id":"MDU6SXNzdWUxMDc1MzQy","number":1043,"title":"java.lang.OutOfMemoryError: unable to create new native thread","user":{"login":"hibnico","id":282823,"node_id":"MDQ6VXNlcjI4MjgyMw==","avatar_url":"https://avatars3.githubusercontent.com/u/282823?v=4","gravatar_id":"","url":"https://api.github.com/users/hibnico","html_url":"https://github.com/hibnico","followers_url":"https://api.github.com/users/hibnico/followers","following_url":"https://api.github.com/users/hibnico/following{/other_user}","gists_url":"https://api.github.com/users/hibnico/gists{/gist_id}","starred_url":"https://api.github.com/users/hibnico/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hibnico/subscriptions","organizations_url":"https://api.github.com/users/hibnico/orgs","repos_url":"https://api.github.com/users/hibnico/repos","events_url":"https://api.github.com/users/hibnico/events{/privacy}","received_events_url":"https://api.github.com/users/hibnico/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2011-06-17T19:18:04Z","updated_at":"2017-09-26T09:11:35Z","closed_at":"2013-04-05T13:24:58Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The use case to reproduce the error is quite unusual, but probably it shows some weak in the management of a thread pool.\n\nI have one elastic search server 0.16.1 and a java webapp client. Both of them discover themselves via broadcast. Everything works fine. Then I close my laptop, it goes to sleep. A while after that I reopen it. From that point I have errors on the elasticsearch server. Every 30 sec I have:\n\n```\n[2011-06-17 20:27:49,752][WARN ][discovery.zen.ping.multicast] [McKenzie, Namor] failed to connect to requesting node [Magician][d-dJv8_MTCW_reKnhnYd-A][inet[/10.0.0.208:9301]]{client=true, data=false}\norg.elasticsearch.transport.ConnectTransportException: [Magician][inet[/10.0.0.208:9301]] connect_timeout[30s]\n    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:512)\n    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:473)\n    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:126)\n    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver$1.run(MulticastZenPing.java:354)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:680)\nCaused by: java.net.ConnectException: connection timed out\n    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.processConnectTimeout(NioClientSocketPipelineSink.java:371)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.run(NioClientSocketPipelineSink.java:283)\n    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)\n    ... 3 more\n```\n\nAfter some time, in the log I then see in the error output stream (not from log4j):\n\n```\nException in thread \"elasticsearch[McKenzie, Namor]discovery#multicast#received-pool-16-thread-1\" java.lang.OutOfMemoryError: unable to create new native thread\n    at java.lang.Thread.start0(Native Method)\n    at java.lang.Thread.start(Thread.java:658)\n    at java.util.concurrent.ThreadPoolExecutor.addIfUnderMaximumPoolSize(ThreadPoolExecutor.java:727)\n    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:657)\n    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver.run(MulticastZenPing.java:350)\n    at java.lang.Thread.run(Thread.java:680)\n```\n\nI have done a jstack on the server, see the attachement.\n\nThen continuing every 30s, I got connection timed out exception. I can then see that the number of thread is diminuing, one less every 30s. In the hight load of logs, I also got this statck trace after a while, not sure if it is relevant:\n\n```\n[2011-06-17 20:29:05,382][WARN ][transport.netty          ] [McKenzie, Namor] Exception caught on netty layer [[id: 0x482d6445, /10.0.0.208:62813 => /10.0.0.208:9300]]\njava.io.IOException: Operation timed out\n    at sun.nio.ch.FileDispatcher.read0(Native Method)\n    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:21)\n    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:202)\n    at sun.nio.ch.IOUtil.read(IOUtil.java:169)\n    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:243)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:321)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)\n    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:680)\n```\n\nOn my webapp side, if I try to use an search feature, I get:\n\n```\n17.06.2011 21:12:43 ERROR [org.mortbay.log:?] /admin/search/user\njava.lang.RuntimeException: org.springframework.web.util.NestedServletException: Request processing failed; nested exception is org.elasticsearch.discovery.MasterNotDiscoveredException: \n[...]\nCaused by: org.elasticsearch.discovery.MasterNotDiscoveredException: \n    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$3.onTimeout(TransportMasterNodeOperationAction.java:162)\n    at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:281)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:680)\n```\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}