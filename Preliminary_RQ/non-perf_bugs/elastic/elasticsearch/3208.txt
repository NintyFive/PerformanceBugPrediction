{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/3208","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3208/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3208/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3208/events","html_url":"https://github.com/elastic/elasticsearch/issues/3208","id":15774834,"node_id":"MDU6SXNzdWUxNTc3NDgzNA==","number":3208,"title":"Elastic Search Random Node High Load","user":{"login":"djovic","id":4742707,"node_id":"MDQ6VXNlcjQ3NDI3MDc=","avatar_url":"https://avatars3.githubusercontent.com/u/4742707?v=4","gravatar_id":"","url":"https://api.github.com/users/djovic","html_url":"https://github.com/djovic","followers_url":"https://api.github.com/users/djovic/followers","following_url":"https://api.github.com/users/djovic/following{/other_user}","gists_url":"https://api.github.com/users/djovic/gists{/gist_id}","starred_url":"https://api.github.com/users/djovic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/djovic/subscriptions","organizations_url":"https://api.github.com/users/djovic/orgs","repos_url":"https://api.github.com/users/djovic/repos","events_url":"https://api.github.com/users/djovic/events{/privacy}","received_events_url":"https://api.github.com/users/djovic/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2013-06-19T23:47:52Z","updated_at":"2013-09-16T23:40:41Z","closed_at":"2013-06-20T15:59:39Z","author_association":"NONE","active_lock_reason":null,"body":"We have a problem with our Elastic Search cluster in every environment. The cluster will sometimes get into an unstable state, wherein certain ES nodes will have load that is several times greater than the load on other nodes.\n\nThis can be reproduced every time very quickly by hitting the cluster with about 40 concurrent threads while running data indexing, and much less quickly by simply running constant searches.\n\nOur cluster setup:\n- 4 no-data client nodes that service search requests\n- 1 no-data client node that sends data indexing requests.\n- 10 data nodes that are called by the 5 for indexing and searching.\n\nWe have verified that all the boxes have:\n- the same hardware\n  - CPU Intel Xeon X5550 (2.67GHz, 64-bit, 16 core)\n  - 96GB high-end physical RAM (64GB JVM heap, 20 GB mem baseline).\n- the same OS\n  - Red Hat 4.1.2-54 (Linux version 2.6.18-348.1.1.el5)\n- the same JVM\n  - Sun/Oracle 1.7.0_09 (64-bit)\n- the same ES version\n  - Currently on 0.20.4\n  - Problem existed since 0.19.8, when we started with ES\n- no other software running\n- the same number of shards\n  - 1 product shard per node\n  - 13M product documents\n  - (product schema has thousands of fields).\n  - 1 entity shard per node\n  - 135M entity documents\n  - (dozens of entity data types, each with several fields).\n- approximately the same amount of data\n  - product index is 25GB on disk.\n  - entity index is 12GB on disk.\n  - All data loads into about 20GB baseline RAM.\n- the exact same logical configuration\n  {\n      product: {\n          settings: {\n              index.translog.flush_threshold_size: 500mb\n              index.refresh_interval: 30s\n              index.number_of_replicas: 1\n              index.translog.disable_flush: false\n              index.version.created: 190999\n              index.number_of_shards: 5\n              index.routing.allocation.total_shards_per_node: 1\n              index.translog.flush_threshold_period: 60m\n              index.translog.flush_threshold_ops: 5000\n          }\n      }\n      entity: {\n          settings: {\n              index.translog.flush_threshold_size: 500mb\n              index.refresh_interval: 30s\n              index.number_of_replicas: 1\n              index.translog.disable_flush: false\n              index.version.created: 190999\n              index.number_of_shards: 5\n              index.routing.allocation.total_shards_per_node: 1\n              index.translog.flush_threshold_period: 60m\n              index.translog.flush_threshold_ops: 5000\n          }\n      }\n  }\n\nHere is what our load graphs look like.\nNotice the high load ES node. Odd.\n![graphite load test graphs](https://f.cloud.github.com/assets/4742707/678545/6ba6f212-d93a-11e2-9d67-abca2035f3e9.gif)\n\nHere is our cluster distribution.\nAll the shards are similar in size and evenly distributed.\n![es head server profile](https://f.cloud.github.com/assets/4742707/678547/72aebffe-d93a-11e2-9a9c-466ca4ab93ed.gif)\n","closed_by":{"login":"djovic","id":4742707,"node_id":"MDQ6VXNlcjQ3NDI3MDc=","avatar_url":"https://avatars3.githubusercontent.com/u/4742707?v=4","gravatar_id":"","url":"https://api.github.com/users/djovic","html_url":"https://github.com/djovic","followers_url":"https://api.github.com/users/djovic/followers","following_url":"https://api.github.com/users/djovic/following{/other_user}","gists_url":"https://api.github.com/users/djovic/gists{/gist_id}","starred_url":"https://api.github.com/users/djovic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/djovic/subscriptions","organizations_url":"https://api.github.com/users/djovic/orgs","repos_url":"https://api.github.com/users/djovic/repos","events_url":"https://api.github.com/users/djovic/events{/privacy}","received_events_url":"https://api.github.com/users/djovic/received_events","type":"User","site_admin":false},"performed_via_github_app":null}