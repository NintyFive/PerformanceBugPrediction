{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/17005","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17005/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17005/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17005/events","html_url":"https://github.com/elastic/elasticsearch/issues/17005","id":139286869,"node_id":"MDU6SXNzdWUxMzkyODY4Njk=","number":17005,"title":"Elasticsearch 2.2.0 spends a lot more time in merges than 1.7.3","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"labels":[{"id":144797810,"node_id":"MDU6TGFiZWwxNDQ3OTc4MTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Infra/Core","name":":Core/Infra/Core","color":"0e8a16","default":false,"description":"Core issues without another label"},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2016-03-08T14:09:14Z","updated_at":"2016-03-31T18:21:11Z","closed_at":"2016-03-09T16:18:56Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Elasticsearch version**:\n\n```\n# elasticsearch --version\nVersion: 2.2.0, Build: 8ff36d1/2016-01-27T13:32:39Z, JVM: 1.8.0_72-internal\n```\n\n**JVM version**:\n\n```\n# java -version\nopenjdk version \"1.8.0_72-internal\"\nOpenJDK Runtime Environment (build 1.8.0_72-internal-b15)\nOpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode)\n```\n\n**OS version**: Debian Jessie on kernel 4.1.3.\n\n**Description of the problem including expected versus actual behavior**:\n\nI'm trying to migrate existing 1.7.3 installation to 2.2.0 and it seems that 2.2.0 has a regression with merging with large indices. To validate that observation, I ran both 1.7.3 and 2.2.0 and tried to index the same data on both to compare resource usage for both.\n\nTest machine specs: 2 x Intel e5-2630v2 (24 threads), 128GB RAM, 128GB SSD for state, OS in RAM, 12 HDD in 6 RAID0 (striping) arrays (only 2 used for testing).\n\n**Steps to reproduce**:\n1. Run both 1.7.3 and 2.2.0 in docker containers on separate RAID0 arrays:\n\n```\n# docker run -d --net host -v /disk/data1/elasticsearch/es-1.7.3:/usr/share/elasticsearch/data -e ES_HEAP_SIZE=16g --name es-1.7.3 elasticsearch:1.7.3 -Des.http.port=9201 -Des.transport.tcp.port=9301 -Des.cluster.name=es-1.7.3 -Des.discovery.zen.ping.multicast.enabled=false -Des.index.store.throttle.type=none -Des.index.merge.scheduler.max_thread_count=4\n\n# docker run -d --net host -v /disk/data2/elasticsearch/es-2.2.0:/usr/share/elasticsearch/data -e ES_HEAP_SIZE=16g --name es-2.2.0 elasticsearch:2.2.0 -Des.http.port=9202 -Des.transport.tcp.port=9302 -Des.cluster.name=es-2.2.0 -Des.discovery.zen.ping.multicast.enabled=false -Des.index.translog.durability=async -Des.index.translog.sync_interval=10s\n```\n1. Set up the same mapping to enable doc values:\n\n```\n# curl -X PUT http://127.0.0.1:9201/_template/star?pretty -d '{\"template\":\"*\",\"settings\":{\"number_of_shards\":10,\"number_of_replicas\":1,\"index.query.default_field\":\"request_uri\",\"index.refresh_interval\":\"5s\"},\"mappings\":{\"_default_\":{\"_all\":{\"enabled\":false},\"dynamic_templates\":[{\"string_fields\":{\"match\":\"*\",\"match_mapping_type\":\"string\",\"mapping\":{\"type\":\"string\",\"index\":\"not_analyzed\",\"ignore_above\":256,\"doc_values\":true}}},{\"long_fields\":{\"match\":\"*\",\"match_mapping_type\":\"long\",\"mapping\":{\"type\":\"long\",\"doc_values\":true}}},{\"double_fields\":{\"match\":\"*\",\"match_mapping_type\":\"double\",\"mapping\":{\"type\":\"double\",\"doc_values\":true}}},{\"date_fields\":{\"match\":\"*\",\"match_mapping_type\":\"date\",\"mapping\":{\"type\":\"date\",\"doc_values\":true}}}],\"properties\":{\"@timestamp\":{\"type\":\"date\",\"doc_values\":true},\"@version\":{\"type\":\"long\",\"doc_values\":true}}}}}'\n\n# curl -X PUT http://127.0.0.1:9202/_template/star?pretty -d '{\"template\":\"*\",\"settings\":{\"number_of_shards\":10,\"number_of_replicas\":1,\"index.query.default_field\":\"request_uri\",\"index.refresh_interval\":\"5s\"},\"mappings\":{\"_default_\":{\"_all\":{\"enabled\":false},\"dynamic_templates\":[{\"string_fields\":{\"match\":\"*\",\"match_mapping_type\":\"string\",\"mapping\":{\"type\":\"string\",\"index\":\"not_analyzed\",\"ignore_above\":256,\"doc_values\":true}}},{\"long_fields\":{\"match\":\"*\",\"match_mapping_type\":\"long\",\"mapping\":{\"type\":\"long\",\"doc_values\":true}}},{\"double_fields\":{\"match\":\"*\",\"match_mapping_type\":\"double\",\"mapping\":{\"type\":\"double\",\"doc_values\":true}}},{\"date_fields\":{\"match\":\"*\",\"match_mapping_type\":\"date\",\"mapping\":{\"type\":\"date\",\"doc_values\":true}}}],\"properties\":{\"@timestamp\":{\"type\":\"date\",\"doc_values\":true},\"@version\":{\"type\":\"long\",\"doc_values\":true}}}}}'\n```\n1. Take 135M / 800M (114GB) docs from existing index on live cluster and reindex them on both test instances. Source index has the same mapping as destination indices, so 10 shards, 1 replica, doc values. Scroll gives 10k packs of docs, we index them in single bulk request per pack.\n\n```\n# docker run -it --net host --name es-1.7.3-index bobrik/esreindexer -src http://myhost/myindex-2016.03.07 -dst http://127.0.0.1:9201/myindex -pack 1000 -pool 5 -query '{\"query_string\":{\"query\":\"hostname:myhost\"}}'\n\n# docker run -it --net host --name es-2.2.0-index bobrik/esreindexer -src http://myhost/myindex-2016.03.07 -dst http://127.0.0.1:9202/myindex -pack 1000 -pool 5 -query '{\"query_string\":{\"query\":\"hostname:myhost\"}}'\n```\n\nExpected results: both 1.7.3 and 2.2.0 have roughly the same time spent on CPU and in merging.\n\nActual results:\n\n**1.7.3**:\n\nIndexing time: 2h34m6s `2016/03/08 11:05:00 .. 2016/03/08 13:39:06`\n\nIndex stats from `_cat` API right after indexing:\n\n```\nindex   pri docs.count docs.deleted pri.store.size segments.count flush.total_time indexing.index_time merges.total_time refresh.time search.query_time suggest.time \nmyindex  10  133575122            0         23.5gb            262             5.2m                6.3h              1.2h        24.5m                0s           0s \n```\n\nTime to optimize down to 2 segments per shard:\n\n```\nivan@36s10:~$ time curl -s 'http://127.0.0.1:9201/myindex/_optimize?max_num_segments=2&pretty'\n{\n  \"_shards\" : {\n    \"total\" : 20,\n    \"successful\" : 10,\n    \"failed\" : 0\n  }\n}\n\nreal    13m37.409s\nuser    0m0.023s\nsys 0m0.014s\n```\n\nIndex stats from `_cat` API right after optimizing:\n\n```\nindex   pri docs.count docs.deleted pri.store.size segments.count flush.total_time indexing.index_time merges.total_time refresh.time search.query_time suggest.time \nmyindex  10  133575122            0         20.5gb             20             5.9m                6.3h              1.4h        24.5m                0s           0s \n```\n\nTime spent on CPU for the time of test, including optimizing (from cgroup): 2889721 user, 136150 system.\n\n**2.2.0**:\n\nIndexing time: 2h31m33s `2016/03/08 11:05:00 .. 2016/03/08 13:36:33`\n\nIndex stats from `_cat` API right after indexing:\n\n```\nindex   pri docs.count docs.deleted pri.store.size segments.count flush.total_time indexing.index_time merges.total_time refresh.time search.query_time suggest.time \nmyindex  10  133575122            0         19.2gb            255             5.9m                6.9h              2.1h        26.2m                0s           0s \n```\n\nTime to optimize down to 2 segments per shard:\n\n```\n# time curl -s 'http://127.0.0.1:9202/myindex/_optimize?max_num_segments=2&pretty'\n{\n  \"_shards\" : {\n    \"total\" : 20,\n    \"successful\" : 10,\n    \"failed\" : 0\n  }\n}\n\nreal    10m22.218s\nuser    0m0.018s\nsys 0m0.010s\n```\n\nIndex stats from `_cat` API right after optimizing:\n\n```\nindex   pri docs.count docs.deleted pri.store.size segments.count flush.total_time indexing.index_time merges.total_time refresh.time search.query_time suggest.time \nmyindex  10  133575122            0         19.2gb             20               6m                6.9h              2.2h        26.2m                0s           0s \n```\n\nTime spent on CPU for the time of test, including optimizing (from cgroup): 3022558 user, 118353 system.\n\n**Provide logs (if relevant)**:\n\n**1.7.3**:\n\n```\n[2016-03-08 11:02:29,631][INFO ][node                     ] [Ramrod] version[1.7.3], pid[1], build[05d4530/2015-10-15T09:14:17Z]\n[2016-03-08 11:02:29,632][INFO ][node                     ] [Ramrod] initializing ...\n[2016-03-08 11:02:29,692][INFO ][plugins                  ] [Ramrod] loaded [], sites []\n[2016-03-08 11:02:29,723][INFO ][env                      ] [Ramrod] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/md127)]], net usable_space [6.8tb], net total_space [7.2tb], types [ext4]\n[2016-03-08 11:02:31,704][INFO ][node                     ] [Ramrod] initialized\n[2016-03-08 11:02:31,704][INFO ][node                     ] [Ramrod] starting ...\n[2016-03-08 11:02:31,825][INFO ][transport                ] [Ramrod] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/1.2.3.4:9301]}\n[2016-03-08 11:02:31,833][INFO ][discovery                ] [Ramrod] es-1.7.3/eVU3RXfXRvqDPvNoaxIpKQ\n[2016-03-08 11:02:34,849][INFO ][cluster.service          ] [Ramrod] new_master [Ramrod][eVU3RXfXRvqDPvNoaxIpKQ][36s10][inet[/1.2.3.4:9301]], reason: zen-disco-join (elected_as_master)\n[2016-03-08 11:02:34,889][INFO ][http                     ] [Ramrod] bound_address {inet[/0:0:0:0:0:0:0:0:9201]}, publish_address {inet[/1.2.3.4:9201]}\n[2016-03-08 11:02:34,889][INFO ][node                     ] [Ramrod] started\n[2016-03-08 11:02:38,239][INFO ][gateway                  ] [Ramrod] recovered [0] indices into cluster_state\n[2016-03-08 11:05:01,962][INFO ][cluster.metadata         ] [Ramrod] [myindex] creating index, cause [auto(bulk api)], templates [star], shards [10]/[1], mappings [_default_, access-logs]\n[2016-03-08 11:05:06,643][INFO ][cluster.metadata         ] [Ramrod] [myindex] update_mapping [access-logs] (dynamic)\n```\n\n**2.2.0**:\n\n```\n[2016-03-08 11:02:39,101][INFO ][node                     ] [Shinobi Shaw] version[2.2.0], pid[1], build[8ff36d1/2016-01-27T13:32:39Z]\n[2016-03-08 11:02:39,102][INFO ][node                     ] [Shinobi Shaw] initializing ...\n[2016-03-08 11:02:39,502][INFO ][plugins                  ] [Shinobi Shaw] modules [lang-expression, lang-groovy], plugins [], sites []\n[2016-03-08 11:02:39,522][INFO ][env                      ] [Shinobi Shaw] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/md126)]], net usable_space [6.8tb], net total_space [7.2tb], spins? [possibly], types [ext4]\n[2016-03-08 11:02:39,522][INFO ][env                      ] [Shinobi Shaw] heap size [15.8gb], compressed ordinary object pointers [true]\n[2016-03-08 11:02:41,255][INFO ][node                     ] [Shinobi Shaw] initialized\n[2016-03-08 11:02:41,255][INFO ][node                     ] [Shinobi Shaw] starting ...\n[2016-03-08 11:02:41,418][INFO ][transport                ] [Shinobi Shaw] publish_address {1.2.3.4:9302}, bound_addresses {[::]:9302}\n[2016-03-08 11:02:41,426][INFO ][discovery                ] [Shinobi Shaw] es-2.2.0/Gg_471eJRt2lrNM9Tf3bBA\n[2016-03-08 11:02:44,447][INFO ][cluster.service          ] [Shinobi Shaw] new_master {Shinobi Shaw}{Gg_471eJRt2lrNM9Tf3bBA}{1.2.3.4}{1.2.3.4:9302}, reason: zen-disco-join(elected_as_master, [0] joins received)\n[2016-03-08 11:02:44,482][INFO ][http                     ] [Shinobi Shaw] publish_address {1.2.3.4:9202}, bound_addresses {[::]:9202}\n[2016-03-08 11:02:44,482][INFO ][node                     ] [Shinobi Shaw] started\n[2016-03-08 11:02:47,545][INFO ][gateway                  ] [Shinobi Shaw] recovered [0] indices into cluster_state\n[2016-03-08 11:05:03,030][INFO ][cluster.metadata         ] [Shinobi Shaw] [myindex] creating index, cause [auto(bulk api)], templates [star], shards [10]/[1], mappings [_default_, access-logs]\n[2016-03-08 11:05:07,886][INFO ][cluster.routing.allocation] [Shinobi Shaw] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[myindex][9], [myindex][9]] ...]).\n[2016-03-08 11:05:07,946][INFO ][cluster.metadata         ] [Shinobi Shaw] [myindex] update_mapping [access-logs]\n```\n\nHow bad does it get on real clusters for daily indices?\n- 400M docs: 19.2h (on 1.7.3, optimized) -> 1.6d (on 2.2.0, optimized)\n- 3B docs: 12.2d (on 1.7.3, optimized) -> 23.8d (on 2.2.0, not even optimized!)\n\nIt seems that the bigger the index, the bigger the difference.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}