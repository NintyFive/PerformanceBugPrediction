{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/432","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/432/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/432/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/432/events","html_url":"https://github.com/elastic/elasticsearch/issues/432","id":363200,"node_id":"MDU6SXNzdWUzNjMyMDA=","number":432,"title":"Rare node corruption during reallocation","user":{"login":"ppearcy","id":307414,"node_id":"MDQ6VXNlcjMwNzQxNA==","avatar_url":"https://avatars3.githubusercontent.com/u/307414?v=4","gravatar_id":"","url":"https://api.github.com/users/ppearcy","html_url":"https://github.com/ppearcy","followers_url":"https://api.github.com/users/ppearcy/followers","following_url":"https://api.github.com/users/ppearcy/following{/other_user}","gists_url":"https://api.github.com/users/ppearcy/gists{/gist_id}","starred_url":"https://api.github.com/users/ppearcy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ppearcy/subscriptions","organizations_url":"https://api.github.com/users/ppearcy/orgs","repos_url":"https://api.github.com/users/ppearcy/repos","events_url":"https://api.github.com/users/ppearcy/events{/privacy}","received_events_url":"https://api.github.com/users/ppearcy/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":14,"created_at":"2010-10-14T22:33:25Z","updated_at":"2010-11-10T16:46:26Z","closed_at":"2010-11-10T16:46:26Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Issue details:\n- Seen on 0.10-Snapshot and today on 0.11.0\n- There was a rolling Node restart occurring that triggered this and this was done to add a new analyzer (I'd stated on IRC that the cluster was steady state and that is not the case)\n- This implies to me that the corruption occurs when recovering from another node as opposed to recovery from the gateway.\n- Starts with a single node. Can be corrected by clearing work directory for that node and forcing recovery from good node.\n- Can be propogated to gateway and other nodes during recovery\n- The REST API appears to route around the troubled index, while the java API toggles between success and failure as it round robins\n- No exceptions show up in the elastic search log files. I had log levels up at DEBUG when this started and can provide any of the shard relocation details, but I don't see anything meaningful. \n\nWalking the logs, here is everything that I see related:\n\nhttp://gist.github.com/627215\n\nI can help out in anyway necessary to help track this down. \n","closed_by":null,"performed_via_github_app":null}