{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/40814","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/40814/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/40814/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/40814/events","html_url":"https://github.com/elastic/elasticsearch/issues/40814","id":428970704,"node_id":"MDU6SXNzdWU0Mjg5NzA3MDQ=","number":40814,"title":"[CI] ML Tests in UpgradeClusterClientYamlTestSuiteIT fail periodically","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"labels":[{"id":912833043,"node_id":"MDU6TGFiZWw5MTI4MzMwNDM=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:ml","name":":ml","color":"0e8a16","default":false,"description":"Machine learning"},{"id":148612629,"node_id":"MDU6TGFiZWwxNDg2MTI2Mjk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Etest-failure","name":">test-failure","color":"207de5","default":false,"description":"Triaged test failures from CI"}],"state":"closed","locked":false,"assignee":{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false},"assignees":[{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2019-04-03T20:55:24Z","updated_at":"2019-04-25T14:38:53Z","closed_at":"2019-04-25T14:38:52Z","author_association":"MEMBER","active_lock_reason":null,"body":"It manifests itself in different sets of failed tests, but the symptoms \r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+intake/2918/consoleText\r\n```\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/20_security/Verify user and role in upgraded cluster}\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/30_ml_jobs_crud/Test get job with function shortcut should expand}\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/30_ml_jobs_crud/Test open old jobs}\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/40_ml_datafeed_crud/Test old and mixed cluster datafeeds with aggs}\r\n```\r\n\r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+7.x+artifactory/118/console\r\n```\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/20_security/Verify user and role in upgraded cluster}\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/30_ml_jobs_crud/Test get job with function shortcut should expand}\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/30_ml_jobs_crud/Test job with pre 6.4 rules}\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/30_ml_jobs_crud/Test open old jobs}\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/40_ml_datafeed_crud/Test old and mixed cluster datafeeds}\r\n```\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+intake/2880/console\r\n```\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/40_ml_datafeed_crud/Test old and mixed cluster datafeeds with aggs}\r\norg.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT test {p0=upgraded_cluster/40_ml_datafeed_crud/Test old and mixed cluster datafeeds without aggs}\r\n```\r\nThe tests are failing with either `resource already exist` exception or `resource not found exception`\r\n```\r\nSuite: org.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT\r\n  1> [2019-04-04T05:45:56,678][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/40_ml_datafeed_crud/Test old and mixed cluster datafeeds without aggs] before test\r\n  1> [2019-04-04T05:45:56,680][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] initializing REST clients against [http://[::1]:46531]\r\n  1> [2019-04-04T05:45:56,952][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] initializing client, minimum es version [8.0.0], master version, [8.0.0], hosts [http://[::1]:46531]\r\n  1> [2019-04-04T05:46:01,088][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  1> [2019-04-04T05:46:01,089][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/40_ml_datafeed_crud/Test old and mixed cluster datafeeds without aggs] after test\r\n  1> [2019-04-04T05:46:01,113][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/30_ml_jobs_crud/Test job with pre 6.4 rules] before test\r\n  1> [2019-04-04T05:46:01,254][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  2> REPRODUCE WITH: ./gradlew :x-pack:qa:rolling-upgrade:v8.0.0#upgradedClusterTestRunner -Dtests.seed=B7B4AC059DCEE076 -Dtests.class=org.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT -Dtests.method=\"test {p0=upgraded_cluster/30_ml_jobs_crud/Test open old jobs}\" -Dtests.security.manager=true -Dtests.locale=de -Dtests.timezone=Australia/Yancowinna -Dcompiler.java=12 -Druntime.java=8 -Dtests.rest.suite=upgraded_cluster\r\n  1> [2019-04-04T05:46:01,255][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/30_ml_jobs_crud/Test job with pre 6.4 rules] after test\r\n  1> [2019-04-04T05:46:01,274][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/70_ilm/Test Lifecycle Still There And Indices Are Still Managed] before test\r\n  1> [2019-04-04T05:46:01,554][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  1> [2019-04-04T05:46:01,555][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/70_ilm/Test Lifecycle Still There And Indices Are Still Managed] after test\r\n  2> REPRODUCE WITH: ./gradlew :x-pack:qa:rolling-upgrade:v8.0.0#upgradedClusterTestRunner -Dtests.seed=B7B4AC059DCEE076 -Dtests.class=org.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT -Dtests.method=\"test {p0=upgraded_cluster/20_security/Verify user and role in upgraded cluster}\" -Dtests.security.manager=true -Dtests.locale=de -Dtests.timezone=Australia/Yancowinna -Dcompiler.java=12 -Druntime.java=8 -Dtests.rest.suite=upgraded_cluster\r\n  1> [2019-04-04T05:46:01,572][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/60_watcher/CRUD watch APIs] before test\r\n  2> REPRODUCE WITH: ./gradlew :x-pack:qa:rolling-upgrade:v8.0.0#upgradedClusterTestRunner -Dtests.seed=B7B4AC059DCEE076 -Dtests.class=org.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT -Dtests.method=\"test {p0=upgraded_cluster/30_ml_jobs_crud/Test get job with function shortcut should expand}\" -Dtests.security.manager=true -Dtests.locale=de -Dtests.timezone=Australia/Yancowinna -Dcompiler.java=12 -Druntime.java=8 -Dtests.rest.suite=upgraded_cluster\r\n  1> [2019-04-04T05:46:01,667][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  2> REPRODUCE WITH: ./gradlew :x-pack:qa:rolling-upgrade:v8.0.0#upgradedClusterTestRunner -Dtests.seed=B7B4AC059DCEE076 -Dtests.class=org.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT -Dtests.method=\"test {p0=upgraded_cluster/40_ml_datafeed_crud/Test old and mixed cluster datafeeds with aggs}\" -Dtests.security.manager=true -Dtests.locale=de -Dtests.timezone=Australia/Yancowinna -Dcompiler.java=12 -Druntime.java=8 -Dtests.rest.suite=upgraded_cluster\r\n  2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/workspace/elastic+elasticsearch+master+intake/x-pack/qa/rolling-upgrade/build/testrun/v8.0.0#upgradedClusterTestRunner/J0/temp/org.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT_B7B4AC059DCEE076-001\r\n  2> NOTE: test params are: codec=Asserting(Lucene80): {}, docValues:{}, maxPointsInLeafNode=1213, maxMBSortInHeap=6.49584769910644, sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@5957300d), locale=de, timezone=Australia/Yancowinna\r\n  1> [2019-04-04T05:46:01,668][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/60_watcher/CRUD watch APIs] after test\r\n  2> NOTE: Linux 4.15.0-1028-gcp amd64/Oracle Corporation 1.8.0_202 (64-bit)/cpus=16,threads=1,free=432964208,total=514850816\r\n  2> NOTE: All tests run in this JVM: [IndexingIT, TokenBackwardsCompatibilityIT, MlMappingsUpgradeIT, WatcherRestartIT, RollupIDUpgradeIT, UpgradeClusterClientYamlTestSuiteIT]\r\nIGNOR/A 0.11s | UpgradeClusterClientYamlTestSuiteIT.test {p0=upgraded_cluster/60_watcher/CRUD watch APIs}\r\n   > Assumption #1: [upgraded_cluster/60_watcher/CRUD watch APIs] skipped, reason: [https://github.com/elastic/elasticsearch/issues/33185]\r\n  1> [2019-04-04T05:46:01,684][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/30_ml_jobs_crud/Test open old jobs] before test\r\n  1> [2019-04-04T05:46:03,423][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] Stash dump on test failure [{\r\n  1>   \"stash\" : {\r\n  1>     \"body\" : {\r\n  1>       \"error\" : {\r\n  1>         \"root_cause\" : [\r\n  1>           {\r\n  1>             \"type\" : \"resource_already_exists_exception\",\r\n  1>             \"reason\" : \"task with id {job-old-cluster-job} already exist\",\r\n  1>             \"stack_trace\" : \"ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist]\r\n  1> \tat org.elasticsearch.persistent.PersistentTasksClusterService$1.execute(PersistentTasksClusterService.java:101)\r\n  1> \tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\r\n  1> \tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\r\n  1> \tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\r\n  1> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:677)\r\n  1> \tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\r\n  1> \tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n  1> \tat java.lang.Thread.run(Thread.java:748)\r\n  1> \"\r\n  1>           }\r\n  1>         ],\r\n  1>         \"type\" : \"status_exception\",\r\n  1>         \"reason\" : \"Cannot open job [old-cluster-job] because it has already been opened\",\r\n  1>         \"caused_by\" : {\r\n  1>           \"type\" : \"resource_already_exists_exception\",\r\n  1>           \"reason\" : \"task with id {job-old-cluster-job} already exist\",\r\n  1>           \"stack_trace\" : \"ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist]\r\n  1> \tat org.elasticsearch.persistent.PersistentTasksClusterService$1.execute(PersistentTasksClusterService.java:101)\r\n  1> \tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\r\n  1> \tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\r\n  1> \tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\r\n  1> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:677)\r\n  1> \tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\r\n  1> \tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n  1> \tat java.lang.Thread.run(Thread.java:748)\r\n  1> \"\r\n  1>         },\r\n  1>         \"stack_trace\" : \"ElasticsearchStatusException[Cannot open job [old-cluster-job] because it has already been opened]; nested: ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist];\r\n  1> \tat org.elasticsearch.xpack.ml.action.TransportOpenJobAction$1.onFailure(TransportOpenJobAction.java:437)\r\n  1> \tat org.elasticsearch.action.ActionListener$1.onFailure(ActionListener.java:70)\r\n  1> \tat org.elasticsearch.action.ActionListener$1.onFailure(ActionListener.java:70)\r\n  1> \tat org.elasticsearch.action.support.ContextPreservingActionListener.onFailure(ContextPreservingActionListener.java:50)\r\n  1> \tat org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:74)\r\n  1> \tat org.elasticsearch.action.support.ContextPreservingActionListener.onFailure(ContextPreservingActionListener.java:50)\r\n  1> \tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$1.onFailure(TransportMasterNodeAction.java:201)\r\n  1> \tat org.elasticsearch.persistent.StartPersistentTaskAction$TransportAction$1.onFailure(StartPersistentTaskAction.java:229)\r\n  1> \tat org.elasticsearch.persistent.PersistentTasksClusterService$1.onFailure(PersistentTasksClusterService.java:113)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService$SafeClusterStateTaskListener.onFailure(MasterService.java:499)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService$TaskOutputs.notifyFailedTasks(MasterService.java:432)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:211)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\r\n  1> \tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\r\n  1> \tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\r\n  1> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:677)\r\n  1> \tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\r\n  1> \tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n  1> \tat java.lang.Thread.run(Thread.java:748)\r\n  1> Caused by: ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist]\r\n  1> \tat org.elasticsearch.persistent.PersistentTasksClusterService$1.execute(PersistentTasksClusterService.java:101)\r\n  1> \tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\r\n  1> \t... 9 more\r\n  1> \"\r\n  1>       },\r\n  1>       \"status\" : 409\r\n  1>     }\r\n  1>   }\r\n  1> }]\r\n  1> [2019-04-04T05:46:04,061][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  1> [2019-04-04T05:46:04,062][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/30_ml_jobs_crud/Test open old jobs] after test\r\nFAILURE 2.40s | UpgradeClusterClientYamlTestSuiteIT.test {p0=upgraded_cluster/30_ml_jobs_crud/Test open old jobs} <<< FAILURES!\r\n   > Throwable #1: java.lang.AssertionError: Failure at [upgraded_cluster/30_ml_jobs_crud:11]: expected [2xx] status code but api [ml.open_job] returned [409 Conflict] [{\"error\":{\"root_cause\":[{\"type\":\"resource_already_exists_exception\",\"reason\":\"task with id {job-old-cluster-job} already exist\",\"stack_trace\":\"ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist]\\n\\tat org.elasticsearch.persistent.PersistentTasksClusterService$1.execute(PersistentTasksClusterService.java:101)\\n\\tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\\n\\tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\\n\\tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\\n\\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\\n\\tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\\n\\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:677)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\"}],\"type\":\"status_exception\",\"reason\":\"Cannot open job [old-cluster-job] because it has already been opened\",\"caused_by\":{\"type\":\"resource_already_exists_exception\",\"reason\":\"task with id {job-old-cluster-job} already exist\",\"stack_trace\":\"ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist]\\n\\tat org.elasticsearch.persistent.PersistentTasksClusterService$1.execute(PersistentTasksClusterService.java:101)\\n\\tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\\n\\tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\\n\\tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\\n\\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\\n\\tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\\n\\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:677)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\"},\"stack_trace\":\"ElasticsearchStatusException[Cannot open job [old-cluster-job] because it has already been opened]; nested: ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist];\\n\\tat org.elasticsearch.xpack.ml.action.TransportOpenJobAction$1.onFailure(TransportOpenJobAction.java:437)\\n\\tat org.elasticsearch.action.ActionListener$1.onFailure(ActionListener.java:70)\\n\\tat org.elasticsearch.action.ActionListener$1.onFailure(ActionListener.java:70)\\n\\tat org.elasticsearch.action.support.ContextPreservingActionListener.onFailure(ContextPreservingActionListener.java:50)\\n\\tat org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:74)\\n\\tat org.elasticsearch.action.support.ContextPreservingActionListener.onFailure(ContextPreservingActionListener.java:50)\\n\\tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$1.onFailure(TransportMasterNodeAction.java:201)\\n\\tat org.elasticsearch.persistent.StartPersistentTaskAction$TransportAction$1.onFailure(StartPersistentTaskAction.java:229)\\n\\tat org.elasticsearch.persistent.PersistentTasksClusterService$1.onFailure(PersistentTasksClusterService.java:113)\\n\\tat org.elasticsearch.cluster.service.MasterService$SafeClusterStateTaskListener.onFailure(MasterService.java:499)\\n\\tat org.elasticsearch.cluster.service.MasterService$TaskOutputs.notifyFailedTasks(MasterService.java:432)\\n\\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:211)\\n\\tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\\n\\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:677)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\nCaused by: ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist]\\n\\tat org.elasticsearch.persistent.PersistentTasksClusterService$1.execute(PersistentTasksClusterService.java:101)\\n\\tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\\n\\tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\\n\\tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\\n\\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\\n\\t... 9 more\\n\"},\"status\":409}]\r\n   > \tat __randomizedtesting.SeedInfo.seed([B7B4AC059DCEE076:3FE093DF33328D8E]:0)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:393)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:370)\r\n   > \tat java.lang.Thread.run(Thread.java:748)\r\n   > Caused by: java.lang.AssertionError: expected [2xx] status code but api [ml.open_job] returned [409 Conflict] [{\"error\":{\"root_cause\":[{\"type\":\"resource_already_exists_exception\",\"reason\":\"task with id {job-old-cluster-job} already exist\",\"stack_trace\":\"ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist]\\n\\tat org.elasticsearch.persistent.PersistentTasksClusterService$1.execute(PersistentTasksClusterService.java:101)\\n\\tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\\n\\tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\\n\\tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\\n\\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\\n\\tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\\n\\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:677)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\"}],\"type\":\"status_exception\",\"reason\":\"Cannot open job [old-cluster-job] because it has already been opened\",\"caused_by\":{\"type\":\"resource_already_exists_exception\",\"reason\":\"task with id {job-old-cluster-job} already exist\",\"stack_trace\":\"ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist]\\n\\tat org.elasticsearch.persistent.PersistentTasksClusterService$1.execute(PersistentTasksClusterService.java:101)\\n\\tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\\n\\tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\\n\\tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\\n\\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\\n\\tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\\n\\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:677)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\"},\"stack_trace\":\"ElasticsearchStatusException[Cannot open job [old-cluster-job] because it has already been opened]; nested: ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist];\\n\\tat org.elasticsearch.xpack.ml.action.TransportOpenJobAction$1.onFailure(TransportOpenJobAction.java:437)\\n\\tat org.elasticsearch.action.ActionListener$1.onFailure(ActionListener.java:70)\\n\\tat org.elasticsearch.action.ActionListener$1.onFailure(ActionListener.java:70)\\n\\tat org.elasticsearch.action.support.ContextPreservingActionListener.onFailure(ContextPreservingActionListener.java:50)\\n\\tat org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:74)\\n\\tat org.elasticsearch.action.support.ContextPreservingActionListener.onFailure(ContextPreservingActionListener.java:50)\\n\\tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$1.onFailure(TransportMasterNodeAction.java:201)\\n\\tat org.elasticsearch.persistent.StartPersistentTaskAction$TransportAction$1.onFailure(StartPersistentTaskAction.java:229)\\n\\tat org.elasticsearch.persistent.PersistentTasksClusterService$1.onFailure(PersistentTasksClusterService.java:113)\\n\\tat org.elasticsearch.cluster.service.MasterService$SafeClusterStateTaskListener.onFailure(MasterService.java:499)\\n\\tat org.elasticsearch.cluster.service.MasterService$TaskOutputs.notifyFailedTasks(MasterService.java:432)\\n\\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:211)\\n\\tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\\n\\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:677)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\nCaused by: ResourceAlreadyExistsException[task with id {job-old-cluster-job} already exist]\\n\\tat org.elasticsearch.persistent.PersistentTasksClusterService$1.execute(PersistentTasksClusterService.java:101)\\n\\tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\\n\\tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\\n\\tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\\n\\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\\n\\t... 9 more\\n\"},\"status\":409}]\r\n   > \tat org.elasticsearch.test.rest.yaml.section.DoSection.execute(DoSection.java:261)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:386)\r\n   > \t... 38 more\r\n  1> [2019-04-04T05:46:04,086][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/20_security/Verify user and role in upgraded cluster] before test\r\n  1> [2019-04-04T05:47:14,271][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] Stash dump on test failure [{\r\n  1>   \"stash\" : {\r\n  1>     \"body\" : {\r\n  1>       \"cluster_name\" : \"rolling-upgrade\",\r\n  1>       \"status\" : \"green\",\r\n  1>       \"timed_out\" : true,\r\n  1>       \"number_of_nodes\" : 2,\r\n  1>       \"number_of_data_nodes\" : 2,\r\n  1>       \"active_primary_shards\" : 14,\r\n  1>       \"active_shards\" : 27,\r\n  1>       \"relocating_shards\" : 0,\r\n  1>       \"initializing_shards\" : 0,\r\n  1>       \"unassigned_shards\" : 0,\r\n  1>       \"delayed_unassigned_shards\" : 0,\r\n  1>       \"number_of_pending_tasks\" : 0,\r\n  1>       \"number_of_in_flight_fetch\" : 0,\r\n  1>       \"task_max_waiting_in_queue_millis\" : 0,\r\n  1>       \"active_shards_percent_as_number\" : 100.0\r\n  1>     }\r\n  1>   }\r\n  1> }]\r\n  1> [2019-04-04T05:47:14,288][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  1> [2019-04-04T05:47:14,289][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/20_security/Verify user and role in upgraded cluster] after test\r\nFAILURE 70.2s | UpgradeClusterClientYamlTestSuiteIT.test {p0=upgraded_cluster/20_security/Verify user and role in upgraded cluster} <<< FAILURES!\r\n   > Throwable #1: java.lang.AssertionError: Failure at [upgraded_cluster/20_security:5]: expected [2xx] status code but api [cluster.health] returned [408 Request Timeout] [{\"cluster_name\":\"rolling-upgrade\",\"status\":\"green\",\"timed_out\":true,\"number_of_nodes\":2,\"number_of_data_nodes\":2,\"active_primary_shards\":14,\"active_shards\":27,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":100.0}]\r\n   > \tat __randomizedtesting.SeedInfo.seed([B7B4AC059DCEE076:3FE093DF33328D8E]:0)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:393)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:370)\r\n   > \tat java.lang.Thread.run(Thread.java:748)\r\n   > Caused by: java.lang.AssertionError: expected [2xx] status code but api [cluster.health] returned [408 Request Timeout] [{\"cluster_name\":\"rolling-upgrade\",\"status\":\"green\",\"timed_out\":true,\"number_of_nodes\":2,\"number_of_data_nodes\":2,\"active_primary_shards\":14,\"active_shards\":27,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":100.0}]\r\n   > \tat org.elasticsearch.test.rest.yaml.section.DoSection.execute(DoSection.java:261)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:386)\r\n   > \t... 38 more\r\n  1> [2019-04-04T05:47:14,302][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/10_basic/Continue scroll after upgrade] before test\r\n  1> [2019-04-04T05:47:14,410][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  1> [2019-04-04T05:47:14,410][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/10_basic/Continue scroll after upgrade] after test\r\n  1> [2019-04-04T05:47:14,421][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/60_watcher/Test watcher stats output] before test\r\n  1> [2019-04-04T05:47:14,469][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  1> [2019-04-04T05:47:14,470][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/60_watcher/Test watcher stats output] after test\r\nIGNOR/A 0.06s | UpgradeClusterClientYamlTestSuiteIT.test {p0=upgraded_cluster/60_watcher/Test watcher stats output}\r\n   > Assumption #1: [upgraded_cluster/60_watcher/Test watcher stats output] skipped, reason: [https://github.com/elastic/elasticsearch/issues/33185]\r\n  1> [2019-04-04T05:47:14,481][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/30_ml_jobs_crud/Test get job with function shortcut should expand] before test\r\n  1> [2019-04-04T05:48:24,518][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] Stash dump on test failure [{\r\n  1>   \"stash\" : {\r\n  1>     \"body\" : {\r\n  1>       \"cluster_name\" : \"rolling-upgrade\",\r\n  1>       \"status\" : \"green\",\r\n  1>       \"timed_out\" : true,\r\n  1>       \"number_of_nodes\" : 2,\r\n  1>       \"number_of_data_nodes\" : 2,\r\n  1>       \"active_primary_shards\" : 14,\r\n  1>       \"active_shards\" : 27,\r\n  1>       \"relocating_shards\" : 0,\r\n  1>       \"initializing_shards\" : 0,\r\n  1>       \"unassigned_shards\" : 0,\r\n  1>       \"delayed_unassigned_shards\" : 0,\r\n  1>       \"number_of_pending_tasks\" : 0,\r\n  1>       \"number_of_in_flight_fetch\" : 0,\r\n  1>       \"task_max_waiting_in_queue_millis\" : 0,\r\n  1>       \"active_shards_percent_as_number\" : 100.0\r\n  1>     }\r\n  1>   }\r\n  1> }]\r\n  1> [2019-04-04T05:48:24,537][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  1> [2019-04-04T05:48:24,538][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/30_ml_jobs_crud/Test get job with function shortcut should expand] after test\r\nFAILURE 70.1s | UpgradeClusterClientYamlTestSuiteIT.test {p0=upgraded_cluster/30_ml_jobs_crud/Test get job with function shortcut should expand} <<< FAILURES!\r\n   > Throwable #1: java.lang.AssertionError: Failure at [upgraded_cluster/30_ml_jobs_crud:2]: expected [2xx] status code but api [cluster.health] returned [408 Request Timeout] [{\"cluster_name\":\"rolling-upgrade\",\"status\":\"green\",\"timed_out\":true,\"number_of_nodes\":2,\"number_of_data_nodes\":2,\"active_primary_shards\":14,\"active_shards\":27,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":100.0}]\r\n   > \tat __randomizedtesting.SeedInfo.seed([B7B4AC059DCEE076:3FE093DF33328D8E]:0)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:393)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:361)\r\n   > \tat java.lang.Thread.run(Thread.java:748)\r\n   > Caused by: java.lang.AssertionError: expected [2xx] status code but api [cluster.health] returned [408 Request Timeout] [{\"cluster_name\":\"rolling-upgrade\",\"status\":\"green\",\"timed_out\":true,\"number_of_nodes\":2,\"number_of_data_nodes\":2,\"active_primary_shards\":14,\"active_shards\":27,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":100.0}]\r\n   > \tat org.elasticsearch.test.rest.yaml.section.DoSection.execute(DoSection.java:261)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:386)\r\n   > \t... 38 more\r\n  1> [2019-04-04T05:48:24,551][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/50_token_auth/Get the indexed token and use if to authenticate] before test\r\n  1> [2019-04-04T05:48:24,600][WARN ][o.e.c.RestClient         ] [test] request [GET http://[::1]:46531/token_index/doc/6?error_trace=true] returned 1 warnings: [299 Elasticsearch-8.0.0-SNAPSHOT-cfea348 \"[types removal] Specifying types in document get requests is deprecated, use the /{index}/_doc/{id} endpoint instead.\"]\r\n  1> [2019-04-04T05:48:24,667][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  1> [2019-04-04T05:48:24,667][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/50_token_auth/Get the indexed token and use if to authenticate] after test\r\n  1> [2019-04-04T05:48:24,677][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/40_ml_datafeed_crud/Test old and mixed cluster datafeeds with aggs] before test\r\n  1> [2019-04-04T05:49:34,715][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] Stash dump on test failure [{\r\n  1>   \"stash\" : {\r\n  1>     \"body\" : {\r\n  1>       \"cluster_name\" : \"rolling-upgrade\",\r\n  1>       \"status\" : \"green\",\r\n  1>       \"timed_out\" : true,\r\n  1>       \"number_of_nodes\" : 2,\r\n  1>       \"number_of_data_nodes\" : 2,\r\n  1>       \"active_primary_shards\" : 14,\r\n  1>       \"active_shards\" : 27,\r\n  1>       \"relocating_shards\" : 0,\r\n  1>       \"initializing_shards\" : 0,\r\n  1>       \"unassigned_shards\" : 0,\r\n  1>       \"delayed_unassigned_shards\" : 0,\r\n  1>       \"number_of_pending_tasks\" : 0,\r\n  1>       \"number_of_in_flight_fetch\" : 0,\r\n  1>       \"task_max_waiting_in_queue_millis\" : 0,\r\n  1>       \"active_shards_percent_as_number\" : 100.0\r\n  1>     }\r\n  1>   }\r\n  1> }]\r\n  1> [2019-04-04T05:49:34,732][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] There are still tasks running after this test that might break subsequent tests [xpack/ml/job[c]].\r\n  1> [2019-04-04T05:49:34,733][INFO ][o.e.u.UpgradeClusterClientYamlTestSuiteIT] [test] [p0=upgraded_cluster/40_ml_datafeed_crud/Test old and mixed cluster datafeeds with aggs] after test\r\nFAILURE 70.1s | UpgradeClusterClientYamlTestSuiteIT.test {p0=upgraded_cluster/40_ml_datafeed_crud/Test old and mixed cluster datafeeds with aggs} <<< FAILURES!\r\n   > Throwable #1: java.lang.AssertionError: Failure at [upgraded_cluster/40_ml_datafeed_crud:2]: expected [2xx] status code but api [cluster.health] returned [408 Request Timeout] [{\"cluster_name\":\"rolling-upgrade\",\"status\":\"green\",\"timed_out\":true,\"number_of_nodes\":2,\"number_of_data_nodes\":2,\"active_primary_shards\":14,\"active_shards\":27,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":100.0}]\r\n   > \tat __randomizedtesting.SeedInfo.seed([B7B4AC059DCEE076:3FE093DF33328D8E]:0)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:393)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:361)\r\n   > \tat java.lang.Thread.run(Thread.java:748)\r\n   > Caused by: java.lang.AssertionError: expected [2xx] status code but api [cluster.health] returned [408 Request Timeout] [{\"cluster_name\":\"rolling-upgrade\",\"status\":\"green\",\"timed_out\":true,\"number_of_nodes\":2,\"number_of_data_nodes\":2,\"active_primary_shards\":14,\"active_shards\":27,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":100.0}]\r\n   > \tat org.elasticsearch.test.rest.yaml.section.DoSection.execute(DoSection.java:261)\r\n   > \tat org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:386)\r\n   > \t... 38 more\r\nCompleted [6/6] in 218.09s, 11 tests, 4 failures, 2 skipped <<< FAILURES!\r\n```","closed_by":{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false},"performed_via_github_app":null}