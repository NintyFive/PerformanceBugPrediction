{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/6141","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6141/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6141/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6141/events","html_url":"https://github.com/elastic/elasticsearch/issues/6141","id":33366767,"node_id":"MDU6SXNzdWUzMzM2Njc2Nw==","number":6141,"title":"Shard rebalance not obeying cluster_concurrent_rebalance","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"labels":[{"id":836504707,"node_id":"MDU6TGFiZWw4MzY1MDQ3MDc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Distributed","name":":Distributed/Distributed","color":"0e8a16","default":false,"description":"A catch all label for anything in the Distributed Area. If you aren't sure, use this one."}],"state":"closed","locked":false,"assignee":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"assignees":[{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false}],"milestone":null,"comments":11,"created_at":"2014-05-13T03:47:29Z","updated_at":"2018-02-13T19:28:50Z","closed_at":"2015-10-27T16:01:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"During full cluster restart relocating shards steadily increased and did not respond to setting cluster.routing.allocation.cluster_concurrent_rebalance. This behavior is pretty clearly cause by having disk.threshold_enabled set.\n- 00:30: Relocating shards was around 86 causing us to get tight on disk space (a node ran out of space and we deleted a number of shards with rm before restarting it).\n- 00:57: Set cluster_concurrent_rebalance to 10 (trying to limit relocating shards)\n- 01:00 Set to 4 (after realizing it was already set to 6 in the config).\n- 01:33 Set to 2 (in case something had changed and it was now per node not for the whole cluster)\n- 02:00 Relocating shards hit its peak of ~140\n\n![image](https://cloud.githubusercontent.com/assets/820871/2953053/8521b826-da4c-11e3-8b56-749eca93943a.png)\n\nI'm guessing there is an interaction with disk threshold and/or shard allocation. We're seeing a lot of disk space taken up by old shards that haven't yet been cleaned up (note this is from 03:38):\n\n![image](https://cloud.githubusercontent.com/assets/820871/2953176/4722b9f4-da50-11e3-8469-b661b9d9ff9f.png)\n\nSystem config:\n- 175 primary shards (plus 2 replicas per shard) = 525 shards total\n- 36 data nodes (3 master nodes).\n- Total disk space in cluster is 26TB. Total of all primaries is about 4.6TB (x3 = 13.8TB). Should be plenty of space.\n- Some shards are pretty big. Max of about 45GB. Generally range from 15-40GB. \n- Just ran a full cluster restart to go from v0.90.12 to v1.1.1 (went pretty smoothly otherwise)\n- Shard allocation awareness: DC = dfw, iad, sat; PARITY=0,1 (so 6 zones with 6 servers in each)\n\nRelevant config settings:\n\n```\ncluster:\n  routing:\n    allocation:\n      node_initial_primaries_recoveries: 8\n      node_concurrent_recoveries: 15\n      cluster_concurrent_rebalance: 6\n      awareness.attributes: dc, parity\n      disk.threshold_enabled: true\n\nindices:\n  recovery:\n    max_bytes_per_sec: 100mb\n    concurrent_streams: 5\n```\n\n```\n{\n  \"cluster_name\" : \"es_glbl_cluster\",\n  \"status\" : \"yellow\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 39,\n  \"number_of_data_nodes\" : 36,\n  \"active_primary_shards\" : 175,\n  \"active_shards\" : 469,\n  \"relocating_shards\" : 135,\n  \"initializing_shards\" : 56,\n  \"unassigned_shards\" : 0\n}\n```\n","closed_by":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"performed_via_github_app":null}