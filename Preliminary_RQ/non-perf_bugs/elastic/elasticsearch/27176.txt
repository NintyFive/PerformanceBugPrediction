{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/27176","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27176/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27176/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27176/events","html_url":"https://github.com/elastic/elasticsearch/issues/27176","id":269700633,"node_id":"MDU6SXNzdWUyNjk3MDA2MzM=","number":27176,"title":"Make large indexes easier to manage","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"labels":[{"id":163824881,"node_id":"MDU6TGFiZWwxNjM4MjQ4ODE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Features/Indices%20APIs","name":":Core/Features/Indices APIs","color":"0e8a16","default":false,"description":"APIs to create and manage indices"},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2017-10-30T18:10:18Z","updated_at":"2018-02-13T20:37:13Z","closed_at":"2017-11-10T16:15:11Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Since large is subjective, you can interpret it as an index measured in terabytes.\r\n\r\n## Problem\r\nFor non-time series data, it can be difficult to find a value to split your data on when defining your indexes. You need a value that is relatively low cardinality and always has a single value on your documents. If you don't have such a field then you'll probably end up creating a single index to store all of your data. Even when you can split your data into indexes, you may still find yourself with large indexes simply due to scale and/or poor data distributions.\r\n\r\nAs part of your cluster operations, there will likely come a time where you need to temporarily duplicate an index. For example, recreating the index to change options like the shard count, mappings, or other settings that can only be changed at index creation time. Another example is recovery when you lose some shards and restore from backups to a new index.\r\n\r\nIn these situations, you'll need enough hardware to sustain two fully replicated versions of your index before you shift your traffic from your old index to your new index then drop the old index. You'll need to potentially double the nodes in your cluster assuming you weren't heavily over-scaled to begin with. If you like to live dangerously then you could probably avoid this by disabling replicas but I'll assume that is not the norm considering the data loss risk.\r\n\r\nFor small clusters, temporarily going from 2 nodes to 4 nodes won't be a big deal but extrapolating this to a larger scale then it can start to become problematic - temporarily doubling the size of 50 / 100 / 200 node clusters is undesirable.\r\n\r\nI think it would be helpful if there was a built-in way to break such indexes down into smaller pieces, so they can be worked with individually and iterated over rather than trying to do everything in one shot.\r\n\r\nThere are ways to do this from the client side today but I'd see that as a workaround rather than a solution.\r\n\r\n## Proposal\r\nOne way to tackle this problem is to add support for an index of indexes, or what I'll call a composite index. This would function similar to an alias except it enforces some additional constraints and it would always be writable.\r\n\r\nA composite index is created with a list of 2 or more existing indexes and the number of indexes cannot be changed after creation. For example, creating composite index `foo` with 2 underlying real indexes:\r\n\r\n```javascript\r\nPUT /_composites/foo\r\n{\r\n  \"indexes\": [\r\n    \"foo_0_v1\",\r\n    \"foo_1_v1\"\r\n  ]\r\n}\r\n```\r\n\r\nIndexing a document against `foo` will do the calculation `hash(_id, seed) % num_indexes` to pick between `foo_0_v1` and `foo_1_v1`.\r\n* If `_routing` was provided, that would be used instead of `_id`\r\n* A composite index must use a different `seed` than the current default to ensure a good data distribution across shards\r\n\r\nSearching against `foo` will go across all indexes unless a `_routing` value is provided, in which case it will pick the index based on the above calculation.\r\n\r\nOnce an index is selected then the request will be forwarded to it and it will handle the request as it does today as if you had made the request against it directly.\r\n\r\nIf we later recreate or restore an underlying index then it can be swapped out. For example, replacing `foo_0_v1` with `foo_0_v2` might look like:\r\n\r\n```javascript\r\nPUT /_composites/foo\r\n{\r\n  \"indexes\": [\r\n    \"foo_0_v2\",\r\n    \"foo_1_v1\"\r\n  ]\r\n}\r\n```\r\n\r\nOnce you have swapped in `foo_0_v2` then `foo_0_v1` can be deleted and work can begin on creating `foo_1_v2`.\r\n\r\nWith this approach we can do a rolling rebuild of large indexes. For this simple example, since we have two real indexes then we only need enough hardware to handle a copy that is 50% of the original size at any given time.\r\n\r\nWith 20 underlying indexes, we'd never need a copy larger than 5% of the original size and potentially not even have to temporarily add nodes since you'd probably already have your cluster slightly over-scaled giving you enough buffer for small index rebuilds.","closed_by":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"performed_via_github_app":null}