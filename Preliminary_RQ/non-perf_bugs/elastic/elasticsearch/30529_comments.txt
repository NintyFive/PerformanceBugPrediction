[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388346830","html_url":"https://github.com/elastic/elasticsearch/issues/30529#issuecomment-388346830","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30529","id":388346830,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODM0NjgzMA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-05-11T12:15:58Z","updated_at":"2018-05-11T12:15:58Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-search-aggs","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388360607","html_url":"https://github.com/elastic/elasticsearch/issues/30529#issuecomment-388360607","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30529","id":388360607,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODM2MDYwNw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-05-11T13:17:33Z","updated_at":"2018-05-11T13:17:33Z","author_association":"CONTRIBUTOR","body":"Your number does lose precision, but not the way you think. This is due to how floating-point numbers work: `9.62` can't be expressed as `a * 2 ^ b` so neither doubles nor floats can represent it accurately.\r\n\r\nIf you print your float, then it will seem to work because the system prints the shortest string whose value is precise enough to distinguish it from adjacent float values. So you only happen to have more decimals in the `terms` aggregation output because it is stored as a double under the hood, so more digits need to be printed for it to be distinguish from adjacent double values.\r\n\r\nBecause floats and doubles cannot accurately represent a value, it is generally a bad idea to run `terms` aggregations on them.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/423383919","html_url":"https://github.com/elastic/elasticsearch/issues/30529#issuecomment-423383919","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30529","id":423383919,"node_id":"MDEyOklzc3VlQ29tbWVudDQyMzM4MzkxOQ==","user":{"login":"maihde","id":229922,"node_id":"MDQ6VXNlcjIyOTkyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/229922?v=4","gravatar_id":"","url":"https://api.github.com/users/maihde","html_url":"https://github.com/maihde","followers_url":"https://api.github.com/users/maihde/followers","following_url":"https://api.github.com/users/maihde/following{/other_user}","gists_url":"https://api.github.com/users/maihde/gists{/gist_id}","starred_url":"https://api.github.com/users/maihde/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/maihde/subscriptions","organizations_url":"https://api.github.com/users/maihde/orgs","repos_url":"https://api.github.com/users/maihde/repos","events_url":"https://api.github.com/users/maihde/events{/privacy}","received_events_url":"https://api.github.com/users/maihde/received_events","type":"User","site_admin":false},"created_at":"2018-09-21T01:22:34Z","updated_at":"2018-09-21T01:22:34Z","author_association":"NONE","body":"@jpountz I know this ticket is closed but I wanted to add a bit of additional information that might help people who run across this ticket in the future better understand what's going on.\r\n\r\n**1. Index settings and insert two documents**\r\n\r\n```\r\nPUT /term-test\r\n{\r\n  \"settings\": {\r\n    \"refresh_interval\": \"1s\",\r\n    \"number_of_shards\": 1,\r\n    \"number_of_replicas\": 0\r\n  },\r\n  \"mappings\": {\r\n    \"demo_type\": {\r\n      \"properties\": {\r\n        \"float_field\": {\r\n          \"type\": \"float\",\r\n          \"store\": true\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nPOST term-test/demo_type\r\n{\r\n  \"float_field\": 0.62\r\n}\r\nPOST term-test/demo_type\r\n{\r\n  \"float_field\": 0.620000004\r\n}\r\n```\r\n\r\n**2. Execute search**\r\n```\r\nGET term-test/_search\r\n{\r\n  \"query\": {\r\n    \"match_all\": {}\r\n  }\r\n}\r\n...\r\n        \"_source\": {\r\n          \"float_field\": 0.62\r\n        }\r\n...\r\n        \"_source\": {\r\n          \"float_field\": 0.620000004\r\n        }\r\n...\r\n```\r\n\r\nThe `_source` field returns the exact values passed in because it is the original JSON so it's reasonable that it behaves as currently implemented.\r\n\r\n**3. Execute search with stored field**\r\n```\r\nGET term-test/_search\r\n{\r\n  \"stored_fields\": [\r\n    \"float_field\"\r\n  ],\r\n  \"query\": {\r\n    \"match_all\": {}\r\n  }\r\n}\r\n...\r\n        \"fields\": {\r\n          \"float_field\": [\r\n            0.62\r\n          ]\r\n        }\r\n...\r\n        \"fields\": {\r\n          \"float_field\": [\r\n            0.62\r\n          ]\r\n        }\r\n...\r\n```\r\n\r\nThis is the confusing part.  If you use stored fields the hits and the bucket keys are not consistent with each other.  My personal opinion is that either: (a) the stored fields response should return `0.6200000047683716` so that it matches the bucket key, or (b) the bucket key should return `0.62` so that it matches the stored fields.  With either of these approaches ElasticSearch would at least be internally consistent when it returns results.\r\n\r\nAn easy way to achieve the former behavior is to modify the `FieldsVisitor` to look like this:\r\n\r\n```java\r\n    @Override\r\n    public void floatField(FieldInfo fieldInfo, float value) throws IOException {\r\n        addValue(fieldInfo.name, (double) value);\r\n    }\r\n```\r\n\r\nAn easy (but potentially inefficient) way to achieve the latter behavior is to change the conversion of stored float values to look like this.\r\n\r\n```java\r\nstatic final class SingleFloatValues extends NumericDoubleValues {\r\n...\r\n        @Override\r\n        public double doubleValue() throws IOException {\r\n        \tString floatValue = Float.toString(\r\n            \t\t\tNumericUtils.sortableIntToFloat((int) in.longValue())\r\n            \t    );\r\n            return Double.parseDouble(floatValue);\r\n            \t\r\n        }\r\n...\r\n}\r\n```\r\n\r\nWith either of these options the search results and the aggregation results will be internally consistent.  I'm inclined to the former because it accurately represents the actual value of stored in the field when you do the query.\r\n\r\nThe reason I needed to use the `terms` aggreation instead of the `histogram` aggregation is that I have a large number of floating point values where I need to create buckets with `interval: 0.00001` resolution.  I want to aggregate to find the top N and then match that with the responses returned from the search (i.e. was the hit one of the top N or not).   The `histogram` aggregation reaches the 10,000 bucket limit, even with `min_doc_count` set to non-zero.  What I really need is the ability to have a `size` parameter on the histogram aggregation that behaves like the `size` parameter on the `terms` aggregation.  Do you think such an enhancement would be accepted into the baseline?  If so, I will take a stab at implementing it.\r\n\r\nThe workaround I used is use the `terms` aggregation and the call `Math.fround()` on both the bucket key and the hit values to make them consistent with each other.","performed_via_github_app":null}]