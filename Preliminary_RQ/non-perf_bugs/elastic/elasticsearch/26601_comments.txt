[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328857132","html_url":"https://github.com/elastic/elasticsearch/issues/26601#issuecomment-328857132","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26601","id":328857132,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODg1NzEzMg==","user":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T13:42:46Z","updated_at":"2017-09-12T14:05:32Z","author_association":"MEMBER","body":"@quilin thanks for raising this, indeed looks weird. I was able to reproduce on 5.4.3, not sure if it is a bug yet but two things to note:\r\n\r\n* I initially thought the nesting of the `{\"bool\":{\"must\":[{\"bool\":{\"should\":[{\"match\":{\"ricName.ricName\": ....` in your reproduction was part of the problem, but it turns out a simple `must` clause also exhibits the problem\r\n* when I run this simplified query using the `_validate` API you can see that the query string gets split right before the \"f\" that matches your test document:\r\n\r\n```\r\nGET /test_index_bug/ric/_validate/query?explain=true\r\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"must\": [\r\n        {\r\n          \"match\": {\r\n            \"ricName.ricName\": {\r\n              \"query\": \"dgsdlruityhaekljeha;izudyvklaejrbt09834576hgadagyhrtertkghsldkjrhtweiorugysdokljagvalkjdrthwieuartyad78z967agyhdajklskjrntjre.skdauygusldbnalwkeruty57840w67yrukshgdajklsghkdajghslkduayhtweuiry68593470498576wiuergпавыапргцыкше8рполдваяыловраолпдфлгвангшфщыгывнаолдыоамлсичрядлывороалфдцоукрегкшвщышгапниоладчитоваирыолфdhsgjsksrtsrtkdghkdhgkdhkdgkhaasadjlknknkjsgdnm,.sdagnn,mhjlkwerthjlksdgahjklyoiugaksgnm.,hjlkyoiudbnjgadkbkjgadbhkjadgshkjdaghjksdahgshdagjkshgkjahskjlhgakjshgakjsdgdshaglkjsdhgakjdshgakjdslgf\"\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n-->\r\n\r\n{\r\n  \"valid\": true,\r\n  \"_shards\": {\r\n    \"total\": 1,\r\n    \"successful\": 1,\r\n    \"failed\": 0\r\n  },\r\n  \"explanations\": [\r\n    {\r\n      \"index\": \"test_index_bug\",\r\n      \"valid\": true,\r\n      \"explanation\": \"+(+(ricName.ricName:dgsdlruityhaekljeha;izudyvklaejrbt09834576hgadagyhrtertkghsldkjrhtweiorugysdokljagvalkjdrthwieuartyad78z967agyhdajklskjrntjre.skdauygusldbnalwkeruty57840w67yrukshgdajklsghkdajghslkduayhtweuiry68593470498576wiuergпавыапргцыкше8рполдваяыловраолпдфлгвангшфщы ricName.ricName:гывнаолдыоамлсичрядлывороалфдцоукрегкшвщышгапниоладчитоваирыолфdhsgjsksrtsrtkdghkdhgkdhkdgkhaasadjlknknkjsgdnm,.sdagnn,mhjlkwerthjlksdgahjklyoiugaksgnm.,hjlkyoiudbnjgadkbkjgadbhkjadgshkjdaghjksdahgshdagjkshgkjahskjlhgakjshgakjsdgdshaglkjsdhgakjdshgakjdslg \r\nricName.ricName:f)) #_type:ric\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThat explains why removing any character or adding more characters doesn't produce a match. My question is why the analyzer (whould be `whitespace` here) splits the query string and if this is expected. Will mark as bug for now.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328862880","html_url":"https://github.com/elastic/elasticsearch/issues/26601#issuecomment-328862880","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26601","id":328862880,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODg2Mjg4MA==","user":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T14:03:21Z","updated_at":"2017-09-12T14:03:21Z","author_association":"MEMBER","body":"Okay, seems like the `whitespace` tokenizer has a maximum token length of 255 after which it starts a new token. This is what the `_analysis` output of your query string looks like:\r\n\r\n```\r\n{\r\n  \"tokens\": [\r\n    {\r\n      \"token\": \"dgsdlruityhaekljeha;izudyvklaejrbt09834576hgadagyhrtertkghsldkjrhtweiorugysdokljagvalkjdrthwieuartyad78z967agyhdajklskjrntjre.skdauygusldbnalwkeruty57840w67yrukshgdajklsghkdajghslkduayhtweuiry68593470498576wiuergпавыапргцыкше8рполдваяыловраолпдфлгвангшфщы\",\r\n      \"start_offset\": 0,\r\n      \"end_offset\": 255,\r\n      \"type\": \"word\",\r\n      \"position\": 0\r\n    },\r\n    {\r\n      \"token\": \"гывнаолдыоамлсичрядлывороалфдцоукрегкшвщышгапниоладчитоваирыолфdhsgjsksrtsrtkdghkdhgkdhkdgkhaasadjlknknkjsgdnm,.sdagnn,mhjlkwerthjlksdgahjklyoiugaksgnm.,hjlkyoiudbnjgadkbkjgadbhkjadgshkjdaghjksdahgshdagjkshgkjahskjlhgakjshgakjsdgdshaglkjsdhgakjdshgakjdslg\",\r\n      \"start_offset\": 255,\r\n      \"end_offset\": 510,\r\n      \"type\": \"word\",\r\n      \"position\": 1\r\n    },\r\n    {\r\n      \"token\": \"f\",\r\n      \"start_offset\": 510,\r\n      \"end_offset\": 511,\r\n      \"type\": \"word\",\r\n      \"position\": 2\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThis seems to be the default behaviour for many tokenizers and is e.g. documented for the [Standard tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-tokenizer.html) with examples of how to override this default using the `max_token_length` parameter. I tried this approach for the whitespace tokenizer just now but wasn't succeful so far. Looking at the code it seems we don't support this configuration option. Changing from \"bug\" to discussion to get more feedback if we should support this. Also changing the title to reflect the cause of this issue.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328880497","html_url":"https://github.com/elastic/elasticsearch/issues/26601#issuecomment-328880497","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26601","id":328880497,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODg4MDQ5Nw==","user":{"login":"quilin","id":5960226,"node_id":"MDQ6VXNlcjU5NjAyMjY=","avatar_url":"https://avatars3.githubusercontent.com/u/5960226?v=4","gravatar_id":"","url":"https://api.github.com/users/quilin","html_url":"https://github.com/quilin","followers_url":"https://api.github.com/users/quilin/followers","following_url":"https://api.github.com/users/quilin/following{/other_user}","gists_url":"https://api.github.com/users/quilin/gists{/gist_id}","starred_url":"https://api.github.com/users/quilin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/quilin/subscriptions","organizations_url":"https://api.github.com/users/quilin/orgs","repos_url":"https://api.github.com/users/quilin/repos","events_url":"https://api.github.com/users/quilin/events{/privacy}","received_events_url":"https://api.github.com/users/quilin/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T15:01:55Z","updated_at":"2017-09-12T15:01:55Z","author_association":"NONE","body":"That totally makes sense now. Thanks a lot!","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328934163","html_url":"https://github.com/elastic/elasticsearch/issues/26601#issuecomment-328934163","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26601","id":328934163,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODkzNDE2Mw==","user":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T18:01:50Z","updated_at":"2017-09-12T18:01:50Z","author_association":"MEMBER","body":"@quilin out of curiosity: is this something you ran into in a \"real\" scenario or did this happen just playing around? I'm asking because I'd like to understand in which use cases tokens of this length might appear. I haven't seen this very often so far. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328934583","html_url":"https://github.com/elastic/elasticsearch/issues/26601#issuecomment-328934583","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26601","id":328934583,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODkzNDU4Mw==","user":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T18:03:34Z","updated_at":"2017-09-12T18:03:58Z","author_association":"MEMBER","body":"I think the current behaviour should be added to our documentation. Even if such long tokens are edge cases, we should allow overriding the \"max_token_length\" for the whitespace tokenizer like we do for several other tokenizers.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328980197","html_url":"https://github.com/elastic/elasticsearch/issues/26601#issuecomment-328980197","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26601","id":328980197,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODk4MDE5Nw==","user":{"login":"quilin","id":5960226,"node_id":"MDQ6VXNlcjU5NjAyMjY=","avatar_url":"https://avatars3.githubusercontent.com/u/5960226?v=4","gravatar_id":"","url":"https://api.github.com/users/quilin","html_url":"https://github.com/quilin","followers_url":"https://api.github.com/users/quilin/followers","following_url":"https://api.github.com/users/quilin/following{/other_user}","gists_url":"https://api.github.com/users/quilin/gists{/gist_id}","starred_url":"https://api.github.com/users/quilin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/quilin/subscriptions","organizations_url":"https://api.github.com/users/quilin/orgs","repos_url":"https://api.github.com/users/quilin/repos","events_url":"https://api.github.com/users/quilin/events{/privacy}","received_events_url":"https://api.github.com/users/quilin/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T20:49:08Z","updated_at":"2017-09-12T20:49:08Z","author_association":"NONE","body":"It was found by an accident. Like somebody fell on a keyboard and then found the result.\r\nI agree that the \"max_token_length\" should be available for overriding, but it's my fault that I did not read the documentation thoroughly.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329431183","html_url":"https://github.com/elastic/elasticsearch/issues/26601#issuecomment-329431183","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26601","id":329431183,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTQzMTE4Mw==","user":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"created_at":"2017-09-14T09:45:14Z","updated_at":"2017-09-14T09:45:14Z","author_association":"MEMBER","body":"> It was found by an accident. Like somebody fell on a keyboard and then found the result.\r\n\r\nI'll have to add that to my set bug-hunting techniques. Root cause analysis: \"My cat slept on the keyboard\".\r\nThanks again for raising this, I opened two follow up issues about documenting this better (#26641) and maybe enable the `max_token_length` parameter for Whitespace tokenizer (#26643) so I'm going to close this.","performed_via_github_app":null}]