{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/58064","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/58064/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/58064/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/58064/events","html_url":"https://github.com/elastic/elasticsearch/issues/58064","id":637890230,"node_id":"MDU6SXNzdWU2Mzc4OTAyMzA=","number":58064,"title":"Elasticsearch no longer writing data after upgrading from 7.6 to 7.7","user":{"login":"danfinn","id":5891946,"node_id":"MDQ6VXNlcjU4OTE5NDY=","avatar_url":"https://avatars1.githubusercontent.com/u/5891946?v=4","gravatar_id":"","url":"https://api.github.com/users/danfinn","html_url":"https://github.com/danfinn","followers_url":"https://api.github.com/users/danfinn/followers","following_url":"https://api.github.com/users/danfinn/following{/other_user}","gists_url":"https://api.github.com/users/danfinn/gists{/gist_id}","starred_url":"https://api.github.com/users/danfinn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danfinn/subscriptions","organizations_url":"https://api.github.com/users/danfinn/orgs","repos_url":"https://api.github.com/users/danfinn/repos","events_url":"https://api.github.com/users/danfinn/events{/privacy}","received_events_url":"https://api.github.com/users/danfinn/received_events","type":"User","site_admin":false},"labels":[{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null},{"id":2042400575,"node_id":"MDU6TGFiZWwyMDQyNDAwNTc1","url":"https://api.github.com/repos/elastic/elasticsearch/labels/needs:triage","name":"needs:triage","color":"c5def5","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2020-06-12T17:01:54Z","updated_at":"2020-06-16T16:27:35Z","closed_at":"2020-06-15T08:18:02Z","author_association":"NONE","active_lock_reason":null,"body":"Ubuntu 16.04\r\nVersion: 7.7.1, Build: default/deb/ad56dce891c901a492bb1ee393f12dfff473a423/2020-05-28T16:30:01.040088Z, JVM: 1.8.0_201\r\n\r\n\r\njava version \"1.8.0_201\"\r\nJava(TM) SE Runtime Environment (build 1.8.0_201-b09)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)\r\n\r\n\r\nRecently we upgraded (via apt and default Ubuntu repos) from 7.6 to 7.7.\r\n\r\nOur ELK pipeline looks like so:\r\n\r\nfilebeat shipping logs on all our nodes -> ELK on a single server\r\n\r\nAfter upgrading to 7.7 as far as I can tell elasticsearch is no longer writing data to any of our indexes.  I enabled ruby debug logging in logstash and everything looks fine there, logs are flowing in and look normal, no errors.  I enabled debug logging in elasticsearch and I can see the connections from logstash sending logs to elasticsearch, no errors there.\r\n\r\n```\r\n[2020-06-11T18:30:20,989][TRACE][o.e.t.T.tracer           ] [ps-dev-elk] [841][indices:data/write/bulk[s][p]] received response from [{ps-dev-elk}{qmfFc9wtTn20jWjM6mCQMw}{iExEVJUJSJa36HZgIL-gOw}{127.0.0.1}{127.0.0.1:9300}{dilmrt}{ml.machine_memory=16808701952, xpack.installed=true, transform.node=true, ml.max_open_jobs=20}]\r\n[2020-06-11T18:30:20,990][TRACE][o.e.t.T.tracer           ] [ps-dev-elk] [843][indices:admin/seq_no/global_checkpoint_sync[p]] sent to [{ps-dev-elk}{qmfFc9wtTn20jWjM6mCQMw}{iExEVJUJSJa36HZgIL-gOw}{127.0.0.1}{127.0.0.1:9300}{dilmrt}{ml.machine_memory=16808701952, xpack.installed=true, transform.node=true, ml.max_open_jobs=20}] (timeout: [null])\r\n[2020-06-11T18:30:20,991][TRACE][o.e.t.T.tracer           ] [ps-dev-elk] [843][indices:admin/seq_no/global_checkpoint_sync[p]] received request\r\n[2020-06-11T18:30:20,991][TRACE][o.e.t.T.tracer           ] [ps-dev-elk] [842][indices:data/write/bulk[s][p]] sent response\r\n[2020-06-11T18:30:20,991][TRACE][o.e.t.T.tracer           ] [ps-dev-elk] [842][indices:data/write/bulk[s][p]] received response from [{ps-dev-elk}{qmfFc9wtTn20jWjM6mCQMw}{iExEVJUJSJa36HZgIL-gOw}{127.0.0.1}{127.0.0.1:9300}{dilmrt}{ml.machine_memory=16808701952, xpack.installed=true, transform.node=true, ml.max_open_jobs=20}]\r\n[2020-06-11T18:30:20,991][TRACE][o.e.t.T.tracer           ] [ps-dev-elk] [843][indices:admin/seq_no/global_checkpoint_sync[p]] sent response\r\n[2020-06-11T18:30:20,991][TRACE][o.e.t.T.tracer           ] [ps-dev-elk] [843][indices:admin/seq_no/global_checkpoint_sync[p]] received response from [{ps-dev-elk}{qmfFc9wtTn20jWjM6mCQMw}{iExEVJUJSJa36HZgIL-gOw}{127.0.0.1}{127.0.0.1:9300}{dilmrt}{ml.machine_memory=16808701952, xpack.installed=true, transform.node=true, ml.max_open_jobs=20}]\r\n```\r\n\r\nWe use daily indexes and the logstash-* index pattern.  Yesterday I removed the index for that specific day.  Normally this would result in the index quickly being recreated.  That never happened.  This morning I checked and there has not been an index created for today.\r\n\r\nAs best as I can tell, logs are flowing into elasticsearch but are not being written anywhere.  Looking at the output for shards I don't see any document counts increasing on any of the shards.\r\n\r\n```\r\nroot@ps-dev-elk:/var/log/logstash# curl -XGET localhost:9200/_cat/shards?v\r\nindex                    shard prirep state          docs   store ip        node\r\n.kibana_8                0     p      STARTED          91 126.6kb 127.0.0.1 ps-dev-elk\r\n.kibana_task_manager_1   0     p      STARTED           2   7.2kb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.03      0     p      STARTED    14712122   7.3gb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.03      0     r      UNASSIGNED\r\nlogstash-2020.06.02      0     p      STARTED    14813812   7.3gb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.02      0     r      UNASSIGNED\r\nlogstash-2020.06.10      0     p      STARTED    12666539   5.7gb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.10      0     r      UNASSIGNED\r\nlogstash                 0     p      STARTED     3057574   1.8gb 127.0.0.1 ps-dev-elk\r\nlogstash                 0     r      UNASSIGNED\r\n.kibana_task_manager_3   0     p      STARTED           6    21kb 127.0.0.1 ps-dev-elk\r\n.apm-custom-link         0     p      STARTED           0    230b 127.0.0.1 ps-dev-elk\r\n.apm-agent-configuration 0     p      STARTED           0    283b 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.08      0     p      STARTED    12663722   5.7gb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.08      0     r      UNASSIGNED\r\n.kibana_task_manager_2   0     p      STARTED           3  25.9kb 127.0.0.1 ps-dev-elk\r\n.kibana_10               0     p      STARTED         241   169kb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.04      0     p      STARTED    14423582   7.1gb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.04      0     r      UNASSIGNED\r\n.async-search            0     p      STARTED           2 685.9kb 127.0.0.1 ps-dev-elk\r\n.kibana_9                0     p      STARTED         227 220.2kb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.06      0     p      STARTED    12435442   5.5gb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.06      0     r      UNASSIGNED\r\nlogstash-2020.06.07      0     p      STARTED    12436608   5.5gb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.07      0     r      UNASSIGNED\r\n.kibana-6                0     p      STARTED          75  78.3kb 127.0.0.1 ps-dev-elk\r\n.kibana-6                0     r      UNASSIGNED\r\nlogstash-2020.06.05      0     p      STARTED    12650352   5.7gb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.05      0     r      UNASSIGNED\r\n.tasks                   0     p      STARTED           1   6.6kb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.09      0     p      STARTED    12686744   5.8gb 127.0.0.1 ps-dev-elk\r\nlogstash-2020.06.09      0     r      UNASSIGNED\r\n.kibana_7                0     p      STARTED          86  87.2kb 127.0.0.1 ps-dev-elk\r\n```\r\n\r\nI am not seeing any errors in the logstash logs or the elasticsearch logs and am not sure what else might be causing this.\r\n","closed_by":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"performed_via_github_app":null}