{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/8730","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8730/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8730/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8730/events","html_url":"https://github.com/elastic/elasticsearch/issues/8730","id":50546250,"node_id":"MDU6SXNzdWU1MDU0NjI1MA==","number":8730,"title":"Master node is sending the same cluster state again and again upon shard failure.","user":{"login":"miccon","id":455015,"node_id":"MDQ6VXNlcjQ1NTAxNQ==","avatar_url":"https://avatars3.githubusercontent.com/u/455015?v=4","gravatar_id":"","url":"https://api.github.com/users/miccon","html_url":"https://github.com/miccon","followers_url":"https://api.github.com/users/miccon/followers","following_url":"https://api.github.com/users/miccon/following{/other_user}","gists_url":"https://api.github.com/users/miccon/gists{/gist_id}","starred_url":"https://api.github.com/users/miccon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/miccon/subscriptions","organizations_url":"https://api.github.com/users/miccon/orgs","repos_url":"https://api.github.com/users/miccon/repos","events_url":"https://api.github.com/users/miccon/events{/privacy}","received_events_url":"https://api.github.com/users/miccon/received_events","type":"User","site_admin":false},"labels":[{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2014-12-01T16:50:45Z","updated_at":"2015-03-02T10:00:00Z","closed_at":"2015-03-02T10:00:00Z","author_association":"NONE","active_lock_reason":null,"body":"After upgrading from 1.0.2 to 1.4.1 we had a shard failing to initialize. For this issue the failing of the shard is not the issue, but that the master node always sends the same cluster state with the failed state again and again, causing lots of unnecessary traffic, especially on a large cluster with a lot of indices.\n\nHere is the log from the master node related to the failed index:\n\n```\n[2014-12-01 17:38:39,575][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [430], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:38:39,577][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:38:40,112][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:38:40,154][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [432], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:38:40,156][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:38:42,654][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:38:42,697][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [434], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:38:42,699][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:38:56,543][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:38:56,584][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [436], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:38:56,587][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:40:12,654][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:40:12,696][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [438], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n[2014-12-01 17:40:12,699][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]\n```\n\nIt seems that the master node is not correctly preventing the cluster state from being distributed, as it does not change as the same index is failing again and again on the same node with the same error message.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}