[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/24709708","html_url":"https://github.com/elastic/elasticsearch/issues/3733#issuecomment-24709708","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3733","id":24709708,"node_id":"MDEyOklzc3VlQ29tbWVudDI0NzA5NzA4","user":{"login":"thomasj02","id":2532431,"node_id":"MDQ6VXNlcjI1MzI0MzE=","avatar_url":"https://avatars3.githubusercontent.com/u/2532431?v=4","gravatar_id":"","url":"https://api.github.com/users/thomasj02","html_url":"https://github.com/thomasj02","followers_url":"https://api.github.com/users/thomasj02/followers","following_url":"https://api.github.com/users/thomasj02/following{/other_user}","gists_url":"https://api.github.com/users/thomasj02/gists{/gist_id}","starred_url":"https://api.github.com/users/thomasj02/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/thomasj02/subscriptions","organizations_url":"https://api.github.com/users/thomasj02/orgs","repos_url":"https://api.github.com/users/thomasj02/repos","events_url":"https://api.github.com/users/thomasj02/events{/privacy}","received_events_url":"https://api.github.com/users/thomasj02/received_events","type":"User","site_admin":false},"created_at":"2013-09-19T00:25:04Z","updated_at":"2013-09-19T00:25:04Z","author_association":"NONE","body":"Apparently it's a heap space error, but it's not clear to me why turning off compression makes it work okay\n\n[2013-09-18 19:23:32,764][WARN ][http.netty               ] [Gorgilla] Caught exception while handling client http traffic, closing connection [id: 0xbcfe3956, /192.168.1.92:34270 => /192.168.1.60:9200]\norg.elasticsearch.common.netty.handler.codec.embedder.CodecEmbedderException: java.lang.OutOfMemoryError: Java heap space\n        at org.elasticsearch.common.netty.handler.codec.embedder.AbstractCodecEmbedder$EmbeddedChannelPipeline.notifyHandlerException(AbstractCodecEmbedder.java:242)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:599)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)\n        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:704)\n        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:671)\n        at org.elasticsearch.common.netty.handler.codec.embedder.EncoderEmbedder.offer(EncoderEmbedder.java:70)\n        at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.encode(HttpContentEncoder.java:201)\n        at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:121)\n        at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)\n        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:704)\n        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:671)\n        at org.elasticsearch.common.netty.channel.AbstractChannel.write(AbstractChannel.java:248)\n        at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:158)\n        at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:100)\n        at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:92)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:190)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:172)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:152)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:146)\n        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:346)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:146)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:133)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:724)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n        at org.elasticsearch.common.netty.buffer.HeapChannelBuffer.<init>(HeapChannelBuffer.java:42)\n        at org.elasticsearch.common.netty.buffer.BigEndianHeapChannelBuffer.<init>(BigEndianHeapChannelBuffer.java:34)\n        at org.elasticsearch.common.netty.buffer.ChannelBuffers.buffer(ChannelBuffers.java:134)\n        at org.elasticsearch.common.netty.buffer.HeapChannelBufferFactory.getBuffer(HeapChannelBufferFactory.java:68)\n        at org.elasticsearch.common.netty.buffer.DynamicChannelBuffer.<init>(DynamicChannelBuffer.java:58)\n        at org.elasticsearch.common.netty.buffer.ChannelBuffers.dynamicBuffer(ChannelBuffers.java:221)\n        at org.elasticsearch.common.netty.handler.codec.compression.JdkZlibEncoder.encode(JdkZlibEncoder.java:184)\n        at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:66)\n        at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneStrictEncoder.doEncode(OneToOneStrictEncoder.java:35)\n        at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)\n        at org.elasticsearch.common.netty.handler.codec.compression.JdkZlibEncoder.handleDownstream(JdkZlibEncoder.java:221)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)\n        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:704)\n        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:671)\n        at org.elasticsearch.common.netty.handler.codec.embedder.EncoderEmbedder.offer(EncoderEmbedder.java:70)\n        at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.encode(HttpContentEncoder.java:201)\n        at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:121)\n        at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)\n        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:704)\n        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:671)\n        at org.elasticsearch.common.netty.channel.AbstractChannel.write(AbstractChannel.java:248)\n        at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:158)\n        at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:100)\n        at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:92)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:190)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:172)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:152)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:146)\n        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:346)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/24723030","html_url":"https://github.com/elastic/elasticsearch/issues/3733#issuecomment-24723030","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3733","id":24723030,"node_id":"MDEyOklzc3VlQ29tbWVudDI0NzIzMDMw","user":{"login":"spinscale","id":667544,"node_id":"MDQ6VXNlcjY2NzU0NA==","avatar_url":"https://avatars2.githubusercontent.com/u/667544?v=4","gravatar_id":"","url":"https://api.github.com/users/spinscale","html_url":"https://github.com/spinscale","followers_url":"https://api.github.com/users/spinscale/followers","following_url":"https://api.github.com/users/spinscale/following{/other_user}","gists_url":"https://api.github.com/users/spinscale/gists{/gist_id}","starred_url":"https://api.github.com/users/spinscale/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spinscale/subscriptions","organizations_url":"https://api.github.com/users/spinscale/orgs","repos_url":"https://api.github.com/users/spinscale/repos","events_url":"https://api.github.com/users/spinscale/events{/privacy}","received_events_url":"https://api.github.com/users/spinscale/received_events","type":"User","site_admin":false},"created_at":"2013-09-19T08:06:29Z","updated_at":"2013-09-19T08:06:29Z","author_association":"MEMBER","body":"Hey,\n\nMost likely the data you are trying to send to the client needs to be loaded into memory before being compressed. Loading 390000 documents into memory can cause these issues.\n\nI do not think that this is the best approach to get that much documents from elasticsearch (your RAM needs to scale with the amount of documents returned, a second parallel request can kill the application). You should take a look at the scan search type at http://www.elasticsearch.org/guide/reference/api/search/search-type/\nThat type allows you to easily retrieve such a big amount of documents.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/24743482","html_url":"https://github.com/elastic/elasticsearch/issues/3733#issuecomment-24743482","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3733","id":24743482,"node_id":"MDEyOklzc3VlQ29tbWVudDI0NzQzNDgy","user":{"login":"thomasj02","id":2532431,"node_id":"MDQ6VXNlcjI1MzI0MzE=","avatar_url":"https://avatars3.githubusercontent.com/u/2532431?v=4","gravatar_id":"","url":"https://api.github.com/users/thomasj02","html_url":"https://github.com/thomasj02","followers_url":"https://api.github.com/users/thomasj02/followers","following_url":"https://api.github.com/users/thomasj02/following{/other_user}","gists_url":"https://api.github.com/users/thomasj02/gists{/gist_id}","starred_url":"https://api.github.com/users/thomasj02/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/thomasj02/subscriptions","organizations_url":"https://api.github.com/users/thomasj02/orgs","repos_url":"https://api.github.com/users/thomasj02/repos","events_url":"https://api.github.com/users/thomasj02/events{/privacy}","received_events_url":"https://api.github.com/users/thomasj02/received_events","type":"User","site_admin":false},"created_at":"2013-09-19T14:39:23Z","updated_at":"2013-09-19T14:39:23Z","author_association":"NONE","body":"Unfortunately I can't use scan since I need to get the documents back in sorted order.\n\nI can understand if it takes a lot of RAM to load and send the documents, but it doesn't make sense that it requires more memory to send back a compressed version of the documents than it does to send an uncompressed version. GZip is a streaming compression protocol, so ES should be able to send out the data compressed while using almost zero additional memory.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/51630916","html_url":"https://github.com/elastic/elasticsearch/issues/3733#issuecomment-51630916","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3733","id":51630916,"node_id":"MDEyOklzc3VlQ29tbWVudDUxNjMwOTE2","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-08-08T17:13:49Z","updated_at":"2014-08-08T17:13:49Z","author_association":"CONTRIBUTOR","body":"Sorted scrolling can now be done almost as efficiently as scanning\n","performed_via_github_app":null}]