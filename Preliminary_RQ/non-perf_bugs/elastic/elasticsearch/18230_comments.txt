[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218106464","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-218106464","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":218106464,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODEwNjQ2NA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-05-10T09:32:05Z","updated_at":"2016-05-10T09:32:05Z","author_association":"CONTRIBUTOR","body":"Hi @mahdibh \n\nWe have added eg the in-flight requests circuit breaker (https://github.com/elastic/elasticsearch/pull/17133) which may or may not help in this situation.  It depends exactly where memory is being consumed here.  eg the `requests` circuit breaker is applied during the query phase, but not during the fetch phase and not on the coordinating node.  So large `_source` fields might consume too much memory and result in an OOM.\n\nGiving us access to the heap dump would be helpful.  Also, would you mind trying this same test out on 5.0.0-alpha2?\n\nWe may be able to find other safeguards (see https://github.com/elastic/elasticsearch/issues/11511) that will help us in these situations\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218150870","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-218150870","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":218150870,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODE1MDg3MA==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-10T13:05:03Z","updated_at":"2016-05-10T13:05:03Z","author_association":"NONE","body":"Hi @clintongormley \n\nWe will try to repro this without requesting _source and see if that helps. Our document _source is usually 10kb. Not sure if that actually will matter here since we have only 5 search threads per node.\n\nWould the number of shards (128) coupled with the fact that we are retrieving 5000 records (sorted) contribute to the memory bloat when the system isn't able to keep up ? My understanding is that ES will have to sort and return the top 5000 docs on each shard to the coordinating node which will have to sort across 128x5000=640k docs. Our sort field has doc values enabled and it's a long timestamp.\n\nAlso, would the ThreadPoolRejection exceptions cause extra allocations (since there is an error per shard). When the system is overloaded, it's likely all shards will reject and the response object will contain 128 shard failure messages.\n\nI'll follow up regarding the heapdump and trying this out on 5.0.0-alpha2\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218374774","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-218374774","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":218374774,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODM3NDc3NA==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-11T06:38:53Z","updated_at":"2016-05-11T06:38:53Z","author_association":"NONE","body":"I tried removing the sort in the query, reducing the number of results fetched from 5k to 100, not fetching _source and using no regex in the query but still no luck. I can get the 3 data nodes to OOM in under 3 minutes of sustained load.\n\nI then tried to use the same index but with 8 primary shards instead of 128 (+2 replica). In that case, I couldn't get the cluster to crash (nodes disappeared from the cluster temporarily, but then came back once I stopped the load).\n\nI've also noticed that the cluster held-up more when I disabled the logging of ThreadPoolRejectionExceptions on the ES side PUT _cluster/settings\" -d'{\"transient\" : {\"logger.action.search.type\" : \"ERROR\"}}'). Those were logged at a peak rate of 10k/second. No wonder that would consume CPU. I'm not sure why stack traces are logged for those exceptions by default, seems like shooting oneself in the foot when ES is trying to shed itself from further load.\n\n@clintongormley any luck finding anything suspicious in the heap dump ?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218580746","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-218580746","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":218580746,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODU4MDc0Ng==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-11T20:31:11Z","updated_at":"2016-05-11T20:31:11Z","author_association":"NONE","body":"More data points:\nWe have upgraded to 1.7.5, but still run into the same issue.\n\nWe switched our query traffic from using the data nodes directly to just using client nodes only instead. This shifted the OOM to the client nodes (which is good, since those are way much cheaper to restart). I'll share the heap dump we got on the client nodes.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218593548","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-218593548","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":218593548,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODU5MzU0OA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-05-11T21:19:28Z","updated_at":"2016-05-11T21:19:28Z","author_association":"CONTRIBUTOR","body":"> Would the number of shards (128) coupled with the fact that we are retrieving 5000 records (sorted) contribute to the memory bloat when the system isn't able to keep up ? My understanding is that ES will have to sort and return the top 5000 docs on each shard to the coordinating node which will have to sort across 128x5000=640k docs. Our sort field has doc values enabled and it's a long timestamp.\n\nmay I ask why you have 128 shards on 3 nodes with 600k doc? What about 1 shard? I also wonder why on earth you are fetching 5k documents instead of using pagination or sorted scroll or search_after?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218607939","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-218607939","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":218607939,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODYwNzkzOQ==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-11T22:21:38Z","updated_at":"2016-05-11T22:21:38Z","author_association":"NONE","body":"> may I ask why you have 128 shards on 3 nodes with 600k doc? What about 1 shard?\n\nThis is just a load test environment. The number of shards is artificially high on so that we can determine whether that impact the memory issues we are observing. In my third update, I pointed out that we weren't able to repro this on an index with 8 shards.\n\n> I also wonder why on earth you are fetching 5k documents instead of using pagination or sorted scroll or search_after?\n- Pagination would add latency, although we have never tried that. Would doing that be beneficial ? Latency aside, can you explain why that would be more efficient than retrieving 5k in one shot.\n- Sorted scrolls would end up being costly due to the scroll context that has to be maintained.\n- search_after isn't available in 1.7 afaik. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218680317","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-218680317","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":218680317,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODY4MDMxNw==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-05-12T07:26:10Z","updated_at":"2016-05-12T07:26:10Z","author_association":"CONTRIBUTOR","body":"> This is just a load test environment. The number of shards is artificially high on so that we can determine whether that impact the memory issues we are observing. \n\nI see - is having 128 shards per node a realistic test? I mean I can put a ton of shards on a node and stuff will likely go bad? Nevertheless, I would be interested in the heapdump of a client node here.\n\n> In my third update, I pointed out that we weren't able to repro this on an index with 8 shards.\n\nsorry missed that....\n\n> Pagination would add latency, although we have never tried that. Would doing that be beneficial ? Latency aside, can you explain why that would be more efficient than retrieving 5k in one shot.\n\nto begin with, why do you need 5k results. Elasticsearch is a top N retrieval engine in the first place where N is small like 10, 20, 30? I doubt the efficiency here, I think you are overestimating latency given that you are fetching 5k \\* 10kb of json latency becomes a constant factor given the amount of data you are fetching. Also keep in mind elasticsearch has to materialize all that data before sending back ie. it's not streaming. Working with chunks is the way to go.\n\n> Sorted scrolls would end up being costly due to the scroll context that has to be maintained.\n\nwhat cost you are talking about here? Memory? it holds on to the index reader plus an integer per shard and scroll request. The cost is bascially an int per shard since you need the index reader anyway?\n\n> search_after isn't available in 1.7 afaik.\n\nsince you are in a testing phase why don't you go and use 2.x give that 5.x is around the corner?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218890208","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-218890208","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":218890208,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODg5MDIwOA==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-12T21:23:54Z","updated_at":"2016-05-12T21:23:54Z","author_association":"NONE","body":"One other observation that we made during this is that when the client node runs out of memory, OOM exceptions are swallowed and the node stays up in zombie state holding to its connections to other data nodes. This in turn doesn't release the netty resources on the data nodes leading to a snowball effect that causes the data nodes to also run out of memory.\n\n```\n2016-05-12T18:48:01.533Z WARN  [ent_worker][T#7]{New I/O worker #7}] [annel.socket.nio.AbstractNioSelector]: Unexpected exception in the selector loop.\njava.lang.OutOfMemoryError: Java heap space\n```\n\nIf you look at netty's AbstractNioSelector.java code, it's catching throwable, logging it and then moving on. \nIf that exception gets bubbled all the way up to the JVM, the process would exit, or at least we could add an OOM handler in the JVM options so that we can kill the process and release the buffers it's holding into the data nodes.\n\nNote that this pattern is also present in other places in the ES code, for example IndicesTTLService.run(), RestActionListener.onFailure() swallow throwable. I'm not sure if those are done on purpose (in the hope that the OOM was momentary?). It seems to me that the best course of action when handling an OOM is either to release memory or to let the exception go up the stack but not to swallow it.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219064357","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-219064357","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":219064357,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTA2NDM1Nw==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-13T14:45:54Z","updated_at":"2016-05-13T14:45:54Z","author_association":"NONE","body":"One more issue we also noted is the following code in MessageChannelHandler\n\n```\n            ThrowableObjectInputStream ois = new ThrowableObjectInputStream(buffer, transport.settings().getClassLoader());\n            error = (Throwable) ois.readObject();\n```\n\nWhen the data nodes search queues are full, the rejection exception is serialized back to the client node. The deserialization of the EsRejectedExecutionException in ois.readObject() across the different netty worker threads is synchronized in java.lang.ClassLoader.loadClass(ClassLoader.java:357). This blocks all IO worker threads. See two thread dumps here:\n\nhttps://gist.github.com/mahdibh/859ef5fcb509a95a9dd079baef369546\n\nAfter this point, it seems that the system gets stuck (probably trying to allocate memory) but never OOMs (or maybe does, but since it's trying to allocate memory to log the OOM, it is never logged).\n\nThis is what I see in the logs, but never any OOM. I guess if I wait long enough, it'll likely happen on some other thread that is not trying to catch it.\n\n2016-05-13T13:27:19.416Z INFO  [rch[metabase-es-26][scheduler][T#1]] [monitor.jvm                         ]: [metabase-es-26] [gc][old][1158][256] duration [6.1s], collections [1]/[6.6s], total [6.1s]/[11.5m], memory [3.8gb]->[3.8gb]/[3.9gb], all_pools {[young] [148.1mb]->[174.1mb]/[266.2mb]}{[survivor] [0b]->[0b]/[33.2mb]}{[old] [3.6gb]->[3.6gb]/[3.6gb]}\n2016-05-13T13:27:31.022Z INFO  [rch[metabase-es-26][scheduler][T#1]] [monitor.jvm                         ]: [metabase-es-26] [gc][old][1160][258] duration [6.2s], collections [1]/[6.7s], total [6.2s]/[11.7m], memory [3.8gb]->[3.8gb]/[3.9gb], all_pools {[young] [205mb]->[178.8mb]/[266.2mb]}{[survivor] [0b]->[0b]/[33.2mb]}{[old] [3.6gb]->[3.6gb]/[3.6gb]}\n2016-05-13T13:27:42.296Z INFO  [rch[metabase-es-26][scheduler][T#1]] [monitor.jvm                         ]: [metabase-es-26] [gc][old][1162][260] duration [6.3s], collections [1]/[6.5s], total [6.3s]/[11.8m], memory [3.9gb]->[3.9gb]/[3.9gb], all_pools {[young] [241.9mb]->[266.2mb]/[266.2mb]}{[survivor] [0b]->[7mb]/[33.2mb]}{[old] [3.6gb]->[3.6gb]/[3.6gb]}\n\nSee thread dumps below when it reaches this state\nhttps://gist.github.com/mahdibh/f879362f2c4960e401d2c06385aec5d8\n\nA bit after this, the node logs a ton of the following exceptions:\n\n```\n\n2016-05-13T14:33:08.631Z WARN  [rch[metabase-es-26][generic][T#456]] [search.action                       ]: [metabase-es-26] Failed to send release search context\norg.elasticsearch.transport.SendRequestTransportException: [metabase-es-18][inet[/X.X.X.XX:9300]][indices:data/read/search[free_context]]\n        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:286)\n        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:249)\n        at org.elasticsearch.search.action.SearchServiceTransportAction.sendFreeContext(SearchServiceTransportAction.java:143)\n...\nCaused by: org.elasticsearch.transport.NodeNotConnectedException: [metabase-es-18][inet[/X.X.X.X:9300]] Node not connected\n        at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:964)\n```\n\nMost likely because the node has been kicked out of the cluster and doesn't see the other data nodes.\n\n\"Node not connected\" is a bit misleading here since it indicates a networking problem reaching the node whereas it's actually just not finding the channel in the node mapping. This could better be characterized as \"can't find {node} in the list of connected nodes\". I'm mentioning this, because we wasted some time checking our networking when we saw in this issue in production.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219065061","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-219065061","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":219065061,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTA2NTA2MQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2016-05-13T14:48:12Z","updated_at":"2016-05-13T14:48:12Z","author_association":"MEMBER","body":"> When the data nodes search queues are full, the rejection exception is serialized back to the client node. The deserialization of the EsRejectedExecutionException in ois.readObject() across the different netty worker threads is synchronized in java.lang.ClassLoader.loadClass(ClassLoader.java:357). This blocks all IO worker threads.\n\nWe don't use Java serialization for exceptions anymore. This is not the case in 2.x and master.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219179970","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-219179970","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":219179970,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTE3OTk3MA==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-13T23:04:52Z","updated_at":"2016-05-13T23:05:23Z","author_association":"NONE","body":"> We don't use Java serialization for exceptions anymore. This is not the case in 2.x and master.\n\nThat's great to hear. I would prefer that those do not even get serialized (especially the stack trace). For exceptions that are designed to shed the system from further load, generating them and sending them back to the client should be the cheapest possible. I think a status code indicating a thread pool rejection would be good enough. Not sure what the behavior is in 2.x/master.\n\n2.x is not an option for us at this point because we can't do a rolling upgrade and it'll take us forever to do a full upgrade (and the plan-b if that fails is rather scary ;) ).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219508460","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-219508460","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":219508460,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTUwODQ2MA==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-16T18:38:53Z","updated_at":"2016-05-16T18:38:53Z","author_association":"NONE","body":"Another thing that could help fix this is to check the netty channel's isWritable() status (see http://normanmaurer.me/presentations/2014-facebook-eng-netty/slides.html#10.0). If the channel isn't writable, the response/request will be queued, hence holding to memory that won't be released until the client/server responds. Under load, data nodes may become slow leading to more queuing on the coordinator side. Also, if the coordinator node is slow receiving, more memory will be held on the data node side. In both cases, OOMs are likely to happen.\n\nThe ES code (1.7.5 branch) doesn't have any reference to the isWritable method so there is definitely a risk of running out of memory under these conditions.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219639163","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-219639163","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":219639163,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTYzOTE2Mw==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-05-17T07:23:05Z","updated_at":"2016-05-17T07:23:05Z","author_association":"CONTRIBUTOR","body":"just to manage expectations, we won't do another 1.7.x release unless there is a really serious issue coming up. There won't be any features or larger modifications happening to 1.7.x. \nI appreciate your  comments but it's still unclear what exactly is causing the problems on your side? Why is the node going OOM? I also wonder if you are producing an artificial problem here since there haven't been issues reported related to this to my knowledge (I can be wrong). \n\nThere are certainly improvements possible to the network code but I think we should contain them and have a clear purpose. We are more than happy to accept PRs / improvements like the `isWritable()` suggestion!\n\n> 2.x is not an option for us at this point because we can't do a rolling upgrade and it'll take us forever to do a full upgrade (and the plan-b if that fails is rather scary ;) ).\n\nyou should really reconsider your strategy - 2.x to 5.x will require a full cluster restart too. I recommend you build some infrastructure to make the move to newer version possible.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219708234","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-219708234","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":219708234,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTcwODIzNA==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-05-17T12:53:25Z","updated_at":"2016-05-18T09:36:04Z","author_association":"MEMBER","body":"Here's a summary of what I found out so far:\n\n### Heap dumps\n\nI looked at your heap dumps and the two major heap consumers are:\n- `TransportService.clientHandlers` (consuming roughly 1.6GB or 40% of your heap)\n- 144 instances of `NioAcceptedSocketChannel` (consuming roughly 2.2GB or 54% of your heap). These instances get created when we accept the network connection. However, if there are not enough workers available they get queued by Netty and the really bad thing is that the queue is an unbounded queue.\n\nThis indicates that there are too many open client connections (due to search requests). \n\n### Reproduction\n\nI tried reproducing the problem with the information that I could find in the ticket. Although I was able to force `EsRejectedExecutionException`s easily, I could not get the system into a state where it OOMs (I am not saying at all that they don't happen; I could just not reproduce it locally).\n\n### Mitigation\n\nHowever, I think you can do a couple of things to mitigate this situation:\n1. First of all, clients should back off when they get `EsRejectedExecutionException` in a response. Since Elasticsearch 2.2, we use an exponential backoff in the Java client (see #14829 and #15513) and you should do something similar if you use another language in your application.\n2. You could look whether you can optimize your query. It is fairly complex so maybe you can do something there.\n3. You should also look into pagination. Do you show 5.000 search results at once to your users?\n\nAn upgrade to 2.x could help you for multiple reasons:\n1. We have included additional circuit breakers which could maybe help in your situation. The top memory consumers are due to request processing or rather queued requests but it maybe the case that the inflight request breaker can still help in this situation (will be available in release 2.4) although this is not the root cause of the problem here.\n2. We have added a query profiler which can help you to find out why your query is slow and help you tune it.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219930160","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-219930160","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":219930160,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTkzMDE2MA==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-18T05:40:07Z","updated_at":"2016-05-18T05:49:33Z","author_association":"NONE","body":"> just to manage expectations, we won't do another 1.7.x release unless there is a really serious issue coming up\n\nThis is a serious issue IMHO. If a single node OOMs and gets into this zombie state this is what happens:\n1. it completely stalls search requests in the whole cluster. ES becomes completely un-usable.\n2. if the node is a coordinator node, it will hold to its connections on data nodes increasing the risk of data nodes also running out of memory. \n\nSwallowing an OOM is pretty serious too if you talk to any decent java engineer :)  It's a bad practice and leads to all sorts of subtle, hard to diagnose problems. \n\nBtw, there have been issues reported around this in the past, see #2528 and more specifically #9018\n\n> you should really reconsider your strategy - 2.x to 5.x will require a full cluster restart too. I recommend you build some infrastructure to make the move to newer version possible.\n\nWe are already doing that. Building that infrastructure is expensive ! The problem is that the 1.x and 2.x clients aren't compatible, so we have to maintain two client versions with two clusters of different versions while the data is changing underneath. If the clients were kept compatible, that would have saved us a lot of boilerplate work.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219931091","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-219931091","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":219931091,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTkzMTA5MQ==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-18T05:47:53Z","updated_at":"2016-05-18T05:47:53Z","author_association":"NONE","body":"thanks @danielmitterdorfer for taking a look at the dumps.\n\n> if there are not enough workers available they get queued by Netty and the really bad thing is that the queue is an unbounded queue.\n> Is there a setting to make this queue bounded ?\n\nWe have already implemented a backoff strategy to relieve the cluster when requests are being rejected. The result of the queries are actually not showed to users, but used by internal backend systems for other purposes.\n\nWhat about search results? What happens when a client is too slow to consume a response. Do those also get queued on a queue with an unbounded size ?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219972635","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-219972635","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":219972635,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTk3MjYzNQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-05-18T09:25:11Z","updated_at":"2016-05-18T09:25:11Z","author_association":"MEMBER","body":"> > if there are not enough workers available they get queued by Netty and the really bad thing is that the queue is an unbounded queue.\n> \n>   Is there a setting to make this queue bounded ?\n\nNo, there is no setting in Netty, see [AbstractNioSelector.java#L87](https://github.com/netty/netty/blob/3.10/src/main/java/org/jboss/netty/channel/socket/nio/AbstractNioSelector.java#L87).\n\n> We have already implemented a backoff strategy to relieve the cluster when requests are being rejected.\n\nHow do you backoff? Linearly, exponentially, some other strategy? It's also hard to tell without knowing your use case but it seems also that there are quite a lot of clients hitting the cluster (I don't know if you just used 1.000 as an example to produce the condition more quickly or if you really have that many clients hitting it in production). Nevertheless, it seems even with your backoff strategy too many requests reach the server as there are more than one hundred queued connections.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/220004490","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-220004490","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":220004490,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMDAwNDQ5MA==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2016-05-18T11:56:37Z","updated_at":"2016-05-19T01:02:24Z","author_association":"MEMBER","body":"@mahdibh We agree with you that Elasticsearch should be terminated on any `OutOfMemoryError`. However, the situation is not simple.\n\nFirst, I think that you're missing that effectively all of the work in Elasticsearch is not done on the main thread. Even without catching `Throwable`, a death of one of these threads due to `OutOfMemoryError` or other `VirtualMachineError`s would (by default) not bring the process down anyway. An option would be to register an `UncaughtExceptionHandler` and terminate the process if this handler is invoked with an `OutOfMemoryError` with the cost of having to unwind all of the code blocks today that are catching `Throwable`.\n\nSo maybe a better option is that in later versions of Java (>= 8u92) we will be able to take advantage of `ExitOnOutOfMemoryError` to immediately exit the process if an `OutOfMemoryError` is detected, but that is something that we need to wait until we require Java 9 for.\n\nEven this has complications. For example, maybe it's best that when handling a remote request, we catch everything so that we can best-effort notify listeners and flush before proceeding to tear the process down? Or maybe not and we should just let the listeners be notified via `NodeDisconnectedException`?\n\nThen there's the operations side of this which is that immediately tearing the process down on `OutOfMemoryError` means that end-users have to be prepared to automatically restart nodes or they could have nodes dropping out, not getting restarted in a timely fashion and so missing delayed allocation windows triggering a bunch of recoveries potentially exacerbating memory issues on other nodes leading to a cluster-wide outage. And maybe that's something that we just want to provide out of the box with our packaging?\n\nNothing is ever simple.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/220232374","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-220232374","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":220232374,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMDIzMjM3NA==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-19T05:40:23Z","updated_at":"2016-05-19T05:40:32Z","author_association":"NONE","body":"> How do you backoff?\n\nWe do exponential backoffs.\n\n> I don't know if you just used 1.000 as an example to produce the condition more quickly or if you really have that many clients hitting it in production. \n\nSince the client is doing async IO, it's actually the number of clients times the number of concurrent queries issued by client (till the IO threads are saturated) that determines the overall load on the system. If you multiply that by the number of shards in the index, the number quickly gets big.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/220238073","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-220238073","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":220238073,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMDIzODA3Mw==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-05-19T06:25:00Z","updated_at":"2016-05-19T06:25:00Z","author_association":"NONE","body":"The core of the issue here is the un-bounded connection queues. That's what netty's `isWritable()` method is for. The handling of the OOM is secondary. It's important that the system protects itself against OOMs. The ES team have been putting a lot of these protections in place as part of the OOM resiliency work and that's great. I think this is another one of these issues that also needs to be added to the list.\n\nRe. swallowing OOM, I think it's just the wrong thing to do regardless of the complications of bubbling up the exception all the way up to the main thread. The JDK documentation for `VirtualMachineError` clearly states: `Thrown to indicate that the Java Virtual Machine is broken or has run out of resources necessary for it to continue operating.`\n\nI would completely understand catching this exception and then for example trying some last resort actions like freeing up memory or closing connections, etc.. However, it should be re-thrown because it's one of those exceptions that you can't just handle and continue to function properly.\nThe intent of the developer here is to save the system at any cost: catch the OOM and ignore it in the hope that memory will get freed-up somewhere else and the system will get back to working normally. However, that's the best case scenario. Keeping the system up and running in that state may do more harm than good. For example: data corruption (see #12041, #10066) or subtle deadlocks/race conditions that only happen during those low memory conditions.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/220291061","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-220291061","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":220291061,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMDI5MTA2MQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2016-05-19T10:51:59Z","updated_at":"2016-05-19T10:51:59Z","author_association":"MEMBER","body":"Did you see where I said we agree with you? I should add, we had discussions about this awhile ago and already came to the conclusion we should handle it differently than today.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/220320546","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-220320546","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":220320546,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMDMyMDU0Ng==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-05-19T13:15:59Z","updated_at":"2016-05-19T13:17:42Z","author_association":"MEMBER","body":"@mahdibh I think before we can fix something, we need be able to reproduce the issue (otherwise it's hard to tell we've really fixed the right thing). Therefore, I have started a gist where you can see the reproduction scenario I have created based on your information: https://gist.github.com/danielmitterdorfer/636d1fb1f6a2ec4353a4e0b5d552a391\n\nIt would be great if you could fork it and adapt it so we can get a scenario which we can reproduce on AWS. One thing I've already noticed: I have not used async IO as I didn't know it before.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/224805605","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-224805605","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":224805605,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNDgwNTYwNQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-06-09T05:47:39Z","updated_at":"2016-06-09T05:47:39Z","author_association":"MEMBER","body":"@mahdibh Did you have any chance to look at the reproduction scenario?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/224807607","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-224807607","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":224807607,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNDgwNzYwNw==","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"created_at":"2016-06-09T06:04:31Z","updated_at":"2016-06-09T06:04:31Z","author_association":"NONE","body":"Sorry @danielmitterdorfer I got pulled into other things. I think this will be hard to reproduce with the python client (I'm assuming it doesn't do any async IO). I'll try to spend some cycles on this next week and will update the ticket.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/224808029","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-224808029","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":224808029,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNDgwODAyOQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-06-09T06:07:50Z","updated_at":"2016-06-09T06:07:50Z","author_association":"MEMBER","body":"@mahdibh No worries. You can use whatever is most convenient to you, I'm sure we can figure it out then. I just used the Python client when I originally created the scenario because I wasn't aware that you do it async back then (and btw the Python client does not do async IO but Honza has created an async io extension).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/230420787","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-230420787","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":230420787,"node_id":"MDEyOklzc3VlQ29tbWVudDIzMDQyMDc4Nw==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-07-05T08:45:11Z","updated_at":"2016-07-05T08:45:11Z","author_association":"MEMBER","body":"Due to lack of a reproduction scenario we cannot make progress and I am closing this ticket now. But please feel free to reopen when you've got a reproduction scenario. Btw, @jasontedor started to eliminate the `catch Throwable` blocks in #19231.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328963081","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-328963081","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":328963081,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODk2MzA4MQ==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T19:41:54Z","updated_at":"2017-09-12T19:41:54Z","author_association":"NONE","body":"Hi,\r\n\r\nCan you guys comment if this is a similar instance of OOM exception gets swallowed cases? I have a nested date histogram + cardinality aggregation that would explode the elasticsearch node heap memory. On 5.4.2 ES, I am seeing the OOM exception stack trace thrown properly. I didn't see the OOM stack trace when I upgrade to ES 5.6. Instead Netty exception log is what I see.\r\n\r\n[2017-09-12T11:47:27,228][ERROR][o.e.t.n.Netty4Utils      ] fatal error on the network layer\r\n        at org.elasticsearch.transport.netty4.Netty4Utils.maybeDie(Netty4Utils.java:179)\r\n        at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.exceptionCaught(Netty4HttpRequestHandler.java:81)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:285)\r\n        at io.netty.channel.AbstractChannelHandlerContext.notifyHandlerException(AbstractChannelHandlerContext.java:850)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:364)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\r\n        at org.elasticsearch.http.netty4.pipelining.HttpPipeliningHandler.channelRead(HttpPipeliningHandler.java:68)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\r\n        at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\r\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\r\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\r\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\r\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926)\r\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498)\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n[2017-09-12T11:47:27,229][WARN ][o.e.m.j.JvmGcMonitorService] [node01-es-dev] [gc][383] overhead, spent [1.3m] collecting in the last [1.3m]\r\n[2017-09-12T11:47:27,227][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [node01-es-dev] fatal error in thread [elasticsearch[node01-es-dev][search][T#3]], exiting\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2017-09-12T11:47:31,496][INFO ][o.e.n.Node               ] [node01-es-dev] initializing ...\r\n[2017-09-12T11:47:31,570][INFO ][o.e.e.NodeEnvironment    ] [node01-es-dev] using [1] data paths, mounts [[/elastic/data (/dev/mapper/elastic-data)]], net usable_space [563.8gb], net total_space [565.7gb], spins? [possibly], types [xfs]\r\n[2017-09-12T11:47:31,570][INFO ][o.e.e.NodeEnvironment    ] [node01-es-dev] heap size [7.9gb], compressed ordinary object pointers [true]\r\n[2017-09-12T11:47:31,598][INFO ][o.e.n.Node               ] [node01-es-dev] node name [node01-es-dev], node ID [MAoYvdPRT02HlKtDD7GQrg]\r\n[2017-09-12T11:47:31,598][INFO ][o.e.n.Node               ] [node01-es-dev] version[5.6.0], pid[16748], build[781a835/2017-09-07T03:09:58.087Z], OS[Linux/3.10.0-514.el7.x86_64/amd64], JVM[Oracle Corporation/OpenJDK 64-Bit Server VM/1.8.0_131/25.131-b12]\r\n[2017-09-12T11:47:31,598][INFO ][o.e.n.Node               ] [node01-es-dev] JVM arguments [-Xms8g, -Xmx8g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+DisableExplicitGC, -XX:+AlwaysPreTouch, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/usr/share/elasticsearch]\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329324137","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-329324137","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":329324137,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTMyNDEzNw==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T23:20:53Z","updated_at":"2017-09-13T23:20:53Z","author_association":"MEMBER","body":"@hexinw The OOM did not get swallowed. Your node stopped and restarted, as expected:\r\n\r\n```\r\n[2017-09-12T11:47:27,227][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [node01-es-dev] fatal error in thread [elasticsearch[node01-es-dev][search][T#3]], exiting\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2017-09-12T11:47:31,496][INFO ][o.e.n.Node ] [node01-es-dev] initializing ...\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329345656","html_url":"https://github.com/elastic/elasticsearch/issues/18230#issuecomment-329345656","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","id":329345656,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTM0NTY1Ng==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-14T01:49:55Z","updated_at":"2017-09-14T01:49:55Z","author_association":"NONE","body":"So the java.lang.OutOfMemoryError stack trace is not getting logged in time due to node restart? On 5.4.2, I've been able to consistently seeing the stack trace when OOM happens.","performed_via_github_app":null}]