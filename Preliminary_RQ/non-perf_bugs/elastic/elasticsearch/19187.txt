{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/19187","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19187/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19187/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19187/events","html_url":"https://github.com/elastic/elasticsearch/issues/19187","id":163160298,"node_id":"MDU6SXNzdWUxNjMxNjAyOTg=","number":19187,"title":"Nested RemoteTransportExceptions flood the logs and fill the disk","user":{"login":"nomoa","id":5939211,"node_id":"MDQ6VXNlcjU5MzkyMTE=","avatar_url":"https://avatars1.githubusercontent.com/u/5939211?v=4","gravatar_id":"","url":"https://api.github.com/users/nomoa","html_url":"https://github.com/nomoa","followers_url":"https://api.github.com/users/nomoa/followers","following_url":"https://api.github.com/users/nomoa/following{/other_user}","gists_url":"https://api.github.com/users/nomoa/gists{/gist_id}","starred_url":"https://api.github.com/users/nomoa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nomoa/subscriptions","organizations_url":"https://api.github.com/users/nomoa/orgs","repos_url":"https://api.github.com/users/nomoa/repos","events_url":"https://api.github.com/users/nomoa/events{/privacy}","received_events_url":"https://api.github.com/users/nomoa/received_events","type":"User","site_admin":false},"labels":[{"id":152510590,"node_id":"MDU6TGFiZWwxNTI1MTA1OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Recovery","name":":Distributed/Recovery","color":"0e8a16","default":false,"description":"Anything around constructing a new shard, either from a local or a remote source."},{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null},{"id":342561351,"node_id":"MDU6TGFiZWwzNDI1NjEzNTE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/v2.4.0","name":"v2.4.0","color":"dddddd","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2016-06-30T13:04:04Z","updated_at":"2016-08-22T14:31:24Z","closed_at":"2016-08-22T14:31:19Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"It happened during a rolling restart needed for a security upgrade. The cluster is running elastic 2.3.3.\nAll nodes are running the same JVM version (OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)).\n\nA RemoteTransportException seemed to \"loop?\" between 2 nodes causing elastic to log bigger and bigger exception traces as a new RemoteException exception seemed to be created with the previous one carrying all its causes.\n\nThe first trace was (on elastic1045) : \n\n```\n[2016-06-30 08:34:20,553][WARN ][org.elasticsearch        ] Exception cause unwrapping ran for 10 levels...\nRemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1045][10.64.48.143:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1045][10.64.48.143:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1045][10.64.48.143:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1045][10.64.48.143:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1045][10.64.48.143:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[elastic1036][10.64.16.45:9300][indices:data/write/bulk[s][p]]]; nested: IllegalIndexShardStateException[CurrentState[POST_RECOVERY] operation only allowed when started/recovering, origin [PRIMARY]];\n[[11 lines of Caused by: RemoteTransportException]]\nCaused by: [itwiki_general_1415230945][[itwiki_general_1415230945][2]] IllegalIndexShardStateException[CurrentState[POST_RECOVERY] operation only allowed when started/recovering, origin [PRIMARY]]\n        at org.elasticsearch.index.shard.IndexShard.ensureWriteAllowed(IndexShard.java:1062)\n        at org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:593)\n        at org.elasticsearch.index.engine.Engine$Index.execute(Engine.java:836)\n        at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:237)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:326)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:389)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:191)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:68)\n        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)\n        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)\n        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n```\n\nThe second one (same root cause) appeared few ms after with also 12 causes.\nThe third and fourth ones had 14 causes, fifth and sixth 16 causes and so on...\nThe last one I've seen had 1982 chained causes.\n\nThe logs were nearly the same on elastic1036 (master) generating 27gig of logs in few minutes on both nodes.\n\nSurprisingly the cluster was still performing relatively well with higher gc activity on these nodes.\n\nThen (maybe 1 hour after the first trace) elastic1045 was dropped from the cluster:\n\n```\n[2016-06-30 09:48:25,953][INFO ][discovery.zen            ] [elastic1045] master_left [{elastic1036}{DUOG0aGqQ3Gajr_wcFTOyw}{10.64.16.45}{10.64.16.45:9300}{rack=B3, row=B, master=true}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]\n```\n\nIt was immediately re-added and the log flood stopped.\n\nI'll comment on this ticket if it happens again.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}