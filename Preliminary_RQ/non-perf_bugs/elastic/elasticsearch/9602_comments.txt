[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73307323","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-73307323","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":73307323,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzA3MzIz","user":{"login":"darsh221","id":1696863,"node_id":"MDQ6VXNlcjE2OTY4NjM=","avatar_url":"https://avatars2.githubusercontent.com/u/1696863?v=4","gravatar_id":"","url":"https://api.github.com/users/darsh221","html_url":"https://github.com/darsh221","followers_url":"https://api.github.com/users/darsh221/followers","following_url":"https://api.github.com/users/darsh221/following{/other_user}","gists_url":"https://api.github.com/users/darsh221/gists{/gist_id}","starred_url":"https://api.github.com/users/darsh221/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/darsh221/subscriptions","organizations_url":"https://api.github.com/users/darsh221/orgs","repos_url":"https://api.github.com/users/darsh221/repos","events_url":"https://api.github.com/users/darsh221/events{/privacy}","received_events_url":"https://api.github.com/users/darsh221/received_events","type":"User","site_admin":false},"created_at":"2015-02-06T20:33:01Z","updated_at":"2015-02-06T20:36:14Z","author_association":"NONE","body":"I see cluster started initializing replica shards after 2 hr. For 2 hours there was nothing in pending tasks other than \nreroute_after_cluster_update_settings task. Didn't see anything useful in the master server logs. Why is this so slow ?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73492860","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-73492860","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":73492860,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDkyODYw","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-09T11:16:35Z","updated_at":"2015-02-09T11:16:35Z","author_association":"MEMBER","body":"@darsh221 something is holding the replicas from being re-assigned. It may be the DiskThresholdAllocator protecting for disk space. You can get an explanation of the current decisions using: curl -XPOST \"http://localhost:9200/_cluster/reroute?explain\"\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73568158","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-73568158","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":73568158,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTY4MTU4","user":{"login":"darsh221","id":1696863,"node_id":"MDQ6VXNlcjE2OTY4NjM=","avatar_url":"https://avatars2.githubusercontent.com/u/1696863?v=4","gravatar_id":"","url":"https://api.github.com/users/darsh221","html_url":"https://github.com/darsh221","followers_url":"https://api.github.com/users/darsh221/followers","following_url":"https://api.github.com/users/darsh221/following{/other_user}","gists_url":"https://api.github.com/users/darsh221/gists{/gist_id}","starred_url":"https://api.github.com/users/darsh221/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/darsh221/subscriptions","organizations_url":"https://api.github.com/users/darsh221/orgs","repos_url":"https://api.github.com/users/darsh221/repos","events_url":"https://api.github.com/users/darsh221/events{/privacy}","received_events_url":"https://api.github.com/users/darsh221/received_events","type":"User","site_admin":false},"created_at":"2015-02-09T19:02:07Z","updated_at":"2015-02-09T19:28:12Z","author_association":"NONE","body":"We are not using any disk threshold watermark settings. May be whatever default it is. Disks in our cluster are only 5 % full of its total size. Currently cluster is in green status so i will not be able to see reroute info.\n\nI tried running \"http://localhost:9200/_cluster/reroute?explain\" but i got \n\n{\nerror: \"ElasticsearchIllegalArgumentException[No feature for name [reroute]]\",\nstatus: 400\n}\n\nMore info about our cluster\n40 physical servers with 200 GB of RAM and 32 cpus.\n120 data nodes. 3 nodes on each physical server with 30 GB RAM each.\n5 master nodes.\nWe are using one 3 \\* 2 and two 2 \\* 2 RAIDs on each server. \nWe are indexing 10-12 TB of data everyday. 2 different indices per hour with 53 shards for each index. Currently each shard is 3-5 gb in size. \n\nConfigs we are using \n\ndiscovery.zen.ping.multicast.enabled: false\nnode.master : false\nnode.data : true\ntransport.tcp.port: 9311\nindex.number_of_shards: 53\nindex.number_of_replicas: 1\nindex.refresh_interval: 30s\naction.disable_delete_all_indices: true\ndiscovery.zen.minimum_master_nodes: 3\ndiscovery.zen.ping_timeout : 1m\ndiscovery.zen.join_timeout : 10m\nscript.disable_dynamic : false\ngateway.recover_after_data_nodes: 110\ngateway.expected_nodes: 117\ngateway.local.auto_import_dangled: yes\ndiscovery.zen.ping.unicast.hosts: [“5 “master nodes]\ncluster.routing.allocation.same_shard.host: true\ncluster.routing.allocation.cluster_concurrent_rebalance: 10\ncluster.routing.allocation.node_initial_primaries_recoveries: 20\ncluster.routing.allocation.node_concurrent_recoveries: 10\nindices.recovery.concurrent_streams: 8\nindices.recovery.max_bytes_per_sec: 100mb\nthreadpool.bulk.type: fixed\nthreadpool.bulk.size: 75\nthreadpool.bulk.queue_size: -1\nthreadpool.index.type: fixed\nthreadpool.index.size: 75\nthreadpool.index.queue_size: -1\nindices.store.throttle.max_bytes_per_sec : 200mb\nindex.merge.scheduler.max_thread_count: 8\nindex.store.type: mmapfs\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73787453","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-73787453","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":73787453,"node_id":"MDEyOklzc3VlQ29tbWVudDczNzg3NDUz","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-10T21:28:15Z","updated_at":"2015-02-10T21:28:15Z","author_association":"MEMBER","body":"> I tried running \"http://localhost:9200/_cluster/reroute?explain\" but i got\n\nThis should be a POST not a get. Note the curl command I sent.\n\nI initially misread the ticket- I thought your shards were not initializing at all for 2h. Now I read they started  intializing after 2h. In these two hours, did all data nodes successfully join the cluster? When you run pending_tasks did you see any task with the `executing` flag set to true? \n\nA couple of comments about your settings\n\n> discovery.zen.ping_timeout : 1m\n\nthis is quite high. Any reason for that?\n\n> threadpool.bulk.type: fixed\n> threadpool.bulk.size: 75\n> threadpool.bulk.queue_size: -1\n> threadpool.index.type: fixed\n> threadpool.index.size: 75\n\nWe have smart defaults in ES based on the number of cores. I think you can remove this. Especially the unbound queue is a recipe for memory issues.\n\n> index.store.type: mmapfs \n\nWhy did you change from the default? ES now uses smarter compound dir format (mmapfs for smaller files, NIO for bigger).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/77465068","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-77465068","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":77465068,"node_id":"MDEyOklzc3VlQ29tbWVudDc3NDY1MDY4","user":{"login":"btecu","id":7517745,"node_id":"MDQ6VXNlcjc1MTc3NDU=","avatar_url":"https://avatars2.githubusercontent.com/u/7517745?v=4","gravatar_id":"","url":"https://api.github.com/users/btecu","html_url":"https://github.com/btecu","followers_url":"https://api.github.com/users/btecu/followers","following_url":"https://api.github.com/users/btecu/following{/other_user}","gists_url":"https://api.github.com/users/btecu/gists{/gist_id}","starred_url":"https://api.github.com/users/btecu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/btecu/subscriptions","organizations_url":"https://api.github.com/users/btecu/orgs","repos_url":"https://api.github.com/users/btecu/repos","events_url":"https://api.github.com/users/btecu/events{/privacy}","received_events_url":"https://api.github.com/users/btecu/received_events","type":"User","site_admin":false},"created_at":"2015-03-05T22:21:48Z","updated_at":"2015-03-05T22:21:48Z","author_association":"NONE","body":"I'm having the same issue. I've restarted my master and the other node (slave) become the master, but my slave now doesn't get any shard. have **n** shards assigned on master and **n** unassigned shards (slave doesn't have anything).\n\nIdeas?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/96426618","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-96426618","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":96426618,"node_id":"MDEyOklzc3VlQ29tbWVudDk2NDI2NjE4","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-04-26T19:45:48Z","updated_at":"2015-04-26T19:45:48Z","author_association":"CONTRIBUTOR","body":"No more info from original poster, so closing.  \n\n@btecu, you'd need to provide more info than you have for us to have any chance of diagnosing. Please feel free to open a new ticket if you're still seeing this issue.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/127332136","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-127332136","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":127332136,"node_id":"MDEyOklzc3VlQ29tbWVudDEyNzMzMjEzNg==","user":{"login":"allthedrones","id":2430029,"node_id":"MDQ6VXNlcjI0MzAwMjk=","avatar_url":"https://avatars3.githubusercontent.com/u/2430029?v=4","gravatar_id":"","url":"https://api.github.com/users/allthedrones","html_url":"https://github.com/allthedrones","followers_url":"https://api.github.com/users/allthedrones/followers","following_url":"https://api.github.com/users/allthedrones/following{/other_user}","gists_url":"https://api.github.com/users/allthedrones/gists{/gist_id}","starred_url":"https://api.github.com/users/allthedrones/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/allthedrones/subscriptions","organizations_url":"https://api.github.com/users/allthedrones/orgs","repos_url":"https://api.github.com/users/allthedrones/repos","events_url":"https://api.github.com/users/allthedrones/events{/privacy}","received_events_url":"https://api.github.com/users/allthedrones/received_events","type":"User","site_admin":false},"created_at":"2015-08-03T17:02:38Z","updated_at":"2015-08-03T17:04:36Z","author_association":"NONE","body":"I've experienced this same thing a couple of times now in 1.4.5. I'll try to provide as much detail as I can...\n\n1 - Disable shard allocation, \n2 - reboot data nodes one at a time, waiting for cluster to return to yellow in between (primaries online, replicas unassigned)\n3 - after last node & all primaries online, re-enable allocation to bring replicas back online.\n\nBut the replicas don't start assigning, not for  a _long_ time (update: in the most recent case, the `reroute_after_cluster_update_settings` task was executing for ~30 minutes, after which time replica assignment began). At the top of the pending tasks queue is this:\n\n``` javascript\n{\n    \"insert_order\" : 4581,\n    \"priority\" : \"URGENT\",\n    \"source\" : \"reroute_after_cluster_update_settings\",\n    \"executing\" : true,\n    \"time_in_queue_millis\" : 736479,\n    \"time_in_queue\" : \"12.2m\"\n}\n```\n\nTrying to execute the curl command as noted here results in an error:\n`curl -XPOST \"http://localhost:9200/_cluster/reroute?explain\"`\n\n``` javascript\n{\"error\":\"RemoteTransportException[[acme-stage-em3][inet[/10.0.0.10:9300]][cluster:admin/reroute]]; nested: ProcessClusterEventTimeoutException[failed to process cluster event (cluster_reroute (api)) within 30s]; \",\"status\":503}\n```\n\nOther relevant details ... [heavy] indexing is still going on during this period (for one index only out of about 100). The cluster has 3 master-only, 2 client-only, and 4 data-only nodes.\n\nThe only possibly suspicious log entry is a timeout complaint in the elected master's log file, a few minutes after re-enabling allocation:\n\n```\n[2015-08-03 16:21:11,378][INFO ][cluster.routing.allocation.decider] [acme-stage-em3] updating [cluster.routing.allocation.enable] from [NONE] to [ALL]\n[2015-08-03 16:25:08,625][WARN ][gateway.local            ] [acme-stage-em3] [c1-15.7.16][1]: failed to list shard stores on node [uqW-DKN2QtWtB3go3hufiQ]\norg.elasticsearch.action.FailedNodeException: Failed node [uqW-DKN2QtWtB3go3hufiQ]\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onFailure(TransportNodesOperationAction.java:206)\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$1000(TransportNodesOperationAction.java:97)\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$4.handleException(TransportNodesOperationAction.java:178)\n        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:366)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.transport.ReceiveTimeoutTransportException: [acme-stage-es3][inet[/10.0.0.6:9300]][internal:cluster/nodes/indices/shard/store[n]] request_id [7443495] timed out after [30006ms]\n        ... 4 more\n[2015-08-03 16:25:09,625][WARN ][transport                ] [acme-stage-em3] Received response for a request that has timed out, sent [31006ms] ago, timed out [1000ms] ago, action [internal:cluster/nodes/indices/shard/store[n]], node [[acme-stage-es3][uqW-DKN2QtWtB3go3hufiQ][acme-stage-es3][inet[/10.0.0.6:9300]]{master=false}], id [7443495]\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/127983069","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-127983069","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":127983069,"node_id":"MDEyOklzc3VlQ29tbWVudDEyNzk4MzA2OQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-08-05T12:37:30Z","updated_at":"2015-08-05T12:37:30Z","author_association":"CONTRIBUTOR","body":"@allthedrones I suggest upgrading - recent versions have greatly improved this situation\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/128112627","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-128112627","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":128112627,"node_id":"MDEyOklzc3VlQ29tbWVudDEyODExMjYyNw==","user":{"login":"allthedrones","id":2430029,"node_id":"MDQ6VXNlcjI0MzAwMjk=","avatar_url":"https://avatars3.githubusercontent.com/u/2430029?v=4","gravatar_id":"","url":"https://api.github.com/users/allthedrones","html_url":"https://github.com/allthedrones","followers_url":"https://api.github.com/users/allthedrones/followers","following_url":"https://api.github.com/users/allthedrones/following{/other_user}","gists_url":"https://api.github.com/users/allthedrones/gists{/gist_id}","starred_url":"https://api.github.com/users/allthedrones/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/allthedrones/subscriptions","organizations_url":"https://api.github.com/users/allthedrones/orgs","repos_url":"https://api.github.com/users/allthedrones/repos","events_url":"https://api.github.com/users/allthedrones/events{/privacy}","received_events_url":"https://api.github.com/users/allthedrones/received_events","type":"User","site_admin":false},"created_at":"2015-08-05T19:08:12Z","updated_at":"2015-08-05T19:08:37Z","author_association":"NONE","body":"Thanks @clintongormley! We're rolling out 1.6 in our pre-production environments now. Are there some better issue #'s that more specifically document _which_ improvements were made that address this particular circumstance? Thanks again!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/128113758","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-128113758","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":128113758,"node_id":"MDEyOklzc3VlQ29tbWVudDEyODExMzc1OA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-08-05T19:12:16Z","updated_at":"2015-08-05T19:12:16Z","author_association":"CONTRIBUTOR","body":"@allthedrones I'd recommend to move to the latest 1.7.1 instead!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/220861222","html_url":"https://github.com/elastic/elasticsearch/issues/9602#issuecomment-220861222","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9602","id":220861222,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMDg2MTIyMg==","user":{"login":"cywjackson","id":704150,"node_id":"MDQ6VXNlcjcwNDE1MA==","avatar_url":"https://avatars3.githubusercontent.com/u/704150?v=4","gravatar_id":"","url":"https://api.github.com/users/cywjackson","html_url":"https://github.com/cywjackson","followers_url":"https://api.github.com/users/cywjackson/followers","following_url":"https://api.github.com/users/cywjackson/following{/other_user}","gists_url":"https://api.github.com/users/cywjackson/gists{/gist_id}","starred_url":"https://api.github.com/users/cywjackson/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cywjackson/subscriptions","organizations_url":"https://api.github.com/users/cywjackson/orgs","repos_url":"https://api.github.com/users/cywjackson/repos","events_url":"https://api.github.com/users/cywjackson/events{/privacy}","received_events_url":"https://api.github.com/users/cywjackson/received_events","type":"User","site_admin":false},"created_at":"2016-05-22T22:44:42Z","updated_at":"2016-05-22T22:44:42Z","author_association":"NONE","body":"The purpose of this comment is to bring some more awareness to this problem, and share what I have done to recover. I feel this unassigned shards could happen in many diff forms, so my soln in the end may not be your soln.\n\nWe had faced this problem in previous upgrade, (could have been 0.90.x to 0.90.y, or 0.90.y to 1.0.x, or 1.0.x to 1.3.x , don't remember). Tried the reroute b4, there was a guy even posted a nice script to try to find all the unassigned shards and allocate them: http://www.unknownerror.org/opensource/elastic/elasticsearch/q/stackoverflow/19967472/elasticsearch-unassigned-shards-how-to-fix , at the very bottom , and notice how he has `\"allow_primary\": true` , so be careful using this. I don't recall how we fixed this last time (not sure if the script fixed for us)\n\nWe ARE facing this issue today again, when upgrading from 1.3.x to 1.7.y.  :cry:  \n\nI first tried that script without the `\"allow_primary\": true` , don't want to result in any data lost! I have 195 unassigned shards \n\n```\n  \"number_of_data_nodes\" : 24,\n  \"active_primary_shards\" : 2249,\n  \"active_shards\" : 6022,\n...\n\"unassigned_shards\" : 195,\n```\n\nIt returns a lot of results , for each execution, but it didn't work. I still have 195 shards unassigned. So I add the explain parameter and only execute on 1 shard. My result looks similar to http://stackoverflow.com/questions/32685188/elasticsearch-shard-relocation-not-working , which has:\n\n```\n{\n>       \"decider\" : \"awareness\",\n>       \"decision\" : \"NO\",\n>       \"explanation\" : \"too many shards on nodes for attribute: [dc]\"  }\n```\n\nMine is: \n\n```\n          \"decider\": \"awareness\",\n          \"decision\": \"NO\",\n          \"explanation\": \"too many shards on nodes for attribute: [aws_availability_zone]\"\n```\n\nBtw, @s1monw , correct me if I am wrong, but I don't think the `explain` parameter work on a simple GET call with empty body,  otherwise I end up getting the same 400 like others. (haven't gone through everything in https://github.com/elastic/elasticsearch/pull/5027 to confirm yet) . I got the above result via the POST method with the allocate cmd in the body. (This method has such a harsh body)\n\nAnyway, I was curious why there are `\"too many shards on nodes for attribute`, so I followed some tip here: https://www.elastic.co/guide/en/elasticsearch/guide/current/_cluster_health.html . After wait for a while, my cluster is in `yellow` state, instead of `red`. So that means none of the unassigned shards are `primary`. which is good, Then I follow the guide from https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-shards.html , i've found my shards in 2 of the usw2c, and 1 unassigned. \n\n```\n/_cat/shards/feed_v1 | grep \" 16 \"\nfeed_v1 16 p STARTED    167399 53.6mb IP1  usw2c-.... \nfeed_v1 16 r STARTED    167399 53.6mb IP2  usw2c-... \nfeed_v1 16 r UNASSIGNED\n```\n\nAnd I start looking across from all the nodes's data directory (chef + bash), i've found 1 of my nodes in usw2a has the shard `16` folder. Interesting about this node is it was restarted at first, but it seems it failed to pick up init the shards. (key moment here for later)\n\nIn fact, this node has been doing A LOT of relocation since the restart. Almost as if it's starting a new node. Here are some more details about our upgrade/restart:\n\nWe were running hybrid nodes, and we want to convert into using master-only/data-only nodes. 3 Master-eligible nodes were already created and joined the cluster, but none of them was a master yet. They all have `discovery.zen.minimum_master_nodes=3`. Because https://github.com/elastic/elasticsearch/issues/10793 is not yet available, we'd have to restart our hybrid nodes with new settings. We choose to do that as part of our upgrade. We use chef to create/bootstrap the ES node, and those hybrid node were previously started with a `discovery.zen.minimum_master_nodes` value that calculated based on the `nodes.length / 2 + 1`. But the problem with this is the bootstrapping node itself is not included in the nodes.length, and we have been expanding/decomming nodes. And future expansion / removal of nodes would impact this value, but the ES may not have been restarted to take in effect. Basically each node has a value that's not necessary the most up-to-date as the node is started -- **TL;DR: prior to that 2a node was shut down, it was a master-eligible node, another node (lets call this 2b) was a master and has a `discovery.zen.minimum_master_nodes` of X. Once 2a was shut down, total # of master-eligible nodes in the cluster is X-1, and this led to the master (2b) no longer a master! (this whole saga demonstrates the importances of the request #10793 .)**\n\nI immediately shut down 2b (while 2a was starting), made the necessary change (set `node.master=false`, set `discovery.zen.minimum_master_nodes=3` , among other changes as part of the upgrade) . This is when I noticed there are unassigned shards failed to be initialized, after both 2a and 2b finished the init state \n\nAfter letting the relocation running for hours, and study from various posts including this one, and done the analysis (there are more I have done than ^), I have decided to stop the allocation `\"transient\" : {\"cluster.routing.allocation.enable\" : \"all\"}}` , wait and confirm no allocation running `/_cat/shards?v | egrep -v \"START|UNASS\"` , and restart the same 2a again . (refer from key moment earlier when it DIDN'T init those shards). Now all the shards are re-init'ed and started upon the restart. \n","performed_via_github_app":null}]