[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/509560319","html_url":"https://github.com/elastic/elasticsearch/issues/44101#issuecomment-509560319","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/44101","id":509560319,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTU2MDMxOQ==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-07-09T09:10:25Z","updated_at":"2019-07-09T09:10:25Z","author_association":"COLLABORATOR","body":"Pinging @elastic/ml-core","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/509564093","html_url":"https://github.com/elastic/elasticsearch/issues/44101#issuecomment-509564093","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/44101","id":509564093,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTU2NDA5Mw==","user":{"login":"sophiec20","id":4185750,"node_id":"MDQ6VXNlcjQxODU3NTA=","avatar_url":"https://avatars2.githubusercontent.com/u/4185750?v=4","gravatar_id":"","url":"https://api.github.com/users/sophiec20","html_url":"https://github.com/sophiec20","followers_url":"https://api.github.com/users/sophiec20/followers","following_url":"https://api.github.com/users/sophiec20/following{/other_user}","gists_url":"https://api.github.com/users/sophiec20/gists{/gist_id}","starred_url":"https://api.github.com/users/sophiec20/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sophiec20/subscriptions","organizations_url":"https://api.github.com/users/sophiec20/orgs","repos_url":"https://api.github.com/users/sophiec20/repos","events_url":"https://api.github.com/users/sophiec20/events{/privacy}","received_events_url":"https://api.github.com/users/sophiec20/received_events","type":"User","site_admin":false},"created_at":"2019-07-09T09:20:43Z","updated_at":"2019-07-09T09:20:43Z","author_association":"NONE","body":"Sounds good. \r\n\r\nPlease clarify for the benefit of this ticket, what would happen in the event that a lesser percentage of the docs failed to index. (this may be difficult to replicate since #43194, but I would image we will experience it in some form) \r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/509773441","html_url":"https://github.com/elastic/elasticsearch/issues/44101#issuecomment-509773441","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/44101","id":509773441,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTc3MzQ0MQ==","user":{"login":"hendrikmuhs","id":7126422,"node_id":"MDQ6VXNlcjcxMjY0MjI=","avatar_url":"https://avatars3.githubusercontent.com/u/7126422?v=4","gravatar_id":"","url":"https://api.github.com/users/hendrikmuhs","html_url":"https://github.com/hendrikmuhs","followers_url":"https://api.github.com/users/hendrikmuhs/followers","following_url":"https://api.github.com/users/hendrikmuhs/following{/other_user}","gists_url":"https://api.github.com/users/hendrikmuhs/gists{/gist_id}","starred_url":"https://api.github.com/users/hendrikmuhs/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hendrikmuhs/subscriptions","organizations_url":"https://api.github.com/users/hendrikmuhs/orgs","repos_url":"https://api.github.com/users/hendrikmuhs/repos","events_url":"https://api.github.com/users/hendrikmuhs/events{/privacy}","received_events_url":"https://api.github.com/users/hendrikmuhs/received_events","type":"User","site_admin":false},"created_at":"2019-07-09T19:22:08Z","updated_at":"2019-07-09T19:22:46Z","author_association":"CONTRIBUTOR","body":" \r\n> Please clarify for the benefit of this ticket, what would happen in the event that a lesser percentage of the docs failed to index. (this may be difficult to replicate since #43194, but I would image we will experience it in some form)\r\n\r\n\"Retry will re-create/re-index the full page as a whole but not the individual index requests\"\r\n\r\nAfter a successful query/bulk index we advance a \"cursor\", query/index again with the new cursor, advance, query/index again, ... and so on, until the cursor reaches the end. In case of a failure the cursor will not be advanced and retry will start from the old cursor position (same happens if a node dies and we migrate to another node). \r\n\r\nDue to the way bulk indexing works the successful index requests will result in successfully indexed documents. We will not try to roll back those documents. The retry will query and index the full page, which means writing the previously failed documents and overwriting the successful ones (same content, same doc id).\r\n\r\nExample: 8 out of 500 index requests failed: means 492 got indexed. We do not advance the cursor, so we do the exact same query and the exact same bulk index request again, therefore we eventually write 500 docs like in the non-failure case. At lucene level we create 8 new docs (which also could be updates!) and 492 updates with identical content.\r\n\r\n**Alternatives**\r\n\r\nonly retry the failed documents: this would be fairly complex, we would need to remember the failed docs, maybe persist them. I am not expecting to run into this issue often, because we assume stable clusters with stable network connectivity. Therefore this optimization is not worth it.\r\n\r\naccept some loss: I think there is nothing worse than incomplete/wrong data. If there is a usecase for accepting loss (high frequency updates, low-latency) it might be an option to allow this as a setting, my gut feeling: this is not what data frames are made for.\r\n\r\nroll-back: Taking the 8:492 example from above: The data frame isn't in a stable state anyway, it's somewhere in the middle of a checkpoint. Neither do the 500 documents form a transaction, therefore it seems unnecessary trying to roll-back the 492 documents as it would not end in a consistent state, but you would need to rollback the whole checkpoint. Roll-back is not delete, for continuous data frame you would require to restore the old document. This again would be fairly complex.","performed_via_github_app":null}]