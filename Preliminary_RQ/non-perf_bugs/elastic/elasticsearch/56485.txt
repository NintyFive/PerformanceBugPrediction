{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/56485","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56485/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56485/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56485/events","html_url":"https://github.com/elastic/elasticsearch/issues/56485","id":615148689,"node_id":"MDU6SXNzdWU2MTUxNDg2ODk=","number":56485,"title":"ES maybe use ThreadPoolExecutor.toString() too frequently when reject tasks.","user":{"login":"nooneuse","id":8781788,"node_id":"MDQ6VXNlcjg3ODE3ODg=","avatar_url":"https://avatars3.githubusercontent.com/u/8781788?v=4","gravatar_id":"","url":"https://api.github.com/users/nooneuse","html_url":"https://github.com/nooneuse","followers_url":"https://api.github.com/users/nooneuse/followers","following_url":"https://api.github.com/users/nooneuse/following{/other_user}","gists_url":"https://api.github.com/users/nooneuse/gists{/gist_id}","starred_url":"https://api.github.com/users/nooneuse/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nooneuse/subscriptions","organizations_url":"https://api.github.com/users/nooneuse/orgs","repos_url":"https://api.github.com/users/nooneuse/repos","events_url":"https://api.github.com/users/nooneuse/events{/privacy}","received_events_url":"https://api.github.com/users/nooneuse/received_events","type":"User","site_admin":false},"labels":[{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null},{"id":2042400575,"node_id":"MDU6TGFiZWwyMDQyNDAwNTc1","url":"https://api.github.com/repos/elastic/elasticsearch/labels/needs:triage","name":"needs:triage","color":"c5def5","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2020-05-09T10:52:07Z","updated_at":"2020-05-12T05:39:26Z","closed_at":"2020-05-12T05:39:26Z","author_association":"NONE","active_lock_reason":null,"body":"- Elasticsearch version (bin/elasticsearch --version):\r\n\r\n> Version: 7.6.2, Build: default/tar/ef48eb35cf30adf4db14086e8aabd07ef6fb113f/2020-03-26T06:34:37.794943Z, JVM: 1.8.0_131\r\n\r\n- Plugins installed:\r\n\r\n> None\r\n\r\n- JVM version (java -version):\r\n\r\n> openjdk version \"13.0.2\" 2020-01-14\r\n> OpenJDK Runtime Environment AdoptOpenJDK (build 13.0.2+8)\r\n> OpenJDK 64-Bit Server VM AdoptOpenJDK (build 13.0.2+8, mixed mode, sharing)\r\n\r\n- OS version (uname -a if on a Unix-like system):\r\n\r\n> Linux testhaha 4.14.81.bm.17-amd64 #1 SMP Debian 4.14.81.bm.17 Sun Dec 15 02:54:05 UTC 2019 x86_64 GNU/Linux\r\n\r\n- Description of the problem including expected versus actual behavior:\r\n\r\n**expected**: tasks rejected very fast with a simple response\r\n**actual**: threads waits for the lock in ThreadPoolExecutor.toString()\r\n\r\n_I think_ it maybe is **no necessary** to add detailed infos to rejected response **each time**.\r\n\r\n- Steps to reproduce:\r\n\r\n> Please include a minimal but complete recreation of the problem,\r\n> including (e.g.) index creation, mappings, settings, query etc. The easier\r\n> you make for us to reproduce it, the more likely that somebody will take the\r\n> time to look at it.\r\n> \r\n\r\n**hello**, I found a place that maybe affects es response performance. \r\ni think, in the case of high qps, any code that involves locking should be carefully optimized.\r\n# 1.query i uesd:\r\n```json\r\n{\r\n\"from\": 0,\r\n\"size\": 0,\r\n\"query\": {\r\n\"bool\": {\r\n\"filter\": [\r\n{ \"term\": { \"user_id\": \"blabla\" }},\r\n{ \"term\": { \"name\": \"poipoipoi\"}},\r\n{ \"term\": { \"type\": \"testhaha\"}}\r\n]\r\n}\r\n},\r\n\"_source\": {\r\n\"includes\": [\r\n\"SUM\",\r\n\"SUM\"\r\n],\r\n\"excludes\": [\r\n\r\n]\r\n},\r\n\"aggregations\": {\r\n\"SUM(impression_cnt)\": {\r\n\"sum\": {\r\n\"field\": \"impression_cnt\"\r\n}\r\n},\r\n\"SUM(play_cnt)\": {\r\n\"sum\": {\r\n\"field\": \"play_cnt\"\r\n}\r\n}\r\n}\r\n}\r\n```\r\n# 2.my es cluster hardware configuration:\r\nthe cluster has two machine and each machine has:\r\nIntel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz * 2 (total 96 cores)\r\n1TB memory\r\n3576GB nvme SSD * 4\r\nNIC speed 25000Mb/s\r\n# 3.my es cluster software configuration:\r\neach machine has two es node\r\nmaybe useful node confï¼š\r\n```\r\nnode.master: true\r\nnode.data: true\r\n```\r\njvm options:\r\n```\r\n-Xms31g\r\n-Xmx31g\r\n```\r\n> each node has very same configuration.\r\n# 4.my indices data\r\nwe generate some test data\r\n```\r\nhealth status index                                pri rep docs.count docs.deleted store.size pri.store.size\r\ngreen  open   akagiski_and_kagarov-20200417        10   1   19977900            0      2.7gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200416        10   1   19920048            0      2.7gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200415        10   1   19859651            0      2.7gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200414        10   1   19789584            0      2.7gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200419        10   1   20929600            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200418        10   1   20654947            0      2.8gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200430        10   1   21222883            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200413        10   1   19575457            0      2.7gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200412        10   1   19603141            0      2.7gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200411        10   1   19755647            0      2.7gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200410        10   1   19680710            0      2.7gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200505        10   1   21747819            0        3gb          1.5gb\r\ngreen  open   akagiski_and_kagarov-20200406        10   1   19237776            0      2.6gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200428        10   1   21574421            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200427        10   1   21404682            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200504        10   1   21889895            0        3gb          1.5gb\r\ngreen  open   akagiski_and_kagarov-20200503        10   1   21607366            0        3gb          1.5gb\r\ngreen  open   akagiski_and_kagarov-20200502        10   1   21214568            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200425        10   1   21312079            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200409        10   1   19599835            0      2.7gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200408        10   1   19374761            0      2.7gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200507        10   1   21467157            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200506        10   1   21401544            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200407        10   1   19330520            0      2.6gb          1.3gb\r\ngreen  open   akagiski_and_kagarov-20200420        10   1   21025548            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200501        10   1   21156376            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200424        10   1   21030666            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200423        10   1   20801871            0      2.8gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200422        10   1   20950070            0      2.9gb          1.4gb\r\ngreen  open   akagiski_and_kagarov-20200421        10   1   21032101            0      2.9gb          1.4gb\r\n```\r\n# 5.I use Locust load testing framework to test the es cluster with the query above \r\ntarget indices is akagiski_and_kagarov-*\r\nLocust running test at 3000user & 8 slaves\r\nLocust send requests to a node in this cluster(use ip:port), it **not** random pick nodes to send request.\r\n# 6.my goal is make the cluster response as fast as possible. \r\n> Any way to improve es performance is welcome.\r\n\r\ni got 400 ~ 800 qps waving quickly.  cpu usage 60% ~ 80%, load 60% ~ 80%. 20% ~ 30% of response is returned with 429 code, \r\nwhich means task queue full and reject, I think this cluster can have **better qps performance**.\r\n\r\n# Provide logs (if relevant):\r\nlets see what happend in stack:\r\n![image](https://user-images.githubusercontent.com/8781788/81471951-77d44000-9227-11ea-87e7-7007b0fcd8b2.png)\r\ni suspected there is a lock in EsThreadPoolExecutor.toString\r\nso i find the code:\r\n```\r\n@Override\r\n    public final String toString() {\r\n        StringBuilder b = new StringBuilder();\r\n        b.append(getClass().getSimpleName()).append('[');\r\n        b.append(\"name = \").append(name).append(\", \");\r\n        if (getQueue() instanceof SizeBlockingQueue) {\r\n            @SuppressWarnings(\"rawtypes\")\r\n            SizeBlockingQueue queue = (SizeBlockingQueue) getQueue();\r\n            b.append(\"queue capacity = \").append(queue.capacity()).append(\", \");\r\n        }\r\n        appendThreadPoolExecutorDetails(b);\r\n        /*\r\n         * ThreadPoolExecutor has some nice information in its toString but we\r\n         * can't get at it easily without just getting the toString.\r\n         */\r\n        b.append(super.toString()).append(']');\r\n        return b.toString();\r\n    }\r\n```\r\nthis method use super.toString(), lets see the code of super.toString():\r\nit came from ThreadPoolExecutor, a jdk class.\r\n```\r\npublic String toString() {\r\n        ReentrantLock mainLock = this.mainLock;\r\n        mainLock.lock();\r\n\r\n        long ncompleted;\r\n        int nworkers;\r\n        int nactive;\r\n        try {\r\n            ncompleted = this.completedTaskCount;\r\n            nactive = 0;\r\n            nworkers = this.workers.size();\r\n            Iterator var6 = this.workers.iterator();\r\n\r\n            while(var6.hasNext()) {\r\n                ThreadPoolExecutor.Worker w = (ThreadPoolExecutor.Worker)var6.next();\r\n                ncompleted += w.completedTasks;\r\n                if (w.isLocked()) {\r\n                    ++nactive;\r\n                }\r\n            }\r\n        } finally {\r\n            mainLock.unlock();\r\n        }\r\n\r\n        int c = this.ctl.get();\r\n        String runState = isRunning(c) ? \"Running\" : (runStateAtLeast(c, 1610612736) ? \"Terminated\" : \"Shutting down\");\r\n        return super.toString() + \"[\" + runState + \", pool size = \" + nworkers + \", active threads = \" + nactive + \", queued tasks = \" + this.workQueue.size() + \", completed tasks = \" + ncompleted + \"]\";\r\n    }\r\n```\r\nto make sure the infos is right, the ThreadPoolExecutor have to lock the mainLock when it collect infos of its workers.So it may be a too slow method to use frequently. But EsThreadPoolExecutor use it every time when toString().\r\nEsThreadPoolExecutor will use super.toString() per reject which cost a lot of time to wait.I still think it is maybe no necessary to add detailed infos to rejected response each time.Can we have an option to skip EsThreadPoolExecutor.toString() when reject?","closed_by":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"performed_via_github_app":null}