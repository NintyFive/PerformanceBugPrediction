[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/520248335","html_url":"https://github.com/elastic/elasticsearch/issues/45427#issuecomment-520248335","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45427","id":520248335,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMDI0ODMzNQ==","user":{"login":"Elias481","id":38512369,"node_id":"MDQ6VXNlcjM4NTEyMzY5","avatar_url":"https://avatars2.githubusercontent.com/u/38512369?v=4","gravatar_id":"","url":"https://api.github.com/users/Elias481","html_url":"https://github.com/Elias481","followers_url":"https://api.github.com/users/Elias481/followers","following_url":"https://api.github.com/users/Elias481/following{/other_user}","gists_url":"https://api.github.com/users/Elias481/gists{/gist_id}","starred_url":"https://api.github.com/users/Elias481/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Elias481/subscriptions","organizations_url":"https://api.github.com/users/Elias481/orgs","repos_url":"https://api.github.com/users/Elias481/repos","events_url":"https://api.github.com/users/Elias481/events{/privacy}","received_events_url":"https://api.github.com/users/Elias481/received_events","type":"User","site_admin":false},"created_at":"2019-08-11T18:06:06Z","updated_at":"2019-08-11T18:06:06Z","author_association":"NONE","body":"I just found some other facts:\r\nAt the same time the Index Writer memory usage climbs to about 1,4GBs (out of 2GB) the Version Map memory raises in same relation (to a max of about 50MB, thought).\r\nThere are these well known throtelling because segment writing can't keep up messages shortly when the memory usage starts to raise.\r\nThis time I managed to kill the bul-loading process before OOM actually happened (Full-GC was still looping) and the memory usage went down after some time.\r\nThe throtelling ended messages appeared.\r\nAnyway ES did not recover, despite no error messages in log the CPU usage kept to be at about 1 core all the time and a clean shutdown was not possible (after many minutes of waiting). The monitoring view was not updating anymore.\r\n\r\nI will now try with index refresh intervall disables and debug logging for indices, but anyway expected behaviour is to be able to set some hard limit at which elasticsearch throttles requests (not deny but just keep requester on hold till buffers are available). So this is a still a bug (at least a documentation bug).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/520249072","html_url":"https://github.com/elastic/elasticsearch/issues/45427#issuecomment-520249072","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45427","id":520249072,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMDI0OTA3Mg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-08-11T18:16:49Z","updated_at":"2019-08-11T18:16:49Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/520249217","html_url":"https://github.com/elastic/elasticsearch/issues/45427#issuecomment-520249217","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45427","id":520249217,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMDI0OTIxNw==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-08-11T18:19:01Z","updated_at":"2019-08-11T18:19:45Z","author_association":"MEMBER","body":"@Elias481 the fact that the circuit does not account for GC was discussed in https://github.com/elastic/elasticsearch/issues/40115 and we decided not to do anything about this for now.\r\n\r\nThere is ongoing discussion about throttling network reads to get smoother behavior in https://github.com/elastic/elasticsearch/issues/44484 in some situations when running a multi-node cluster (see last point why this is not applicable to single node situations).\r\n\r\nI think your best solution here is to account for the circuit breaker behavior in your client and make it retry (and maybe throttle) when running into the circuit breaker as intended by the functionality.\r\n\r\n> behaviour is to be able to set some hard limit at which elasticsearch throttles requests (not deny but just keep requester on hold till buffers are available)\r\n\r\nThe problem with this idea is that it only really makes sense in a single node cluster doesn't it?\r\nIf you have multiple nodes that you can send your bulk request to, running into `503`s is likely a lot more useful than the node throttling network reads for REST requests. Without those 503s and throttled reads in their place on the REST layer, the latency behavior of ES becomes a lot less predictable and load balancing REST requests across multiple ES nodes becomes much harder doesn't it?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/520251723","html_url":"https://github.com/elastic/elasticsearch/issues/45427#issuecomment-520251723","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45427","id":520251723,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMDI1MTcyMw==","user":{"login":"Elias481","id":38512369,"node_id":"MDQ6VXNlcjM4NTEyMzY5","avatar_url":"https://avatars2.githubusercontent.com/u/38512369?v=4","gravatar_id":"","url":"https://api.github.com/users/Elias481","html_url":"https://github.com/Elias481","followers_url":"https://api.github.com/users/Elias481/followers","following_url":"https://api.github.com/users/Elias481/following{/other_user}","gists_url":"https://api.github.com/users/Elias481/gists{/gist_id}","starred_url":"https://api.github.com/users/Elias481/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Elias481/subscriptions","organizations_url":"https://api.github.com/users/Elias481/orgs","repos_url":"https://api.github.com/users/Elias481/repos","events_url":"https://api.github.com/users/Elias481/events{/privacy}","received_events_url":"https://api.github.com/users/Elias481/received_events","type":"User","site_admin":false},"created_at":"2019-08-11T18:57:45Z","updated_at":"2019-08-11T18:57:45Z","author_association":"NONE","body":"Hmm I have to think about. Currently my problem is the software that was made by our development and gave us to put it in production.\r\nThey do not cope with the circuit-breaker (no retries..) and I have to do initial bulk-load.\r\nSo what is making the memory usage going straight up at certain point in time? Is it just that throtteling? Do I have a chance to calculate needed memory for successfull initial bulk-import?\r\n\r\nSo if the circuit breaker would account for the \"Index Writer Memory Usage\" I would be happy to go to our dev team with this. (We can use it in current mode then so let it stop at 60% usage for all accounted memeory).\r\nOtherwise it copes down to GC tuning (initiation occupancy) and more heap because of such behaviour, at least that is what I expect from our dev.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/520254282","html_url":"https://github.com/elastic/elasticsearch/issues/45427#issuecomment-520254282","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45427","id":520254282,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMDI1NDI4Mg==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-08-11T19:36:12Z","updated_at":"2019-08-11T19:36:12Z","author_association":"MEMBER","body":"@Elias481  \r\n\r\nUnfortunately, this appears to be a user question at this point and we'd like to direct these kinds of things to the [forums](https://discuss.elastic.co/\\). If you can stop by there, we'd appreciate it. This allows us to use GitHub for verified bug reports, feature requests, and pull requests.\r\n\r\nLet me try a few quick answers here though to get you started:\r\n\r\n> So what is making the memory usage going straight up at certain point in time? Is it just that throtteling? Do I have a chance to calculate needed memory for successfull initial bulk-import?\r\n\r\nI don't think there is any viable way of calculating the memory use up front. There's just too many variables going into this like the performance of your underlying storage, CPU performance, GC settings and many many more => Your best bet is to implement retry logic into your application in my opinion instead of trying to work around a lack of it by turning off ES safety features. Trying to guess the amount of load that the system can handle seems like a significantly more complex undertaking than simply implementing retries to me.\r\n\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/520255157","html_url":"https://github.com/elastic/elasticsearch/issues/45427#issuecomment-520255157","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45427","id":520255157,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMDI1NTE1Nw==","user":{"login":"Elias481","id":38512369,"node_id":"MDQ6VXNlcjM4NTEyMzY5","avatar_url":"https://avatars2.githubusercontent.com/u/38512369?v=4","gravatar_id":"","url":"https://api.github.com/users/Elias481","html_url":"https://github.com/Elias481","followers_url":"https://api.github.com/users/Elias481/followers","following_url":"https://api.github.com/users/Elias481/following{/other_user}","gists_url":"https://api.github.com/users/Elias481/gists{/gist_id}","starred_url":"https://api.github.com/users/Elias481/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Elias481/subscriptions","organizations_url":"https://api.github.com/users/Elias481/orgs","repos_url":"https://api.github.com/users/Elias481/repos","events_url":"https://api.github.com/users/Elias481/events{/privacy}","received_events_url":"https://api.github.com/users/Elias481/received_events","type":"User","site_admin":false},"created_at":"2019-08-11T19:49:06Z","updated_at":"2019-08-11T19:49:06Z","author_association":"NONE","body":"@original-brownbear I now read linked tickets. There was no discussion about whether andy why the indexing heap cannot be accounted in circuit breaker (besides the real-mem cb that has the drawback of tripping just because the GC has not finished housekeeping). So for systems with predicatble workload the old cb looks much better - if it would account for such large portions of heap like used for the indexing buffer and one could tune the threshold to account for all the things not caught in circuit breaker but are more predictable as the suddenly extremle raising indexing buffers which also stay high.\r\nSo if we would use the real-mem cb we could not ditinguish between just \"immediate\" retry for letting the GC do it's work or wait some more time because there is really memory pressure because of the disk throtteling issue or such things.","performed_via_github_app":null}]