{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/27491","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27491/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27491/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27491/events","html_url":"https://github.com/elastic/elasticsearch/issues/27491","id":276037112,"node_id":"MDU6SXNzdWUyNzYwMzcxMTI=","number":27491,"title":"\"Dirty\" data of other shards kept on disk after rerouting","user":{"login":"hgfischer","id":120860,"node_id":"MDQ6VXNlcjEyMDg2MA==","avatar_url":"https://avatars1.githubusercontent.com/u/120860?v=4","gravatar_id":"","url":"https://api.github.com/users/hgfischer","html_url":"https://github.com/hgfischer","followers_url":"https://api.github.com/users/hgfischer/followers","following_url":"https://api.github.com/users/hgfischer/following{/other_user}","gists_url":"https://api.github.com/users/hgfischer/gists{/gist_id}","starred_url":"https://api.github.com/users/hgfischer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hgfischer/subscriptions","organizations_url":"https://api.github.com/users/hgfischer/orgs","repos_url":"https://api.github.com/users/hgfischer/repos","events_url":"https://api.github.com/users/hgfischer/events{/privacy}","received_events_url":"https://api.github.com/users/hgfischer/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2017-11-22T11:38:46Z","updated_at":"2017-11-22T20:57:13Z","closed_at":"2017-11-22T11:54:56Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Elasticsearch version** (`bin/elasticsearch --version`): `Version: 2.4.3, Build: d38a34e/2016-12-07T16:28:56Z, JVM: 1.8.0_144`\r\n\r\n**Plugins installed**: []\r\n\r\n**JVM version** (`java -version`):\r\n```\r\njava version \"1.8.0_144\"\r\nJava(TM) SE Runtime Environment (build 1.8.0_144-b01)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)\r\n```\r\n\r\n**OS version** (`uname -a` if on a Unix-like system):\r\n```\r\nLinux els01 4.4.0-92-generic #115-Ubuntu SMP Thu Aug 10 09:04:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.3 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n``` \r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\nI removed 2 nodes from the cluster, using cluster shard allocation exclusion lists, to move shards away before turning them off, to do hardware maintenance. After the maintenance both nodes were added at different times with same name and IPs to the cluster, and the cluster shard allocation exclusion lists were cleaned up to allow shards to move in back again.\r\n\r\nAfter some time reallocating data, one of the rebuilt nodes had too much disk used (~85%) while most of the other nodes are bellow (65%). Then after inspecting the data directory I found lots of data from shards that were not allocated on that node.\r\n\r\nI could not find the reason of why the node is keeping dirty data there that does not belong to that node.\r\n\r\n**Steps to reproduce**:\r\n\r\nI am not sure if the following procedure can help reproduce it, but this is more or less what describes the steps that led to the problem:\r\n\r\n 1. On a average sized cluster, with 100s of indices with 12 shards, and some billions of documents.\r\n 2. Decommission 2 nodes using cluster shard allocation exclusion list\r\n 3. Add a new node with same hostname and IP address\r\n 4. Allow the node to join and the shard allocation to \"finish\"\r\n 5. Add the second node\r\n 6. Check disk usage\r\n\r\n**Provide logs (if relevant)**:\r\n\r\nDon't know what to look for, since the whole \"shard reallocation\" was trusted enough to not have anyone watching. In the next day I found that the node was with the unusual high disk usage.\r\n\r\n**Side quest**:\r\n\r\nApart from reporting the issue, I would like to know what I can do to cleanup the dirty data, without having to rebuild that node. Is there a script that does that? If not, I can write one, but I need to know what and were should I look for, to know what can be deleted. ","closed_by":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"performed_via_github_app":null}