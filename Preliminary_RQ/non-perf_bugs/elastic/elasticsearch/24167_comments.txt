[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/295279516","html_url":"https://github.com/elastic/elasticsearch/issues/24167#issuecomment-295279516","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24167","id":295279516,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NTI3OTUxNg==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2017-04-19T13:57:55Z","updated_at":"2017-04-19T13:57:55Z","author_association":"MEMBER","body":"@jdconrad can you please also assign these to someone?  in this case anyone from the distributed area will serve. I'll take this one.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/295310433","html_url":"https://github.com/elastic/elasticsearch/issues/24167#issuecomment-295310433","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24167","id":295310433,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NTMxMDQzMw==","user":{"login":"jdconrad","id":2126764,"node_id":"MDQ6VXNlcjIxMjY3NjQ=","avatar_url":"https://avatars2.githubusercontent.com/u/2126764?v=4","gravatar_id":"","url":"https://api.github.com/users/jdconrad","html_url":"https://github.com/jdconrad","followers_url":"https://api.github.com/users/jdconrad/followers","following_url":"https://api.github.com/users/jdconrad/following{/other_user}","gists_url":"https://api.github.com/users/jdconrad/gists{/gist_id}","starred_url":"https://api.github.com/users/jdconrad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jdconrad/subscriptions","organizations_url":"https://api.github.com/users/jdconrad/orgs","repos_url":"https://api.github.com/users/jdconrad/repos","events_url":"https://api.github.com/users/jdconrad/events{/privacy}","received_events_url":"https://api.github.com/users/jdconrad/received_events","type":"User","site_admin":false},"created_at":"2017-04-19T15:28:00Z","updated_at":"2017-04-19T15:28:00Z","author_association":"CONTRIBUTOR","body":"@bleskes Will do.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297001297","html_url":"https://github.com/elastic/elasticsearch/issues/24167#issuecomment-297001297","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24167","id":297001297,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzAwMTI5Nw==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T11:31:08Z","updated_at":"2017-04-25T11:31:08Z","author_association":"CONTRIBUTOR","body":"The test encounters a local checkpoint mismatch, probably because the translog is missing an entry (replica cannot advance local checkpoint).\r\n\r\nRelevant log lines:\r\n\r\n```\r\n   > Throwable #1: java.lang.AssertionError: Some expected ids were not found in search results: [ꩣꩺꩪ꩹ꩣ꩸ꩪꩿ꩸a180, եՍճՂմ֏ԽդՍa216, 𓇨𓏀𓆨𓈡aa238, ꓛꓵꓶꓘꓷꓑꓝꓼꓚa226, ㆴㆠㆳㆩㆢㆭㆾㆡㆠa137, 󠄱󠄼󠇗󠄝aa240, 🉢🉡🊪🈵aa126, 〚〄〼〻〃〳」〦〡a122, ᥃ᤔᤪ᤮ᤓ᥆᤿᥆᥉a149, 𐋘𐋒𐋀𐋟aa207, ꥧꥫꥧꥡꥴꥼꥹꥬꥳa206, ︉︁︌︆︀︊️︉︃a121, ╷╄┴┩╺┌┅╖╓a182]. Total shards: 3 Successful shards: 3 & 0 shard failures:\r\n   > Expected: <0>\r\n   >      but: was <13>\r\n   > \tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n   > \tat org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits(ElasticsearchAssertions.java:229)\r\n   > \tat org.elasticsearch.recovery.RelocationIT.testIndexAndRelocateConcurrently(RelocationIT.java:512)\r\n   > \tat java.lang.Thread.run(Thread.java:745)Throwable #2: java.lang.AssertionError: [test][0], node[g-azBk_eSQOcK9hsQcsYow], [R], s[STARTED], a[id=g7JRvvK8RAWPlZIKShHfIQ] local checkpoint mismatch\r\n   > Expected: <73L>\r\n   >      but: was <53L>\r\n   > \tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n   > \tat org.elasticsearch.recovery.RelocationIT.lambda$beforeIndexDeletion$1(RelocationIT.java:132)\r\n   > \tat org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:711)\r\n   > \tat org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:685)\r\n   > \tat org.elasticsearch.recovery.RelocationIT.beforeIndexDeletion(RelocationIT.java:116)\r\n   > \tat org.elasticsearch.test.ESIntegTestCase.afterInternal(ESIntegTestCase.java:579)\r\n   > \tat org.elasticsearch.test.ESIntegTestCase.cleanUpCluster(ESIntegTestCase.java:2058)\r\n   > \tat java.lang.Thread.run(Thread.java:745)\r\n   > \tSuppressed: java.lang.AssertionError: [test][0], node[g-azBk_eSQOcK9hsQcsYow], [R], s[STARTED], a[id=g7JRvvK8RAWPlZIKShHfIQ] local checkpoint mismatch\r\n   > Expected: <73L>\r\n   >      but: was <53L>\r\n   > \t\tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n   > \t\tat org.elasticsearch.recovery.RelocationIT.lambda$beforeIndexDeletion$1(RelocationIT.java:132)\r\n   > \t\tat org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:699)\r\n   > \t\t... 38 more\r\n```\r\n\r\nI'll have to dig deeper what's causing this /cc: @jasontedor \r\nMore recent failure: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=oraclelinux/883/consoleFull","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297399091","html_url":"https://github.com/elastic/elasticsearch/issues/24167#issuecomment-297399091","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24167","id":297399091,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzM5OTA5MQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-04-26T12:59:23Z","updated_at":"2017-04-26T12:59:23Z","author_association":"CONTRIBUTOR","body":"I've closely examined the failure here:\r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=oraclelinux/883/consoleFull\r\n\r\nWhat is happening here is the following. We have 4 nodes:\r\n\r\n```\r\n{node_t0}{L3CpmEfoTMuwIFXLNEy4yA}{olvHdPIXRI-g6hl46IOImA}{127.0.0.1}{127.0.0.1:9400}{color=blue}, master\r\n{node_t1}{OJ_UPhr4S8GEepzK3zNR1Q}{TV4yTxtKRxa_9oF_s9yWBw}{127.0.0.1}{127.0.0.1:9401}{color=blue}\r\n{node_t2}{poCFuyO6Ss6wqlv_amKDKw}{ZWhmdAHmT2uQ9xM1WxSOIA}{127.0.0.1}{127.0.0.1:9402}{color=red}\r\n{node_t3}{cYkbR4D_TCeQ6CxzlUd_RQ}{bgCrxqWzQvOfBWyYNOXW9A}{127.0.0.1}{127.0.0.1:9403}{color=red}\r\n```\r\n\r\nand are in a state where we have a relocating primary (from node_t2 to node_t0) together with a relocating replica (from node_t3 to node_t1):\r\n\r\n```\r\ncluster state version: 14\r\n  --------[test][2], node[poCFuyO6Ss6wqlv_amKDKw], relocating [L3CpmEfoTMuwIFXLNEy4yA], [P], s[RELOCATING], a[id=iAUBqd0rSbyA-B9-f8mbRQ, rId=xKA60_nPQaWDi-sh9NvGHg]\r\n  --------[test][2], node[cYkbR4D_TCeQ6CxzlUd_RQ], relocating [OJ_UPhr4S8GEepzK3zNR1Q], [R], s[RELOCATING], a[id=jrBmMQUoRb6gi1LniVy-3w, rId=mcc4_uPbQi-qmxM9jApG5A]\r\n```\r\n\r\nPrimary relocation completes before the replica relocation, which means that replica recovery is cancelled by changing the allocation id of the replica relocation target (see #23926):\r\n\r\n```\r\ncluster state version: 15\r\n  --------[test][2], node[L3CpmEfoTMuwIFXLNEy4yA], [P], s[STARTED], a[id=xKA60_nPQaWDi-sh9NvGHg]\r\n  --------[test][2], node[cYkbR4D_TCeQ6CxzlUd_RQ], relocating [OJ_UPhr4S8GEepzK3zNR1Q], [R], s[RELOCATING], a[id=jrBmMQUoRb6gi1LniVy-3w, rId=7JPzU857TwepFt0r0Xkkpw]\r\n```\r\n\r\nThis cluster state is first applied on the node that has the replica relocation target, which then removes the shard as the allocation id changed, creates a new initializing shard and restarts recovery from the new primary:\r\n\r\n```\r\n[2017-04-25T15:10:00,165][DEBUG][o.e.i.c.IndicesClusterStateService] [node_t1] [test][2] removing shard (stale allocation id, stale [test][2], node[OJ_UPhr4S8GEepzK3zNR1Q], relocating [cYkbR4D_TCeQ6CxzlUd_RQ], [R], recovery_source[peer recovery], s[INITIALIZING], a[id=mcc4_uPbQi-qmxM9jApG5A, rId=jrBmMQUoRb6gi1LniVy-3w], new [test][2], node[OJ_UPhr4S8GEepzK3zNR1Q], relocating [cYkbR4D_TCeQ6CxzlUd_RQ], [R], recovery_source[peer recovery], s[INITIALIZING], a[id=7JPzU857TwepFt0r0Xkkpw, rId=jrBmMQUoRb6gi1LniVy-3w])\r\n\r\n[2017-04-25T15:10:00,173][TRACE][o.e.i.r.PeerRecoveryTargetService] [node_t1] [test][2] started recovery from {node_t0}{L3CpmEfoTMuwIFXLNEy4yA}{olvHdPIXRI-g6hl46IOImA}{127.0.0.1}{127.0.0.1:9400}{color=blue}, id [448]\r\n\r\n[2017-04-25T15:10:00,175][TRACE][o.e.i.r.PeerRecoveryTargetService] [node_t1] [test][2] preparing for sequence-number-based recovery starting at local checkpoint [0] from [{node_t0}{L3CpmEfoTMuwIFXLNEy4yA}{olvHdPIXRI-g6hl46IOImA}{127.0.0.1}{127.0.0.1:9400}{color=blue}]\r\n```\r\n\r\nThe new primary goes ahead with the recovery using sequence numbers:\r\n\r\n```\r\n[2017-04-25T15:10:00,175][TRACE][o.e.i.r.PeerRecoverySourceService] [node_t0] [test][2] starting recovery to {node_t1}{OJ_UPhr4S8GEepzK3zNR1Q}{TV4yTxtKRxa_9oF_s9yWBw}{127.0.0.1}{127.0.0.1:9401}{color=blue}\r\n\r\n[2017-04-25T15:10:00,176][TRACE][o.e.i.r.RecoverySourceHandler] [node_t0] [test][2][recover to node_t1] performing sequence numbers based recovery. starting at [0]\r\n```\r\n\r\nNote that in contrast to recoveries that go through phase 1 (and call acquireIndexCommit), sequence number based recoveries (afaics) can be done on a shard that is not marked as STARTED yet. The critical part here is that node_t0 which is the primary relocation target has not applied the cluster state yet where it knows that the relocation successfully completed, it's shard state is currently in POST_RECOVERY. Another critical aspect is that recovery validation only checks that the recovery source node knows that there is an initializing shard on the recovery target node but does not check whether that shard has the correct allocation id. This means that the recovery source node thinks in this case that the recovery target on node_t1 has allocation id `mcc4_uPbQi-qmxM9jApG5A` instead of `7JPzU857TwepFt0r0Xkkpw`. This means that other index requests that arrive on the primary after the recovery is started, are replicated to node_t1 by using the wrong allocation id. This leads to an allocation mismatch failure on the target node:\r\n\r\n```\r\n[2017-04-25T15:10:00,195][TRACE][o.e.a.b.TransportShardBulkAction] [node_t0] [[test][2]] failure while performing [indices:data/write/bulk[s]] on replica [test][2], node[OJ_UPhr4S8GEepzK3zNR1Q], relocating [cYkbR4D_TCeQ6CxzlUd_RQ], [R], recovery_source[peer recovery], s[INITIALIZING], a[id=mcc4_uPbQi-qmxM9jApG5A, rId=jrBmMQUoRb6gi1LniVy-3w], request [BulkShardRequest [[test][2]] containing [index {[test][type1][ऺॺॻलु़ॊफॆa121], source[{\"field1\":\"one hundred twenty-one \"}]}]]\r\n  1> org.elasticsearch.transport.RemoteTransportException: [node_t1][127.0.0.1:9401][indices:data/write/bulk[s][r]]\r\n  1> Caused by: org.elasticsearch.index.shard.ShardNotFoundException: expected aID [mcc4_uPbQi-qmxM9jApG5A] but found [7JPzU857TwepFt0r0Xkkpw]\r\n  1> \tat org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncReplicaAction.doRun(TransportReplicationAction.java:568) ~[main/:?]\r\n  1> \tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[main/:?]\r\n  1> \tat ...\r\n```\r\n\r\nAs this is a `ShardNotFoundException`, we just ignore it on the sender site (i.e. don't fail the target shard). This means that that write is not properly replicated to the target shard, resulting in the missing write on the replica:\r\n\r\n```\r\n> Throwable #1: java.lang.AssertionError: Some expected ids were not found in search results: [ऺॺॻलु़ॊफॆa121]. Total shards: 10 Successful shards: 10 & 0 shard failures:\r\n   > Expected: <0>\r\n   >      but: was <1>\r\n   > \tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n   > \tat org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits(ElasticsearchAssertions.java:229)\r\n   > \tat org.elasticsearch.recovery.RelocationIT.testIndexAndRelocateConcurrently(RelocationIT.java:512)\r\n   > \tat java.lang.Thread.run(Thread.java:745)Throwable #2: java.lang.AssertionError: [test][2], node[OJ_UPhr4S8GEepzK3zNR1Q], [R], s[STARTED], a[id=7JPzU857TwepFt0r0Xkkpw] local checkpoint mismatch\r\n   > Expected: <21L>\r\n   >      but: was <15L>\r\n   > \tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n   > \tat org.elasticsearch.recovery.RelocationIT.lambda$beforeIndexDeletion$1(RelocationIT.java:132)\r\n   > \tat org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:711)\r\n   > \tat org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:685)\r\n   > \tat org.elasticsearch.recovery.RelocationIT.beforeIndexDeletion(RelocationIT.java:116)\r\n   > \tat org.elasticsearch.test.ESIntegTestCase.afterInternal(ESIntegTestCase.java:579)\r\n   > \tat org.elasticsearch.test.ESIntegTestCase.cleanUpCluster(ESIntegTestCase.java:2057)\r\n   > \tat java.lang.Thread.run(Thread.java:745)\r\n   > \tSuppressed: java.lang.AssertionError: [test][2], node[OJ_UPhr4S8GEepzK3zNR1Q], [R], s[STARTED], a[id=7JPzU857TwepFt0r0Xkkpw] local checkpoint mismatch\r\n   > Expected: <21L>\r\n   >      but: was <15L>\r\n   > \t\tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n   > \t\tat org.elasticsearch.recovery.RelocationIT.lambda$beforeIndexDeletion$1(RelocationIT.java:132)\r\n   > \t\tat org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:699)\r\n   > \t\t... 38 more\r\n   > \tSuppressed: java.lang.AssertionError: [test][2], node[OJ_UPhr4S8GEepzK3zNR1Q], [R], s[STARTED], a[id=7JPzU857TwepFt0r0Xkkpw] local checkpoint mismatch\r\n   > Expected: <21L>\r\n   >      but: was <15L>\r\n   > \t\tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n   > \t\tat org.elasticsearch.recovery.RelocationIT.lambda$beforeIndexDeletion$1(RelocationIT.java:132)\r\n   > \t\tat org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:699)\r\n   > \t\t... 38 more\r\n```\r\n\r\nI've opened #24333 as a fix","performed_via_github_app":null}]