{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153/events","html_url":"https://github.com/elastic/elasticsearch/issues/26153","id":249525144,"node_id":"MDU6SXNzdWUyNDk1MjUxNDQ=","number":26153,"title":"Reindex API: Reindex task es_rejected_execution_exception search queue failure","user":{"login":"berglh","id":12455338,"node_id":"MDQ6VXNlcjEyNDU1MzM4","avatar_url":"https://avatars1.githubusercontent.com/u/12455338?v=4","gravatar_id":"","url":"https://api.github.com/users/berglh","html_url":"https://github.com/berglh","followers_url":"https://api.github.com/users/berglh/followers","following_url":"https://api.github.com/users/berglh/following{/other_user}","gists_url":"https://api.github.com/users/berglh/gists{/gist_id}","starred_url":"https://api.github.com/users/berglh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/berglh/subscriptions","organizations_url":"https://api.github.com/users/berglh/orgs","repos_url":"https://api.github.com/users/berglh/repos","events_url":"https://api.github.com/users/berglh/events{/privacy}","received_events_url":"https://api.github.com/users/berglh/received_events","type":"User","site_admin":false},"labels":[{"id":145572580,"node_id":"MDU6TGFiZWwxNDU1NzI1ODA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/CRUD","name":":Distributed/CRUD","color":"0e8a16","default":false,"description":"A catch all label for issues around indexing, updating and getting a doc by id. Not search."}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":14,"created_at":"2017-08-11T02:29:24Z","updated_at":"2018-03-20T16:14:49Z","closed_at":"2018-03-20T16:14:49Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Environment\r\nI have a 10 data node, 5 master only node cluster with the request handled by a data node. <br />The index in question has 5 primary shards and 1 replica.\r\n\r\n**Product** | **Version**\r\n---:|:---\r\nElasticsearch (Official Container) | 5.5.1 (Official Elastic Container)\r\nPlugins | X-Pack Removed\r\nOpenJDK | 1.8.0_141\r\nDocker | 17.05.0-ce, build 89658be\r\nOracle Linux Server | 7.3 \r\nKernel | Linux 3.10.0-514.21.2.el7.x86_64\r\n\r\n### Problem Description\r\n\r\nUpon requesting a task to reindex an `elasticsearch 2.x` created index with a size of `~50 GB` and a doc count of `133047546`, the task completes with a status of `true` even though elasticserach produced an error. The `EsThreadPoolExecutor` error reported alludes the search queue capacity as being exceeded, presumably by the Scroll search requests contending for queue space.\r\n\r\nTo me, it appears that the Reindex API is stopping the task on *any* error. I can sort of appreciate you don't want something to silently fail, but actually I believe it should be the job of the Scroll client (in this case the Reindex API) to identify the search queue has been exceeded and continue to retry.\r\n\r\nThis is kind of touched on in this Github issue: [Reindex API : improve robustness in case of error](https://github.com/elastic/elasticsearch/issues/22471)\r\n\r\nIncreasing the Scroll size of the Reindex improves the ability of the Reindex API to make it most of the way through the process. However, on a large enough index, I continue to hit this problem.\r\n\r\nMy suggestion is that the Reindex API should retry on this soft error.\r\n\r\n#### Supplementary Problem\r\nIn addition to this, I have a problem with the description of the `requests_per_second` URI parameter in the documentation: [Reindex API: URL Parameters](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html#_url_parameters_3). \r\n\r\n> requests_per_second can be set to any positive decimal number (1.4, 6, 1000, etc) and throttles the number of requests per second that the reindex issues or it can be set to -1 to disabled throttling. The throttling is done waiting between bulk batches so that it can manipulate the scroll timeout. The wait time is the difference between the time it took the batch to complete and the time requests_per_second * requests_in_the_batch. Since the batch isnâ€™t broken into multiple bulk requests large batch sizes will cause Elasticsearch to create many requests and then wait for a while before starting the next set. This is \"bursty\" instead of \"smooth\". The default is -1.\r\n\r\nI interpret this instruction as the value of `requests_per_second` limiting the number of Scroll searches or ES bulk writes as `\"requests\"` conducted per second. \r\n\r\nWhat I experienced actually was that setting `requests_per_second` to `0.5` resulted in a wait time of `~15500` seconds for a bulk size of `10000`. It seems like this setting actually restricts the number of search `results` per second or is creating bucket loads of write requests for a Scroll size of `10000`. \r\n\r\nI tried to use this to limit the impact of the Reindex on the seach queues, but not until I set this value to `5000` for a Scroll size of `10000` did I start to see the kind of rate limiting that I am after. I can open another issue for this if required, not sure if there is a bug in ES bulk writing or just a disambiguation problem.\r\n\r\n### Reproduction Command\r\n\r\n```javascript\r\nPOST _reindex?wait_for_completion=false&wait_for_active_shards=all\r\n{\r\n  \"source\": {\r\n    \"index\": \"largeindex\",\r\n    \"size\": 10000,\r\n    \"query\": {\r\n      \"bool\": {\r\n        \"must\": [\r\n          {\r\n            \"range\": {\r\n              \"@timestamp\": {\r\n                \"gte\": \"2016-01-01T00:00:00.000\",\r\n                \"lte\": \"2017-10-11T00:00:00.000\",\r\n                \"time_zone\": \"+10:00\"\r\n              }\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  },\r\n  \"dest\": {\r\n    \"index\": \"largeindex.es5\"\r\n  }\r\n}\r\n```\r\n### Output\r\n```javascript\r\n{\r\n  \"_index\" : \".tasks\",\r\n  \"_type\" : \"task\",\r\n  \"_id\" : \"fmVI6xlZQCmhqZqVPIjfXA:71121809\",\r\n  \"_score\" : 1.0,\r\n  \"_source\" : {\r\n    \"completed\" : true,\r\n    \"task\" : {\r\n      \"node\" : \"fmVI6xlZQCmhqZqVPIjfXA\",\r\n      \"id\" : 71121809,\r\n      \"type\" : \"transport\",\r\n      \"action\" : \"indices:data/write/reindex\",\r\n      \"status\" : {\r\n        \"total\" : 133047546,\r\n        \"updated\" : 0,\r\n        \"created\" : 74140000,\r\n        \"deleted\" : 0,\r\n        \"batches\" : 7414,\r\n        \"version_conflicts\" : 0,\r\n        \"noops\" : 0,\r\n        \"retries\" : {\r\n          \"bulk\" : 0,\r\n          \"search\" : 0\r\n        },\r\n        \"throttled_millis\" : 0,\r\n        \"requests_per_second\" : -1.0,\r\n        \"throttled_until_millis\" : 0\r\n      },\r\n      \"description\" : \"reindex from [largeindex] to [largeindex.es5]\",\r\n      \"start_time_in_millis\" : 1502331869750,\r\n      \"running_time_in_nanos\" : 9825436625697,\r\n      \"cancellable\" : true\r\n    },\r\n    \"response\" : {\r\n      \"took\" : 9825436,\r\n      \"timed_out\" : false,\r\n      \"total\" : 133047546,\r\n      \"updated\" : 0,\r\n      \"created\" : 74140000,\r\n      \"deleted\" : 0,\r\n      \"batches\" : 7414,\r\n      \"version_conflicts\" : 0,\r\n      \"noops\" : 0,\r\n      \"retries\" : {\r\n        \"bulk\" : 0,\r\n        \"search\" : 0\r\n      },\r\n      \"throttled_millis\" : 0,\r\n      \"requests_per_second\" : -1.0,\r\n      \"throttled_until_millis\" : 0,\r\n      \"failures\" : [\r\n        {\r\n          \"shard\" : -1,\r\n          \"reason\" : {\r\n            \"type\" : \"es_rejected_execution_exception\",\r\n            \"reason\" : \"rejected execution of org.elasticsearch.transport.TcpTransport$RequestHandler@63806e47 on EsThreadPoolExecutor[search, queue capacity = 1000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@229e062f[Running, pool size = 49, active threads = 49, queued tasks = 1000, completed tasks = 6373350]]\"\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```","closed_by":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"performed_via_github_app":null}