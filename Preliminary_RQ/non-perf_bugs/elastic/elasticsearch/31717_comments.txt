[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/401696063","html_url":"https://github.com/elastic/elasticsearch/issues/31717#issuecomment-401696063","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31717","id":401696063,"node_id":"MDEyOklzc3VlQ29tbWVudDQwMTY5NjA2Mw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-07-02T07:28:21Z","updated_at":"2018-07-02T07:28:21Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/425573147","html_url":"https://github.com/elastic/elasticsearch/issues/31717#issuecomment-425573147","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31717","id":425573147,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNTU3MzE0Nw==","user":{"login":"dliappis","id":1754575,"node_id":"MDQ6VXNlcjE3NTQ1NzU=","avatar_url":"https://avatars0.githubusercontent.com/u/1754575?v=4","gravatar_id":"","url":"https://api.github.com/users/dliappis","html_url":"https://github.com/dliappis","followers_url":"https://api.github.com/users/dliappis/followers","following_url":"https://api.github.com/users/dliappis/following{/other_user}","gists_url":"https://api.github.com/users/dliappis/gists{/gist_id}","starred_url":"https://api.github.com/users/dliappis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dliappis/subscriptions","organizations_url":"https://api.github.com/users/dliappis/orgs","repos_url":"https://api.github.com/users/dliappis/repos","events_url":"https://api.github.com/users/dliappis/events{/privacy}","received_events_url":"https://api.github.com/users/dliappis/received_events","type":"User","site_admin":false},"created_at":"2018-09-28T21:36:49Z","updated_at":"2018-10-03T07:50:01Z","author_association":"CONTRIBUTOR","body":"TL;DR\r\n-----\r\n\r\nIt's hard to come up with general defaults that are optimal for all cases, esp. in multi node environments with different (primary+replica) shard settings.\r\n\r\nthis comment shows two categories of defaults that work well in different scenarios:\r\n\r\n- 5P 1R shard settings, 3node clusters, Network storage, AWS+GCP\r\n- 1P 1R shard settings, 3node clusters, SSD storage, AWS+GCP\r\n\r\nThe next comment will include optimal defaults for:\r\n\r\n- 3P 1R shard settings, 3node clusters, SSD storage, AWS+GCP\r\n\r\nThe final defaults will be based on the results from all the shard setting scenarios, however, the detailed results per category are still useful for documentation and tuning guidance.\r\n\r\nAll benchmarks were done with the same commit from master (https://github.com/elastic/elasticsearch/commit/73417bf09afdfa6eeb7a7d620218b843e8d2653e) that doesn't include the unmerged, as of now, append-only final optimizations in https://github.com/elastic/elasticsearch/pull/34099.\r\n\r\n5P 1R, Network SSD Storage, 3 node cluster\r\n------------------------------------------\r\n\r\n``` json\r\n\"max_concurrent_read_batches\": 5,\r\n\"max_concurrent_write_batches\": 5,\r\n\"max_batch_operation_count\": 32768,\r\n\"max_batch_size_in_bytes\": 4718592,\r\n\"max_write_buffer_size\": 164000\r\n```\r\n\r\n**NOTE**: While these defaults have been benchmarked against a large amount of scenarios, they are still static parameters. Elasticsearch will not auto-adjust based on performance during CCR activity and performance depends also on the amount of shards. Therefore they will likely still need tuning depending on environmental peculiarities.\r\n\r\n### Benchmarks Executed\r\n\r\nAll benchmarks where executed on the Cloud (AWS and GCP).\r\n\r\n<details><summary>GCP environment</summary>\r\n\r\n1 cluster, 3 leader nodes in `europe-west`, each node in different AZ `europe-west1-b`, `europe-west1-c`, `europe-west1-d`.\r\n\r\n1 cluster, 3 follower nodes in `us-central`, each node in different AZ `us-central1-a`, `us-central1-b`, `us-central1-c`.\r\n\r\n1 load gen on europe-west (`europe-west1-b`).\r\n\r\nEach node root disk size: `500GB` EBS SSD.\r\n\r\nEach instance is: `n1-highcpu-16`.\r\n\r\nMinimum allowable processor, Skylake (https://cloud.google.com/compute/docs/machine-types).\r\n\r\n14GiB RAM (7GiB heap)\r\n</details>\r\n\r\n<details><summary>AWS environment</summary>\r\n\r\n1 cluster, 3 leader nodes in `eu-central-1`, each node in different AZ (`us-east-2a`, `us-east-2b`, `us-east-2c`).\r\n\r\n1 cluster, 3 follower nodes in `us-east-2`, each node in different AZ (`eu-central-1a`, `eu-central-1b`, `eu-central-1c`).\r\n\r\nLoad gen node: (`eu-central-1a`).\r\n\r\nEach node root disk size: `500GB` Persistent Disk SSD.\r\n\r\nEach instance is: `c5.2xlarge`.\r\n\r\n15GiB RAM (7GiB heap).\r\n</details>\r\n\r\n<details><summary>Rally Details</summary>\r\n\r\n(see https://github.com/elastic/rally-tracks, https://github.com/elastic/rally-eventdata-track)\r\n\r\nTracks used:\r\n\r\n- geopoint\r\n- pmc\r\n- http_logs\r\n- eventdata\r\n\r\nType of load: indexing only\r\nClients used (see [schedule](https://esrally.readthedocs.io/en/stable/track.html?highlight=clients#schedule)): 4\r\n</details>\r\n\r\n### Results with recommended settings:\r\n\r\nGCP:\r\n\r\n| Track     | # Indices | Docs/index                  | store.size | pri.store.size | Total Duration | Time to catchup after indexing |\r\n| -----     |   ------- | ----------                  | ---------- | -------------- | -------------- | ------------------------------ |\r\n| geopoint  |         1 | 60844404                    | 6.7GB      | 3.3GB          |       00:08:45 |                 0:00:00.289002 |\r\n| pmc       |         1 | 574199                      | 48.3GB     | 24.7GB         |        0:11:15 |                 0:00:00.470718 |\r\n| http_logs |         3 | 12406628/30700742/193424966 | 2/5/36.1GB | 1/2.5/18.1GB   |        0:43:29 |                 0:00:00.948574 |\r\n\r\nAWS:\r\n\r\n| Track     | # Indices |                  Docs/index | store.size | pri.store.size | Total Duration | Time to catchup after indexing |\r\n| -----     |   ------- |                  ---------- | ---------- | -------------- | -------------- | ------------------------------ |\r\n| geopoint  |         1 |                    60844404 | 6.7GB      | 3.3GB          |        0:09:53 |                 0:00:00.209117 |\r\n| pmc       |         1 |                      574199 | 41GB       | 20.5GB         |        0:35:46 |                 0:00:10.334900 |\r\n| http_logs |         3 | 12406628/30700742/193424966 | 2/5/33.1GB | 1/2.5/16.6GB   |        0:41:17 |                 0:01:41.313000 |\r\n| eventdata |         1 |                  1000040000 | 444.4GB    | 223.2GB        |       13:29:16 |                 0:00:15.781900 |\r\n\r\n<details><summary>Other details</summary>\r\nTotal benchmarks executed:\r\n\r\nGCP: 43 (gradually increasing max_concurrent_batch_read/write values and max_batch_size_in_bytes)\r\nAWS: 7\r\n</details>\r\n\r\n1P 1R, Local SSD Storage, 3 node cluster\r\n------------------------------------------\r\n\r\n``` json\r\n\"max_concurrent_read_batches\": 9,\r\n\"max_concurrent_write_batches\": 9,\r\n\"max_batch_operation_count\": 163840,\r\n\"max_batch_size_in_bytes\": 4718592,\r\n\"max_write_buffer_size\": 2942700\r\n```\r\n\r\n### Notes\r\n\r\nThis setup exerts high pressure on **I/O** (primarily) and **cpu** (secondarily) , as essentially only two out of three nodes deal with the load.\r\nThe setup used already with EBS/Persistent Storage proved a bottleneck and results with the same settings, or settings with higher concurrency + buffers didn't improve replication performance (see details).\r\n\r\nTrying locally attached SSD disks shifted the bottleneck to the cpu (see details).\r\n\r\nAfter finding the right instances in terms of cpu and i/o performance it was possible to tune settings to provide the optimal performance.\r\n\r\n### Benchmarks Executed\r\n\r\n<details><summary>Final AWS environment</summary>\r\n1 cluster, 3 leader nodes in `eu-central-1`, each node in different AZ (`us-east-2a`, `us-east-2b`, `us-east-2c`).\r\n\r\n1 cluster, 3 follower nodes in `us-east-2`, each node in different AZ (`eu-central-1a`, `eu-central-1b`, `eu-central-1c`).\r\n\r\nLoad gen node: (`eu-central-1a`).\r\n\r\nEach node root disk size: `100GB` (EBS) but Elasticsearch data dir and Rally `~/.rally` dirs use `/dev/nvme1n1` local SSD of the instance.\r\n\r\nEach instance is: `m5d.2xlarge`.\r\n\r\n30GiB RAM (15GiB heap)\r\n\r\n(Also tried i3.xlarge, which didn't have enough CPU capacity to deal with http_logs benchmarks at max indexing performance).\r\n</details>\r\n\r\n<details><summary>GCP environment</summary>\r\n1 cluster, 3 leader nodes in `europe-west`, each node in different AZ `europe-west4-a`, `europe-west4-b`, `europe-west4-c` (had to switch from europe-west1-* due to lack of resources).\r\n\r\n1 cluster, 3 follower nodes in us-central, each node in different AZ `us-central1-a`, `us-central1-b`, `us-central1-c`.\r\n\r\n1 load gen on europe-west (`europe-west4-a`).\r\n\r\nEach node root disk size: `500`GB` EBS SSD but `~/.rally` and `~/es` reside on locally attached SSD disk.\r\n\r\nEach instance is: `n1-highcpu-16`.\r\n\r\nMinimum allowable processor, Skylake (https://cloud.google.com/compute/docs/machine-types).\r\n\r\n14GiB RAM (7GiB heap).\r\n</details>\r\n\r\n<details><summary>Rally Details</summary>\r\n(see https://github.com/elastic/rally-tracks, https://github.com/elastic/rally-eventdata-track)\r\n\r\n**indexing was throttled to 45000 doc/s**\r\n\r\nTracks used:\r\n\r\n- geopoint\r\n- pmc\r\n- http_logs\r\n- eventdata\r\n\r\nType of load: indexing only\r\nClients used (see [schedule](https://esrally.readthedocs.io/en/stable/track.html?highlight=clients#schedule)): 4\r\n</details>\r\n\r\n### Results with recommended settings:\r\n\r\nGCP:\r\n\r\n| Track     | # Indices | Docs/index                  | store.size   | pri.store.size | Total Duration | Time to catchup after indexing |\r\n| -----     | -------   | ----------                  | ----------   | -------------- | -------------- | ------------------------------ |\r\n| geopoint  | 1         | 60844404                    | 9.6GB        | 4.2GB          | 0:37:53        | 0:00:00.257276                 |\r\n| pmc       | 1         | 574199                      | 40.7GB       | 20.3GB         | 0:54:25        | 0:00:01.204230                 |\r\n| http_logs | 3         | 12406628/30700742/193424966 | 2/4.9/31.9GB | 1/2.4/16.1GB   | 2:35:01        | 0:00:01.885290                 |\r\n\r\nAWS:\r\n\r\n| Track     | # Indices | Docs/index                  | store.size | pri.store.size | Total Duration | Time to catchup after indexing |\r\n| -----     | -------   | ----------                  | ---------- | -------------- | -------------- | ------------------------------ |\r\n| geopoint  | 1         | 60844404                    | 8.1GB      | 4GB            | 0:23:45        | 0:00:00.213884                 |\r\n| pmc       | 1         | 574199                      | 43.1GB     | 22.5GB         | 0:23:29        | 0:00:00.951584                 |\r\n| http_logs | 3         | 12406628/30700742/193424966 | 2/5/33.1GB | 1/2.5/16.6GB   | 1:29:10        | 0:00:18.619700                 |\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/426536331","html_url":"https://github.com/elastic/elasticsearch/issues/31717#issuecomment-426536331","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31717","id":426536331,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNjUzNjMzMQ==","user":{"login":"dliappis","id":1754575,"node_id":"MDQ6VXNlcjE3NTQ1NzU=","avatar_url":"https://avatars0.githubusercontent.com/u/1754575?v=4","gravatar_id":"","url":"https://api.github.com/users/dliappis","html_url":"https://github.com/dliappis","followers_url":"https://api.github.com/users/dliappis/followers","following_url":"https://api.github.com/users/dliappis/following{/other_user}","gists_url":"https://api.github.com/users/dliappis/gists{/gist_id}","starred_url":"https://api.github.com/users/dliappis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dliappis/subscriptions","organizations_url":"https://api.github.com/users/dliappis/orgs","repos_url":"https://api.github.com/users/dliappis/repos","events_url":"https://api.github.com/users/dliappis/events{/privacy}","received_events_url":"https://api.github.com/users/dliappis/received_events","type":"User","site_admin":false},"created_at":"2018-10-03T07:20:12Z","updated_at":"2018-10-03T07:20:12Z","author_association":"CONTRIBUTOR","body":"\r\n3P 1R, Network SSD Storage, 3 node cluster\r\n------------------------------------------\r\n\r\n```json\r\n\"max_concurrent_read_batches\": 5,\r\n\"max_concurrent_write_batches\": 5,\r\n\"max_batch_operation_count\": 32768,\r\n\"max_batch_size_in_bytes\": 4718592,\r\n\"max_write_buffer_size\": 164000\r\n```\r\n\r\n### Notes\r\n\r\nAs expected, using 3p 1r shard settings is lighter on **cpu** and **I/O** pressure than the 1/1 scenario.\r\n\r\nAll benchmarks were executed using local SSD drives and http_logs in particular throttled at 85% of max indexing performance, as otherwise normalized CPU consumption on AWS ranged between 95-98% (the instance on AWS has less cpus).\r\n\r\n<details><summary>AWS environment</summary>\r\n\r\n1 cluster, 3 leader nodes in `eu-central-1`, each node in different AZ (`us-east-2a`, `us-east-2b`, `us-east-2c`).\r\n\r\n1 cluster, 3 follower nodes in `us-east-2`, each node in different AZ (`eu-central-1a`, `eu-central-1b`, `eu-central-1c`).\r\n\r\nLoad gen node: (`eu-central-1a`).\r\n\r\nEach node root disk size: `100GB` (EBS) but Elasticsearch data dir and Rally `~/.rally` dirs use `/dev/nvme1n1` local SSD of the instance.\r\n\r\nEach instance is: `m5d.2xlarge`.\r\n\r\n30GiB RAM (15GiB heap)\r\n\r\n(Also tried i3.xlarge, which didn't have enough CPU capacity to deal with http_logs benchmarks at max indexing performance).\r\n</details>\r\n\r\n<details><summary>GCP environment</summary>\r\n\r\n1 cluster, 3 leader nodes in `europe-west`, each node in different AZ `europe-west4-a`, `europe-west4-b`, `europe-west4-c`.\r\n\r\n1 cluster, 3 follower nodes in us-central, each node in different AZ `us-central1-a`, `us-central1-b`, `us-central1-c`.\r\n\r\n1 load gen on europe-west (`europe-west4-a`).\r\n\r\nEach node root disk size: `500`GB EBS SSD but `.rally/` and `es/` reside on locally attached SSD disk.\r\n\r\nEach instance is: `n1-highcpu-16`.\r\n\r\nMinimum allowable processor, Skylake (https://cloud.google.com/compute/docs/machine-types).\r\n\r\n14GiB RAM (7GiB heap).\r\n</details>\r\n\r\n<details><summary>Rally Details</summary>\r\n\r\n(see https://github.com/elastic/rally-tracks, https://github.com/elastic/rally-eventdata-track)\r\n\r\nTracks used:\r\n\r\n- geopoint\r\n- pmc\r\n- http_logs -> max indexing throughput\r\n- http_logs -> indexing throughput throttled to 85% of max performance (55000docs/s on AWS, 85000docs/s on GCP)\r\n- eventdata -> max indexing throughput; instance's CPU, esp follower was pegged at 98%\r\n- eventdata -> indexing throughput throttled to 85% of max performance\r\n\r\nType of load: indexing only\r\nClients used (see [schedule](https://esrally.readthedocs.io/en/stable/track.html?highlight=clients#schedule)): 4\r\n</details>\r\n\r\n### Results with recommended settings:\r\n\r\nGCP:\r\n\r\n| Track                                     | # Indices | Docs/index                  | store.size   | pri.store.size | Total Duration | Time to catchup after indexing |\r\n| -----                                     | -------   | ----------                  | ----------   | -------------- | -------------- | ------------------------------ |\r\n| geopoint                                  | 1         | 60844404                    | 7.2GB        | 3.5GB          | 0:13:26        | 0:00:00.265027                 |\r\n| pmc                                       | 1         | 574199                      | 70.4GB       | 34.7GB         | 0:13:06        | 0:00:00.477828                 |\r\n| http_logs (max indexing throughput)       | 3         | 12406628/30700742/193424966 | 2/4.9/31.8GB | 1/2.4/15.9GB   | 1:01:33        | 0:00:00.778742                 |\r\n| http_logs (throttled 85% of max indexing) | 3         | 12406628/30700742/193424966 | 2/5/32.1GB   | 1/2.5/16.1GB   | 1:13:01        | 0:00:00.779165                 |\r\n| eventdata (max indexing throughput)       | 3         | 1000040000                  | 427.7GB      | 214GB          | 21:37:14       | 4:48:11.400000                 |\r\n| eventdata (throttled 85% of max indexing) | 3         | 1000040000                  | 428.1GB      | 214.1GB        | 22:14:09       | 0:00:00.532950                 |\r\n\r\nAWS:\r\n\r\n| Track                                     | # Indices | Docs/index                  | store.size   | pri.store.size | Total Duration | Time to catchup after indexing |\r\n| -----                                     | -------   | ----------                  | ----------   | -------------- | -------------- | ------------------------------ |\r\n| geopoint                                  | 1         | 60844404                    | 7.3GB        | 3.6GB          | 0:08:39        | 0:00:27.475100                 |\r\n| pmc                                       | 1         | 574199                      | 61.1GB       | 30.3GB         | 0:12:55        | 0:00:00.269416                 |\r\n| http_logs (max indexing throughput)       | 3         | 12406628/30700742/193424966 | 2/4.9/31.1GB | 1/2.4/15.5GB   | 0:45:06        | 0:05:00.847000                 |\r\n| http_logs (throttled 85% of max indexing) | 3         | 12406628/30700742/193424966 | 2/4.9/31.1GB | 1/2.4/15.5GB   | 0:48:30        | 0:01:00.925000                 |\r\n| eventdata (max indexing throughput)       | 3         | 1000040000                  | 424.1GB      | 212GB          | 18:21:09       | 5:23:26.500000                 |\r\n| eventdata (throttled 85% of max indexing) | 3         | 1000040000                  | 428.2GB      | 214GB          | 15:27:13       | 0:00:01.125280                 |\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/426543331","html_url":"https://github.com/elastic/elasticsearch/issues/31717#issuecomment-426543331","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31717","id":426543331,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNjU0MzMzMQ==","user":{"login":"dliappis","id":1754575,"node_id":"MDQ6VXNlcjE3NTQ1NzU=","avatar_url":"https://avatars0.githubusercontent.com/u/1754575?v=4","gravatar_id":"","url":"https://api.github.com/users/dliappis","html_url":"https://github.com/dliappis","followers_url":"https://api.github.com/users/dliappis/followers","following_url":"https://api.github.com/users/dliappis/following{/other_user}","gists_url":"https://api.github.com/users/dliappis/gists{/gist_id}","starred_url":"https://api.github.com/users/dliappis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dliappis/subscriptions","organizations_url":"https://api.github.com/users/dliappis/orgs","repos_url":"https://api.github.com/users/dliappis/repos","events_url":"https://api.github.com/users/dliappis/events{/privacy}","received_events_url":"https://api.github.com/users/dliappis/received_events","type":"User","site_admin":false},"created_at":"2018-10-03T07:49:19Z","updated_at":"2018-10-03T07:49:19Z","author_association":"CONTRIBUTOR","body":"### Next steps\r\n\r\nWith the append-only follower [optimizations merged](https://github.com/elastic/elasticsearch/pull/34099), next step is to rerun the above benchmarks (at least the ones that demonstrated high resource usage, esp CPU) and evaluate if the common defaults used for 5p/1r, 3p/1r scenario demonstrate less resource usage and whether they are good enough even for 1p/1r.\r\n\r\nFor reference here are some visualizations for CPU usage:\r\n\r\n### AWS http_logs 3P/1R unthrottled\r\n\r\n#### CPU\r\n\r\n![image](https://user-images.githubusercontent.com/1754575/46396106-179a2580-c6f7-11e8-80e1-6b3928e36cb3.png)\r\n\r\n### AWS http_logs 3P/1R throttled to 85% of max indexing throughput\r\n\r\n#### CPU: \r\n![image](https://user-images.githubusercontent.com/1754575/46396205-63e56580-c6f7-11e8-8b80-e2c01f2019a0.png)\r\n\r\n#### Progress of follower vs leader with sequence numbers:\r\n\r\n![image](https://user-images.githubusercontent.com/1754575/46396294-a8710100-c6f7-11e8-87ec-9af8d39128f7.png)\r\n\r\n### GCP http_logs 3P/1R unthrottled\r\n\r\n#### CPU: \r\n![image](https://user-images.githubusercontent.com/1754575/46396566-66948a80-c6f8-11e8-9154-42e3187058e1.png)\r\n\r\n### GCP http_logs 3P/1R throttled to 85% of max indexing throughput\r\n\r\n#### CPU: \r\n\r\n![image](https://user-images.githubusercontent.com/1754575/46396690-c3904080-c6f8-11e8-8ea0-9eb0fa776c8e.png)\r\n\r\n### AWS http_logs 1P/1R throttled to 85% of max indexing throughput\r\n\r\n#### CPU\r\n\r\n![image](https://user-images.githubusercontent.com/1754575/46396876-4dd8a480-c6f9-11e8-8dd3-d2cf852d2cb2.png)\r\n\r\n### GCP http_logs 1P/1R throttled to 85% of max indexing throughput\r\n\r\n#### CPU\r\n\r\n![image](https://user-images.githubusercontent.com/1754575/46397056-d7887200-c6f9-11e8-8101-49f8d67ff52d.png)\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/432219348","html_url":"https://github.com/elastic/elasticsearch/issues/31717#issuecomment-432219348","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31717","id":432219348,"node_id":"MDEyOklzc3VlQ29tbWVudDQzMjIxOTM0OA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-10-23T12:06:33Z","updated_at":"2018-10-23T12:06:33Z","author_association":"MEMBER","body":"@dliappis and I did some more research and here is a summary of our discoveries:\r\n\r\n1) Cross region network becomes more efficient when multiple connections are used concurrently. When we run with one reader the network added roughly 700ms on average to each request. With 2 concurrent readers it dropped to 400ms. With 4 readers it dropped even further. We saw this behavior both on GCP and AWS.\r\n2) The system worked nicely (i.e., the following index kept up nicely) when indexing to a single shard leader index on a 16 core CPU instance with 8 concurrent clients. The indexing rate was roughly 100K http log lines per second. The following index was set up on an identical machine and we used up to 8 concurrent read requests and 8 concurrent write requests. Increasing the the budget to 16r/16w didn't help performance and the read/write budget wasn't fully utilized. All of these experiments used a batch size of 5K ops, 5MB max batch size and an unlimited write buffer.\r\n\r\nA few observations that we have made will doing the experiments:\r\n1) Readers have more work than writers: readers need to unpack lucene stored field blocks and extract the source - this is more work than a write needs to do for small documents as indexing involves parsing them and putting them in an in memory buffer.\r\n2) Both read and write settings are a maximum - concurrent requests will be allocated only if needed.\r\n3) By default we use 6 channels to connect to a remote cluster. This value has been inherited from CCS.\r\n4) On a busy cluster CCR caused ~10% reduction in indexing speed on the leader. On a non-saturated machine (same machine as above but with 4 concurrent writes), CCR cause ~5% slow downs. We are still researching why.\r\n\r\nW.r.t defaults we have discussed multiple options, including being smart and using the number of processors on the machines to figure out our defaults. That said I feel we don't have a good enough grip of all the possible use cases/machines and it will take a considerable effort to get it, with unclear chance of actually getting a clear and simple picture. I'd prefer going with simple numbers that are easy to communicate and change. Once we gather more input we can adapt them.\r\n\r\nWith that in mind, I suggest going with the following defaults:\r\n1) Keep the currently hard coded 6 channels per remote cluster. I think we should consider making it configurable.\r\n2) Default to a maximum of 12 concurrent read requests (double the channel count). This a conservative setting that will allow for some room for faster networks (local LANs) where the ration between disk reads and network is different. \r\n3) Default to a maximum of 8 concurrent write requests. This is less than the number of readers based on expected ratio of work. \r\n\r\n@dliappis did I forget anything?\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/432784491","html_url":"https://github.com/elastic/elasticsearch/issues/31717#issuecomment-432784491","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31717","id":432784491,"node_id":"MDEyOklzc3VlQ29tbWVudDQzMjc4NDQ5MQ==","user":{"login":"dliappis","id":1754575,"node_id":"MDQ6VXNlcjE3NTQ1NzU=","avatar_url":"https://avatars0.githubusercontent.com/u/1754575?v=4","gravatar_id":"","url":"https://api.github.com/users/dliappis","html_url":"https://github.com/dliappis","followers_url":"https://api.github.com/users/dliappis/followers","following_url":"https://api.github.com/users/dliappis/following{/other_user}","gists_url":"https://api.github.com/users/dliappis/gists{/gist_id}","starred_url":"https://api.github.com/users/dliappis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dliappis/subscriptions","organizations_url":"https://api.github.com/users/dliappis/orgs","repos_url":"https://api.github.com/users/dliappis/repos","events_url":"https://api.github.com/users/dliappis/events{/privacy}","received_events_url":"https://api.github.com/users/dliappis/received_events","type":"User","site_admin":false},"created_at":"2018-10-24T18:54:07Z","updated_at":"2018-10-24T18:54:07Z","author_association":"CONTRIBUTOR","body":"@bleskes Thank you for the concise summation.\r\n\r\nMaybe worth mentioning that all experiments were done on instances using a locally attached SSDs (i.e. **not** AWS's EBS/GCP's persistent disks).\r\n\r\nWhat initial defaults should we use for `max_batch_size`? So far we've been using `5MB` but the [current default](https://github.com/bleskes/elasticsearch/blob/2b4a4e1efd64da0bdbc11b918fdf3eefc1414caf/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/TransportResumeFollowAction.java#L55) is `Long.MAX_VALUE`, so we can also choose to leave it unchanged and rely only on the `max_batch_operation_count` == `5120` that you recently introduced in [your PR](https://github.com/elastic/elasticsearch/pull/34793/files#diff-81e8af15f32db238a8e9f870c3a79ddbL55).\r\n\r\nI'd also have added the need to clarify defaults for `max_write_buffer_count` but this is now being worked on https://github.com/elastic/elasticsearch/pull/34797 (and there's a new parameter `max_write_buffer_bytes`).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/439856060","html_url":"https://github.com/elastic/elasticsearch/issues/31717#issuecomment-439856060","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31717","id":439856060,"node_id":"MDEyOklzc3VlQ29tbWVudDQzOTg1NjA2MA==","user":{"login":"dliappis","id":1754575,"node_id":"MDQ6VXNlcjE3NTQ1NzU=","avatar_url":"https://avatars0.githubusercontent.com/u/1754575?v=4","gravatar_id":"","url":"https://api.github.com/users/dliappis","html_url":"https://github.com/dliappis","followers_url":"https://api.github.com/users/dliappis/followers","following_url":"https://api.github.com/users/dliappis/following{/other_user}","gists_url":"https://api.github.com/users/dliappis/gists{/gist_id}","starred_url":"https://api.github.com/users/dliappis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dliappis/subscriptions","organizations_url":"https://api.github.com/users/dliappis/orgs","repos_url":"https://api.github.com/users/dliappis/repos","events_url":"https://api.github.com/users/dliappis/events{/privacy}","received_events_url":"https://api.github.com/users/dliappis/received_events","type":"User","site_admin":false},"created_at":"2018-11-19T11:07:29Z","updated_at":"2018-11-19T11:07:29Z","author_association":"CONTRIBUTOR","body":"Here's a summary of what has happened in the mean time.\r\n\r\nWe devised a [testing plan](https://gist.github.com/dliappis/67615df7622071400846ef890430f57a) comprising 3 stages:\r\n\r\n- Stage 0:\r\n    - Verify the impact of x-pack-security\r\n    - Verify the impact of non-append workloadds\r\n\r\n- Stage 1:\r\n    - Validate `max_` defaults allow the follower to always catch up using 3 workloads ([geopoint](https://github.com/elastic/rally-tracks/tree/master/geopoint) -> very small docs, [http_logs](https://github.com/elastic/rally-tracks/tree/master/http_logs) -> medium doc size, large corpus and [pmc](https://github.com/elastic/rally-tracks/tree/master/pmc) -> very large docs) on both AWS and GCP cloud environments.\r\n    - Compare CCR overhead vs \"no CCR and`soft_deletes: false`\" as well as vs \"no CCR with `soft_deletes: true`\".\r\n\r\n- Stage 2:\r\n    - Examine ability to catch up, system overhead, and stability with a larger worload (1bn docs, continuous execution over 3+ days) on both AWS and GCP cloud environments.\r\n    - Compare CCR overhead.\r\n\r\nObservations/Actions:\r\n\r\n1. We observed [long GC pauses](https://gist.github.com/dliappis/67615df7622071400846ef890430f57a#gistcomment-2747594) with work loads containing large doc sizes (`pmc` track) and changed the [max_read_request_size](https://github.com/elastic/elasticsearch/commit/ddda2d419cb49c8df0df70b8ce8d4c8b7322ef02) to `32GB` (from `Long.MAX_VALUE` i.e. practically unlimited).\r\n\r\n2. Observed a penalty in indexing throughput when CCR deals with conflicts (`25%` conflicts from total size, favoring most recent 25% of docs) as opposed to the same workload without CCR and soft_deletes. Initial calculation on a smaller 1-node environment on GCP showed this penalty to be [~12.5%](https://gist.github.com/dliappis/67615df7622071400846ef890430f57a#gistcomment-2743291), however \r\nmore recent experiments shows this to be [~6%](https://github.com/elastic/elasticsearch/pull/35594#issuecomment-439416214) on a larger 3-node env in AWS.\r\n\r\n    Additionally, we [observed](https://github.com/elastic/elasticsearch/pull/35594#issuecomment-439416214) a `~19%` penalty for `merges_total_time_` and `~108%` penalty in `refresh_total_time`. This [PR](https://github.com/elastic/elasticsearch/pull/35594#issuecomment-439416214) has been raised to improve both metrics via a configurable `soft_deletes.reclaim.delay` parameter, which defaults to `1minute`.\r\n\r\n3. Stage 1 (3 node) benchmarks across all tracks and cloud vendor combinations showed [here](https://gist.github.com/dliappis/67615df7622071400846ef890430f57a#gistcomment-2747594) and [here](https://gist.github.com/dliappis/67615df7622071400846ef890430f57a#gistcomment-2752193) that followers were always able to catch up with  (`<1s` to catch up once indexing was over).\r\n\r\n4. Stage 1 benchmarks [demonstrated](https://gist.github.com/dliappis/67615df7622071400846ef890430f57a#gistcomment-2752193) a `~2.37-5%` penalty in median indexing throughput when CCR is enabled (compared to indexing without CCR and no soft_deletes) on AWS. On GCP we've have gotten more inconsistent results, sometimes exhibiting the same `4-5%` [penalty](https://gist.github.com/dliappis/67615df7622071400846ef890430f57a#gistcomment-2747594) but also [not showing any penalty](https://gist.github.com/dliappis/67615df7622071400846ef890430f57a#gistcomment-2752193) (or higher performance) which isn't clear why. The assumption at this point is that it's due to environmental factors e.g. noisy neighours but requires further investigation with randomized benchmarks at different hours. (See also following bullet 6. for a comparison of CCR overhead using a larger track over a longer period of time).\r\n\r\n5. 3-day benchmarks (Stage 2), using throttled indexing throughput of 3800 docs/s, finished successfully, followers always able to catch up and stable indexing throughput on both AWS and GCP.\r\n\r\n6. 1bn doc workload at maximum indexing throughput (16 clients) also [finished successfully](https://gist.github.com/dliappis/67615df7622071400846ef890430f57a#gistcomment-2751220), following cluster was able to catch up. Comparison of the same workload with CCR and soft_deletes disabled showed that CCR has a `~6%` penalty on AWS and a `~4.3%` penalty on GCP in median indexing throughput.\r\n\r\n7. Finally, as CCR was progressing throughout the execution of the ^^ benchmarks we [re-executed the 1bn doc track](https://gist.github.com/dliappis/67615df7622071400846ef890430f57a#gistcomment-2755833) a few days before the 6.5.0 release to ensure there was no regression in the indexing throughput, ability for the follower to catch up or system resource utilization and didn't spot any issues.\r\n\r\n@bleskes Did I miss anything?\r\n\r\nNOTE: Some of the Kibana links in the gist point to a private Cloud cluster.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/470972724","html_url":"https://github.com/elastic/elasticsearch/issues/31717#issuecomment-470972724","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31717","id":470972724,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3MDk3MjcyNA==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2019-03-08T15:43:20Z","updated_at":"2019-03-08T15:43:20Z","author_association":"MEMBER","body":"I am closing since we have good default parameters now. Thanks @dliappis for the awesome work.","performed_via_github_app":null}]