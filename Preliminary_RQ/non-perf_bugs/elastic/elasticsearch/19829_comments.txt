[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/237865727","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-237865727","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":237865727,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNzg2NTcyNw==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-08-05T14:30:02Z","updated_at":"2016-08-05T14:30:02Z","author_association":"CONTRIBUTOR","body":"can you elaborate what `long time` means for you :) I like 1 min, 10min? Waiting for pending merges to abort can potentially take quite a while, I am double checking though.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/237866848","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-237866848","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":237866848,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNzg2Njg0OA==","user":{"login":"gehel","id":1415765,"node_id":"MDQ6VXNlcjE0MTU3NjU=","avatar_url":"https://avatars1.githubusercontent.com/u/1415765?v=4","gravatar_id":"","url":"https://api.github.com/users/gehel","html_url":"https://github.com/gehel","followers_url":"https://api.github.com/users/gehel/followers","following_url":"https://api.github.com/users/gehel/following{/other_user}","gists_url":"https://api.github.com/users/gehel/gists{/gist_id}","starred_url":"https://api.github.com/users/gehel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gehel/subscriptions","organizations_url":"https://api.github.com/users/gehel/orgs","repos_url":"https://api.github.com/users/gehel/repos","events_url":"https://api.github.com/users/gehel/events{/privacy}","received_events_url":"https://api.github.com/users/gehel/received_events","type":"User","site_admin":false},"created_at":"2016-08-05T14:34:15Z","updated_at":"2016-08-05T14:34:15Z","author_association":"NONE","body":"Long time meaning 1 hour before we killed it. I did take a few threaddumps and can validate that the merge was still doing work. So it would probably have completed eventually. But I would much prefer the merge to abort and the shutdown to complete.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/237872469","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-237872469","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":237872469,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNzg3MjQ2OQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-08-05T14:55:27Z","updated_at":"2016-08-05T14:55:27Z","author_association":"CONTRIBUTOR","body":"@gehel can you tell why the node get's restarted? do you have GC going on or are you under memory pressure? I also wonder how big your index is there and if you potentially have a lot of sparse fields in your index?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/237876248","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-237876248","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":237876248,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNzg3NjI0OA==","user":{"login":"gehel","id":1415765,"node_id":"MDQ6VXNlcjE0MTU3NjU=","avatar_url":"https://avatars1.githubusercontent.com/u/1415765?v=4","gravatar_id":"","url":"https://api.github.com/users/gehel","html_url":"https://github.com/gehel","followers_url":"https://api.github.com/users/gehel/followers","following_url":"https://api.github.com/users/gehel/following{/other_user}","gists_url":"https://api.github.com/users/gehel/gists{/gist_id}","starred_url":"https://api.github.com/users/gehel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gehel/subscriptions","organizations_url":"https://api.github.com/users/gehel/orgs","repos_url":"https://api.github.com/users/gehel/repos","events_url":"https://api.github.com/users/gehel/events{/privacy}","received_events_url":"https://api.github.com/users/gehel/received_events","type":"User","site_admin":false},"created_at":"2016-08-05T15:09:30Z","updated_at":"2016-08-05T15:09:30Z","author_association":"NONE","body":"@s1monw the node got restarted for scheduled maintenance (JVM security upgrade). We have 30Go of heap allocated to the JVM, 128Go of RAM in the machine. I don't think that we have much GC activity, but I need to check on that.\n\nThis elasticsearch cluster is our logstash backend, we have one index per day with ~10-20M document, and pri.store.size between 5 and 40Go, mostly around 10Go.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/237876356","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-237876356","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":237876356,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNzg3NjM1Ng==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2016-08-05T15:09:55Z","updated_at":"2016-08-05T15:09:55Z","author_association":"CONTRIBUTOR","body":"I have a possible explanation: a merge only checks whether it's aborted when its `RateLimiter.pause` is invoked, but since forced merges run un-throttled, this is never called, and the forced merge will block shutdown.\n\nI'll try to confirm with a standalone Lucene test and open an issue...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/237906880","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-237906880","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":237906880,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNzkwNjg4MA==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2016-08-05T17:09:13Z","updated_at":"2016-08-05T17:09:13Z","author_association":"CONTRIBUTOR","body":"> I have a possible explanation: \n\nOK, I was wrong about this: even at un-throttled IO, Lucene 5.5.0 still checks for aborts every 1 MB of bytes written.  And I tested a standalone Lucene case and confirmed that a forced merge is quickly aborted during rollback.\n\nSo I can't explain what's happening here yet.\n\nIf a merge thread runs for a long time but never writes more than 1 MB, that could lead to this behavior, but it's hard to imagine a merge thread running for an _hour_ and not writing 1 MB ;)\n\nDo you have very sparse fields?  How many documents were in the index being force merged?  Were there maybe many deletions?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238028942","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-238028942","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":238028942,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODAyODk0Mg==","user":{"login":"gehel","id":1415765,"node_id":"MDQ6VXNlcjE0MTU3NjU=","avatar_url":"https://avatars1.githubusercontent.com/u/1415765?v=4","gravatar_id":"","url":"https://api.github.com/users/gehel","html_url":"https://github.com/gehel","followers_url":"https://api.github.com/users/gehel/followers","following_url":"https://api.github.com/users/gehel/following{/other_user}","gists_url":"https://api.github.com/users/gehel/gists{/gist_id}","starred_url":"https://api.github.com/users/gehel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gehel/subscriptions","organizations_url":"https://api.github.com/users/gehel/orgs","repos_url":"https://api.github.com/users/gehel/repos","events_url":"https://api.github.com/users/gehel/events{/privacy}","received_events_url":"https://api.github.com/users/gehel/received_events","type":"User","site_admin":false},"created_at":"2016-08-06T15:30:54Z","updated_at":"2016-08-06T15:30:54Z","author_association":"NONE","body":"I don't know how to check how sparse our fields are. As this elasticsearch cluster is backing logstash, I can very well imagine that some fields are only used for some applications. So we probably do have fairly sparse fields, but I don't have an exact measure of that. We don't do deletions on that cluster.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238226210","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-238226210","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":238226210,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODIyNjIxMA==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2016-08-08T12:53:05Z","updated_at":"2016-08-08T12:53:05Z","author_association":"CONTRIBUTOR","body":"I tried to reproduce this, using the ES 2.3.3 install, and indexing the first 50 M documents from the NYC taxi data set: http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\n\nThen I kick off the `forceMerge`, see that it's running, then I send SIGINT to the ES process and it very quickly terminates.  I repeated this and tried closing the index instead of shutting ES process down, and it also quickly terminates.\n\nSo I'm unable to reproduce this so far ... there must be some difference.  Could you pull a diagnostics (https://github.com/elasticsearch/elasticsearch-support-diagnostics) and post somewhere?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238250965","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-238250965","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":238250965,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODI1MDk2NQ==","user":{"login":"gehel","id":1415765,"node_id":"MDQ6VXNlcjE0MTU3NjU=","avatar_url":"https://avatars1.githubusercontent.com/u/1415765?v=4","gravatar_id":"","url":"https://api.github.com/users/gehel","html_url":"https://github.com/gehel","followers_url":"https://api.github.com/users/gehel/followers","following_url":"https://api.github.com/users/gehel/following{/other_user}","gists_url":"https://api.github.com/users/gehel/gists{/gist_id}","starred_url":"https://api.github.com/users/gehel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gehel/subscriptions","organizations_url":"https://api.github.com/users/gehel/orgs","repos_url":"https://api.github.com/users/gehel/repos","events_url":"https://api.github.com/users/gehel/events{/privacy}","received_events_url":"https://api.github.com/users/gehel/received_events","type":"User","site_admin":false},"created_at":"2016-08-08T14:15:20Z","updated_at":"2016-08-08T14:15:20Z","author_association":"NONE","body":"@mikemccand Thanks for the efforts! Diagnostic dump is available at https://phabricator.wikimedia.org/F4345184 Let me know if there is anything else I can provide!\n\nNote that we had the same issue restarting the next node in the same cluster (still for planned maintenance). The [thread dump](https://gist.github.com/gehel/bae0154a740024e54a8e7d04e7c94561) of this other restart looks very similar.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238372896","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-238372896","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":238372896,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODM3Mjg5Ng==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2016-08-08T20:51:04Z","updated_at":"2016-08-08T20:51:04Z","author_association":"CONTRIBUTOR","body":"Thanks @gehel.\n\nHmm, in the hot threads, I see a `forceMerge` running, this time doing `isSingleValued` on a biggish merge (39.8 GB, 16.7 M docs), which is indeed a step of the merge that does all reading and no writing.  But it ought to be fast.\n\nOther parts of merging are read-only (e.g. `checkIntegrity`) but should also be fast.\n\nYour first merge hot thread should have been writing stuff, which should eventually lead to checking for the merge being aborted.\n\nYou have a miniscule number of deletes, so the merge ought to be writing bytes to the new segment as it runs.\n\nWhen this happens again, can you pull many hot threads over time and post all of them?  And also run iostat continuously while the force merge is refusing to stop, and post that? I'd like to see what the merge thread is doing over this hour...\n\nWhat IO system is backing your path.data?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238391261","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-238391261","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":238391261,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODM5MTI2MQ==","user":{"login":"nomoa","id":5939211,"node_id":"MDQ6VXNlcjU5MzkyMTE=","avatar_url":"https://avatars1.githubusercontent.com/u/5939211?v=4","gravatar_id":"","url":"https://api.github.com/users/nomoa","html_url":"https://github.com/nomoa","followers_url":"https://api.github.com/users/nomoa/followers","following_url":"https://api.github.com/users/nomoa/following{/other_user}","gists_url":"https://api.github.com/users/nomoa/gists{/gist_id}","starred_url":"https://api.github.com/users/nomoa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nomoa/subscriptions","organizations_url":"https://api.github.com/users/nomoa/orgs","repos_url":"https://api.github.com/users/nomoa/repos","events_url":"https://api.github.com/users/nomoa/events{/privacy}","received_events_url":"https://api.github.com/users/nomoa/received_events","type":"User","site_admin":false},"created_at":"2016-08-08T22:00:33Z","updated_at":"2016-08-08T22:00:33Z","author_association":"CONTRIBUTOR","body":"@mikemccand here is a set of [5 stackdumps](https://phabricator.wikimedia.org/F4345956) took when elastic was stuck after calling service restart.\nThe first stack was taken at 11:55 and the last one at 12:48. The merge seems to be always stuck while processing DocValues.\nI downloaded one of the shards (10gb) but failed to reproduce the issue with a simple lucene test case, I'll try with a bigger one.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238395836","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-238395836","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":238395836,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODM5NTgzNg==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2016-08-08T22:20:50Z","updated_at":"2016-08-08T22:20:50Z","author_association":"CONTRIBUTOR","body":"Thanks, indeed all 5 of those show doc values merging, specifically sorted set, and sometimes from an older segment (Lucene 4.10.x format).\n\nI'm wondering if your data is such that the compression of the newer doc values formats (Lucene 5.4+) is much better than 4.10.x was and this merge is doing tons of reading and hardly any writing.  The `iostat` output would help confirm this.  Would it be possible to upload one of your shards somewhere?   I can try to confirm this.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238482185","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-238482185","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":238482185,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODQ4MjE4NQ==","user":{"login":"gehel","id":1415765,"node_id":"MDQ6VXNlcjE0MTU3NjU=","avatar_url":"https://avatars1.githubusercontent.com/u/1415765?v=4","gravatar_id":"","url":"https://api.github.com/users/gehel","html_url":"https://github.com/gehel","followers_url":"https://api.github.com/users/gehel/followers","following_url":"https://api.github.com/users/gehel/following{/other_user}","gists_url":"https://api.github.com/users/gehel/gists{/gist_id}","starred_url":"https://api.github.com/users/gehel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gehel/subscriptions","organizations_url":"https://api.github.com/users/gehel/orgs","repos_url":"https://api.github.com/users/gehel/repos","events_url":"https://api.github.com/users/gehel/events{/privacy}","received_events_url":"https://api.github.com/users/gehel/received_events","type":"User","site_admin":false},"created_at":"2016-08-09T08:03:30Z","updated_at":"2016-08-09T08:03:30Z","author_association":"NONE","body":"@mikemccand Thanks for the help! I'm afraid we can't upload the shards as they potential contain critical information. I'll check though.\n\nFor your previous question, our IO system is: 4x1To spinning disk in software RAID0. I can find the specific models, controllers, ... if you think that's useful.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238504085","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-238504085","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":238504085,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODUwNDA4NQ==","user":{"login":"gehel","id":1415765,"node_id":"MDQ6VXNlcjE0MTU3NjU=","avatar_url":"https://avatars1.githubusercontent.com/u/1415765?v=4","gravatar_id":"","url":"https://api.github.com/users/gehel","html_url":"https://github.com/gehel","followers_url":"https://api.github.com/users/gehel/followers","following_url":"https://api.github.com/users/gehel/following{/other_user}","gists_url":"https://api.github.com/users/gehel/gists{/gist_id}","starred_url":"https://api.github.com/users/gehel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gehel/subscriptions","organizations_url":"https://api.github.com/users/gehel/orgs","repos_url":"https://api.github.com/users/gehel/repos","events_url":"https://api.github.com/users/gehel/events{/privacy}","received_events_url":"https://api.github.com/users/gehel/received_events","type":"User","site_admin":false},"created_at":"2016-08-09T09:40:25Z","updated_at":"2016-08-09T09:40:25Z","author_association":"NONE","body":"@mikemccand here is a [dashboard of iostat metrics that we collect](https://grafana.wikimedia.org/dashboard/db/logstash-elasticsearch-investigation). Let me know if you want more metrics, I'll see what I can add. The timings are as follow:\n- 09:42 UTC: starting shutdown of elasticsearch\n- 10:56 UTC: kill -9 elasticsearch and restart\n\nSo it seems that during the whole shutdown, there was almost no IO happening.\n\nNot sure this is directly related, but I did find some errors about optimize just after shutdown: https://gist.github.com/gehel/ba16e9fd7debbcaa70ab6549525c2b1c\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238508072","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-238508072","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":238508072,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODUwODA3Mg==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2016-08-09T09:57:14Z","updated_at":"2016-08-09T09:57:14Z","author_association":"CONTRIBUTOR","body":"> So it seems that during the whole shutdown, there was almost no IO happening.\n\nWow, how strange, that the CPU can be working so hard for an hour merging away and not cause any IO... the reads could all have been cached by the OS, but the writes should periodically (every 5s by default in Linux) be written through to the IO device.  Apparently Lucene is merging away and producing no bytes for your new segment.\n\nDoes this only happen when force-merging older indices (Lucene 4.10.x, ES 1.x)?\n\n> For your previous question, our IO system is: 4x1To spinning disk in software RAID0. I can find the specific models, controllers, ... if you think that's useful.\n\nOK, thanks, no need for specifics.  The IO devices aren't even being written to ...\n\n> I'm afraid we can't upload the shards as they potential contain critical information. I'll check though.\n\nOK, too bad: I'm very curious what sort of indices compress down to 0 bytes on merge ;)  Something odd is happening.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238514030","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-238514030","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":238514030,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODUxNDAzMA==","user":{"login":"gehel","id":1415765,"node_id":"MDQ6VXNlcjE0MTU3NjU=","avatar_url":"https://avatars1.githubusercontent.com/u/1415765?v=4","gravatar_id":"","url":"https://api.github.com/users/gehel","html_url":"https://github.com/gehel","followers_url":"https://api.github.com/users/gehel/followers","following_url":"https://api.github.com/users/gehel/following{/other_user}","gists_url":"https://api.github.com/users/gehel/gists{/gist_id}","starred_url":"https://api.github.com/users/gehel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gehel/subscriptions","organizations_url":"https://api.github.com/users/gehel/orgs","repos_url":"https://api.github.com/users/gehel/repos","events_url":"https://api.github.com/users/gehel/events{/privacy}","received_events_url":"https://api.github.com/users/gehel/received_events","type":"User","site_admin":false},"created_at":"2016-08-09T10:25:21Z","updated_at":"2016-08-09T10:25:21Z","author_association":"NONE","body":"We might be able to send you one of our shard, provided you (@mikemccand) or elastic signs an NDA. I'm checking to see if this is actually possible and if it is easy enough to make sense.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/239296225","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-239296225","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":239296225,"node_id":"MDEyOklzc3VlQ29tbWVudDIzOTI5NjIyNQ==","user":{"login":"gehel","id":1415765,"node_id":"MDQ6VXNlcjE0MTU3NjU=","avatar_url":"https://avatars1.githubusercontent.com/u/1415765?v=4","gravatar_id":"","url":"https://api.github.com/users/gehel","html_url":"https://github.com/gehel","followers_url":"https://api.github.com/users/gehel/followers","following_url":"https://api.github.com/users/gehel/following{/other_user}","gists_url":"https://api.github.com/users/gehel/gists{/gist_id}","starred_url":"https://api.github.com/users/gehel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gehel/subscriptions","organizations_url":"https://api.github.com/users/gehel/orgs","repos_url":"https://api.github.com/users/gehel/repos","events_url":"https://api.github.com/users/gehel/events{/privacy}","received_events_url":"https://api.github.com/users/gehel/received_events","type":"User","site_admin":false},"created_at":"2016-08-11T21:20:12Z","updated_at":"2016-08-11T21:20:12Z","author_association":"NONE","body":"Anything else I can do to help on this? It should not be too hard on our side to get give you an NDA to sign and send one of our problematic shard. Interested?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/239321339","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-239321339","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":239321339,"node_id":"MDEyOklzc3VlQ29tbWVudDIzOTMyMTMzOQ==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2016-08-11T23:14:00Z","updated_at":"2016-08-11T23:14:00Z","author_association":"CONTRIBUTOR","body":"@gehel sure, can you send it to my private email address (see my github profile)?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/240259835","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-240259835","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":240259835,"node_id":"MDEyOklzc3VlQ29tbWVudDI0MDI1OTgzNQ==","user":{"login":"ebernhardson","id":558434,"node_id":"MDQ6VXNlcjU1ODQzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/558434?v=4","gravatar_id":"","url":"https://api.github.com/users/ebernhardson","html_url":"https://github.com/ebernhardson","followers_url":"https://api.github.com/users/ebernhardson/followers","following_url":"https://api.github.com/users/ebernhardson/following{/other_user}","gists_url":"https://api.github.com/users/ebernhardson/gists{/gist_id}","starred_url":"https://api.github.com/users/ebernhardson/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ebernhardson/subscriptions","organizations_url":"https://api.github.com/users/ebernhardson/orgs","repos_url":"https://api.github.com/users/ebernhardson/repos","events_url":"https://api.github.com/users/ebernhardson/events{/privacy}","received_events_url":"https://api.github.com/users/ebernhardson/received_events","type":"User","site_admin":false},"created_at":"2016-08-16T22:30:27Z","updated_at":"2016-08-16T22:40:03Z","author_association":"NONE","body":"I can add that we did have a few very sparse fields in a couple indices. There was a bug where one of the applications logging to these instances created > 25k fields per day by mistakenly using hashes as nested array keys (https://phabricator.wikimedia.org/T141384). That somehow \"just worked\" in 1.7 so we didn't notice the problem, but caused issues once we upgraded to 2.3. That bug has since been fixed, but perhaps having 25k (up to 100k on the worst days) fields that are each used in a single document  could have caused an issue like this?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/240354114","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-240354114","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":240354114,"node_id":"MDEyOklzc3VlQ29tbWVudDI0MDM1NDExNA==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2016-08-17T08:58:59Z","updated_at":"2016-08-17T08:58:59Z","author_association":"CONTRIBUTOR","body":"> That bug has since been fixed, but perhaps having 25k (up to 100k on the worst days) fields that are each used in a single document could have caused an issue like this?\n\nDid these fields have doc values enabled?  If so, that could explain it: older versions of Lucene (4.10.x) did not compress extremely sparse cases very well, but with https://issues.apache.org/jira/browse/LUCENE-6863 (included in ES 2.3.x) we now compress these extremely sparse cases.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/240514204","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-240514204","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":240514204,"node_id":"MDEyOklzc3VlQ29tbWVudDI0MDUxNDIwNA==","user":{"login":"ebernhardson","id":558434,"node_id":"MDQ6VXNlcjU1ODQzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/558434?v=4","gravatar_id":"","url":"https://api.github.com/users/ebernhardson","html_url":"https://github.com/ebernhardson","followers_url":"https://api.github.com/users/ebernhardson/followers","following_url":"https://api.github.com/users/ebernhardson/following{/other_user}","gists_url":"https://api.github.com/users/ebernhardson/gists{/gist_id}","starred_url":"https://api.github.com/users/ebernhardson/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ebernhardson/subscriptions","organizations_url":"https://api.github.com/users/ebernhardson/orgs","repos_url":"https://api.github.com/users/ebernhardson/repos","events_url":"https://api.github.com/users/ebernhardson/events{/privacy}","received_events_url":"https://api.github.com/users/ebernhardson/received_events","type":"User","site_admin":false},"created_at":"2016-08-17T19:05:10Z","updated_at":"2016-08-17T19:05:10Z","author_association":"NONE","body":"Checked our mapping, yes they all had doc values enabled (per defaults). The mapping per field generally looks like:\n\n```\n      \"err_body_req_body_attributes_value_ids_mw9g_dsr_0\": {\n        \"type\": \"string\",\n        \"norms\": {\n          \"enabled\": false\n        },\n        \"fields\": {\n          \"raw\": {\n            \"type\": \"string\",\n            \"index\": \"not_analyzed\",\n            \"ignore_above\": 256\n          }\n        }\n      },\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/240866964","html_url":"https://github.com/elastic/elasticsearch/issues/19829#issuecomment-240866964","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19829","id":240866964,"node_id":"MDEyOklzc3VlQ29tbWVudDI0MDg2Njk2NA==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2016-08-18T21:45:01Z","updated_at":"2016-08-18T21:45:01Z","author_association":"CONTRIBUTOR","body":"OK thanks @ebernhardson; I think given that this is an unusual corner case situation (old Lucene 4.10.x segments being merged, while containing documents caused by an application level bug that created tons of exceptionally sparse fields), there's nothing for us to fix here.\n","performed_via_github_app":null}]