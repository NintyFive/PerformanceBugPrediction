{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/13146","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13146/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13146/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13146/events","html_url":"https://github.com/elastic/elasticsearch/issues/13146","id":103536719,"node_id":"MDU6SXNzdWUxMDM1MzY3MTk=","number":13146,"title":"Add support for ICUTokenizerFactory and customizing the rule file in ICU tokenizer","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"labels":[{"id":142001965,"node_id":"MDU6TGFiZWwxNDIwMDE5NjU=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Analysis","name":":Search/Analysis","color":"0e8a16","default":false,"description":"How text is split into tokens"},{"id":23172,"node_id":"MDU6TGFiZWwyMzE3Mg==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Efeature","name":">feature","color":"006b75","default":false,"description":null},{"id":92913858,"node_id":"MDU6TGFiZWw5MjkxMzg1OA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/good%20first%20issue","name":"good first issue","color":"07569b","default":true,"description":"low hanging fruit"},{"id":110815527,"node_id":"MDU6TGFiZWwxMTA4MTU1Mjc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/help%20wanted","name":"help wanted","color":"207de5","default":true,"description":"adoptme"},{"id":352250322,"node_id":"MDU6TGFiZWwzNTIyNTAzMjI=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/v5.0.0-alpha2","name":"v5.0.0-alpha2","color":"dddddd","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2015-08-27T15:34:05Z","updated_at":"2018-02-13T19:30:17Z","closed_at":"2016-04-22T19:59:06Z","author_association":"MEMBER","active_lock_reason":null,"body":"_From @rslinckx on October 21, 2014 10:35_\n\nThere has been a commit in lucene (http://svn.apache.org/viewvc?view=revision&revision=1416629) that adds a ICUTokenizerFactory which allows to create a ICUTokenizer with a special config argument enabling the customization of the rule based iterator by providing a custom rules files.\n\nIs it possible to expose that argument in the elasticsearch icu plugin ? This would allow to define an elasticsearch tokenizer like:\n\n```\n{\n    \"index\" : {\n        \"analysis\" : {\n            \"tokenizer\" : {\n                \"custom-icu-tokenizer\" : {\n                    \"type\" : \"icu_tokenizer\",\n                    \"rulefiles\": \"Latn:Latin-break-only-on-whitespace.rbbi\"\n                }\n            }\n        }\n    }\n}\n```\n\nThis allows tailoring the output of the tokenization process (in this example, not splitting on hyphens in the latin script)\n\nAlso see the original solr discussion here: https://issues.apache.org/jira/browse/SOLR-4123\n\n_Copied from original issue: elastic/elasticsearch-analysis-icu#42_\n","closed_by":{"login":"xuzha","id":1799964,"node_id":"MDQ6VXNlcjE3OTk5NjQ=","avatar_url":"https://avatars0.githubusercontent.com/u/1799964?v=4","gravatar_id":"","url":"https://api.github.com/users/xuzha","html_url":"https://github.com/xuzha","followers_url":"https://api.github.com/users/xuzha/followers","following_url":"https://api.github.com/users/xuzha/following{/other_user}","gists_url":"https://api.github.com/users/xuzha/gists{/gist_id}","starred_url":"https://api.github.com/users/xuzha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xuzha/subscriptions","organizations_url":"https://api.github.com/users/xuzha/orgs","repos_url":"https://api.github.com/users/xuzha/repos","events_url":"https://api.github.com/users/xuzha/events{/privacy}","received_events_url":"https://api.github.com/users/xuzha/received_events","type":"User","site_admin":false},"performed_via_github_app":null}