{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7288/events","html_url":"https://github.com/elastic/elasticsearch/issues/7288","id":40304191,"node_id":"MDU6SXNzdWU0MDMwNDE5MQ==","number":7288,"title":"Add option to prevent recovering replica shards onto nodes that did not have them locally.","user":{"login":"gibrown","id":820871,"node_id":"MDQ6VXNlcjgyMDg3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820871?v=4","gravatar_id":"","url":"https://api.github.com/users/gibrown","html_url":"https://github.com/gibrown","followers_url":"https://api.github.com/users/gibrown/followers","following_url":"https://api.github.com/users/gibrown/following{/other_user}","gists_url":"https://api.github.com/users/gibrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gibrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gibrown/subscriptions","organizations_url":"https://api.github.com/users/gibrown/orgs","repos_url":"https://api.github.com/users/gibrown/repos","events_url":"https://api.github.com/users/gibrown/events{/privacy}","received_events_url":"https://api.github.com/users/gibrown/received_events","type":"User","site_admin":false},"labels":[{"id":836504707,"node_id":"MDU6TGFiZWw4MzY1MDQ3MDc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Distributed","name":":Distributed/Distributed","color":"0e8a16","default":false,"description":"A catch all label for anything in the Distributed Area. If you aren't sure, use this one."}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":22,"created_at":"2014-08-14T21:59:43Z","updated_at":"2018-02-13T19:28:44Z","closed_at":"2015-08-16T10:44:32Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"For large deployments ES shard recovery after a node goes down or some networking event occurs seems to cause more problems than are necessary because all of the shards end up getting shuffled around the cluster and can cause other performance problems. Seems better to leave some replicas uninitialized until the nodes are alive again. Right now the recovery time from an event affecting a node is proportional to the total amount of data on that node rather than proportional to the amount of changed data on that node if we just waited for the node to be up again so it can recover locally. This behavior also makes it harder to contain a problem from affecting the entire cluster.\n## Backstory\n\nWe had a 4 min network disruption in one of our 3 data centers (14 data nodes per DC) at 23:04. As you would expect ES queries started timing out to that DC giving us a 30% error rate, but then recovered after about 5 minutes.\n\n![](https://i.cloudup.com/0EhM4BnqMu.png)\n\nHowever, after about 30 minutes queries again started timing out and that continued to get worse over the next two hours.\n\nThe root cause of these bad queries was due to shards getting shuffled around within the DC that had the networking problem.\n\n![](https://i.cloudup.com/8mbgBq1PQt.png)\n\nSome nodes actually ended up getting overloaded with shards causing server performance problems as the shards got initialized on those nodes and they could not handle the increasing query load (those first 14 bars are disk space in the affected DC):\n\n![](https://i.cloudup.com/DBVZ2plrzV-3000x3000.png)\n## Proposed Solution\n\nI think a better set of \"production\" settings would say \"do not re-allocate shards to other nodes when a node goes down\".\n\nIt seems like cluster.routing.allocation.enable should have an additional option: recovery_and_new (this would still allow rebalancing). \"Recovery\" in this case would mean that shards can only be initialized on a node that already had them on it. \n\nThe assumption here is that the cluster has enough replication that losing a single replica is not such a large event that it is worth triggering a whole lot of network traffic and potentially causing other problems.\n## Other Workarounds\n\nI don't think there is a way to do this right now. I could disable all allocation, but that would prevent shards for new indices from being allocated. recover_after_time could almost serve this purpose (I could set the timeout to 3 hours to give the ops team time to get a server back up), but that setting seems to only apply on a full cluster restart.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}