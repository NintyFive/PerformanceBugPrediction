[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/205217738","html_url":"https://github.com/elastic/elasticsearch/issues/17501#issuecomment-205217738","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17501","id":205217738,"node_id":"MDEyOklzc3VlQ29tbWVudDIwNTIxNzczOA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-04-04T09:48:36Z","updated_at":"2016-04-04T09:48:36Z","author_association":"MEMBER","body":"It think it's important to understand  why they nodes left the cluster. Do see anything in the logs like long GC pauzes? do you monitor the memory usage of the nodes? Also, when the need leaves, do you see anything of note in the node's logs?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/205233529","html_url":"https://github.com/elastic/elasticsearch/issues/17501#issuecomment-205233529","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17501","id":205233529,"node_id":"MDEyOklzc3VlQ29tbWVudDIwNTIzMzUyOQ==","user":{"login":"NicoYUE","id":9987198,"node_id":"MDQ6VXNlcjk5ODcxOTg=","avatar_url":"https://avatars0.githubusercontent.com/u/9987198?v=4","gravatar_id":"","url":"https://api.github.com/users/NicoYUE","html_url":"https://github.com/NicoYUE","followers_url":"https://api.github.com/users/NicoYUE/followers","following_url":"https://api.github.com/users/NicoYUE/following{/other_user}","gists_url":"https://api.github.com/users/NicoYUE/gists{/gist_id}","starred_url":"https://api.github.com/users/NicoYUE/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/NicoYUE/subscriptions","organizations_url":"https://api.github.com/users/NicoYUE/orgs","repos_url":"https://api.github.com/users/NicoYUE/repos","events_url":"https://api.github.com/users/NicoYUE/events{/privacy}","received_events_url":"https://api.github.com/users/NicoYUE/received_events","type":"User","site_admin":false},"created_at":"2016-04-04T10:30:39Z","updated_at":"2016-04-04T10:30:39Z","author_association":"NONE","body":"I just checked my logs and I don't see anything specific for leaving the cluster. And I have to do something like a query or checking KOPF to realize they are missing.\n\nI remember when I just started setting it up, the nodes would leave if they couldn't find an active host in:\n\ndiscovery.zen.ping.unicast.hosts\n\nAbout memory, I've set an ES_HEAP_SIZE of 32g but I don't know if it's relevant.\n\nIf one of node leave the cluster again, I'll try to get more info\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/205287162","html_url":"https://github.com/elastic/elasticsearch/issues/17501#issuecomment-205287162","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17501","id":205287162,"node_id":"MDEyOklzc3VlQ29tbWVudDIwNTI4NzE2Mg==","user":{"login":"NicoYUE","id":9987198,"node_id":"MDQ6VXNlcjk5ODcxOTg=","avatar_url":"https://avatars0.githubusercontent.com/u/9987198?v=4","gravatar_id":"","url":"https://api.github.com/users/NicoYUE","html_url":"https://github.com/NicoYUE","followers_url":"https://api.github.com/users/NicoYUE/followers","following_url":"https://api.github.com/users/NicoYUE/following{/other_user}","gists_url":"https://api.github.com/users/NicoYUE/gists{/gist_id}","starred_url":"https://api.github.com/users/NicoYUE/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/NicoYUE/subscriptions","organizations_url":"https://api.github.com/users/NicoYUE/orgs","repos_url":"https://api.github.com/users/NicoYUE/repos","events_url":"https://api.github.com/users/NicoYUE/events{/privacy}","received_events_url":"https://api.github.com/users/NicoYUE/received_events","type":"User","site_admin":false},"created_at":"2016-04-04T13:01:42Z","updated_at":"2016-04-04T13:04:40Z","author_association":"NONE","body":"Just happened again, this is what my logs says\n\n```\n[2016-04-04 14:43:59,950][WARN ][cluster.service          ] [hdp1.1.prod2.es.xxx] failed to disconnect to node [{hdp6.2.prod2.es.xxx}{-fIZEZwvTmmHFKGMgxXCdA}{192.168.10.6}{192.168.10.6:9301}]\njava.lang.OutOfMemoryError: unable to create new native thread\n[2016-04-04 14:44:05,318][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry\n[2016-04-04 14:44:05,322][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry\n[2016-04-04 14:44:05,324][DEBUG][action.admin.indices.get ] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry\n[2016-04-04 14:44:05,325][DEBUG][action.admin.cluster.health] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry\n[2016-04-04 14:44:09,901][WARN ][threadpool               ] [hdp1.1.prod2.es.xxx] failed to run [threaded] org.elasticsearch.cluster.service.InternalClusterService$ReconnectToNodes@58f4c6e4\njava.lang.OutOfMemoryError: unable to create new native thread\n[2016-04-04 14:44:16,089][WARN ][threadpool               ] [hdp1.1.prod2.es.xxx] failed to run [threaded] org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout@778292ad\njava.lang.OutOfMemoryError: unable to create new native thread\n[2016-04-04 14:44:29,442][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry\n[2016-04-04 14:44:29,442][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry\n[2016-04-04 14:44:35,320][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] timed out while retrying [cluster:monitor/state] after failure (timeout [30s])\n[2016-04-04 14:44:35,322][WARN ][rest.suppressed          ] /_cluster/state/master_node,routing_table,blocks/ Params: {metric=master_node,routing_table,blocks}\nMasterNotDiscoveredException[null]\n[2016-04-04 14:44:35,323][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] timed out while retrying [cluster:monitor/state] after failure (timeout [30s])\n[2016-04-04 14:44:35,324][WARN ][rest.suppressed          ] /_cluster/settings Params: {}\nMasterNotDiscoveredException[null]\n[2016-04-04 14:44:35,326][WARN ][threadpool               ] [hdp1.1.prod2.es.xxx] failed to run [threaded] org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout@3e9e7840\njava.lang.OutOfMemoryError: unable to create new native thread\n[2016-04-04 14:44:35,325][DEBUG][action.admin.indices.get ] [hdp1.1.prod2.es.xxx] timed out while retrying [indices:admin/get] after failure (timeout [30s])\n[2016-04-04 14:44:35,327][WARN ][rest.suppressed          ] /_aliases Params: {index=_aliases}MasterNotDiscoveredException[null]\n\n[2016-04-04 14:44:35,328][DEBUG][action.admin.indices.get ] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry\n[2016-04-04 14:44:35,332][DEBUG][action.admin.cluster.health] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry\n[2016-04-04 14:44:59,443][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] timed out while retrying [cluster:monitor/state] after failure (timeout [30s])\n[2016-04-04 14:44:59,443][WARN ][threadpool               ] [hdp1.1.prod2.es.xxx] failed to run [threaded] org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout@189b1bb5\njava.lang.OutOfMemoryError: unable to create new native thread\n\n```\n\nIt tells me memory problem but I've set 32g which should be enough and from KOPF, i'm not using a lof of memory either.\n\nEach of my instances have a discovery zen so I don't know why they can't find the master anymore which didnt leave my cluster.\n\nAlso, I noticed that the only nodes that are leaving everytime are those I had set on triple instances on one machine, with 32g for each instances, but I should have at least 100g unused RAM on these machines.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/205325170","html_url":"https://github.com/elastic/elasticsearch/issues/17501#issuecomment-205325170","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17501","id":205325170,"node_id":"MDEyOklzc3VlQ29tbWVudDIwNTMyNTE3MA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-04-04T14:37:05Z","updated_at":"2016-04-04T14:37:05Z","author_association":"MEMBER","body":"This line indicates you are indeed having memory problems:\n\n```\njava.lang.OutOfMemoryError: unable to create new native thread\n```\n\nOnce a node goes out of memory it becomes unreliable and indeed needs to be restarted.  You mention KOPF indicates you have enough memory. Can you elaborate? \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/205328280","html_url":"https://github.com/elastic/elasticsearch/issues/17501#issuecomment-205328280","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17501","id":205328280,"node_id":"MDEyOklzc3VlQ29tbWVudDIwNTMyODI4MA==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2016-04-04T14:44:51Z","updated_at":"2016-04-04T14:49:14Z","author_association":"MEMBER","body":"> `java.lang.OutOfMemoryError: unable to create new native thread`\n\nWhen you see the message \"unable to create new native thread\" this generally means that you have an issue limiting the number of processes that the elasticsearch user can create. The exact resolution varies from system to system, but in general on Linux you would look at `/etc/security/limits.conf` and `ulimit -u`. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/205331019","html_url":"https://github.com/elastic/elasticsearch/issues/17501#issuecomment-205331019","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17501","id":205331019,"node_id":"MDEyOklzc3VlQ29tbWVudDIwNTMzMTAxOQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2016-04-04T14:49:17Z","updated_at":"2016-04-04T14:55:57Z","author_association":"MEMBER","body":"> About memory, I've set an ES_HEAP_SIZE of 32g but I don't know if it's relevant.\n\nIt's not relevant to the issue that you're seeing in the logs (\"unable to create new native threads\"). However, you'll actually see better results if you drop the heap slightly below 32g. On the version of Elasticsearch that you're running, when a node starts up you'll see a message that says\n\n```\n[2016-04-04 10:47:27,618][INFO ][env                      ] [Masked Marauder] heap size [31.9gb], compressed ordinary object pointers [false]\n```\n\nbut if you drop the heap below 32g you'll be able to take advantage of compressed oops\n\n```\n[2016-04-04 10:48:20,133][INFO ][env                      ] [Brute II] heap size [31.1gb], compressed ordinary object pointers [true]\n```\n\nThis actually gives you more useable heap, and the smaller pointers are friendlier to memory bandwidth and CPU caches.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/205350728","html_url":"https://github.com/elastic/elasticsearch/issues/17501#issuecomment-205350728","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17501","id":205350728,"node_id":"MDEyOklzc3VlQ29tbWVudDIwNTM1MDcyOA==","user":{"login":"NicoYUE","id":9987198,"node_id":"MDQ6VXNlcjk5ODcxOTg=","avatar_url":"https://avatars0.githubusercontent.com/u/9987198?v=4","gravatar_id":"","url":"https://api.github.com/users/NicoYUE","html_url":"https://github.com/NicoYUE","followers_url":"https://api.github.com/users/NicoYUE/followers","following_url":"https://api.github.com/users/NicoYUE/following{/other_user}","gists_url":"https://api.github.com/users/NicoYUE/gists{/gist_id}","starred_url":"https://api.github.com/users/NicoYUE/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/NicoYUE/subscriptions","organizations_url":"https://api.github.com/users/NicoYUE/orgs","repos_url":"https://api.github.com/users/NicoYUE/repos","events_url":"https://api.github.com/users/NicoYUE/events{/privacy}","received_events_url":"https://api.github.com/users/NicoYUE/received_events","type":"User","site_admin":false},"created_at":"2016-04-04T15:29:39Z","updated_at":"2016-04-05T09:02:23Z","author_association":"NONE","body":"On KOPF, I can check the HEAP usage where my instances usually are using around 1Gb for 31.81Gb Max, it's far from that.\n\nThat's why I find this really weird. i do have some nodes with a low max RAM but they're not those who leave my cluster and are actually quite stable\n\nHaven't check limit.conf yet, will do it ASAP\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/206272578","html_url":"https://github.com/elastic/elasticsearch/issues/17501#issuecomment-206272578","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17501","id":206272578,"node_id":"MDEyOklzc3VlQ29tbWVudDIwNjI3MjU3OA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-04-06T09:56:59Z","updated_at":"2016-04-06T09:56:59Z","author_association":"CONTRIBUTOR","body":"@jasontedor has already provided the answer, and given that this topic is more suited to the forums than github, I'm going to close\n","performed_via_github_app":null}]