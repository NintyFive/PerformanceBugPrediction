{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/6521","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6521/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6521/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6521/events","html_url":"https://github.com/elastic/elasticsearch/issues/6521","id":35827102,"node_id":"MDU6SXNzdWUzNTgyNzEwMg==","number":6521,"title":"Master node took a long time to remove the problematic data nodes.","user":{"login":"JeffreyZZ","id":7685299,"node_id":"MDQ6VXNlcjc2ODUyOTk=","avatar_url":"https://avatars3.githubusercontent.com/u/7685299?v=4","gravatar_id":"","url":"https://api.github.com/users/JeffreyZZ","html_url":"https://github.com/JeffreyZZ","followers_url":"https://api.github.com/users/JeffreyZZ/followers","following_url":"https://api.github.com/users/JeffreyZZ/following{/other_user}","gists_url":"https://api.github.com/users/JeffreyZZ/gists{/gist_id}","starred_url":"https://api.github.com/users/JeffreyZZ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JeffreyZZ/subscriptions","organizations_url":"https://api.github.com/users/JeffreyZZ/orgs","repos_url":"https://api.github.com/users/JeffreyZZ/repos","events_url":"https://api.github.com/users/JeffreyZZ/events{/privacy}","received_events_url":"https://api.github.com/users/JeffreyZZ/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2014-06-16T19:16:54Z","updated_at":"2014-06-17T06:44:24Z","closed_at":"2014-06-17T06:44:24Z","author_association":"NONE","active_lock_reason":null,"body":"We have an ElasticSearch cluster (ver 1.1.1) that has a dedicated master node (=es-diag-19) and a few data nodes. We noticed that when one of the data nodes (=es-diag-16) was put under high index an search requests pressure, and it then experienced very long gc time. During its long gc period, the master node got many timeouts to ping the data node for more than an hour. \n\nAs we already set discovery.zen.ping.timeout: 25s, I suppose that master should automatically remove the data node when it saw frequent timeouts when connecting the data node, but seems that the master took a long time to have this timeout before finally removed the problematic data node. Is this bug?\n\nHere are the log from the master and data nodes : \n## master (es-diag-19)\n\n2014-06-16 07:09:02,883][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [45538ms] ago, timed out [15530ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7295820]\n[2014-06-16 07:10:25,321][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [55224ms] ago, timed out [25211ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7297000]\n[2014-06-16 07:11:24,406][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [55815ms] ago, timed out [25804ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7297931]\n[2014-06-16 07:12:07,292][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [40825ms] ago, timed out [10820ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data]\n......\n[2014-06-16 08:33:01,184][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [83298ms] ago, timed out [53288ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7362676]\n[2014-06-16 08:33:01,184][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [53288ms] ago, timed out [23275ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7363037]\n[2014-06-16 08:34:00,401][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [58194ms] ago, timed out [28190ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7363693]\n[2014-06-16 08:34:36,793][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [35385ms] ago, timed out [5374ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7364398]\n[2014-06-16 08:36:08,944][INFO ][cluster.service          ] [ES-DIAG-19-master] removed {[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{AvailabilitySet=As-Data-3, master=false},}, reason: zen-disco-node_failed([ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout\n## data(es-diag-16)\n\n[2014-06-16 07:09:02,027][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165282][71] duration [46s], collections [1]/[46.5s], total [46s]/[1.2m], memory [6.8gb]->[6.3gb]/[6.9gb], all_pools {[young] [520.6mb]->[61.2mb]/[532.5mb]}{[survivor] [55.4mb]->[0b]/[66.5mb]}{[old] [6.2gb]->[6.3gb]/[6.3gb]}\n[2014-06-16 07:10:24,498][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165309][73] duration [55.1s], collections [1]/[55.8s], total [55.1s]/[2.1m], memory [6.9gb]->[6.4gb]/[6.9gb], all_pools {[young] [528.1mb]->[142.8mb]/[532.5mb]}{[survivor] [58.3mb]->[0b]/[66.5mb]}{[old] [6.3gb]->[6.3gb]/[6.3gb]}\n[2014-06-16 07:11:23,514][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165312][74] duration [56.6s], collections [1]/[56.8s], total [56.6s]/[3m], memory [6.9gb]->[6.4gb]/[6.9gb], all_pools {[young] [532.5mb]->[64.4mb]/[532.5mb]}{[survivor] [53.6mb]->[0b]/[66.5mb]}{[old] [6.3gb]->[6.3gb]/[6.3gb]}\n......\n[2014-06-16 08:33:00,308][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165449][170] duration [45.7s], collections [1]/[45.8s], total [45.7s]/[1.3h], memory [6.9gb]->[6.9gb]/[6.9gb], all_pools {[young] [532.5mb]->[532.5mb]/[532.5mb]}{[survivor] [58.9mb]->[59mb]/[66.5mb]}{[old] [6.3gb]->[6.3gb]/[6.3gb]}\n[2014-06-16 08:34:35,917][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165450][173] duration [2.2m], collections [3]/[2.2m], total [2.2m]/[1.4h], memory [6.9gb]->[6.9gb]/[6.9gb], all_pools {[young] [532.5mb]->[532.5mb]/[532.5mb]}{[survivor] [59mb]->[61.6mb]/[66.5mb]}{[old] [6.3gb]->[6.3gb]/[6.3gb]}\n[2014-06-16 08:37:24,322][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165451][177] duration [2.8m], collections [4]/[2.8m], total [2.8m]/[1.4h], memory [6.9gb]->[6.9gb]/[6.9gb], all_pools {[young] [532.5mb]->[532.5mb]/[532.5mb]}{[survivor] [61.6mb]->[64.9mb]/[66.5mb]}{[old] [6.3gb]->[6.3gb]/[6.3gb]}\n[2014-06-16 08:40:05,384][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165452][179] duration [1.8m], collections [2]/[1.8m], total [1.8m]/[1.4h], memory [6.9gb]->[6.9gb]/[6.9gb], all_pools {[young] [532.5mb]->[532.5mb]/[532.5mb]}{[survivor] [64.9mb]->[64.5mb]/[66.5mb]}{[old] [6.3gb]->[6.3gb]/[6.3gb]}\n[2014-06-16 08:41:02,775][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165453][181] duration [1.7m], collections [2]/[1.7m], total [1.7m]/[1.5h], memory [6.9gb]->[6.9gb]/[6.9gb], all_pools {[young] [532.5mb]->[532.5mb]/[532.5mb]}{[survivor] [64.5mb]->[63.3mb]/[66.5mb]}{[old] [6.3gb]->[6.3gb]/[6.3gb]}\n[2014-06-16 08:45:35,336][WARN ][discovery.zen.ping.unicast] [ES-DIAG-16-data] failed to send ping to [[#zen_unicast_2#][es-diag-16][inet[es-diag-20/10.1.0.34:9300]]]\norg.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[es-diag-20/10.1.0.34:9300]][discovery/zen/unicast] request_id [5391981] timed out after [59031ms]\n    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:356)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n[2014-06-16 08:46:33,539][WARN ][discovery.zen            ] [ES-DIAG-16-data] failed to connect to master [[ES-DIAG-19-master][J7p7Z9h2QCGvBSeGRbfaSw][es-diag-19][inet[/10.1.0.8:9300]]{data=false, AvailabilitySet=As-Master, master=true}], retrying...\norg.elasticsearch.transport.ConnectTransportException: [ES-DIAG-19-master][inet[/10.1.0.8:9300]] connect_timeout[30s]\n    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:718)\n    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:647)\n    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:615)\n    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:129)\n    at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:338)\n    at org.elasticsearch.discovery.zen.ZenDiscovery.access$500(ZenDiscovery.java:79)\n    at org.elasticsearch.discovery.zen.ZenDiscovery$1.run(ZenDiscovery.java:286)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n[2014-06-16 08:45:35,351][WARN ][discovery.zen.ping.unicast] [ES-DIAG-16-data] failed to send ping to [[#zen_unicast_3#][es-diag-16][inet[es-diag-21/10.1.0.35:9300]]]\norg.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[es-diag-21/10.1.0.35:9300]][discovery/zen/unicast] request_id [5391982] timed out after [59047ms]\n    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:356)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n[2014-06-16 08:46:33,664][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165458][187] duration [58.6s], collections [1]/[58.9s], total [58.6s]/[1.6h], memory [6.8gb]->[6.8gb]/[6.9gb], all_pools {[young] [532.5mb]->[532.5mb]/[532.5mb]}{[survivor] [26.4mb]->[31.1mb]/[66.5mb]}{[old] [6.3gb]->[6.3gb]/[6.3gb]}\n[2014-06-16 08:46:34,101][DEBUG][action.bulk              ] [ES-DIAG-16-data] [perf-searchindexer-2014-06-16][1], node[SWuDp-k4QaC-Lh8X4P3Rfw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@3f536b50]\njava.lang.NullPointerException\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:139)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:76)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:610)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:557)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n[2014-06-16 08:46:34,149][WARN ][transport                ] [ES-DIAG-16-data] Transport response handler not found of id [5391984]\n[2014-06-16 08:47:32,289][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165459][188] duration [58.4s], collections [1]/[58.6s], total [58.4s]/[1.6h], memory [6.8gb]->[6.8gb]/[6.9gb], all_pools {[young] [532.5mb]->[470.6mb]/[532.5mb]}{[survivor] [31.1mb]->[0b]/[66.5mb]}{[old] [6.3gb]->[6.3gb]/[6.3gb]}\n[2014-06-16 08:47:32,842][DEBUG][action.bulk              ] [ES-DIAG-16-data] [logs-sep-2014-06-16][1], node[SWuDp-k4QaC-Lh8X4P3Rfw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@450272fe]\njava.lang.NullPointerException\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:139)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:76)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:610)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:557)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n","closed_by":{"login":"spinscale","id":667544,"node_id":"MDQ6VXNlcjY2NzU0NA==","avatar_url":"https://avatars2.githubusercontent.com/u/667544?v=4","gravatar_id":"","url":"https://api.github.com/users/spinscale","html_url":"https://github.com/spinscale","followers_url":"https://api.github.com/users/spinscale/followers","following_url":"https://api.github.com/users/spinscale/following{/other_user}","gists_url":"https://api.github.com/users/spinscale/gists{/gist_id}","starred_url":"https://api.github.com/users/spinscale/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spinscale/subscriptions","organizations_url":"https://api.github.com/users/spinscale/orgs","repos_url":"https://api.github.com/users/spinscale/repos","events_url":"https://api.github.com/users/spinscale/events{/privacy}","received_events_url":"https://api.github.com/users/spinscale/received_events","type":"User","site_admin":false},"performed_via_github_app":null}