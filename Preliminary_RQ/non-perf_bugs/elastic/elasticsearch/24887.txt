{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/24887","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24887/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24887/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24887/events","html_url":"https://github.com/elastic/elasticsearch/issues/24887","id":231303424,"node_id":"MDU6SXNzdWUyMzEzMDM0MjQ=","number":24887,"title":"3 nodes ES 2.3.2 cluster with Replica 2 goes to red state after bringing down whole cluster and starting only a single node","user":{"login":"SKumarMN","id":2056311,"node_id":"MDQ6VXNlcjIwNTYzMTE=","avatar_url":"https://avatars2.githubusercontent.com/u/2056311?v=4","gravatar_id":"","url":"https://api.github.com/users/SKumarMN","html_url":"https://github.com/SKumarMN","followers_url":"https://api.github.com/users/SKumarMN/followers","following_url":"https://api.github.com/users/SKumarMN/following{/other_user}","gists_url":"https://api.github.com/users/SKumarMN/gists{/gist_id}","starred_url":"https://api.github.com/users/SKumarMN/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SKumarMN/subscriptions","organizations_url":"https://api.github.com/users/SKumarMN/orgs","repos_url":"https://api.github.com/users/SKumarMN/repos","events_url":"https://api.github.com/users/SKumarMN/events{/privacy}","received_events_url":"https://api.github.com/users/SKumarMN/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2017-05-25T10:36:44Z","updated_at":"2017-05-25T11:53:32Z","closed_at":"2017-05-25T10:53:45Z","author_association":"NONE","active_lock_reason":null,"body":"ES version ES 2.3.2\r\nJRE version\":\"1.8.0_112\r\nOS: Windows server 2012\r\n\r\nI have a cluster with 3 nodes.  I have a index with 2 replicas. I brought down all the es nodes. However when i restart one es node, the shards are unassigned and the index/cluster is in red state. if i bring up one more node then my cluster becomes yellow and operational. \r\n\r\nThe issue here is even though my replica is 2 for a 3 node cluster and after full cluster shutdown and a single node startup, the cluster/index should have been yellow than red.  \r\n\r\nA similar use case with 2 nodes and replica as 1 works fine.  \r\n\r\n\r\n_cat/indices\r\nhealth status index   pri rep docs.count docs.deleted store.size pri.store.size \r\ngreen  open   twitter   5   2         37            0    199.7kb         66.5kb \r\n\r\n_cat/nodes\r\nhost        ip          heap.percent ram.percent  load node.role master name               \r\n10.196.18.1 10.196.18.1           13          24 -1.00 d         m      Alpha Ray          \r\n10.196.18.1 10.196.18.1            5          24 -1.00 d         *      Zebediah Killgrave \r\n10.196.18.1 10.196.18.1           14          24 -1.00 d         m      Chance    \r\n\r\n\r\n_cat/shards\r\nindex   shard prirep state   docs  store ip          node               \r\ntwitter 3     p      STARTED    9   20kb 10.196.18.1 Zebediah Killgrave \r\ntwitter 3     r      STARTED    9 17.2kb 10.196.18.1 Chance             \r\ntwitter 3     r      STARTED    9   20kb 10.196.18.1 Alpha Ray          \r\ntwitter 1     r      STARTED   11 14.6kb 10.196.18.1 Zebediah Killgrave \r\ntwitter 1     p      STARTED   11 14.6kb 10.196.18.1 Chance             \r\ntwitter 1     r      STARTED   11 17.4kb 10.196.18.1 Alpha Ray          \r\ntwitter 2     r      STARTED    8 14.4kb 10.196.18.1 Zebediah Killgrave \r\ntwitter 2     r      STARTED    8 14.4kb 10.196.18.1 Chance             \r\ntwitter 2     p      STARTED    8 14.4kb 10.196.18.1 Alpha Ray          \r\ntwitter 4     r      STARTED    7 11.6kb 10.196.18.1 Zebediah Killgrave \r\ntwitter 4     p      STARTED    7 11.6kb 10.196.18.1 Chance             \r\ntwitter 4     r      STARTED    7 11.6kb 10.196.18.1 Alpha Ray          \r\ntwitter 0     p      STARTED    2  5.7kb 10.196.18.1 Zebediah Killgrave \r\ntwitter 0     r      STARTED    2  5.7kb 10.196.18.1 Chance             \r\ntwitter 0     r      STARTED    2  5.7kb 10.196.18.1 Alpha Ray   \r\n\r\n\r\n\r\nNow all the nodes are shut down and i just bring up one node  output as shown below. As you see in the below output i dont see recovery of index and change of status from red to yellow like example \r\n\"cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[twitter]\"\r\n\r\n[2017-05-25 04:21:21,767][INFO ][plugins                  ] [Leo] modules [reind\r\nex], plugins [analysis-phonetic, delete-by-query], sites []\r\n[2017-05-25 04:21:21,812][INFO ][env                      ] [Leo] using [1] data\r\n paths, mounts [[(C:)]], net usable_space [150.4gb], net total_space [239.6gb],\r\nspins? [unknown], types [NTFS]\r\n[2017-05-25 04:21:21,813][INFO ][env                      ] [Leo] heap size [910\r\n.5mb], compressed ordinary object pointers [true]\r\n[2017-05-25 04:21:25,449][INFO ][node                     ] [Leo] initialized\r\n[2017-05-25 04:21:25,449][INFO ][node                     ] [Leo] starting ...\r\n[2017-05-25 04:21:25,696][INFO ][transport                ] [Leo] publish_addres\r\ns {slc12acw.us.oracle.com/10.196.18.1:9300}, bound_addresses {10.196.18.1:9300},\r\n {[fe80::9813:c9a8:3ce6:a365]:9300}\r\n[2017-05-25 04:21:25,709][INFO ][discovery                ] [Leo] sigcluster1/GQ\r\nNpeJo2QlyuI75sZDvMPQ\r\n[2017-05-25 04:21:28,784][INFO ][cluster.service          ] [Leo] new_master {Le\r\no}{GQNpeJo2QlyuI75sZDvMPQ}{10.196.18.1}{slc12acw.us.oracle.com/10.196.18.1:9300}\r\n, reason: zen-disco-join(elected_as_master, [0] joins received)\r\n[2017-05-25 04:21:28,817][INFO ][http                     ] [Leo] publish_addres\r\ns {slc12acw.us.oracle.com/10.196.18.1:9200}, bound_addresses {10.196.18.1:9200},\r\n {[fe80::9813:c9a8:3ce6:a365]:9200}\r\n[2017-05-25 04:21:28,818][INFO ][node                     ] [Leo] started\r\n[2017-05-25 04:21:28,947][INFO ][gateway                  ] [Leo] recovered [1]\r\nindices into cluster_state\r\n \r\n \r\n_cat/shards with only single node restart after complete cluster shutdown\r\nindex   shard prirep state      docs store ip node \r\ntwitter 3     p      UNASSIGNED                    \r\ntwitter 3     r      UNASSIGNED                    \r\ntwitter 3     r      UNASSIGNED                    \r\ntwitter 1     p      UNASSIGNED                    \r\ntwitter 1     r      UNASSIGNED                    \r\ntwitter 1     r      UNASSIGNED                    \r\ntwitter 2     p      UNASSIGNED                    \r\ntwitter 2     r      UNASSIGNED                    \r\ntwitter 2     r      UNASSIGNED                    \r\ntwitter 4     p      UNASSIGNED                    \r\ntwitter 4     r      UNASSIGNED                    \r\ntwitter 4     r      UNASSIGNED                    \r\ntwitter 0     p      UNASSIGNED                    \r\ntwitter 0     r      UNASSIGNED                    \r\ntwitter 0     r      UNASSIGNED  \r\n","closed_by":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"performed_via_github_app":null}