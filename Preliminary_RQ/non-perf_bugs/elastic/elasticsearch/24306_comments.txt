[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298146310","html_url":"https://github.com/elastic/elasticsearch/issues/24306#issuecomment-298146310","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24306","id":298146310,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODE0NjMxMA==","user":{"login":"abeyad","id":1631297,"node_id":"MDQ6VXNlcjE2MzEyOTc=","avatar_url":"https://avatars2.githubusercontent.com/u/1631297?v=4","gravatar_id":"","url":"https://api.github.com/users/abeyad","html_url":"https://github.com/abeyad","followers_url":"https://api.github.com/users/abeyad/followers","following_url":"https://api.github.com/users/abeyad/following{/other_user}","gists_url":"https://api.github.com/users/abeyad/gists{/gist_id}","starred_url":"https://api.github.com/users/abeyad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abeyad/subscriptions","organizations_url":"https://api.github.com/users/abeyad/orgs","repos_url":"https://api.github.com/users/abeyad/repos","events_url":"https://api.github.com/users/abeyad/events{/privacy}","received_events_url":"https://api.github.com/users/abeyad/received_events","type":"User","site_admin":false},"created_at":"2017-04-29T04:27:19Z","updated_at":"2017-04-29T04:27:19Z","author_association":"CONTRIBUTOR","body":"@apepper thanks for reporting this.  Did you recently upgrade your ES version to 2.4.4 or have you been on it for a while (longer than the problem has been manifesting)?  Also, why are you recreating the repository every time you want to take a snapshot?  That is not needed, you can simply take snapshots once the repository has been created once for the cluster.\r\n\r\nThe error is happening during repository verification after creation - by default when a repository is created in a cluster, a test is run to ensure that every node in the cluster is able to write to the repository, so the master node writes a new folder off the base_path called `tests-{seed}` and has each node write a blob to that folder, and once it verified that it can write to the repository, it subsequently deletes that `tests-{seed}` folder off the base_path.  The error is happening as a result of attempting to delete `tests-{seed}` and all the blobs created in it - it seems some of the blobs could not be deleted, or there was a concurrent request and the blobs were attempted to be deleted twice.  Its not clear to me yet how this can happen, given the code and the stack trace shown (unless some side process outside of ES deleted those objects).\r\n\r\nNote that you can sidestep the verification process by passing in the request param `verify=false`, and this may be a good solution for you in the interim, i.e.\r\n```\r\nPUT _snapshot/my_s3_repository?verify=false\r\n{\r\n  \"type\": \"s3\",\r\n  \"settings\": {\r\n    \"bucket\": \"my_bucket_name\",\r\n    \"region\": \"eu-west\",\r\n    \"base_path\": \"my_base_path\",\r\n    \"server_side_encryption\": true,\r\n  }\r\n}\r\n```\r\n\r\nBut as I mentioned, you don't need to call this every time - once you've created the repository once in the cluster, all you need to do is use the [create snapshot API](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-snapshots.html#_snapshot) to create snapshots.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298597009","html_url":"https://github.com/elastic/elasticsearch/issues/24306#issuecomment-298597009","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24306","id":298597009,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODU5NzAwOQ==","user":{"login":"apepper","id":86275,"node_id":"MDQ6VXNlcjg2Mjc1","avatar_url":"https://avatars0.githubusercontent.com/u/86275?v=4","gravatar_id":"","url":"https://api.github.com/users/apepper","html_url":"https://github.com/apepper","followers_url":"https://api.github.com/users/apepper/followers","following_url":"https://api.github.com/users/apepper/following{/other_user}","gists_url":"https://api.github.com/users/apepper/gists{/gist_id}","starred_url":"https://api.github.com/users/apepper/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apepper/subscriptions","organizations_url":"https://api.github.com/users/apepper/orgs","repos_url":"https://api.github.com/users/apepper/repos","events_url":"https://api.github.com/users/apepper/events{/privacy}","received_events_url":"https://api.github.com/users/apepper/received_events","type":"User","site_admin":false},"created_at":"2017-05-02T10:27:45Z","updated_at":"2017-05-02T10:27:45Z","author_association":"CONTRIBUTOR","body":"@abeyad: Thanks for getting back to me.\r\n\r\n>Did you recently upgrade your ES version to 2.4.4 or have you been on it for a while (longer than the problem has been manifesting)?\r\n\r\nWe do use ES 2.4.4 in production since early march 2017. So about two months now. The problem did manifest twice during that time.\r\n\r\n> Also, why are you recreating the repository every time you want to take a snapshot? That is not needed, you can simply take snapshots once the repository has been created once for the cluster.\r\n\r\nBecause so far we did not have problems with just registering the snapshot repository, whenever we want to create a new snapshot. But we can modify our backup script to be a noop, if the snapshot repository is already created. So instead of 24 times a day, only 1 time a day the endpoint in question is called. So the bug can than only occur 1/24 of the time.\r\n\r\n> Note that you can sidestep the verification process by passing in the request param verify=false, and this may be a good solution for you in the interim [...]\r\n\r\nThanks for pointing out a workaround. But I don't feel comfortable to disable the verification just to avoid a warning.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298706778","html_url":"https://github.com/elastic/elasticsearch/issues/24306#issuecomment-298706778","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24306","id":298706778,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODcwNjc3OA==","user":{"login":"abeyad","id":1631297,"node_id":"MDQ6VXNlcjE2MzEyOTc=","avatar_url":"https://avatars2.githubusercontent.com/u/1631297?v=4","gravatar_id":"","url":"https://api.github.com/users/abeyad","html_url":"https://github.com/abeyad","followers_url":"https://api.github.com/users/abeyad/followers","following_url":"https://api.github.com/users/abeyad/following{/other_user}","gists_url":"https://api.github.com/users/abeyad/gists{/gist_id}","starred_url":"https://api.github.com/users/abeyad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abeyad/subscriptions","organizations_url":"https://api.github.com/users/abeyad/orgs","repos_url":"https://api.github.com/users/abeyad/repos","events_url":"https://api.github.com/users/abeyad/events{/privacy}","received_events_url":"https://api.github.com/users/abeyad/received_events","type":"User","site_admin":false},"created_at":"2017-05-02T17:38:10Z","updated_at":"2017-05-02T17:38:10Z","author_association":"CONTRIBUTOR","body":"> We do use ES 2.4.4 in production since early march 2017. So about two months now. The problem did manifest twice during that time.\r\n\r\nDid the problem manifest before upgrading to 2.4.4 as well?  Or only since you've upgraded to 2.4.4?  And from which version did you upgrade?\r\n\r\nAlso, did you notice a new master node election around the time the failure occurred?  The logs would've indicated something like \"new master\"","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/298842911","html_url":"https://github.com/elastic/elasticsearch/issues/24306#issuecomment-298842911","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24306","id":298842911,"node_id":"MDEyOklzc3VlQ29tbWVudDI5ODg0MjkxMQ==","user":{"login":"apepper","id":86275,"node_id":"MDQ6VXNlcjg2Mjc1","avatar_url":"https://avatars0.githubusercontent.com/u/86275?v=4","gravatar_id":"","url":"https://api.github.com/users/apepper","html_url":"https://github.com/apepper","followers_url":"https://api.github.com/users/apepper/followers","following_url":"https://api.github.com/users/apepper/following{/other_user}","gists_url":"https://api.github.com/users/apepper/gists{/gist_id}","starred_url":"https://api.github.com/users/apepper/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apepper/subscriptions","organizations_url":"https://api.github.com/users/apepper/orgs","repos_url":"https://api.github.com/users/apepper/repos","events_url":"https://api.github.com/users/apepper/events{/privacy}","received_events_url":"https://api.github.com/users/apepper/received_events","type":"User","site_admin":false},"created_at":"2017-05-03T07:42:23Z","updated_at":"2017-05-03T07:42:23Z","author_association":"CONTRIBUTOR","body":"> Did the problem manifest before upgrading to 2.4.4 as well? Or only since you've upgraded to 2.4.4? And from which version did you upgrade?\r\n\r\nThe problem manifested after upgrading to `2.4.4`. Previously we had several different versions of elasticsearch running, all without problems regarding S3 snapshots. The latest prior to `2.4.4` was `2.4.1`.\r\n\r\n> Also, did you notice a new master node election around the time the failure occurred? The logs would've indicated something like \"new master\"\r\n\r\nAccording to the logs there where no master node election during that time. The specific cluster consists of three AWS EC2 instances running in a VPC and is running for about two months now. During that time no instance was rebooted or added after the initial start. So master node election is probably not triggering this bug.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/376179410","html_url":"https://github.com/elastic/elasticsearch/issues/24306#issuecomment-376179410","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24306","id":376179410,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NjE3OTQxMA==","user":{"login":"tlrx","id":642733,"node_id":"MDQ6VXNlcjY0MjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/642733?v=4","gravatar_id":"","url":"https://api.github.com/users/tlrx","html_url":"https://github.com/tlrx","followers_url":"https://api.github.com/users/tlrx/followers","following_url":"https://api.github.com/users/tlrx/following{/other_user}","gists_url":"https://api.github.com/users/tlrx/gists{/gist_id}","starred_url":"https://api.github.com/users/tlrx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tlrx/subscriptions","organizations_url":"https://api.github.com/users/tlrx/orgs","repos_url":"https://api.github.com/users/tlrx/repos","events_url":"https://api.github.com/users/tlrx/events{/privacy}","received_events_url":"https://api.github.com/users/tlrx/received_events","type":"User","site_admin":false},"created_at":"2018-03-26T14:10:48Z","updated_at":"2018-03-26T14:10:48Z","author_association":"MEMBER","body":"We failed to reproduce this issue on our side and we didn't have any other bug report like this since this issue was created. We talked about this with @ywelsch today and we suggest that you upgrade your cluster to a more recent version (2.4 is not supported anymore) and see if the issue appears again.","performed_via_github_app":null}]