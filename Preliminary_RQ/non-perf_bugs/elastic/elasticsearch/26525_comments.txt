[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/327623220","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-327623220","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":327623220,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNzYyMzIyMA==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2017-09-06T21:55:40Z","updated_at":"2017-09-06T21:55:40Z","author_association":"MEMBER","body":"I believe this is related to #26012","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/327733857","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-327733857","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":327733857,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNzczMzg1Nw==","user":{"login":"costin","id":76245,"node_id":"MDQ6VXNlcjc2MjQ1","avatar_url":"https://avatars3.githubusercontent.com/u/76245?v=4","gravatar_id":"","url":"https://api.github.com/users/costin","html_url":"https://github.com/costin","followers_url":"https://api.github.com/users/costin/followers","following_url":"https://api.github.com/users/costin/following{/other_user}","gists_url":"https://api.github.com/users/costin/gists{/gist_id}","starred_url":"https://api.github.com/users/costin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/costin/subscriptions","organizations_url":"https://api.github.com/users/costin/orgs","repos_url":"https://api.github.com/users/costin/repos","events_url":"https://api.github.com/users/costin/events{/privacy}","received_events_url":"https://api.github.com/users/costin/received_events","type":"User","site_admin":false},"created_at":"2017-09-07T08:46:53Z","updated_at":"2017-09-07T08:47:17Z","author_association":"MEMBER","body":"_updated the initial request and added minimal formatting to make the post readable_\r\n\r\nThanks @dakrone, indeed it looks like a duplicate of #26012 ; @hexinw can you please confirm?\r\n\r\nCheers,\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/327865136","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-327865136","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":327865136,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNzg2NTEzNg==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-07T17:14:33Z","updated_at":"2017-09-07T17:14:33Z","author_association":"NONE","body":"<img width=\"632\" alt=\"screen shot 2017-09-07 at 10 11 26 am\" src=\"https://user-images.githubusercontent.com/13575408/30175870-292d0c48-93b5-11e7-9475-e27b6024c5c7.png\">\r\n\r\nThis is the eclipse MAT report on the heap usage when elastic search crashed. Yes it looks like a dupe to #26012.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328622506","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-328622506","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":328622506,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODYyMjUwNg==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-11T18:47:26Z","updated_at":"2017-09-11T18:47:26Z","author_association":"NONE","body":"I changed the \"min_doc_count\" to avoid empty bucks in date histogram. I still run into the OOM problem. So I am actually not sure if solution discussed in #26012 would solve this problem. @colings86 @jimczi, could you comment?\r\n\r\nNote I also tried out the \"global_ordinals_hash\" mentioned in #24359 and it didn't help, either\r\n\r\nGET nusights_metric_cfs_collector_memcpu_stats_2017_*/_search\r\n{\r\n  \"size\" : 0,\r\n  \"query\" : {\r\n    \"bool\" : {\r\n      \"filter\" : [\r\n        {\r\n          \"range\" : {\r\n            \"timestamp\" : {\r\n              \"from\" : \"1504126054711\",\r\n              \"to\" : \"1504298854711\",\r\n              \"include_lower\" : true,\r\n              \"include_upper\" : true,\r\n              \"format\" : \"epoch_millis\",\r\n              \"boost\" : 1.0\r\n            }\r\n          }\r\n        },\r\n        {\r\n          \"query_string\" : {\r\n            \"query\" : \"(process: cfs)\",\r\n            \"fields\" : [ ],\r\n            \"use_dis_max\" : true,\r\n            \"tie_breaker\" : 0.0,\r\n            \"default_operator\" : \"or\",\r\n            \"auto_generate_phrase_queries\" : false,\r\n            \"max_determinized_states\" : 10000,\r\n            \"enable_position_increments\" : true,\r\n            \"fuzziness\" : \"AUTO\",\r\n            \"fuzzy_prefix_length\" : 0,\r\n            \"fuzzy_max_expansions\" : 50,\r\n            \"phrase_slop\" : 0,\r\n            \"analyze_wildcard\" : true,\r\n            \"escape\" : false,\r\n            \"split_on_whitespace\" : true,\r\n            \"boost\" : 1.0\r\n          }\r\n        }\r\n      ],\r\n      \"disable_coord\" : false,\r\n      \"adjust_pure_negative\" : true,\r\n      \"boost\" : 1.0\r\n    }\r\n  },\r\n  \"aggregations\" : {\r\n    \"4\" : {\r\n      \"terms\" : {\r\n        \"field\" : \"node_uuid\",\r\n        \"size\" : 500,\r\n        \"execution_hint\": \"global_ordinals_hash\",\r\n        \"min_doc_count\" : 1,\r\n        \"shard_min_doc_count\" : 0,\r\n        \"show_term_doc_count_error\" : false,\r\n        \"order\" : [\r\n          {\r\n            \"_term\" : \"desc\"\r\n          }\r\n        ]\r\n      },\r\n      \"aggregations\" : {\r\n        \"2\" : {\r\n          \"date_histogram\" : {\r\n            \"field\" : \"timestamp\",\r\n            \"format\" : \"epoch_millis\",\r\n            \"interval\" : \"5m\",\r\n            \"offset\" : 0,\r\n            \"order\" : {\r\n              \"_key\" : \"asc\"\r\n            },\r\n            \"keyed\" : false,\r\n            \"min_doc_count\" : 1,\r\n            \"extended_bounds\" : {\r\n              \"min\" : \"1504126054711\",\r\n              \"max\" : \"1504298854711\"\r\n            }\r\n          },\r\n          \"aggregations\" : {\r\n            \"1\" : {\r\n                \"cardinality\": {\r\n                  \"field\": \"pid\"\r\n                }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328683135","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-328683135","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":328683135,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODY4MzEzNQ==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-11T23:03:31Z","updated_at":"2017-09-11T23:10:46Z","author_association":"NONE","body":"Noticed this cardinality aggregator memory usage model on current release.\r\n\r\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html\r\n\r\n>>> Start excerpt >>>\r\nFor a precision threshold of c, the implementation that we are using requires about c * 8 bytes.\r\n>>> End excerpt >>>\r\n\r\nIf this is the case, memory estimate for default 'prevision_threshold' of 30000 * 8 ~= 240KB. This is way bigger than the 5KB default weight in AggregatorBase. Does it mean we need to tweak the circuit breaker adjustment in CardinalityAggregator as well?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328683423","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-328683423","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":328683423,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODY4MzQyMw==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-11T23:05:21Z","updated_at":"2017-09-11T23:05:21Z","author_association":"NONE","body":"Also feel free to close this issue if you guys think it is a dup. I went through some of the initial issues report in GitHub and Elastic discussions before opening up this ticket. Saw something similar but not sure they are exactly the same.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328684968","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-328684968","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":328684968,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODY4NDk2OA==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2017-09-11T23:14:52Z","updated_at":"2017-09-11T23:14:52Z","author_association":"MEMBER","body":"> This is way bigger than the 5KB default weight in AggregatorBase. Does it mean we need to tweak the circuit breaker adjustment in CardinalityAggregator as well?\r\n\r\nThe `CardinalityAggregator` directly uses `BigArrays` which has its own circuit breaking accounting, it should not need to be adjusted.\r\n\r\nI believe this particular issue has to do with top-level histogram aggregations (of course I could be mistaken)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328795355","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-328795355","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":328795355,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODc5NTM1NQ==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T09:22:35Z","updated_at":"2017-09-12T09:22:35Z","author_association":"MEMBER","body":"Adding `global_ordinals_hash` to the request will only prevent the OOME in the cases where the query restricts the number of terms buckets created to a small number compared with the total number of terms int he field (i.e. the sparse terms case). If the query matches a lot of terms then it still has the potential to cause problems. I think we should try to find out if this is the case here.\r\n\r\n@hexinw could you run the following request and paste the output on this issue?\r\n```\r\nGET nusights_metric_cfs_collector_memcpu_stats_2017_*/_search{  \r\n   \"size\":0,\r\n   \"query\":{  \r\n      \"bool\":{  \r\n         \"filter\":[  \r\n            {  \r\n               \"range\":{  \r\n                  \"timestamp\":{  \r\n                     \"from\":\"1504126054711\",\r\n                     \"to\":\"1504298854711\",\r\n                     \"include_lower\":true,\r\n                     \"include_upper\":true,\r\n                     \"format\":\"epoch_millis\",\r\n                     \"boost\":1.0\r\n                  }\r\n               }\r\n            },\r\n            {  \r\n               \"query_string\":{  \r\n                  \"query\":\"(process: cfs)\",\r\n                  \"fields\":[  \r\n\r\n                  ],\r\n                  \"use_dis_max\":true,\r\n                  \"tie_breaker\":0.0,\r\n                  \"default_operator\":\"or\",\r\n                  \"auto_generate_phrase_queries\":false,\r\n                  \"max_determinized_states\":10000,\r\n                  \"enable_position_increments\":true,\r\n                  \"fuzziness\":\"AUTO\",\r\n                  \"fuzzy_prefix_length\":0,\r\n                  \"fuzzy_max_expansions\":50,\r\n                  \"phrase_slop\":0,\r\n                  \"analyze_wildcard\":true,\r\n                  \"escape\":false,\r\n                  \"split_on_whitespace\":true,\r\n                  \"boost\":1.0\r\n               }\r\n            }\r\n         ],\r\n         \"disable_coord\":false,\r\n         \"adjust_pure_negative\":true,\r\n         \"boost\":1.0\r\n      }\r\n   },\r\n   \"aggregations\":{  \r\n      \"node_uuid_cardinality\":{  \r\n         \"cardinality\":{  \r\n            \"field\":\"node_uuid\"\r\n         }\r\n      }\r\n   }\r\n}\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328807769","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-328807769","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":328807769,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODgwNzc2OQ==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T10:11:31Z","updated_at":"2017-09-12T10:19:40Z","author_association":"MEMBER","body":"I don't think this is related to https://github.com/elastic/elasticsearch/issues/26012. \r\nAt least I don't think the solution proposed in #26012 could catch this better than the circuit breaker.\r\n\r\nThe circuit breaker for requests (`indices.breaker.request.limit`) defaults to 60% of the heap.\r\nWhat size of heap are you using in your nodes ? 60% is quite high so there is a chance that you hit an OOME before that just because your node needs more than 40% of the total heap to survive.\r\nIf you want to protect against memory intensive aggregation you could try to lower this value. \r\nThe aggregation in the example can create at most `144,000` buckets (500 for the terms agg * 288 for the five minutes interval). That's big if all buckets need to compute a cardinality.\r\nThere are other data structures that are not count in the circuit breaker but I doubt that they are the main cause. The cardinality agg alone would need `38`GB of accounted memory to finish (with 144,000 buckets). \r\nCan you try to adjust the `indices.breaker.request.limit` and report if it worked ?\r\n\r\n\r\n \r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328813009","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-328813009","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":328813009,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODgxMzAwOQ==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T10:35:01Z","updated_at":"2017-09-12T10:35:01Z","author_association":"MEMBER","body":"The default value for the `precision_threshold` is `3000` and not `30000` so the cardinality agg would need `3.8GB` and not `38GB` like said in the previous comment. Although it's still big and accounts for a single query so I guess that you have a small heap size or a lot of concurrent requests ?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328899543","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-328899543","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":328899543,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODg5OTU0Mw==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T16:01:23Z","updated_at":"2017-09-12T16:01:23Z","author_association":"NONE","body":"@colings86 Yes there are more than 500 distinct nodes in the indices. \r\n\r\nThe output to the node cardinality aggregation query:\r\n\r\n{\r\n  \"took\": 2,\r\n  \"timed_out\": false,\r\n  \"_shards\": {\r\n    \"total\": 10,\r\n    \"successful\": 10,\r\n    \"failed\": 0\r\n  },\r\n  \"hits\": {\r\n    \"total\": 237294,\r\n    \"max_score\": 0,\r\n    \"hits\": []\r\n  },\r\n  \"aggregations\": {\r\n    \"node_uuid_cardinality\": {\r\n      \"value\": 512\r\n    }\r\n  }\r\n}","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328900987","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-328900987","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":328900987,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODkwMDk4Nw==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T16:06:17Z","updated_at":"2017-09-12T16:06:17Z","author_association":"NONE","body":"@jimczi Yes I have a pretty small elastic search single node cluster with small memory footprint (8G memory and 4G heap for ES), just for experiment on the date histogram and cardinality aggregation stuff. Not for production yet. Just a bit unprepared to realize that the system can consume a large memory consumption by the aggregations.\r\n\r\nSorry for the typo, with 3000 precision, 24KB per bucket consumption is not bad to me.  \r\n\r\nI think reducing indices.breaker.request.limit should help for the circuit breaker to come in. Let me confirm.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328910771","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-328910771","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":328910771,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODkxMDc3MQ==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T16:38:26Z","updated_at":"2017-09-12T16:38:26Z","author_association":"NONE","body":"I changed indices.breaker.request.limit to 30% and it triggered the circuit break.\r\n\r\n{\r\n  \"error\": {\r\n    \"root_cause\": [\r\n      {\r\n        \"type\": \"circuit_breaking_exception\",\r\n        \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556064832/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n        \"bytes_wanted\": 2556064832,\r\n        \"bytes_limit\": 2556061286\r\n      },\r\n      {\r\n        \"type\": \"circuit_breaking_exception\",\r\n        \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556080808/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n        \"bytes_wanted\": 2556080808,\r\n        \"bytes_limit\": 2556061286\r\n      },\r\n      {\r\n        \"type\": \"circuit_breaking_exception\",\r\n        \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556130048/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n        \"bytes_wanted\": 2556130048,\r\n        \"bytes_limit\": 2556061286\r\n      },\r\n      {\r\n        \"type\": \"circuit_breaking_exception\",\r\n        \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556097504/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n        \"bytes_wanted\": 2556097504,\r\n        \"bytes_limit\": 2556061286\r\n      },\r\n      {\r\n        \"type\": \"circuit_breaking_exception\",\r\n        \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556080680/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n        \"bytes_wanted\": 2556080680,\r\n        \"bytes_limit\": 2556061286\r\n      },\r\n      {\r\n        \"type\": \"circuit_breaking_exception\",\r\n        \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556064960/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n        \"bytes_wanted\": 2556064960,\r\n        \"bytes_limit\": 2556061286\r\n      },\r\n      {\r\n        \"type\": \"circuit_breaking_exception\",\r\n        \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556064792/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n        \"bytes_wanted\": 2556064792,\r\n        \"bytes_limit\": 2556061286\r\n      },\r\n      {\r\n        \"type\": \"circuit_breaking_exception\",\r\n        \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556097608/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n        \"bytes_wanted\": 2556097608,\r\n        \"bytes_limit\": 2556061286\r\n      },\r\n      {\r\n        \"type\": \"circuit_breaking_exception\",\r\n        \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556097280/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n        \"bytes_wanted\": 2556097280,\r\n        \"bytes_limit\": 2556061286\r\n      }\r\n    ],\r\n    \"type\": \"search_phase_execution_exception\",\r\n    \"reason\": \"all shards failed\",\r\n    \"phase\": \"query\",\r\n    \"grouped\": true,\r\n    \"failed_shards\": [\r\n      {\r\n        \"shard\": 0,\r\n        \"index\": \"nusights_metric_cfs_collector_memcpu_stats_2017_08\",\r\n        \"node\": \"bfIPOYfyTg-GTU5CKY9ggA\",\r\n        \"reason\": {\r\n          \"type\": \"circuit_breaking_exception\",\r\n          \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556064832/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n          \"bytes_wanted\": 2556064832,\r\n          \"bytes_limit\": 2556061286\r\n        }\r\n      },\r\n      {\r\n        \"shard\": 0,\r\n        \"index\": \"nusights_metric_cfs_collector_memcpu_stats_2017_09\",\r\n        \"node\": \"bfIPOYfyTg-GTU5CKY9ggA\",\r\n        \"reason\": {\r\n          \"type\": \"circuit_breaking_exception\",\r\n          \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556080808/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n          \"bytes_wanted\": 2556080808,\r\n          \"bytes_limit\": 2556061286\r\n        }\r\n      },\r\n      {\r\n        \"shard\": 1,\r\n        \"index\": \"nusights_metric_cfs_collector_memcpu_stats_2017_08\",\r\n        \"node\": \"bfIPOYfyTg-GTU5CKY9ggA\",\r\n        \"reason\": {\r\n          \"type\": \"circuit_breaking_exception\",\r\n          \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556130048/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n          \"bytes_wanted\": 2556130048,\r\n          \"bytes_limit\": 2556061286\r\n        }\r\n      },\r\n      {\r\n        \"shard\": 1,\r\n        \"index\": \"nusights_metric_cfs_collector_memcpu_stats_2017_09\",\r\n        \"node\": \"bfIPOYfyTg-GTU5CKY9ggA\",\r\n        \"reason\": {\r\n          \"type\": \"circuit_breaking_exception\",\r\n          \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556097504/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n          \"bytes_wanted\": 2556097504,\r\n          \"bytes_limit\": 2556061286\r\n        }\r\n      },\r\n      {\r\n        \"shard\": 2,\r\n        \"index\": \"nusights_metric_cfs_collector_memcpu_stats_2017_09\",\r\n        \"node\": \"bfIPOYfyTg-GTU5CKY9ggA\",\r\n        \"reason\": {\r\n          \"type\": \"circuit_breaking_exception\",\r\n          \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556080680/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n          \"bytes_wanted\": 2556080680,\r\n          \"bytes_limit\": 2556061286\r\n        }\r\n      },\r\n      {\r\n        \"shard\": 3,\r\n        \"index\": \"nusights_metric_cfs_collector_memcpu_stats_2017_08\",\r\n        \"node\": \"bfIPOYfyTg-GTU5CKY9ggA\",\r\n        \"reason\": {\r\n          \"type\": \"circuit_breaking_exception\",\r\n          \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556064960/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n          \"bytes_wanted\": 2556064960,\r\n          \"bytes_limit\": 2556061286\r\n        }\r\n      },\r\n      {\r\n        \"shard\": 3,\r\n        \"index\": \"nusights_metric_cfs_collector_memcpu_stats_2017_09\",\r\n        \"node\": \"bfIPOYfyTg-GTU5CKY9ggA\",\r\n        \"reason\": {\r\n          \"type\": \"circuit_breaking_exception\",\r\n          \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556064792/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n          \"bytes_wanted\": 2556064792,\r\n          \"bytes_limit\": 2556061286\r\n        }\r\n      },\r\n      {\r\n        \"shard\": 4,\r\n        \"index\": \"nusights_metric_cfs_collector_memcpu_stats_2017_08\",\r\n        \"node\": \"bfIPOYfyTg-GTU5CKY9ggA\",\r\n        \"reason\": {\r\n          \"type\": \"circuit_breaking_exception\",\r\n          \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556097608/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n          \"bytes_wanted\": 2556097608,\r\n          \"bytes_limit\": 2556061286\r\n        }\r\n      },\r\n      {\r\n        \"shard\": 4,\r\n        \"index\": \"nusights_metric_cfs_collector_memcpu_stats_2017_09\",\r\n        \"node\": \"bfIPOYfyTg-GTU5CKY9ggA\",\r\n        \"reason\": {\r\n          \"type\": \"circuit_breaking_exception\",\r\n          \"reason\": \"[request] Data too large, data for [<reused_arrays>] would be [2556097280/2.3gb], which is larger than the limit of [2556061286/2.3gb]\",\r\n          \"bytes_wanted\": 2556097280,\r\n          \"bytes_limit\": 2556061286\r\n        }\r\n      }\r\n    ]\r\n  },\r\n  \"status\": 503\r\n}","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329571047","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-329571047","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":329571047,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTU3MTA0Nw==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-14T18:34:00Z","updated_at":"2017-09-14T18:34:00Z","author_association":"NONE","body":"I enabled the trace log to understand how the request circuit breaker is doing the heap usage estimation. I noticed that the OOM seems to happen way after the heap usage estimation going down to 0gb. \r\n\r\nNoticed the heap estimation by circuit breaker reaches peak 3.9gb around 10:49:11. It goes down to \r\n0 around 10:49:21,922. But OOM happens in 10:50:42.\r\n\r\nDoes it mean some other circuit breaker logic is required to catch this case? \r\n\r\n\r\n>>>> Heap estimation reaches peak\r\n\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adding [568b][<reused_arrays>] to used bytes [new used: [3.9gb], limit: 5112122572 [4.7gb], estimate: 4188695184 [3.9gb]]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adjusted breaker by [-504] bytes, now [4188694680]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.e.E.TMP            ] [node01-es-dev] [.monitoring-kibana-2-2017.09.14][0] elasticsearch[node01-es-dev][refresh][T#3] TMP:   seg=_5hw(6.5.1):c1 size=0.013 MB [floored]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adding [328b][<reused_arrays>] to used bytes [new used: [3.9gb], limit: 5112122572 [4.7gb], estimate: 4188695008 [3.9gb]]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adjusted breaker by [-296] bytes, now [4188694712]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adding [328b][<reused_arrays>] to used bytes [new used: [3.9gb], limit: 5112122572 [4.7gb], estimate: 4188695040 [3.9gb]]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adjusted breaker by [-296] bytes, now [4188694744]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adding [568b][<reused_arrays>] to used bytes [new used: [3.9gb], limit: 5112122572 [4.7gb], estimate: 4188695312 [3.9gb]]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adjusted breaker by [-504] bytes, now [4188694808]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adding [144kb][<reused_arrays>] to used bytes [new used: [3.9gb], limit: 5112122572 [4.7gb], estimate: 4188842264 [3.9gb]]\r\n[2017-09-14T10:49:11,388][TRACE][o.e.i.b.request          ] [request] Adding [64b][<reused_arrays>] to used bytes [new used: [3.8gb], limit: 5112122572 [4.7gb], estimate: 4184527824 [3.8gb]]\r\n[2017-09-14T10:49:11,388][TRACE][o.e.i.b.request          ] [request] Adding [64b][<reused_arrays>] to used bytes [new used: [3.8gb], limit: 5112122572 [4.7gb], estimate: 4184527824 [3.8gb]]\r\n[2017-09-14T10:49:11,388][TRACE][o.e.i.b.request          ] [request] Adding [64b][<reused_arrays>] to used bytes [new used: [3.8gb], limit: 5112122572 [4.7gb], estimate: 4184527824 [3.8gb]]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adding [64b][<reused_arrays>] to used bytes [new used: [3.9gb], limit: 5112122572 [4.7gb], estimate: 4188842328 [3.9gb]]\r\n[2017-09-14T10:49:11,391][TRACE][o.e.i.b.request          ] [request] Adding [64b][<reused_arrays>] to used bytes [new used: [3.9gb], limit: 5112122572 [4.7gb], estimate: 4188842392 [3.9gb]]\r\n[2017-09-14T10:49:11,392][TRACE][o.e.i.b.request          ] [request] Adjusted breaker by [-64] bytes, now [4188842328]\r\n[2017-09-14T10:49:11,388][TRACE][o.e.i.b.request          ] [request] Adding [64b][<reused_arrays>] to used bytes [new used: [3.8gb], limit: 5112122572 [4.7gb], estimate: 4184527824 [3.8gb]]\r\n[2017-09-14T10:49:11,392][TRACE][o.e.i.b.request          ] [request] Adding [64b][<reused_arrays>] to used bytes [new used: [3.9gb], limit: 5112122572 [4.7gb], estimate: 4188842328 [3.9gb]]\r\n[2017-09-14T10:49:11,388][TRACE][o.e.i.b.request          ] [request] Adding [64b][<reused_arrays>] to used bytes [new used: [3.8gb], limit: 5112122572 [4.7gb], estimate: 4184527824 [3.8gb]]\r\n[2017-09-14T10:49:11,392][TRACE][o.e.i.b.request          ] [request] Adding [64b][<reused_arrays>] to used bytes [new used: [3.9gb],\r\n\r\n>>>> Heap estimation goes down to 0\r\n[2017-09-14T10:49:21,922][TRACE][o.e.i.b.request          ] [request] Adjusted breaker by [-96] bytes, now [0]\r\n[2017-09-14T10:49:21,922][TRACE][o.e.t.TaskManager        ] [node01-es-dev] unregister task for id: 875170\r\n[2017-09-14T10:49:21,922][TRACE][o.e.t.T.tracer           ] [node01-es-dev] [312235][indices:data/read/search[phase/query]] sent response\r\n[2017-09-14T10:49:21,922][TRACE][o.e.t.T.tracer           ] [node01-es-dev] [312235][indices:data/read/search[phase/query]] received response from [{node01-es-dev}{B_AYP0NDRD2LErSiyA1H9Q}{VktFHTHDSZeqjNiVZilxaw}{10.4.244.78}{10.4.244.78:9300}{ml.enabled=true}]\r\n[2017-09-14T10:49:21,922][TRACE][o.e.a.s.TransportSearchAction] [node01-es-dev] got first-phase result from [B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_08][3]\r\n[2017-09-14T10:49:21,922][TRACE][o.e.a.s.TransportSearchAction] [node01-es-dev] [query] Moving to next phase: [fetch], based on results from: [B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_08][0],[B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_09][0],[B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_08][1],[B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_09][1],[B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_08][2],[B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_09][2],[B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_08][3],[B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_09][3],[B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_08][4],[B_AYP0NDRD2LErSiyA1H9Q][nusights_metric_cfs_collector_memcpu_stats_2017_09][4] (cluster state version: 34)\r\n[2017-09-14T10:49:21,929][TRACE][o.e.i.IndexService       ] [node01-es-dev] [.monitoring-kibana-2-2017.09.14] scheduling refresh every 1s\r\n[2017-09-14T10:49:22,429][TRACE][o.e.x.w.t.s.e.TickerScheduleTriggerEngine] [node01-es-dev] checking jobs [2017-09-14T17:49:22.429Z]\r\n\r\n>>>> OOM happens\r\n[2017-09-14T10:50:42,858][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [node01-es-dev] fatal error in thread [elasticsearch[node01-es-dev][search][T#9]], exiting\r\njava.lang.OutOfMemoryError: Java heap space\r\n        at org.elasticsearch.common.util.BigArrays.newByteArray(BigArrays.java:481) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.common.util.BigArrays.newByteArray(BigArrays.java:490) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.metrics.cardinality.HyperLogLogPlusPlus.<init>(HyperLogLogPlusPlus.java:171) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.metrics.cardinality.InternalCardinality.doReduce(InternalCardinality.java:90) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.InternalAggregation.reduce(InternalAggregation.java:119) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:139) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.bucket.histogram.InternalDateHistogram$Bucket.reduce(InternalDateHistogram.java:113) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.bucket.histogram.InternalDateHistogram.reduceBuckets(InternalDateHistogram.java:287) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.bucket.histogram.InternalDateHistogram.doReduce(InternalDateHistogram.java:378) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.InternalAggregation.reduce(InternalAggregation.java:119) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:139) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.bucket.terms.InternalTerms$Bucket.reduce(InternalTerms.java:142) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.bucket.terms.InternalTerms.doReduce(InternalTerms.java:271) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.InternalAggregation.reduce(InternalAggregation.java:119) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:139) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.search.SearchPhaseController.reduceAggs(SearchPhaseController.java:513) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.search.SearchPhaseController.reducedQueryPhase(SearchPhaseController.java:490) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.search.SearchPhaseController.reducedQueryPhase(SearchPhaseController.java:408) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.search.SearchPhaseController$1.reduce(SearchPhaseController.java:725) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.search.FetchSearchPhase.innerRun(FetchSearchPhase.java:102) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.search.FetchSearchPhase.access$000(FetchSearchPhase.java:45) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.search.FetchSearchPhase$1.doRun(FetchSearchPhase.java:87) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:638) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_131]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_131]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]\r\n[2017-09-14T10:50:42,879][TRACE][o.e.i.b.request          ] [request] Adjusted breaker by [16440] bytes, now [21904]\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329590653","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-329590653","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":329590653,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTU5MDY1Mw==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2017-09-14T19:53:00Z","updated_at":"2017-09-14T19:53:00Z","author_association":"MEMBER","body":"Aggregations are performed in two phases. The first phase runs the aggregation on all shards and the second phase reduces the shard responses to a single result. The circuit breaker checks the memory during the first phase but it is not used for the reduce phase in the coordinating node. In your example the node ran out of memory during the reduce phase where memory is not checked.\r\nAll shards were able to perform the aggregation without reaching the circuit breaker limit but the coordinating node was not able to load all the results at once to create the final response.\r\nIn 5.4 we introduced a new option to limit the number of shard results that should be reduced at once on the coordinating node:\r\nhttps://www.elastic.co/guide/en/elasticsearch/reference/5.4/search-request-body.html\r\nYou can try to change the default value (255) in order to reduce the memory needed for the coordinating node. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329837627","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-329837627","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":329837627,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTgzNzYyNw==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-15T16:50:58Z","updated_at":"2017-09-15T17:35:02Z","author_association":"NONE","body":"What is the reason that we don't put circuit breaker in the reduced phase?\r\nOr just catch the OOM exception in reducer stage and return failure to user. This seems a better solution than crashing ES node.\r\n\r\nThe batched_reduce_size parameter is a pretty coarse parameter as different data sets and query type can allow different number of shards to be reduced at a same time.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329951635","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-329951635","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":329951635,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTk1MTYzNQ==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-16T07:18:23Z","updated_at":"2017-09-16T07:24:34Z","author_association":"NONE","body":"Do you guys see any issue for the following code change to catch the OOM in the reduce phase?\r\n\r\n======> git diff core/src/main/java/org/elasticsearch/action/search/FetchSearchPhase.java \r\n```\r\ndiff --git a/core/src/main/java/org/elasticsearch/action/search/FetchSearchPhase.java b/core/src/main/java/org/elasticsearch/action/search/FetchSearchPhase.java\r\nindex 25231ef..d91f5d3 100644\r\n--- a/core/src/main/java/org/elasticsearch/action/search/FetchSearchPhase.java\r\n+++ b/core/src/main/java/org/elasticsearch/action/search/FetchSearchPhase.java\r\n@@ -84,7 +84,12 @@ final class FetchSearchPhase extends SearchPhase {\r\n                 // we do the heavy lifting in this inner run method where we reduce aggs etc. that's why we fork this phase\r\n                 // off immediately instead of forking when we send back the response to the user since there we only need\r\n                 // to merge together the fetched results which is a linear operation.\r\n-                innerRun();\r\n+                try {\r\n+                    innerRun();\r\n+                } catch (OutOfMemoryError exc) {\r\n+                    context.getLogger().warn(\"Catch OOM.\");\r\n+                    throw new SearchPhaseExecutionException(\"FetchSearch\", \"Out of memory error.\", ShardSearchFailure.EMPTY_ARRAY);\r\n+                }\r\n             }\r\n \r\n             @Override\r\n```\r\n====> Now OutOfMemoryError is caught without causing ES node to go down.\r\n\r\n```\r\n[2017-09-16T00:11:12,851][WARN ][o.e.a.s.TransportSearchAction] [node01-es-dev] Catch OOM.\r\n[2017-09-16T00:11:13,404][WARN ][o.e.x.w.e.ExecutionService] [node01-es-dev] Failed to execute watch [CsSKhUSkTPSLM8c56fVsNw_kibana_version_mismatch_5b73b833-8199-4e40-a620-fd7d4ffdff64-2017-09-16T07:10:11.309Z]\r\n[2017-09-16T00:11:13,383][WARN ][r.suppressed             ] path: /nusights_metric_cfs_collector_memcpu_stats_2017_*/_search, params: {index=nusights_metric_cfs_collector_memcpu_stats_2017_*}\r\norg.elasticsearch.action.search.SearchPhaseExecutionException: \r\n        at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:272) [elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at org.elasticsearch.action.search.FetchSearchPhase$1.onFailure(FetchSearchPhase.java:97) [elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:623) [elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39) [elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]\r\nCaused by: org.elasticsearch.action.search.SearchPhaseExecutionException: Out of memory error.\r\n        at org.elasticsearch.action.search.FetchSearchPhase$1.doRun(FetchSearchPhase.java:91) ~[elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:638) ~[elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        ... 3 more\r\n[2017-09-16T00:11:14,381][WARN ][o.e.m.j.JvmGcMonitorService] [node01-es-dev] [gc][227] overhead, spent [10.1s] collecting in the last [11.1s]\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329960838","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-329960838","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":329960838,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTk2MDgzOA==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-09-16T10:42:40Z","updated_at":"2017-09-16T16:51:39Z","author_association":"MEMBER","body":"We are not going to catch OOM, we worked hard to remove catching errors that can not be recovered from: #19231. Dying when the node hits such an error is intentional, the JVM is suspect at that point: #19272.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329979003","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-329979003","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":329979003,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTk3OTAwMw==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-16T16:28:58Z","updated_at":"2017-09-19T15:36:33Z","author_association":"NONE","body":"Sorry I am late to this. Can you elaborate the JVM statement? Do you mean that GC is slow in recycling the memory? Coming from C/C++ world, I'd think it normal for application to handle memory malloc failure. Is there any doc or discussion thread that talk about elastic search memory management model in general? \r\n\r\nJust to recap, I run into two OOM problems in my test.\r\n1) OOM is hit in per shard search phase, where circuit breaker fails to kick in due to request circuit limit is higher than current actual available heap space.\r\n2) OOM happens in the reduce phase on the coordinating node. \r\n\r\nBoth OOMs are bad as they bring down the ES node and cause the ES cluster to not quite usable. And to me it'd be better to fail the query rather than crashing the ES node if we calculate the memory requirement upfront or react to the OOM dynamically.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330046600","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-330046600","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":330046600,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDA0NjYwMA==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2017-09-17T13:53:41Z","updated_at":"2017-09-17T13:53:41Z","author_association":"MEMBER","body":"An out of memory exception can be thrown on any thread, whether executing:\r\n - JDK code\r\n - Elasticsearch server code\r\n - code in a third-party dependency\r\n\r\nWe have zero guarantees that a thread suffering an out of memory exception can gracefully cleanup any shared data structures it was in the middle of manipulating, there might not even be enough memory to gracefully carry out such cleanup. With no guarantees, we have to assume that the state of the JVM is suspect at this point.\r\n\r\nIt gets worse. Imagine threads waiting on an object monitor or a lock held by a thread that suffers an out of memory exception. The offending thread might not be able to notify the monitor or unlock these threads leaving them deadlocked. Trying to deal with this comes at the cost of significantly more complicated code. In fact, even if you tried to employ a strategy for dealing with this, because of the above remarks on shared data structures, there's no guarantee that this can be done safely; we can not unblock a waiting thread and let it start messing with a shared data structure when it's not in a good state.\r\n\r\nSimilar reasoning applies to other errors (stack overflow, etc.). \r\n\r\nIn such a stage, the JVM simply must die with dignity instead of limping along in an unknown unreliable state.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330276294","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-330276294","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":330276294,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDI3NjI5NA==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-18T16:22:38Z","updated_at":"2017-09-18T16:22:38Z","author_association":"NONE","body":"@jasontedor I agreed with you on the OOM decision. Is there any improvement in the talk to pre-estimate memory consumption (I understand it is still a best effort) and fail the query request rather than letting OOM kick in to bring down the ES node?\r\n\r\nAlso with respect to the circuit breaker setting, do you guys see any value in making the static limit a dynamic setting so the limit is dynamically decided by the time how much heap is available?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/332331172","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-332331172","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":332331172,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMjMzMTE3Mg==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2017-09-26T20:49:05Z","updated_at":"2017-09-26T20:49:05Z","author_association":"MEMBER","body":"> Is there any improvement in the talk to pre-estimate memory consumption (I understand it is still a best effort) and fail the query request rather than letting OOM kick in to bring down the ES node?\r\n\r\nWe're definitely always interested in improving the pre-estimation of memory consumption, and adding more circuit breakers as needed.\r\n\r\n> Also with respect to the circuit breaker setting, do you guys see any value in making the static limit a dynamic setting so the limit is dynamically decided by the time how much heap is available?\r\n\r\nI think this would be too hard to debug since it would vary widely whether the node could handle the request. It's also impossible to know whether a heap at 79% usage could easily be GC'd to 20% usage, or whether all the objects are live and cannot be GC'd. I'm in favor of a static (albeit configurable) limit on breakers.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/332337722","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-332337722","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":332337722,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMjMzNzcyMg==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-26T21:12:13Z","updated_at":"2017-09-26T21:12:13Z","author_association":"NONE","body":"Thanks @dakrone What was the reason we don't have circuit breaker in reduced phase currently? Actually why is reducing phase taking more memory?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/332343607","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-332343607","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":332343607,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMjM0MzYwNw==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2017-09-26T21:36:44Z","updated_at":"2017-09-26T21:36:44Z","author_association":"MEMBER","body":"> What was the reason we don't have circuit breaker in reduced phase currently?\r\n\r\nIt simply hasn't been added, I don't think there's a reason anyone purposely didn't add one there.\r\n\r\n> Actually why is reducing phase taking more memory?\r\n\r\nThe reducing phase usually does not take more memory, this is likely the reason why a circuit breaker hasn't been proposed for this prior to this. I haven't seen any other instances where a node was getting an OOME during the reduction phase.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/332345014","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-332345014","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":332345014,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMjM0NTAxNA==","user":{"login":"hexinw","id":13575408,"node_id":"MDQ6VXNlcjEzNTc1NDA4","avatar_url":"https://avatars1.githubusercontent.com/u/13575408?v=4","gravatar_id":"","url":"https://api.github.com/users/hexinw","html_url":"https://github.com/hexinw","followers_url":"https://api.github.com/users/hexinw/followers","following_url":"https://api.github.com/users/hexinw/following{/other_user}","gists_url":"https://api.github.com/users/hexinw/gists{/gist_id}","starred_url":"https://api.github.com/users/hexinw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexinw/subscriptions","organizations_url":"https://api.github.com/users/hexinw/orgs","repos_url":"https://api.github.com/users/hexinw/repos","events_url":"https://api.github.com/users/hexinw/events{/privacy}","received_events_url":"https://api.github.com/users/hexinw/received_events","type":"User","site_admin":false},"created_at":"2017-09-26T21:43:11Z","updated_at":"2017-09-26T21:43:11Z","author_association":"NONE","body":"The stack trace I posted 11 days back is a OOM in reduced phase.\r\n\r\n```\r\n[2017-09-16T00:11:12,851][WARN ][o.e.a.s.TransportSearchAction] [node01-es-dev] Catch OOM.\r\n[2017-09-16T00:11:13,404][WARN ][o.e.x.w.e.ExecutionService] [node01-es-dev] Failed to execute watch [CsSKhUSkTPSLM8c56fVsNw_kibana_version_mismatch_5b73b833-8199-4e40-a620-fd7d4ffdff64-2017-09-16T07:10:11.309Z]\r\n[2017-09-16T00:11:13,383][WARN ][r.suppressed             ] path: /nusights_metric_cfs_collector_memcpu_stats_2017_*/_search, params: {index=nusights_metric_cfs_collector_memcpu_stats_2017_*}\r\norg.elasticsearch.action.search.SearchPhaseExecutionException: \r\n        at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:272) [elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at org.elasticsearch.action.search.FetchSearchPhase$1.onFailure(FetchSearchPhase.java:97) [elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:623) [elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39) [elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]\r\nCaused by: org.elasticsearch.action.search.SearchPhaseExecutionException: Out of memory error.\r\n        at org.elasticsearch.action.search.FetchSearchPhase$1.doRun(FetchSearchPhase.java:91) ~[elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:638) ~[elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-5.4.2.jar:5.4.2-SNAPSHOT]\r\n        ... 3 more\r\n[2017-09-16T00:11:14,381][WARN ][o.e.m.j.JvmGcMonitorService] [node01-es-dev] [gc][227] overhead, spent [10.1s] collecting in the last [11.1s]\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/528776377","html_url":"https://github.com/elastic/elasticsearch/issues/26525#issuecomment-528776377","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26525","id":528776377,"node_id":"MDEyOklzc3VlQ29tbWVudDUyODc3NjM3Nw==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2019-09-06T09:06:53Z","updated_at":"2019-09-06T09:06:53Z","author_association":"MEMBER","body":"A part of the issue described here has been tackled in https://github.com/elastic/elasticsearch/pull/27581. Enhancing request-level circuit-breaking on the coordinating node is tracked in #37182 and thus I'm closing the issue here in favor or #37182.","performed_via_github_app":null}]