{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/8397","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8397/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8397/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8397/events","html_url":"https://github.com/elastic/elasticsearch/issues/8397","id":48142101,"node_id":"MDU6SXNzdWU0ODE0MjEwMQ==","number":8397,"title":"logstash processors keep dying with \"nil?\" redirect?","user":{"login":"Vehyla","id":4624840,"node_id":"MDQ6VXNlcjQ2MjQ4NDA=","avatar_url":"https://avatars3.githubusercontent.com/u/4624840?v=4","gravatar_id":"","url":"https://api.github.com/users/Vehyla","html_url":"https://github.com/Vehyla","followers_url":"https://api.github.com/users/Vehyla/followers","following_url":"https://api.github.com/users/Vehyla/following{/other_user}","gists_url":"https://api.github.com/users/Vehyla/gists{/gist_id}","starred_url":"https://api.github.com/users/Vehyla/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Vehyla/subscriptions","organizations_url":"https://api.github.com/users/Vehyla/orgs","repos_url":"https://api.github.com/users/Vehyla/repos","events_url":"https://api.github.com/users/Vehyla/events{/privacy}","received_events_url":"https://api.github.com/users/Vehyla/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2014-11-07T23:11:05Z","updated_at":"2014-11-08T12:17:49Z","closed_at":"2014-11-08T12:17:49Z","author_association":"NONE","active_lock_reason":null,"body":"All of our logs go to a kafka cluster, then we have some logstash processors that push them into elasticsearch. We keep noticing a weird error, then the processor tends to die. The process is up mind you. But the CPU/Load goes to about 1% and there is almost no data going in our out of the ethernet port. I believe elasticsearch is telling the processor to go to a server, but is sending bad or missing data about said server. But that's just my theory.\n\nSome info that will hopefully help shine a lot on the problem.\n\nrpm:\nlogstash-1.4.2-1_2c0f5a1.noarch\nlogstash-kafka-1.2.1-1.noarch\n\nMy conf:\ninput {\nkafka {\nzk_connect => \"zoo01:2111,zoo02:2111,zoo03:2111\"\ngroup_id => \"access\"\ntopic_id => \"prod.access\"\nconsumer_threads => 1\n}\n}\n\nfilter {\nmutate {\nremove_field => [ \"_type\", \"_id\", \"_index\", \"logdate\" ]\n}\n\nif [type] == \"apache_access\" {\nruby {\ncode => 'event[\"msec\"] = event[\"usec\"] / 1000.0 if event[\"usec\"]'\nremove_field => [ \"usec\" ]\n}\n} \n}\n\noutput {\nelasticsearch {\nhost => \"es-access\"\nport => \"9200\"\nprotocol => \"http\"\nflush_size => '10000'\nindex => \"%{type}-%{+YYYY.MM.dd}\"\ncluster => \"elastic-access\"\nworkers => \"4\"\n}\n}\n\nThe error we are seeing:\n{:timestamp=>\"2014-11-07T00:04:19.463000+0000\", :message=>\"Failed to flush outgoing items\", :outgoing_count=>246, :exception=>#, :backtrace=>[\"/opt/logstash/vendor/bundle/jruby/1.9/gems/ftw-0.0.39/lib/ftw/agent.rb:336:in execute'\", \"/opt/logstash/vendor/bundle/jruby/1.9/gems/ftw-0.0.39/lib/ftw/agent.rb:217:inpost!'\", \"/opt/logstash/lib/logstash/outputs/elasticsearch/protocol.rb:106:in bulk_ftw'\", \"/opt/logstash/lib/logstash/outputs/elasticsearch/protocol.rb:80:inbulk'\", \"/opt/logstash/lib/logstash/outputs/elasticsearch.rb:315:in flush'\", \"/opt/logstash/vendor/bundle/jruby/1.9/gems/stud-0.0.17/lib/stud/buffer.rb:219:inbuffer_flush'\", \"org/jruby/RubyHash.java:1339:in each'\", \"/opt/logstash/vendor/bundle/jruby/1.9/gems/stud-0.0.17/lib/stud/buffer.rb:216:inbuffer_flush'\", \"/opt/logstash/vendor/bundle/jruby/1.9/gems/stud-0.0.17/lib/stud/buffer.rb:193:in buffer_flush'\", \"/opt/logstash/vendor/bundle/jruby/1.9/gems/stud-0.0.17/lib/stud/buffer.rb:159:inbuffer_receive'\", \"/opt/logstash/lib/logstash/outputs/elasticsearch.rb:311:in receive'\", \"/opt/logstash/lib/logstash/outputs/base.rb:86:inhandle'\", \"/opt/logstash/lib/logstash/outputs/base.rb:78:in `worker_setup'\"], :level=>:warn}\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}