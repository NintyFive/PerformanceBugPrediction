{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/61843","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/61843/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/61843/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/61843/events","html_url":"https://github.com/elastic/elasticsearch/issues/61843","id":690925947,"node_id":"MDU6SXNzdWU2OTA5MjU5NDc=","number":61843,"title":"Reduced contention in translog add operation","user":{"login":"nitinverma2510","id":1314643,"node_id":"MDQ6VXNlcjEzMTQ2NDM=","avatar_url":"https://avatars1.githubusercontent.com/u/1314643?v=4","gravatar_id":"","url":"https://api.github.com/users/nitinverma2510","html_url":"https://github.com/nitinverma2510","followers_url":"https://api.github.com/users/nitinverma2510/followers","following_url":"https://api.github.com/users/nitinverma2510/following{/other_user}","gists_url":"https://api.github.com/users/nitinverma2510/gists{/gist_id}","starred_url":"https://api.github.com/users/nitinverma2510/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nitinverma2510/subscriptions","organizations_url":"https://api.github.com/users/nitinverma2510/orgs","repos_url":"https://api.github.com/users/nitinverma2510/repos","events_url":"https://api.github.com/users/nitinverma2510/events{/privacy}","received_events_url":"https://api.github.com/users/nitinverma2510/received_events","type":"User","site_admin":false},"labels":[{"id":836542781,"node_id":"MDU6TGFiZWw4MzY1NDI3ODE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Engine","name":":Distributed/Engine","color":"0e8a16","default":false,"description":"Anything around managing Lucene and the Translog in an open shard."},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null},{"id":1967496670,"node_id":"MDU6TGFiZWwxOTY3NDk2Njcw","url":"https://api.github.com/repos/elastic/elasticsearch/labels/Team:Distributed","name":"Team:Distributed","color":"fef2c0","default":false,"description":"Meta label for distributed team"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2020-09-02T11:01:04Z","updated_at":"2020-10-07T01:35:41Z","closed_at":"2020-10-07T01:35:40Z","author_association":"NONE","active_lock_reason":null,"body":"Hi Team,\r\n\r\nI did a quick and dirty POC to reduce contention in `Translog.add()` operation so as to allow write threads to parallelize the lucene indexing work within a bulk request instead of being blocked on this critical section.\r\n\r\nI did the POC only for indexing flow and I achieved this by introducing a `LinkedBlockingQueue` in `TranslogWriter` to queue `Translog.add` tasks. I added a separate queue processor to batch and add the data to `BufferedChannelOutputStream` on a separate single thread instead of doing it synchronously as it happens today. Additionally, I introduced callback listeners to let `TransportShardBulkAction` class know that all the individual requests within a *bulk* have been written to translog and a translog sync call can now be issued.\r\n\r\nI used customized *nyc_taxis* rally track with a consistent search, range and aggregation query traffic and different number of parallel bulk indexing clients to benchmark the improvement, and saw below results.\r\n\r\n**ES version:** 7.4\r\n**Data nodes:** 4 nodes with 32 CPUs each (i3.8xlarge)\r\n**Shard count:** 4 primaries with 1 replica each\r\n\r\n*Consistent traffic of 350 search requests/second, 15 aggregation requests/second and 14 range queries/second.*\r\n\r\n| Parallel indexing clients  | Max indexing throughput without optimization| Max. indexing throughput with optimization | Percentage improvement |\r\n| ------------- | ------------- |-------|-------------|\r\n| 16  | 198447 docs/s | 205659 docs/s | 3.63% |\r\n| 32  | 222809 docs/s | 253013 docs/s | 13.55% |\r\n| 64  | 228919 docs/s | 268094 docs/s | 17.11% |\r\n| 128  | 230343 docs/s | 272445 docs/s | 18.27% |\r\n\r\n**Question to ES team:**\r\n\r\nWhile browsing through open PRs, I noticed that @tbrooks8 already has PR #61562 open for same optimization and I was wondering if ES team is already actively working on this change or is it just experimental and the team is looking for community contribution?","closed_by":{"login":"tbrooks8","id":862472,"node_id":"MDQ6VXNlcjg2MjQ3Mg==","avatar_url":"https://avatars3.githubusercontent.com/u/862472?v=4","gravatar_id":"","url":"https://api.github.com/users/tbrooks8","html_url":"https://github.com/tbrooks8","followers_url":"https://api.github.com/users/tbrooks8/followers","following_url":"https://api.github.com/users/tbrooks8/following{/other_user}","gists_url":"https://api.github.com/users/tbrooks8/gists{/gist_id}","starred_url":"https://api.github.com/users/tbrooks8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tbrooks8/subscriptions","organizations_url":"https://api.github.com/users/tbrooks8/orgs","repos_url":"https://api.github.com/users/tbrooks8/repos","events_url":"https://api.github.com/users/tbrooks8/events{/privacy}","received_events_url":"https://api.github.com/users/tbrooks8/received_events","type":"User","site_admin":false},"performed_via_github_app":null}