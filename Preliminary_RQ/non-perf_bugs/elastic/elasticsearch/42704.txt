{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/42704","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42704/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42704/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42704/events","html_url":"https://github.com/elastic/elasticsearch/issues/42704","id":450250068,"node_id":"MDU6SXNzdWU0NTAyNTAwNjg=","number":42704,"title":"ElasticSearch - Shards not splitting equally","user":{"login":"YakobovLior","id":33580144,"node_id":"MDQ6VXNlcjMzNTgwMTQ0","avatar_url":"https://avatars3.githubusercontent.com/u/33580144?v=4","gravatar_id":"","url":"https://api.github.com/users/YakobovLior","html_url":"https://github.com/YakobovLior","followers_url":"https://api.github.com/users/YakobovLior/followers","following_url":"https://api.github.com/users/YakobovLior/following{/other_user}","gists_url":"https://api.github.com/users/YakobovLior/gists{/gist_id}","starred_url":"https://api.github.com/users/YakobovLior/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/YakobovLior/subscriptions","organizations_url":"https://api.github.com/users/YakobovLior/orgs","repos_url":"https://api.github.com/users/YakobovLior/repos","events_url":"https://api.github.com/users/YakobovLior/events{/privacy}","received_events_url":"https://api.github.com/users/YakobovLior/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-05-30T10:58:58Z","updated_at":"2019-05-30T20:50:00Z","closed_at":"2019-05-30T20:50:00Z","author_association":"NONE","active_lock_reason":null,"body":"Hello,\r\nI'm running an RPM based on-prem cluster with 21 data nodes:\r\n![image](https://user-images.githubusercontent.com/33580144/58627915-097dc280-82e1-11e9-8342-bf77afb7b10c.png)\r\n\r\nAs shows, one node specifically has very high load, while logstashes complaining on bulk retries issue due to many writes to this node:\r\n![image](https://user-images.githubusercontent.com/33580144/58628046-61b4c480-82e1-11e9-9fd9-08d462395ef4.png)\r\n\r\nCorrespondingly, this node gets excessive GC operations: (data nodes are 30GB heap, CMS GC)\r\n![image](https://user-images.githubusercontent.com/33580144/58628136-a2acd900-82e1-11e9-93ef-24d12b857f98.png)\r\n\r\nSo I wrote a short script to see the spread of active shards (from today which getting writes) among the cluster, and for some reason us-elkdb15 is having much more indices than the others:\r\n![image](https://user-images.githubusercontent.com/33580144/58628628-e94f0300-82e2-11e9-9dec-b63c6237b443.png)\r\n\r\nAny explanation to this behavior? the high load on this node causes significant performance issues to the cluster.\r\n\r\nThanks,\r\nLior\r\n","closed_by":{"login":"gwbrown","id":1522844,"node_id":"MDQ6VXNlcjE1MjI4NDQ=","avatar_url":"https://avatars1.githubusercontent.com/u/1522844?v=4","gravatar_id":"","url":"https://api.github.com/users/gwbrown","html_url":"https://github.com/gwbrown","followers_url":"https://api.github.com/users/gwbrown/followers","following_url":"https://api.github.com/users/gwbrown/following{/other_user}","gists_url":"https://api.github.com/users/gwbrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gwbrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gwbrown/subscriptions","organizations_url":"https://api.github.com/users/gwbrown/orgs","repos_url":"https://api.github.com/users/gwbrown/repos","events_url":"https://api.github.com/users/gwbrown/events{/privacy}","received_events_url":"https://api.github.com/users/gwbrown/received_events","type":"User","site_admin":false},"performed_via_github_app":null}