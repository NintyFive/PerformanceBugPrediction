[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/697266317","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-697266317","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":697266317,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzI2NjMxNw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2020-09-23T10:06:06Z","updated_at":"2020-09-23T10:06:06Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-search (:Search/Search)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/698225729","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-698225729","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":698225729,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODIyNTcyOQ==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2020-09-24T09:20:27Z","updated_at":"2020-09-24T09:21:18Z","author_association":"CONTRIBUTOR","body":"If we go ahead with the idea of indexing less than 100% of ngrams in values (to save disk space) then I'd like to drop the existing support for fuzzy queries. \r\n#### Why? - we lose the optimisation that made fuzzy queries fast\r\nThe current implementation relies on the fact that the max fuzzy query edit distance of 2 can be neatly translated to an ngram OR query with a minimum-should-match of `numNgrams - 2`. This drastically reduces the number of candidate matches we need to verify. This logic does not hold if we drop ngrams from the index and we would have to revert to a match_all type ngram query meaning we would have to decompress and parse every binary document value in the index to verify matches.\r\n#### At what cost? - Little, it was likely never very useful anyway\r\nWildcard fields are typically long and the search string can only be 2 edit distances max different from the value. This seems like an unlikely search requirement - people are normally looking for fuzzy matching on short strings.\r\n\r\nThis does start to break our guarantees that `wildcard fields == keyword fields` in terms of feature compatibility though.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/698278510","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-698278510","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":698278510,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODI3ODUxMA==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2020-09-24T11:13:39Z","updated_at":"2020-09-24T15:46:30Z","author_association":"CONTRIBUTOR","body":"### The dangers of index-time thinning\r\nThe problem with dropping terms at index-time (as opposed to the current search-time optimisations) is that you may make future searches hugely inefficient.\r\nAs an example - I experimented with dropping tokens where `token.hashcode()%2 == 0` - roughly one in every 2 tokens.\r\nThe difficulty is that a query like `*.exe*` may be unlucky enough to throw away both the `.ex` and `exe` ngrams due to the way they hash. This is quite likely given the probability is the same as tossing heads twice in a row. When the coin tosses don't come up in your favour you have no tokens in the index with which to accelerate the search and then we have to perform a linear scan of all compressed binary doc values which would be very expensive.\r\nThere's an unwelcome lack of predictability to searches then - why should `*.exe*` take hours to find 10 results but `*.foo*` take milliseconds? Having this distinction be on the roll of a dice is not something with a precedent in elasticsearch.\r\n\r\nUnfortunately a position-independent means of indexing is required. For example, we can't just drop every other token in a stream because the document value and search string may not start with the same characters - the doc may have opted to index tokens  `123` `34_` in the value `1234` but the user searched for `*234*` so would be requiring a match on `234` which is not indexed. Each token needs to be verifiably indexed or not based purely on examining the 3 characters it contains and not any surrounding content. \r\nAnother deterministic approach that was suggested was to drop tokens that contain any punctuation. This may be more sympathetic to the sorts of searches users might generally run but might not be as effective at reducing disk space.\r\nWhatever algorithm we choose there is the real danger that it won't be aligned with user searches and cause huge difference in response times compared to searches that are lucky enough to contain indexed terms. Maybe admins could take control of thinning choices and use a custom Analyzer?\r\nEither way, part of the appeal of the wildcard field compared to the text field is that users do not have to know what indexing strategies have been employed when writing queries.\r\n\r\n### Unacceptable trade off?\r\nInitial results suggest we can save ~35% of disk space indexing only half the ngrams but this benefit is paid for with the cost outlined above that searches can take massively longer to execute if they are unlucky enough to use the wrong terms.\r\nThis feels like a trade-off we shouldn't make by default.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/698395061","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-698395061","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":698395061,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODM5NTA2MQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2020-09-24T14:50:55Z","updated_at":"2020-09-24T15:09:20Z","author_association":"CONTRIBUTOR","body":"@markharwood I agree that fuzzy support is unlikely important but I would avoid dropping support for fuzzy queries entirely as the compatibility between keyword and wildcard is an important feature of the wildcard field. I would rather make fuzzy queries operate purely on doc values.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/698407390","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-698407390","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":698407390,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQwNzM5MA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2020-09-24T15:10:44Z","updated_at":"2020-09-24T15:11:47Z","author_association":"CONTRIBUTOR","body":"The hash/modulo dropping approach looks interesting to me but I wonder how it would compare with other approaches, I have the following ideas in mind for instance:\r\n - More aggressively normalizing characters in order to have fewer, denser postings lists, e.g. normalizing all spaces (line breaks, tabs, ...) to a white space and all punctuation characters to a single character e.g. `_`.\r\n - The above idea is pretty conservative, we could look into how much disk space we'd save and how much slower queries would be if we e.g. normalized all even digits to the previous digit (1->0, 3->2, 5->4, etc.) and possibly the same for characters (b->a, d->c, etc.).\r\n - Not dropping ngrams based on their hashes but based on heuristics, e.g. dropping all 3-grams that have a space or punctuation character in the middle, or all 3-grams that contain 2 spaces or punctuation characters. I wonder if we could reason about the 3-grams that are less likely to be useful for queries?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/698421008","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-698421008","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":698421008,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQyMTAwOA==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2020-09-24T15:33:13Z","updated_at":"2020-09-24T15:33:27Z","author_association":"CONTRIBUTOR","body":"Being so data/use-case dependent was why I floated the idea of having user-definable TokenFilters.\r\n\r\nMaybe we limit choices to pre-defined token filters we ship for this trimming purpose","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/700053247","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-700053247","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":700053247,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMDA1MzI0Nw==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2020-09-28T14:44:13Z","updated_at":"2020-09-28T14:56:31Z","author_association":"CONTRIBUTOR","body":"I got hold of some log data and related queries to measure the effects of index-time token dropping on queries.\r\n\r\nIronically,  my tests failed for the query string `*ERROR*`.  The hash modulos of this string's ngrams (`err`, `rro`, `ror`) all come up as tokens to drop and my code doesn't currently handle this properly. The fix isn't great either because it would need to fall back to a `match_all` query that does a brute force scan of all unique values in the index.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/700265584","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-700265584","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":700265584,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMDI2NTU4NA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2020-09-28T20:32:19Z","updated_at":"2020-09-28T20:32:19Z","author_association":"CONTRIBUTOR","body":"Agreed that this sounds bad. This is why I prefer skipping ngrams based on their actual content than based on the result of a hashing function.\r\n\r\nI hope we could find a way to improve space efficiency with very minimal assumptions so that we wouldn't need to expose options to tailor the behavior of this field differently for every use-case.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/700598022","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-700598022","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":700598022,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMDU5ODAyMg==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2020-09-29T09:55:25Z","updated_at":"2020-09-29T13:54:52Z","author_association":"CONTRIBUTOR","body":"I trialled dropping tokens with spaces in them with my test logs dataset.\r\nThis reduced the field size by 18% from 3.4GB to 2.8GB:\r\n\r\n``` \r\n==== wildcard field on master\r\n\r\ntotal disk:      4,540,272,889\r\nnum docs:           18,910,699\r\nstored fields:     870,096,528\r\nterm vectors:                0\r\nnorms:                       0\r\ndocvalues:       1,166,315,683\r\npostings:        2,358,532,109\r\nprox:                        0\r\npoints:             38,816,955\r\nterms:             106,510,273\r\n\r\n        field           total      terms dict        postings       proximity          points       docvalues       % with dv                       features\r\n        =====           =====      ==========        ========       =========       =========       =========        ========                       ========\r\n         line   3,487,256,622         306,098   2,358,532,031               0               0   1,128,418,493          100.0%                    docs binary\r\n          _id     106,204,253     106,204,175              78               0               0               0            0.0%                           docs\r\n      _seq_no      37,896,728               0               0               0               0      37,896,728          100.0%              8bytes/1D numeric\r\n_primary_term             231               0               0               0               0             231          100.0%                        numeric\r\n     _version             231               0               0               0               0             231          100.0%                        numeric\r\n      _source               0               0               0               0               0               0            0.0%              \r\n\r\n\r\n====== wildcard field, master modified to trim tokens containing spaces\r\n\r\ntotal disk:      3,845,563,101\r\nnum docs:           18,910,699\r\nstored fields:     870,224,450\r\nterm vectors:                0\r\nnorms:                       0\r\ndocvalues:       1,166,288,842\r\npostings:        1,663,539,771\r\nprox:                        0\r\npoints:             38,811,842\r\nterms:             106,696,855\r\n\r\n        field           total      terms dict        postings       proximity          points       docvalues       % with dv                       features\r\n        =====           =====      ==========        ========       =========       =========       =========        ========                       ========\r\n         line   2,792,248,234         275,929   1,663,539,693               0               0   1,128,432,612          100.0%                    docs binary\r\n          _id     106,421,004     106,420,926              78               0               0               0            0.0%                           docs\r\n      _seq_no      37,855,768               0               0               0               0      37,855,768          100.0%              8bytes/1D numeric\r\n_primary_term             231               0               0               0               0             231          100.0%                        numeric\r\n     _version             231               0               0               0               0             231          100.0%                        numeric\r\n      _source               0               0               0               0               0               0            0.0%                    \r\n```\r\n\r\n\r\nThe example queries I had weren't noticeably slower as a result of dropping tokens with spaces.\r\nWhile the strategy may be useful for my example dataset and queries, a worst-case scenario might be on docs that record sequences of data with spaces e.g,\r\n\r\n``` 1, 3, 4, 10, 9, 3, 4, 6.... ```\r\n\r\nIn this scenario there would be no tokens indexed other than `10,` \r\n\r\n\r\n>, we could look into how much disk space we'd save and how much slower queries would be if we e.g. normalized all even digits to the previous digit \r\n\r\nThe bulk of the disk space costs look to be tied up in postings. By normalising terms in the way suggested above I expect we just move the costs around rather than make a significant impact. If `bbc` and `abc` are both normalised to `abc` then that will have the same number of postings - perhaps just denser and therefore compressing a bit better. The postings are denser than a typical index already because they are only 3 character terms (median num postings is 13 in my test data).\r\n\r\nDropping terms will likely have a bigger disk space reduction than merging terms but its side-effect when mis-aligned with searches is potentially bigger. If all search terms are missing (like in my `1, 3, 4` example) the cost is a full scan Vs if search terms are only ever merged (`bbc` == `abc`) then we just have more false positive matches to be filtered. \r\n\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/700810408","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-700810408","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":700810408,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMDgxMDQwOA==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2020-09-29T16:10:24Z","updated_at":"2020-09-29T16:10:24Z","author_association":"CONTRIBUTOR","body":">By normalising terms in the way suggested above I expect we just move the costs around rather than make a significant impact.\r\n\r\nI tried normalisation on my test data set and as suspected, we halved the number of unique terms but maintained roughly the same posting size. Probably not worth pursuing this approach. FYI @jpountz \r\n\r\n```\r\ntotal disk:      4,312,685,895\r\nnum docs:           18,910,699\r\nstored fields:     869,976,159\r\nterm vectors:                0\r\nnorms:                       0\r\ndocvalues:       1,166,296,514\r\npostings:        2,131,185,546\r\nprox:                        0\r\npoints:             38,814,908\r\nterms:             106,411,427\r\n\r\n        field           total      terms dict        postings       proximity          points       docvalues       % with dv                       features\r\n        =====           =====      ==========        ========       =========       =========       =========        ========                       ========\r\n         line   3,259,723,192         105,632   2,131,185,468               0               0   1,128,432,092          100.0%                    docs binary\r\n          _id     106,305,873     106,305,795              78               0               0               0            0.0%                           docs\r\n      _seq_no      37,863,960               0               0               0               0      37,863,960          100.0%              8bytes/1D numeric\r\n_primary_term             231               0               0               0               0             231          100.0%                        numeric\r\n     _version             231               0               0               0               0             231          100.0%                        numeric\r\n      _source               0               0               0               0               0               0            0.0%                        ```\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/700821598","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-700821598","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":700821598,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMDgyMTU5OA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2020-09-29T16:28:01Z","updated_at":"2020-09-29T16:28:01Z","author_association":"CONTRIBUTOR","body":"It's still more 6%, which I don't consider negligible. I suspect we will want to look into combinations of (hopefully low-impact) tricks that help save space in the ngram index.\r\n\r\nOne thing I'm also considering is that Lucene uses FOR rather than PFOR for doc IDs in postings, so we might be wasting some disk there as well. For sparse postings lists, spending 20 bits per value rather than 18 is not a big deal, but when the choice is between 4 and 6 the trade-off is a bit different as this could be a 33% saving. (Postings for ngrams are typically denser than postings of text fields.)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/701312761","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-701312761","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":701312761,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMTMxMjc2MQ==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2020-09-30T10:46:22Z","updated_at":"2020-09-30T10:46:22Z","author_association":"CONTRIBUTOR","body":"I trialled an even more aggressive normalizer based on this code:\r\n```\r\n        private int normalize(int codepoint) {\r\n            // Normalize  space ! \" # $ % &  ' ( } * + , - . chars to / \r\n            if (codepoint >=32 && codepoint < 48) {\r\n                return 47; \r\n            }\r\n            // Normalize  [ \\ ] ^ _ ` chars to / \r\n            if (codepoint >=91 && codepoint <= 96) {\r\n                return 47; \r\n            }\r\n            // Normalize  { | } ~ chars to / \r\n            if (codepoint >=123 && codepoint <= 126) {\r\n                return 47; \r\n            }\r\n            // All other ascii characters, normalize odd numbers to even.\r\n            if (codepoint >= 48 && codepoint <= 128 && codepoint % 2 ==0) {\r\n                // Even ascii chars in 0-9 a-z range.\r\n                return codepoint -1;\r\n            } else {\r\n                //return even ascii char or non-ascii chars\r\n                return codepoint;\r\n            }                \r\n        } \r\n```\r\nThis led to some more reductions in disk space - 3.4GB master wildcard to 3GB with the modification. \r\nSearch times were not noticeably slower for the test queries provided.\r\n\r\n```\r\n====== wildcard field, master modified to further normalise ascii tokens (most normal punctuation)\r\n\r\ntotal disk:      4,020,035,717\r\nnum docs:           18,910,699\r\nstored fields:     870,274,144\r\nterm vectors:                0\r\nnorms:                       0\r\ndocvalues:       1,166,316,164\r\npostings:        1,838,346,697\r\nprox:                        0\r\npoints:             38,813,894\r\nterms:             106,283,477\r\n\r\n        field           total      terms dict        postings       proximity          points       docvalues       % with dv                       features\r\n        =====           =====      ==========        ========       =========       =========       =========        ========                       ========\r\n         line   2,966,833,153          59,368   1,838,346,619               0               0   1,128,427,166          100.0%                    docs binary\r\n          _id     106,224,187     106,224,109              78               0               0               0            0.0%                           docs\r\n      _seq_no      37,888,536               0               0               0               0      37,888,536          100.0%              8bytes/1D numeric\r\n_primary_term             231               0               0               0               0             231          100.0%                        numeric\r\n     _version             231               0               0               0               0             231          100.0%                        numeric\r\n      _source               0               0               0               0               0               0            0.0% \r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/701435770","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-701435770","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":701435770,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQzNTc3MA==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2020-09-30T14:41:21Z","updated_at":"2020-09-30T14:41:21Z","author_association":"CONTRIBUTOR","body":"With this new approach of normalizing tokens (rather than dropping them) we can still accelerate fuzzy queries the way we used to.\r\nHowever, the range query is a casualty because the scrambling of tokens in the index means we can't rely on querying a contiguous range the way we used to accelerate range queries. @jpountz shall we drop range query support or revert to a match-all approach (if allow expensive queries flag permits)?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/701485559","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-701485559","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":701485559,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQ4NTU1OQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2020-09-30T16:01:20Z","updated_at":"2020-09-30T16:01:20Z","author_association":"CONTRIBUTOR","body":"> @jpountz shall we drop range query support or revert to a match-all approach?\r\n\r\nLet's not drop as the compatibility with the `keyword` field is very important. How are we handling lowercasing today, which already shuffles ordering? Could the current approach be generalized to handle these new normalizations? If not I think it'd be nice to at least support the case when the lower term and the higher term share a common prefix of 2 chars or more so that if you search for terms between `abcd` and `abxy` then we could use documents that match `\\0ab` as an approximation?\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/701529836","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-701529836","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":701529836,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMTUyOTgzNg==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2020-09-30T17:21:47Z","updated_at":"2020-09-30T17:21:47Z","author_association":"CONTRIBUTOR","body":">How are we handling lowercasing today, which already shuffles ordering? \r\n\r\nWe have 2 strategies for accelerating range queries:\r\n1) Long prefix eg `c:/LongPrefix/a.txt TO c:/LongPrefix/z.txt`\r\nHere we make an AND of all the leading ngrams in common with the start and end \r\n2) Short prefix eg `a.txt TO z.txt`\r\nHere we use a `TermRangeQuery` to get all the values from `\\0a*` to `\\0z*`\r\n\r\nIt's the second strategy which will no longer work because the normalised values may not exist in a contiguous range.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/701541088","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-701541088","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":701541088,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMTU0MTA4OA==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2020-09-30T17:42:45Z","updated_at":"2020-09-30T17:42:45Z","author_association":"CONTRIBUTOR","body":"I identified another block of characters that can be normalized ( `:   ; < = > ? @`) and adding these brought a small decrease in index size:\r\n```\r\ntotal disk:      4,007,497,483\r\nnum docs:           18,910,699\r\nstored fields:     870,386,123\r\nterm vectors:                0\r\nnorms:                       0\r\ndocvalues:       1,166,321,896\r\npostings:        1,825,648,928\r\nprox:                        0\r\npoints:             38,815,932\r\nterms:             106,323,263\r\n\r\n        field           total      terms dict        postings       proximity          points       docvalues       % with dv                       features\r\n        =====           =====      ==========        ========       =========       =========       =========        ========                       ========\r\n         line   2,954,132,832          59,276   1,825,648,850               0               0   1,128,424,706          100.0%                    docs binary\r\n          _id     106,264,065     106,263,987              78               0               0               0            0.0%                           docs\r\n      _seq_no      37,896,728               0               0               0               0      37,896,728          100.0%              8bytes/1D numeric\r\n_primary_term             231               0               0               0               0             231          100.0%                        numeric\r\n     _version             231               0               0               0               0             231          100.0%                        numeric\r\n      _source               0               0               0               0               0               0            0.0%                               \r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/702015515","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-702015515","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":702015515,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMjAxNTUxNQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2020-10-01T09:36:13Z","updated_at":"2020-10-01T09:36:13Z","author_association":"CONTRIBUTOR","body":"I think we have a bug due to this in our current implementation of rangeQuery? For instance `A.txt TO a.txt` should match `B.txt` but we use `\\0a*` as an acceleration query, which is wrong?\r\n\r\nRegardless of that I'm+1 on only keeping the 1st strategy to accelerate range queries and removing the 2nd which is harder to make correct with this change.\r\n\r\nAggressive normalization looks like it yields significant savings without hurting queries too much, maybe we should move forward with it and evaluate dropping ngrams based on heuristics on top of it in a separate issue since it seems to have more implications on query times and search-time complexity (e.g. the point about fuzzy queries that you brought up)?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/702124779","html_url":"https://github.com/elastic/elasticsearch/issues/62817#issuecomment-702124779","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/62817","id":702124779,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMjEyNDc3OQ==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2020-10-01T13:12:22Z","updated_at":"2020-10-01T13:12:22Z","author_association":"MEMBER","body":"> One thing I'm also considering is that Lucene uses FOR rather than PFOR for doc IDs in postings, so we might be wasting some disk there as well. For sparse postings lists, spending 20 bits per value rather than 18 is not a big deal, but when the choice is between 4 and 6 the trade-off is a bit different as this could be a 33% saving. (Postings for ngrams are typically denser than postings of text fields.)\r\n\r\nI wrote a quick and dirty patch to check the possible gain on the dataset we use for testing:\r\n\r\nEncoding|Postings size\r\n------------ | -------------\r\nFOR|2,0GB\r\nPFOR|1,5GB\r\nFOR + Normalize|1.7GB\r\nPFOR + Normalize|1.4GB\r\n\r\nPFOR saves 25% on the raw ngram index and 23% on the normalized one (lowercase + character folding) so that seems like a great follow up if we want to optimize further.","performed_via_github_app":null}]