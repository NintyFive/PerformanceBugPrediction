{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/18729","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18729/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18729/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18729/events","html_url":"https://github.com/elastic/elasticsearch/issues/18729","id":158406963,"node_id":"MDU6SXNzdWUxNTg0MDY5NjM=","number":18729,"title":"Replace replica count, auto expand and rack awareness with topology constraints","user":{"login":"apatrida","id":182340,"node_id":"MDQ6VXNlcjE4MjM0MA==","avatar_url":"https://avatars3.githubusercontent.com/u/182340?v=4","gravatar_id":"","url":"https://api.github.com/users/apatrida","html_url":"https://github.com/apatrida","followers_url":"https://api.github.com/users/apatrida/followers","following_url":"https://api.github.com/users/apatrida/following{/other_user}","gists_url":"https://api.github.com/users/apatrida/gists{/gist_id}","starred_url":"https://api.github.com/users/apatrida/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apatrida/subscriptions","organizations_url":"https://api.github.com/users/apatrida/orgs","repos_url":"https://api.github.com/users/apatrida/repos","events_url":"https://api.github.com/users/apatrida/events{/privacy}","received_events_url":"https://api.github.com/users/apatrida/received_events","type":"User","site_admin":false},"labels":[{"id":836504707,"node_id":"MDU6TGFiZWw4MzY1MDQ3MDc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Distributed","name":":Distributed/Distributed","color":"0e8a16","default":false,"description":"A catch all label for anything in the Distributed Area. If you aren't sure, use this one."},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2016-06-03T16:38:10Z","updated_at":"2018-02-13T19:27:54Z","closed_at":"2016-06-10T09:45:14Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The current replica count, auto expand and rack awareness are really all competing for the same idea of shard safety, and not succeeding.  They try to give shard safety but fail to be flexible and cause either hardship to managing the system, false errors, or situations where things just don't appear to work.  \n### The following would replace these concepts in their entirety:\n#### _Introducing \"Elasticsearch Topology Contraints\"_\n\nYou can guide or force the distribution of replicas by setting the following constraints on combinations of node attributes.  The use of \"replicas count\" below indicates primary + replica shards as a whole as is no longer \"extra copies\".\n- DESIRE MIN REPLICAS <count>   (soft minimum)\n- DESIRE MAX REPLICAS <count>  (soft maximum)\n- REQUIRE MIN REPLICAS <count> \n- REQUIRE MAX REPLICAS <count> \n- DESIRE PRIMARY _(primary shards)_\n- REQUIRE PRIMARY _(primary shards)_\n- DISALLOW PRIMARY  _(primary shards)_\n- DESIRE COMPLETE SHARDS _(shard sets)_\n- REQUIRE COMPLETE SHARDS _(shard sets)_\n- DISALLOW COMPLETE SHARDS _(shard sets)_\n- DESIRE INCOMPLETE SHARDS _(shard sets)_\n- REQUIRE INCOMPLETE SHARDS _(shard sets)_\n- DISALLOW INCOMPLETE SHARDS _(shard sets)_\n- DESIRE PRIMARY AND REPLICA SHARDS COLOCATED _(but not within same node)_\n- REQUIRE PRIMARY AND REPLICA SHARDS COLOCATED _(but not within same node)_\n- DISALLOW PRIMARY AND REPLICA SHARDS COLOCATED _(default for not within same node)_\n- REQUIRE INDEX APPROVAL _(for indexes to be on node)_\n- INCLUDE INDEX, EXCLUDE INDEX, REQUIRE INDEX, APPROVE INDEX _(for indexes on node)_\n\nNode attributes are defined by the configuration as:\n- what are the list of possible attributes for the entire cluster, such as \"zone\", \"rack\", \"nodetype\"\n- what are the known attribute values for a given node, such as \"zone: us-east\", \"rack: 1a\", \"nodetype: webapp\"\n- special values are allowed to be matched as node attributes:  `_name`, `_host_ip`, `_publish_ip`, `_ip`, `_host`,  `_index`, `_alias` -- where the special value of `_index` matches index name, and `_alias` matches any index that has the alias attached.  Others are the same as used in the old allocation filtering.  \n\nAnd node attribute values are matched against constraints using:\n- literal values\n- wildcards\n- some special values such as `_all`\n### Deprecated Features\n\nThese features are deprecated as they are encompassed in Topology Constraints:\n- replica count in index settings\n- auto-expand replicas in index settings\n- shard allocation awareness\n- forced shard allocation awareness\n- index shard allocation include, exclude, require\n- total shards per node\n## Example\n### Example Cluster\n\nFor all examples below, here is a cluster definition given each node and the node attributes set.\n- cluster node attributes:  `zone`, `rack`, `nodetype`\n- cluster nodes:\n  - `main-node-1`:  zone `us-east`, rack `1a`, nodetype `main`\n  - `main-node-2`:  zone `us-east`, rack `1c`,  nodetype `main`\n  - `main-node-3`:  zone `us-east`, rack `1d`,  nodetype `main`\n  - `main-node-4`:  zone `us-west`, rack `1a`,  nodetype `main`\n  - `main-node-5`:  zone `us-west`, rack `1c`,  nodetype `main`\n  - `main-node-6`:  zone `us-west`, rack `1d`,  nodetype `main`\n  - `webapp-node-1`: zone `us-east`, rack `1a`, nodetype `webapp`\n  - `webapp-node-2`:  zone `us-east`, rack `1c`, nodetype `webapp`\n  - `webapp-node-3`:  zone `us-west`, rack `1a`, nodetype `webapp`\n  - `webapp-node-4`:  zone `us-west`, rack `1d`, nodetype `webapp`\n  - `batch-node-1`: zone `us-east`, rack `1d`, nodetype `batch`\n- created indexes:\n  - `Alpha` with no initial replica count setting\n  - `Beta` with no initial replica count setting\n  - `ReadHeavy` with no initial replica count setting\n  - `VeryImportant` with initial REQUIRE MINIMUM REPLICAS count `3` defined in index settings\n\n_note: the above uses zones that may imply a WAN is present between data centers, and that may be a bad idea for a single cluster, so ignore that and focus on the example and not what those words mean in any given attribute, this is just to make an example that makes sense._\n### USE CASE 1:\n\nI require each index to have minimum of 2 replicas, and each zone to have at least 1 replica\n\nTopology constraints:\n- ATTRIBUTE `[\"_index\"]` VALUE `[\"*\"]` REQUIRE MIN REPLICAS `2`  \n- ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE MIN REPLICAS `1`\n\nIn this case a `Alpha`, `Beta`, `ReadHeavy` would have 2 or more replicas (primary+copies) and with one in each of `us-east` and `us-west`, while `VeryImportant` would have 3 replicas (primary+copies) with one spread across the zones.  To avoid this shard split, you can add the constraint:\n- ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE COMPLETE SHARDS\n\nNow the 3rd replica set will be in one of the two zones, if you want to lean it towards zone `us-east` then have a higher desired count there.\n- ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"VeryImportant\"]` DESIRE MIN REPLICAS `2`\n\nThe planner would check all of the constraints and given that all the REQUIRED can be satisfied while meeting the DESIRED as well, it would put 2 full copies of `VeryImportant` index in zone `us-east` and one copy in `us-west` to satisfy the required count of `3` for this index.\n\nFinal constraints are therefore:\n- ATTRIBUTE `[\"_index\"]` VALUE `[\"*\"]` REQUIRE MIN REPLICAS `2`  \n- ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE MIN REPLICAS `1`\n- ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE COMPLETE SHARDS\n- ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"VeryImportant\"]` DESIRE MIN REPLICAS `2`\n\n_note:  the use of `_index` in each attribute is for clarity, it could be allowed to be omitted when an index setting is used, therefore assuming `*` or all indexes._\n### USE CASE 2:\n\nSame as use case 1 but I also desire that all primary shards to be in zone `us-east` unless that is not possible.\n- from case 1:\n  - ATTRIBUTE `[\"_index\"]` VALUE `[\"*\"]` REQUIRE MIN REPLICAS `2`  \n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE MIN REPLICAS `1`\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE COMPLETE SHARDS\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"VeryImportant\"]` DESIRE MIN REPLICAS `2`\n- for this case add:\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"*\"]` DESIRE PRIMARY\n### USE CASE 3:\n\nSame as use case 2 but I also desire that all nodes have replicas of a `VeryImportant` and `ReadHeavy` indexes.\n- from case 2:\n  - ATTRIBUTE `[\"_index\"]` VALUE `[\"*\"]` REQUIRE MIN REPLICAS `2`  \n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE MIN REPLICAS `1`\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE COMPLETE SHARDS\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"VeryImportant\"]` DESIRE MIN REPLICAS `2`\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"*\"]` DESIRE PRIMARY\n- for this case add:\n  - ATTRIBUTE `[\"_index]` VALUE `[\"VeryImportant,ReadHeavy\"]` DESIRE MIN REPLICAS `_all`\n### USE CASE 4:\n\nSame as use case 3 but I also want each individual `rack` to have at least 1 replica of each index for safety.\n- from case 3:\n  - ATTRIBUTE `[\"_index\"]` VALUE `[\"*\"]` REQUIRE MIN REPLICAS `2`  \n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE MIN REPLICAS `1`\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE COMPLETE SHARDS\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"VeryImportant\"]` DESIRE MIN REPLICAS `2`\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"*\"]` DESIRE PRIMARY\n  - ATTRIBUTE `[\"_index]` VALUE `[\"VeryImportant,ReadHeavy\"]` DESIRE MIN REPLICAS `_all`\n- for this case add:\n  - ATTRIBUTE `[\"zone\", \"rack\", \"_index\"]` VALUE `[\"*\", \"*\", \"*\"]` DESIRE MIN REPLICAS `1`\n### USE CASE 5:\n\nSame as use case 4 but I do not want any indexes on `webapp` or `batch` nodes that are not approved for those nodes.  I only want `ReadHeavy` index on those nodes but without any primary shards.\n- from case 4:\n  - ATTRIBUTE `[\"_index\"]` VALUE `[\"*\"]` REQUIRE MIN REPLICAS `2`  \n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE MIN REPLICAS `1`\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE COMPLETE SHARDS\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"VeryImportant\"]` DESIRE MIN REPLICAS `2`\n  - ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"*\"]` DESIRE PRIMARY\n  - ATTRIBUTE `[\"_index]` VALUE `[\"VeryImportant,ReadHeavy\"]` DESIRE MIN REPLICAS `_all`\n  - ATTRIBUTE `[\"zone\", \"rack\", \"_index\"]` VALUE `[\"*\", \"*\", \"*\"]` DESIRE MIN REPLICAS `1`\n- for this case add:\n  - ATTRIBUTE `[\"nodetype\", \"_index\"]` VALUE `[\"webapp, batch\", \"*\"]` REQUIRE INDEX APPROVAL\n  - ATTRIBUTE `[\"nodetype\"]` VALUE `[\"webapp, batch\"]` APPROVE INDEX `ReadHeavy`\n  - ATTRIBUTE `[\"nodetype\", \"_index\"]` VALUE `[\"webapp, batch\", \"ReadHeavy\"]` DISALLOW PRIMARY\n\nWe now have `ReadHeavy` index spreading some of its shards across `webapp` and `batch` nodes excluding any primaries.  But what we actually want is to try and have a full copy per `webapp` machine and surely must have a copy per `batch` machine.  The `webapp` machines could query through to the main cluster if not yet replicated their copy, but the `batch` machines should not to avoid pounding the main cluster nodes from heavy querying.  So we add these constraints:\n- ATTRIBUTE `[\"nodetype\", \"_host\", \"_index\"]` VALUE `[\"webapp\", \"*\", \"ReadHeavy\"]` DESIRE MIN REPLICAS `1`\n- ATTRIBUTE `[\"nodetype\", \"_host\", \"_index\"]` VALUE `[\"batch\", \"*\", \"ReadHeavy\"]` REQUIRE MIN REPLICAS `1`\n\nNow we have caused each host to have a full copy of the index for `webapp` and `batch` nodes, where it is a hard requirement for `batch` and soft for `webapp`.\n\nFinal constraints are therefore:\n- ATTRIBUTE `[\"_index\"]` VALUE `[\"*\"]` REQUIRE MIN REPLICAS `2`  \n- ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE MIN REPLICAS `1`\n- ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"*\", \"*\"]` REQUIRE COMPLETE SHARDS\n- ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"VeryImportant\"]` DESIRE MIN REPLICAS `2`\n- ATTRIBUTE `[\"zone\", \"_index\"]` VALUE `[\"us-east\", \"*\"]` DESIRE PRIMARY\n- ATTRIBUTE `[\"_index]` VALUE `[\"VeryImportant,ReadHeavy\"]` DESIRE MIN REPLICAS `_all`\n- ATTRIBUTE `[\"zone\", \"rack\", \"_index\"]` VALUE `[\"*\", \"*\", \"*\"]` DESIRE MIN REPLICAS `1`\n- ATTRIBUTE `[\"nodetype\", \"_index\"]` VALUE `[\"webapp, batch\", \"*\"]` REQUIRE INDEX APPROVAL\n- ATTRIBUTE `[\"nodetype\"]` VALUE `[\"webapp, batch\"]` APPROVE INDEX `ReadHeavy`\n- ATTRIBUTE `[\"nodetype\", \"_index\"]` VALUE `[\"webapp, batch\", \"ReadHeavy\"]` DISALLOW PRIMARY\n- ATTRIBUTE `[\"nodetype\", \"_host\", \"_index\"]` VALUE `[\"webapp\", \"*\", \"ReadHeavy\"]` DESIRE MIN REPLICAS `1`\n- ATTRIBUTE `[\"nodetype\", \"_host\", \"_index\"]` VALUE `[\"batch\", \"*\", \"ReadHeavy\"]` REQUIRE MIN REPLICAS `1`\n\nWhich is fairly complex, but incredibly powerful.\n### USE CASE 6:\n\nWhat if I have the reverse case of this `ReadHeavy` index and have an index `WriteHeavy` where i want to write mostly to special nodes and then replicate to the others.  The same type of constraints could apply by REQUIRE PRIMARY to a few write-heavy nodes and setting DESIRE MIN REPLICAS to `1` for the same nodes so that they have a complete set of PRIMARY \n\nIf I want to hold replication to other nodes, I could add a REQUIRE INDEX APPROVAL to the other nodes (_note:  need a negation for values so can express \"everything but this attribute values\"_), do the full index, then remove that constraint letting it replicate across once complete.\n### USE CASE 7:\n\nI add a daily index that needs to have constraints, so the name suffix changes on each index, how do constraints apply?  Use wildcards at end of index naming, for example:\n- ATTRIBUTE `[\"nodetype\"]` VALUE `[\"webapp, batch\"]` APPROVE INDEX `DailySomething-*`\n\nor\n- ATTRIBUTE `[\"nodetype\", \"_host\", \"_index\"]` VALUE `[\"webapp\", \"*\", \"DailySomething-*\"]` DESIRE MIN REPLICAS `1`\n\nor if the indexes are all added to an alias, constraints could be based on the alias.  The only issue is that sometimes these type of indexes are created and then added to the index after a while, so maybe the temporary index has constraints that match wildcards, and the final alias has constraints that match on it.\n- ATTRIBUTE `[\"zone\", \"_alias\"]` VALUE `[\"us-east\", \"DailyAll\"]` DESIRE MIN REPLICAS `3`\n## Constraints on specific indexes:\n\nShould there be constraints on indexes other than starting minimum replicas?  And should these override topology constraints?  For example, restore a daily generated index with initial REQUIRE MAX REPLICAS `1` replicas and then once restored increase by changing to an index specific DESIRED MIN REPLICAS `_all`; or remove my index specific MAX of 1 and let the topology constraints take over.\n\nPer index constraints are the beginner case, and special case.  Topology should be the production norm.  But allowing index overrides (warning when in conflict with Topology) for special cases allows cases described above.\n## Constraint weighting\n\nShould conflicts between constraints be resolved by some weight given to constraints?  If same weight it is a problem, if different weight then the higher weight wins.  You put your safety constraints as the highest.\n## Constraint precedence:\n\nShould there be precedence rules for constraints?\n\nFor example, for replica count there is precedence here when multiple constraints overlap.  MIN wins over DESIRED and MAX, MAX wins over DESIRED.  A conflict with MAX results in a cluster warning that can be viewed from cluster state.  Only failures on MIN to be satisfied result in index state of RED.  \n## GREEN/YELLOW/RED Cluster and Index Status:\n\nCluster and index status should be queried a bit differently. \n- If you ask at the cluster level you have a rollup of all index status and the constraints that apply. \n- If you ask at the index level you have a rollup of all constraints that apply to the index status and the general index health (all primaries are valid and assigned).  \n- If you ask at the node attribute matching level, you have a rollup of all matching indexes/nodes\n- If you ask at the specific constraint level, you have the results of that constraint\n\nSo in use case 5 where `batch` nodes MUST have a full copy of the index, and `webapp` SHOULD have a full copy.  A `batch` process would wait for yellow state on `ReadHeavy` index by querying status for:\n- STATUS OF CONSTRAINTS ATTRIBUTE `[\"nodetype\", \"_host\", \"_index\"]` VALUE `[\"batch\", \"_me\", \"ReadHeavy\"]`\n\nSo if index health of `ReadHeavy` is generally ok (all primaries exist) and the constraints that affect nodetype `batch` on \"_me\" (my) host for index `ReadHeavy` are all satisfied then I receive a GREEN response.  And my batch would continue.\n\nTODO:  this needs heavy spec'ing to figure out how status vs. constraints are met.  But I think the index base health is only based on index specific constraints and not the global ones.  Other health checks should be from the perspective of the user of the index to ensure the constraints that matter to them are satisfied.  You could always say \"for index ReadHeavy are ALL constraints satisfied EVERYWHERE\" and do a higher level index check.  But more fine grained makes sense, for example one data center being YELLOW but mine being GREEN should allow me to do what I want to do in my app.\n## For Later, DISTANCE Constraints\n\nAdding distance constraints later will allow the system to do smart things about how to know what is SAME rack, NEAR in datacenter, or FAR such as WAN, or with a numeric value so that index replication across WANs can be handled differently such as a topology constraint for FAR sets of nodes use async replication instead of sync.   But they are not needed for the above use cases.\n## Related:\n\nsee related: #18723 which isn't needed if this issue is done.  Maybe #18723 is a stopgap for 5.x while this issue is for 6.x of Elasticsearch.\n","closed_by":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"performed_via_github_app":null}