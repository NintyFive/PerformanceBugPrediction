{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/29504","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29504/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29504/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29504/events","html_url":"https://github.com/elastic/elasticsearch/issues/29504","id":314000208,"node_id":"MDU6SXNzdWUzMTQwMDAyMDg=","number":29504,"title":"new generation too small / excessive garbage collection of young generation","user":{"login":"davidkarlsen","id":18299,"node_id":"MDQ6VXNlcjE4Mjk5","avatar_url":"https://avatars0.githubusercontent.com/u/18299?v=4","gravatar_id":"","url":"https://api.github.com/users/davidkarlsen","html_url":"https://github.com/davidkarlsen","followers_url":"https://api.github.com/users/davidkarlsen/followers","following_url":"https://api.github.com/users/davidkarlsen/following{/other_user}","gists_url":"https://api.github.com/users/davidkarlsen/gists{/gist_id}","starred_url":"https://api.github.com/users/davidkarlsen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidkarlsen/subscriptions","organizations_url":"https://api.github.com/users/davidkarlsen/orgs","repos_url":"https://api.github.com/users/davidkarlsen/repos","events_url":"https://api.github.com/users/davidkarlsen/events{/privacy}","received_events_url":"https://api.github.com/users/davidkarlsen/received_events","type":"User","site_admin":false},"labels":[{"id":144797810,"node_id":"MDU6TGFiZWwxNDQ3OTc4MTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Infra/Core","name":":Core/Infra/Core","color":"0e8a16","default":false,"description":"Core issues without another label"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2018-04-13T07:41:02Z","updated_at":"2020-01-16T07:05:09Z","closed_at":"2018-04-13T12:56:24Z","author_association":"NONE","active_lock_reason":null,"body":"Wrongly reported in https://github.com/elastic/elasticsearch-docker/issues/155 1st, now adding here instead.\r\n\r\nThe sizing for the young-generation seems off in the standard jvm.options seems not very well chosen. When reading docs/guidelines from elastic, the guideline is to not change the GC-settings, only change the heap-size. I am using the standard elastic image: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html (or rather https://hub.docker.com/r/davidkarlsen/elasticsearchhq/ which adds a plugin, but that's the only change).\r\n\r\n```\r\nfgrep MaxNewSize *\r\ngc.log.0.current:CommandLine flags: -XX:+AlwaysPreTouch -XX:CMSInitiatingOccupancyFraction=75 -XX:GCLogFileSize=67108864 -XX:+HeapDumpOnOutOfMemoryError -XX:InitialHeapSize=19327352832 -XX:MaxHeapSize=19327352832 -XX:MaxNewSize=174485504 -XX:MaxTenuringThreshold=6 -XX:NewSize=174485504 -XX:NumberOfGCLogFiles=32 -XX:OldPLABSize=16 -XX:OldSize=348971008 -XX:-OmitStackTraceInFastThrow -XX:+PrintGC -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution -XX:ThreadStackSize=1024 -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseGCLogFileRotation -XX:+UseParNewGC\r\n```\r\n\r\ngclog:\r\n```\r\n2018-04-13T09:29:39.897+0200: 130451.177: [GC (Allocation Failure) 2018-04-13T09:29:39.897+0200: 130451.178: [ParNew\r\nDesired survivor size 8716288 bytes, new threshold 2 (max 6)\r\n- age   1:    6692208 bytes,    6692208 total\r\n- age   2:    3618576 bytes,   10310784 total\r\n- age   3:    3794016 bytes,   14104800 total\r\n: 145061K->16167K(153344K), 0.0685724 secs] 11959807K->11830914K(18857344K), 0.0686832 secs] [Times: user=0.12 sys=0.00, real=0.07 secs] \r\n2018-04-13T09:29:39.966+0200: 130451.246: Total time for which application threads were stopped: 0.0691450 seconds, Stopping threads took: 0.0001155 seconds\r\n2018-04-13T09:29:41.634+0200: 130452.914: [GC (Allocation Failure) 2018-04-13T09:29:41.634+0200: 130452.914: [ParNew\r\nDesired survivor size 8716288 bytes, new threshold 2 (max 6)\r\n- age   1:    6921680 bytes,    6921680 total\r\n- age   2:    6343104 bytes,   13264784 total\r\n: 152487K->17024K(153344K), 0.0926382 secs] 11967234K->11839050K(18857344K), 0.0927670 secs] [Times: user=0.15 sys=0.00, real=0.10 secs] \r\n2018-04-13T09:29:41.726+0200: 130453.006: Total time for which application threads were stopped: 0.0931676 seconds, Stopping threads took: 0.0000598 seconds\r\n2018-04-13T09:29:45.003+0200: 130456.283: [GC (Allocation Failure) 2018-04-13T09:29:45.003+0200: 130456.283: [ParNew\r\nDesired survivor size 8716288 bytes, new threshold 2 (max 6)\r\n- age   1:    7243696 bytes,    7243696 total\r\n- age   2:    5887968 bytes,   13131664 total\r\n: 153344K->17024K(153344K), 0.1166746 secs] 11975370K->11845278K(18857344K), 0.1167974 secs] [Times: user=0.14 sys=0.00, real=0.12 secs] \r\n2018-04-13T09:29:45.120+0200: 130456.400: Total time for which application threads were stopped: 0.1172260 seconds, Stopping threads took: 0.0000525 seconds\r\n2018-04-13T09:29:45.289+0200: 130456.569: [GC (Allocation Failure) 2018-04-13T09:29:45.289+0200: 130456.569: [ParNew\r\nDesired survivor size 8716288 bytes, new threshold 2 (max 6)\r\n- age   1:    7422480 bytes,    7422480 total\r\n- age   2:    5845600 bytes,   13268080 total\r\n: 152937K->17024K(153344K), 0.1204861 secs] 11981191K->11850982K(18857344K), 0.1206075 secs] [Times: user=0.15 sys=0.00, real=0.12 secs] \r\n2018-04-13T09:29:45.410+0200: 130456.690: Total time for which application threads were stopped: 0.1210332 seconds, Stopping threads took: 0.0000611 seconds\r\n2018-04-13T09:29:45.504+0200: 130456.784: [GC (Allocation Failure) 2018-04-13T09:29:45.504+0200: 130456.784: [ParNew\r\nDesired survivor size 8716288 bytes, new threshold 2 (max 6)\r\n- age   1:    7577560 bytes,    7577560 total\r\n- age   2:    7010416 bytes,   14587976 total\r\n: 153344K->17024K(153344K), 0.0757820 secs] 11987302K->11859466K(18857344K), 0.0759058 secs] [Times: user=0.14 sys=0.00, real=0.07 secs] \r\n2018-04-13T09:29:45.580+0200: 130456.860: Total time for which application threads were stopped: 0.0773335 seconds, Stopping threads took: 0.0010613 seconds\r\n2018-04-13T09:29:45.715+0200: 130456.995: [GC (Allocation Failure) 2018-04-13T09:29:45.715+0200: 130456.995: [ParNew\r\nDesired survivor size 8716288 bytes, new threshold 2 (max 6)\r\n- age   1:    4911008 bytes,    4911008 total\r\n- age   2:    6793904 bytes,   11704912 total\r\n: 153344K->13913K(153344K), 0.0846712 secs] 11995786K->11863057K(18857344K), 0.0848011 secs] [Times: user=0.15 sys=0.00, real=0.08 secs] \r\n2018-04-13T09:29:45.800+0200: 130457.080: Total time for which application threads were stopped: 0.0852069 seconds, Stopping threads took: 0.0000540 seconds\r\n2018-04-13T09:29:45.960+0200: 130457.240: [GC (Allocation Failure) 2018-04-13T09:29:45.960+0200: 130457.240: [ParNew\r\nDesired survivor size 8716288 bytes, new threshold 6 (max 6)\r\n- age   1:    2795432 bytes,    2795432 total\r\n- age   2:    4395024 bytes,    7190456 total\r\n: 150189K->12051K(153344K), 0.0770999 secs] 11999332K->11867784K(18857344K), 0.0772358 secs] [Times: user=0.13 sys=0.00, real=0.08 secs] \r\n```\r\n\r\nThe gc-monitor complains:\r\n```\r\nsudo tail -f /var/log/messages|grep -i monitor\r\nApr 13 09:30:23 alp-aot-ccm03 docker/fa5c9da092fe[25991]: [2018-04-13T07:30:23,968][INFO ][o.e.m.j.JvmGcMonitorService] [HUCBRNR] [gc][129959] overhead, spent [268ms] collecting in the last [1s]\r\nApr 13 09:30:29 alp-aot-ccm03 docker/fa5c9da092fe[25991]: [2018-04-13T07:30:29,971][INFO ][o.e.m.j.JvmGcMonitorService] [HUCBRNR] [gc][129965] overhead, spent [346ms] collecting in the last [1s]\r\nApr 13 09:30:41 alp-aot-ccm03 docker/fa5c9da092fe[25991]: [2018-04-13T07:30:41,047][INFO ][o.e.m.j.JvmGcMonitorService] [HUCBRNR] [gc][129976] overhead, spent [286ms] collecting in the last [1s]\r\n```\r\n\r\nI am seeing that old gen is collected around every 40minutes, but the young gen is collecting like crazy (the number is around \"30\" in x-pack monitoring view in kibana monitoring app - but not so easy to understand what that figure is, probably frequency between gc's)\r\n\r\nfrom the stats api after running for > 24hrs:\r\n```\r\n\"pools\" : {\r\n            \"young\" : {\r\n              \"used_in_bytes\" : 139303520,\r\n              \"max_in_bytes\" : 139591680, <-- only 139 Mb\r\n              \"peak_used_in_bytes\" : 139591680,\r\n              \"peak_max_in_bytes\" : 139591680\r\n            },\r\n            \"survivor\" : {\r\n              \"used_in_bytes\" : 17432576,\r\n              \"max_in_bytes\" : 17432576, <-- only 17 Mb!\r\n              \"peak_used_in_bytes\" : 17432576,\r\n              \"peak_max_in_bytes\" : 17432576\r\n            },\r\n            \"old\" : {\r\n              \"used_in_bytes\" : 4988199848,\r\n              \"max_in_bytes\" : 19152896000,\r\n              \"peak_used_in_bytes\" : 14391151600,\r\n              \"peak_max_in_bytes\" : 19152896000\r\n            }\r\n          }\r\n        },\r\n        \"threads\" : {\r\n          \"count\" : 57,\r\n          \"peak_count\" : 61\r\n        },\r\n        \"gc\" : {\r\n          \"collectors\" : {\r\n            \"young\" : {\r\n              \"collection_count\" : 110044,\r\n              \"collection_time_in_millis\" : 8392102\r\n            },\r\n            \"old\" : {\r\n              \"collection_count\" : 41,\r\n              \"collection_time_in_millis\" : 4808\r\n            }\r\n          }\r\n```\r\n\r\n\r\n**Elasticsearch version** (`bin/elasticsearch --version`):\r\nelasticsearch --version\r\nVersion: 6.2.3, Build: c59ff00/2018-03-13T10:06:29.741383Z, JVM: 1.8.0_161\r\n\r\n**Plugins installed**: []\r\n* https://github.com/vvanholl/elasticsearch-prometheus-exporter\r\n* x-pack (from the default image)\r\n\r\n**JVM version** (`java -version`):\r\n/usr/lib/jvm/jre-1.8.0-openjdk/bin/java -version\r\nopenjdk version \"1.8.0_161\"\r\nOpenJDK Runtime Environment (build 1.8.0_161-b14)\r\nOpenJDK 64-Bit Server VM (build 25.161-b14, mixed mode)\r\n\r\n\r\n**OS version** (`uname -a` if on a Unix-like system):\r\nuname -a\r\nLinux fa5c9da092fe 3.10.0-693.1.1.el7.x86_64 #1 SMP Thu Aug 3 08:15:31 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\nSee discussion (and log-excerpts) from \r\nhttps://discuss.elastic.co/t/excessive-garbage-collection/127527/6\r\n\r\n**Steps to reproduce**:\r\n\r\nPlease include a *minimal* but *complete* recreation of the problem, including\r\n(e.g.) index creation, mappings, settings, query etc.  The easier you make for\r\nus to reproduce it, the more likely that somebody will take the time to look at it.\r\n\r\n 1. use standard logstash index mapping\r\n 2. add -Xmx18g -Xms18g\r\n 3. push in events at a rate of 300e/s\r\n\r\n**Provide logs (if relevant)**:\r\nSee the forum for output, or ask for anything and I can add more info.\r\n","closed_by":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"performed_via_github_app":null}