[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/85364970","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-85364970","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":85364970,"node_id":"MDEyOklzc3VlQ29tbWVudDg1MzY0OTcw","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2015-03-24T06:49:44Z","updated_at":"2015-03-24T06:49:44Z","author_association":"MEMBER","body":"@drewdahlke The nested feature heavily leans on in-memory bitsets. Essentially each parent nested field needs an in-memory bitset. In the case all the nested fields are in the root level, only one bitset needs to be in memory, but once you have multiple levels of nested fields more bitsets need to be loaded in memory and this can become expensive. In your case do you have multiple levels of nested fields?\n\nAre you using nested sorting? Unfortunately at the moment this is quite memory in-efficient. It has been fixed in 2.0: #9199\n\nIn the case that you don't use all of your nested fields you may want to disable eager loading by setting `index.load_fixed_bitset_filters_eagerly` to false in your elasticsearch.yml file.\n\nIf you have many documents marked as deleted it may make sense to prune those documents by running an optimize. (this can also be found in node stats under docs/deleted) This can reduce the space these in-memory bitsets need.\n\nOther than that you can always increase the maximum heap space a node is allowed to use or add more nodes.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/85507977","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-85507977","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":85507977,"node_id":"MDEyOklzc3VlQ29tbWVudDg1NTA3OTc3","user":{"login":"drewdahlke","id":9967543,"node_id":"MDQ6VXNlcjk5Njc1NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/9967543?v=4","gravatar_id":"","url":"https://api.github.com/users/drewdahlke","html_url":"https://github.com/drewdahlke","followers_url":"https://api.github.com/users/drewdahlke/followers","following_url":"https://api.github.com/users/drewdahlke/following{/other_user}","gists_url":"https://api.github.com/users/drewdahlke/gists{/gist_id}","starred_url":"https://api.github.com/users/drewdahlke/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/drewdahlke/subscriptions","organizations_url":"https://api.github.com/users/drewdahlke/orgs","repos_url":"https://api.github.com/users/drewdahlke/repos","events_url":"https://api.github.com/users/drewdahlke/events{/privacy}","received_events_url":"https://api.github.com/users/drewdahlke/received_events","type":"User","site_admin":false},"created_at":"2015-03-24T14:03:14Z","updated_at":"2015-03-24T14:03:14Z","author_association":"NONE","body":"Hey, thanks for getting back to me. Our doc structure only nests one layer deep right now, but we are sorting on a nested date field. It's good to know there's an improvement for that in a future release. I've got include_in_parent set to false on the nested docs. Our docs get updated typically 8-10 times but are never deleted.\n\nWe're on 1.4.4 and it looks like in 1.4.1 the eager loading of bitsets was disabled by default so we should be good there. However, I have a question about that. Is it holding onto 1 bitset per field or one bitset per field per term filter value ever used while querying with _cache=true? \n\nAlso is there a way to see a breakdown on field name to bit set overhead? It would be handy to see which fields have the most overhead. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/85536122","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-85536122","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":85536122,"node_id":"MDEyOklzc3VlQ29tbWVudDg1NTM2MTIy","user":{"login":"drewdahlke","id":9967543,"node_id":"MDQ6VXNlcjk5Njc1NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/9967543?v=4","gravatar_id":"","url":"https://api.github.com/users/drewdahlke","html_url":"https://github.com/drewdahlke","followers_url":"https://api.github.com/users/drewdahlke/followers","following_url":"https://api.github.com/users/drewdahlke/following{/other_user}","gists_url":"https://api.github.com/users/drewdahlke/gists{/gist_id}","starred_url":"https://api.github.com/users/drewdahlke/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/drewdahlke/subscriptions","organizations_url":"https://api.github.com/users/drewdahlke/orgs","repos_url":"https://api.github.com/users/drewdahlke/repos","events_url":"https://api.github.com/users/drewdahlke/events{/privacy}","received_events_url":"https://api.github.com/users/drewdahlke/received_events","type":"User","site_admin":false},"created_at":"2015-03-24T14:45:15Z","updated_at":"2015-03-24T14:45:15Z","author_association":"NONE","body":"Scratch what I said about 1.4.4 defaulting to false. I must have missread https://github.com/elastic/elasticsearch/issues/8394 because setting index.load_fixed_bitset_filters_eagerly=false seems to have done the trick. fixed_bit_set_memory_in_bytes shrunk 2 orders of magnitude from that. Sweet!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/85536536","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-85536536","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":85536536,"node_id":"MDEyOklzc3VlQ29tbWVudDg1NTM2NTM2","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2015-03-24T14:45:46Z","updated_at":"2015-03-24T14:53:30Z","author_association":"MEMBER","body":"> However, I have a question about that. Is it holding onto 1 bitset per field or one bitset per field per term filter value ever used while querying with _cache=true?\n\nThe bitset cache caches on a per nested field basis.\n\n> Also is there a way to see a breakdown on field name to bit set overhead?\n\nNo, at the moment that isn't possible\n\nIf a document get deleted then under the hood the Lucene document(s) get marked as deleted and are re-added. So perhaps it does make sense to prune deleted docs at some point.\n\nIn 1.4.4 only the bitset for parent level nested field are eagerly loaded, in your case that is only one bitset, so that should be fine. The nested sorting also causes the nested fields referenced in the nested sorting to be cached, so that adds up.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/85539491","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-85539491","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":85539491,"node_id":"MDEyOklzc3VlQ29tbWVudDg1NTM5NDkx","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2015-03-24T14:53:50Z","updated_at":"2015-03-24T14:53:50Z","author_association":"MEMBER","body":"One note about the disabling the eager loading is that unfortunately will not prevent jvm heap issues. If a search request gets executed that does need the bitsets, it will load it and you may run into the same issues. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/85632441","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-85632441","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":85632441,"node_id":"MDEyOklzc3VlQ29tbWVudDg1NjMyNDQx","user":{"login":"drewdahlke","id":9967543,"node_id":"MDQ6VXNlcjk5Njc1NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/9967543?v=4","gravatar_id":"","url":"https://api.github.com/users/drewdahlke","html_url":"https://github.com/drewdahlke","followers_url":"https://api.github.com/users/drewdahlke/followers","following_url":"https://api.github.com/users/drewdahlke/following{/other_user}","gists_url":"https://api.github.com/users/drewdahlke/gists{/gist_id}","starred_url":"https://api.github.com/users/drewdahlke/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/drewdahlke/subscriptions","organizations_url":"https://api.github.com/users/drewdahlke/orgs","repos_url":"https://api.github.com/users/drewdahlke/repos","events_url":"https://api.github.com/users/drewdahlke/events{/privacy}","received_events_url":"https://api.github.com/users/drewdahlke/received_events","type":"User","site_admin":false},"created_at":"2015-03-24T18:27:43Z","updated_at":"2015-03-24T18:27:43Z","author_association":"NONE","body":"Makes sense. Any chance we could make this a feature request to persist those bitsets on disk and LRU cache them in memory rotated by a configurable max memory cap? As is with time series data (we have 1 index per day of data) old data, less frequently queried, has the same bitset memory cost as the hot recent data and it'd be nice to get some control over that. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/87418541","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-87418541","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":87418541,"node_id":"MDEyOklzc3VlQ29tbWVudDg3NDE4NTQx","user":{"login":"holm","id":467209,"node_id":"MDQ6VXNlcjQ2NzIwOQ==","avatar_url":"https://avatars2.githubusercontent.com/u/467209?v=4","gravatar_id":"","url":"https://api.github.com/users/holm","html_url":"https://github.com/holm","followers_url":"https://api.github.com/users/holm/followers","following_url":"https://api.github.com/users/holm/following{/other_user}","gists_url":"https://api.github.com/users/holm/gists{/gist_id}","starred_url":"https://api.github.com/users/holm/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/holm/subscriptions","organizations_url":"https://api.github.com/users/holm/orgs","repos_url":"https://api.github.com/users/holm/repos","events_url":"https://api.github.com/users/holm/events{/privacy}","received_events_url":"https://api.github.com/users/holm/received_events","type":"User","site_admin":false},"created_at":"2015-03-29T14:04:07Z","updated_at":"2015-03-29T14:04:07Z","author_association":"NONE","body":"We have been trying to upgrade to 1.4 from 1.2, but are currently unable to because the we run out of heap, even with more than 50% additional heap being made available. Based on testing it seems this is the issue that is causing the increased memory usage. \n\nWe use nested documents for sorting. Is there any chance the fix in 2.0 will be backported or are there any other ways to get around this issue?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/88054900","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-88054900","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":88054900,"node_id":"MDEyOklzc3VlQ29tbWVudDg4MDU0OTAw","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2015-03-31T11:38:42Z","updated_at":"2015-03-31T11:38:42Z","author_association":"MEMBER","body":"@holm sorry to hear that nested sorting is preventing you from upgrading... We decided not to backport the nested sorting fix into 1.x, because of a change in Lucene 5 (2.0 will use this Lucene version) on which the fix relies on. Basically documents are guaranteed to always score in order in Lucene 5, in Lucene 4 this isn't always to case and the nested sorting fix relies on that. \n\n@drewdahlke The reason that these bitsets are on the jvm heap space is because the nested query, nested sorting etc. require random access to it.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/88070473","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-88070473","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":88070473,"node_id":"MDEyOklzc3VlQ29tbWVudDg4MDcwNDcz","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2015-03-31T12:44:22Z","updated_at":"2015-03-31T12:44:22Z","author_association":"CONTRIBUTOR","body":"I don't think we should try to backport this change, this would be dangerous. I would rather recommend increasing the heap size or adding more nodes to the cluster until we have a better solution for this issue (ie. 2.0).\n\nBut even on 2.0, nested documents will still be a heavy feature since we will still require a full bit set for each nested field. In general, `nested` should not be considered an improved version of `object` but rather a useful feature that comes with a non-negligible cost. This needs to be taken into account when designing mappings. Also, `nested` docs don't only increase memory usage, they also make the index sparser, which in-turn hurts compression of the index on disk and increase memory usage of all data-structures that key by doc ID (such as fielddata).\n\n@holm As a side note, I see that you have 6015 segments on a single node, which is quite large for a single node. Do you know how many shards this maps to (I would guess around 120 since it is common for shards to have about 50 segments)? Maybe you could save memory by having fewer shards, eg. by allocating fewer shards per index, merging read-only indices to a single segment per shard, or having eg. weekly indices instead of daily if you have time-based data.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/88103623","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-88103623","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":88103623,"node_id":"MDEyOklzc3VlQ29tbWVudDg4MTAzNjIz","user":{"login":"drewdahlke","id":9967543,"node_id":"MDQ6VXNlcjk5Njc1NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/9967543?v=4","gravatar_id":"","url":"https://api.github.com/users/drewdahlke","html_url":"https://github.com/drewdahlke","followers_url":"https://api.github.com/users/drewdahlke/followers","following_url":"https://api.github.com/users/drewdahlke/following{/other_user}","gists_url":"https://api.github.com/users/drewdahlke/gists{/gist_id}","starred_url":"https://api.github.com/users/drewdahlke/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/drewdahlke/subscriptions","organizations_url":"https://api.github.com/users/drewdahlke/orgs","repos_url":"https://api.github.com/users/drewdahlke/repos","events_url":"https://api.github.com/users/drewdahlke/events{/privacy}","received_events_url":"https://api.github.com/users/drewdahlke/received_events","type":"User","site_admin":false},"created_at":"2015-03-31T14:15:49Z","updated_at":"2015-03-31T14:15:49Z","author_association":"NONE","body":"Keeping the bitsets on heap makes total sense. It's my impression from https://github.com/elastic/elasticsearch/pull/7037 that bitsets moved from being in field cache with an LRU eviction policy to something separate & non-evicting in 1.4.0. \n\nBreaking the bitsets out from field cache is a huge improvement b/c bitsets & field data are different enough that giving them different eviction policies makes a lot of sense. However, omitting the eviction policy all-together on bitsets seems atypical for the project. I'd like to see a cap similar to indices.fielddata.cache.size but for those bitsets. For us a slow query here and there trumps OOMs or the expense of growing the cluster to compensate. \n\nSans you all adding a cap we're looking at either having to fork ES 1.4.4 to add a cap or downgrading to 1.3.x to keep our ec2 costs down. \n\n@jpountz Merging old shards down to a single segment is a great idea; putting it on my list. The bitsets have my attention right now b/c upgrading from 1.1.x to 1.4.x with no other changes immediately started costing us more money to host b/c we have to have so much more heap available. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/88241482","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-88241482","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":88241482,"node_id":"MDEyOklzc3VlQ29tbWVudDg4MjQxNDgy","user":{"login":"cdmicacc","id":431068,"node_id":"MDQ6VXNlcjQzMTA2OA==","avatar_url":"https://avatars2.githubusercontent.com/u/431068?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmicacc","html_url":"https://github.com/cdmicacc","followers_url":"https://api.github.com/users/cdmicacc/followers","following_url":"https://api.github.com/users/cdmicacc/following{/other_user}","gists_url":"https://api.github.com/users/cdmicacc/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmicacc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmicacc/subscriptions","organizations_url":"https://api.github.com/users/cdmicacc/orgs","repos_url":"https://api.github.com/users/cdmicacc/repos","events_url":"https://api.github.com/users/cdmicacc/events{/privacy}","received_events_url":"https://api.github.com/users/cdmicacc/received_events","type":"User","site_admin":false},"created_at":"2015-03-31T20:54:57Z","updated_at":"2015-03-31T21:38:50Z","author_association":"NONE","body":"We are in the same situation -- the memory used by the bitsets grows until the entire heap is consumed (we have 12 nodes, currently, each with a 32GB heap).  \n\nBy chance, we discovered that issuing `POST https://localhost:9200/index/_cache/clear` will clear the bitset cache (but there is no corresponding explicit parameter to clear it!).  \n\nHowever, without some form of cache expiry (LRU or otherwise), we're either going to have to periodically flush all the caches across the cluster, or downgrade back to 1.3.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/90533684","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-90533684","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":90533684,"node_id":"MDEyOklzc3VlQ29tbWVudDkwNTMzNjg0","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2015-04-07T12:33:52Z","updated_at":"2015-04-07T12:33:52Z","author_association":"MEMBER","body":"One note to @jpountz's note about memory usage for nested docs: Nested documents requires a full bit set for each nested field that is a parent of another nested field. This applies for both 2.0 and 1.x (the nested sorting is just an exception here).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/90536461","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-90536461","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":90536461,"node_id":"MDEyOklzc3VlQ29tbWVudDkwNTM2NDYx","user":{"login":"holm","id":467209,"node_id":"MDQ6VXNlcjQ2NzIwOQ==","avatar_url":"https://avatars2.githubusercontent.com/u/467209?v=4","gravatar_id":"","url":"https://api.github.com/users/holm","html_url":"https://github.com/holm","followers_url":"https://api.github.com/users/holm/followers","following_url":"https://api.github.com/users/holm/following{/other_user}","gists_url":"https://api.github.com/users/holm/gists{/gist_id}","starred_url":"https://api.github.com/users/holm/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/holm/subscriptions","organizations_url":"https://api.github.com/users/holm/orgs","repos_url":"https://api.github.com/users/holm/repos","events_url":"https://api.github.com/users/holm/events{/privacy}","received_events_url":"https://api.github.com/users/holm/received_events","type":"User","site_admin":false},"created_at":"2015-04-07T12:46:07Z","updated_at":"2015-04-07T12:46:07Z","author_association":"NONE","body":"We use the nested document feature to associate a personal \"score\" for each user to a document. We then use the score for the active user when sorting the results. For most documents only very few users will have a score, so the length of the nested documents are quite short. \n\nI think having an eviction policy for the bitsets sounds like a good idea. Barring that, I would love to know if there are other ways to solve the scoring, outside a nested sorting. I guess a function score could work also, although less direct.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/110821899","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-110821899","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":110821899,"node_id":"MDEyOklzc3VlQ29tbWVudDExMDgyMTg5OQ==","user":{"login":"schlosna","id":54594,"node_id":"MDQ6VXNlcjU0NTk0","avatar_url":"https://avatars0.githubusercontent.com/u/54594?v=4","gravatar_id":"","url":"https://api.github.com/users/schlosna","html_url":"https://github.com/schlosna","followers_url":"https://api.github.com/users/schlosna/followers","following_url":"https://api.github.com/users/schlosna/following{/other_user}","gists_url":"https://api.github.com/users/schlosna/gists{/gist_id}","starred_url":"https://api.github.com/users/schlosna/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/schlosna/subscriptions","organizations_url":"https://api.github.com/users/schlosna/orgs","repos_url":"https://api.github.com/users/schlosna/repos","events_url":"https://api.github.com/users/schlosna/events{/privacy}","received_events_url":"https://api.github.com/users/schlosna/received_events","type":"User","site_admin":false},"created_at":"2015-06-10T16:25:33Z","updated_at":"2015-06-10T16:25:33Z","author_association":"NONE","body":"I am experiencing bloat of the fixed bit set caches which has led to OutOfMemoryErrors on our data nodes and ES cluster instability. We make heavy use of nested documents to provide access controls for parent documents, and our queries include filters for these nested documents.\n\nSimilar to @drewdahlke, we may have to fork ES if we cannot bound the `FixedBitSetFilterCache` so I am beginning to look into implementing size and/or time bounding for the `FixedBitSetFilterCache`, and wanted to gauge community interest in a possible PR and get feedback on what might be an acceptable implementation. I am planning on an implementation similar to `IndicesFieldDataCache`. I am also interested in thoughts around if the `FixedBitSetFilterCache` should participate in the existing circuit breakers (i.e. indices.breaker.total.limit per\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-fielddata.html#circuit-breaker), and possibly add `indices.breaker.filter.limit` and `indices.breaker.filter.overhead`.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/111865048","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-111865048","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":111865048,"node_id":"MDEyOklzc3VlQ29tbWVudDExMTg2NTA0OA==","user":{"login":"jepettoruti","id":4490499,"node_id":"MDQ6VXNlcjQ0OTA0OTk=","avatar_url":"https://avatars2.githubusercontent.com/u/4490499?v=4","gravatar_id":"","url":"https://api.github.com/users/jepettoruti","html_url":"https://github.com/jepettoruti","followers_url":"https://api.github.com/users/jepettoruti/followers","following_url":"https://api.github.com/users/jepettoruti/following{/other_user}","gists_url":"https://api.github.com/users/jepettoruti/gists{/gist_id}","starred_url":"https://api.github.com/users/jepettoruti/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jepettoruti/subscriptions","organizations_url":"https://api.github.com/users/jepettoruti/orgs","repos_url":"https://api.github.com/users/jepettoruti/repos","events_url":"https://api.github.com/users/jepettoruti/events{/privacy}","received_events_url":"https://api.github.com/users/jepettoruti/received_events","type":"User","site_admin":false},"created_at":"2015-06-14T19:11:22Z","updated_at":"2015-06-14T19:11:22Z","author_association":"NONE","body":"Is there any official word of when this will be fixed? Is it for ES 2.x or would it come in some previous release?\n@schlosna I don't think we have the resources to help with the development of a fork, but happy to test it.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/111949974","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-111949974","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":111949974,"node_id":"MDEyOklzc3VlQ29tbWVudDExMTk0OTk3NA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2015-06-15T06:36:31Z","updated_at":"2015-06-15T06:36:31Z","author_association":"CONTRIBUTOR","body":"Making this cache bounded is a bit trappy because then even very simple queries that match few documents might need to scan the entire index to load these bit sets that we use for nested queries/sorting.\n\nLike @martijnvg said, this situation will improve a lot in 2.0 thanks to #9199. However I don't think we can do anything in 1.x.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/112046083","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-112046083","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":112046083,"node_id":"MDEyOklzc3VlQ29tbWVudDExMjA0NjA4Mw==","user":{"login":"drewdahlke","id":9967543,"node_id":"MDQ6VXNlcjk5Njc1NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/9967543?v=4","gravatar_id":"","url":"https://api.github.com/users/drewdahlke","html_url":"https://github.com/drewdahlke","followers_url":"https://api.github.com/users/drewdahlke/followers","following_url":"https://api.github.com/users/drewdahlke/following{/other_user}","gists_url":"https://api.github.com/users/drewdahlke/gists{/gist_id}","starred_url":"https://api.github.com/users/drewdahlke/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/drewdahlke/subscriptions","organizations_url":"https://api.github.com/users/drewdahlke/orgs","repos_url":"https://api.github.com/users/drewdahlke/repos","events_url":"https://api.github.com/users/drewdahlke/events{/privacy}","received_events_url":"https://api.github.com/users/drewdahlke/received_events","type":"User","site_admin":false},"created_at":"2015-06-15T12:31:05Z","updated_at":"2015-06-15T12:31:05Z","author_association":"NONE","body":"@jpountz If I recall doc values were added b/c un-inverting the index into field data cache on demand was expensive. It sounds like building the bitsets on demand are in the same boat, so wouldn't it make sense to persist them to disk as well? Putting a bounded cache on something disk backed would make it less trappy and allow people to take the performance hit on indexing/merging rather than heap usage at query time. \n\nI could see defaulting it to unbounded (prioritizing query performance), but providing a bounded option for people who need more control over heap usage. \n\n@schlosna If you end up forking before an official fix comes out, I could spare some cycles for code review/testing. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/112099826","html_url":"https://github.com/elastic/elasticsearch/issues/10224#issuecomment-112099826","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10224","id":112099826,"node_id":"MDEyOklzc3VlQ29tbWVudDExMjA5OTgyNg==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2015-06-15T15:05:21Z","updated_at":"2015-06-15T15:05:21Z","author_association":"CONTRIBUTOR","body":"@drewdahlke Totally agreed, we shouldn't have to load all this information about the nested structure in memory. It should reside on disk and we should read from there directly.\n","performed_via_github_app":null}]