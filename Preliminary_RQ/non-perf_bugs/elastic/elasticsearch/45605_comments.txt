[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/521604535","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-521604535","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":521604535,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMTYwNDUzNQ==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-08-15T11:04:52Z","updated_at":"2019-08-15T11:04:52Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/521617146","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-521617146","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":521617146,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMTYxNzE0Ng==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2019-08-15T12:00:23Z","updated_at":"2019-08-15T12:17:45Z","author_association":"CONTRIBUTOR","body":"I just noticed this is a more detailed investigation of one specific case of #45600, so this may not be a rare problem at all.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/521622447","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-521622447","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":521622447,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMTYyMjQ0Nw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-08-15T12:23:42Z","updated_at":"2019-08-15T12:23:42Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-core-infra","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/521623237","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-521623237","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":521623237,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMTYyMzIzNw==","user":{"login":"alpar-t","id":2565652,"node_id":"MDQ6VXNlcjI1NjU2NTI=","avatar_url":"https://avatars1.githubusercontent.com/u/2565652?v=4","gravatar_id":"","url":"https://api.github.com/users/alpar-t","html_url":"https://github.com/alpar-t","followers_url":"https://api.github.com/users/alpar-t/followers","following_url":"https://api.github.com/users/alpar-t/following{/other_user}","gists_url":"https://api.github.com/users/alpar-t/gists{/gist_id}","starred_url":"https://api.github.com/users/alpar-t/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alpar-t/subscriptions","organizations_url":"https://api.github.com/users/alpar-t/orgs","repos_url":"https://api.github.com/users/alpar-t/repos","events_url":"https://api.github.com/users/alpar-t/events{/privacy}","received_events_url":"https://api.github.com/users/alpar-t/received_events","type":"User","site_admin":false},"created_at":"2019-08-15T12:26:48Z","updated_at":"2019-08-15T12:26:48Z","author_association":"CONTRIBUTOR","body":"Could it be that somehow the clusters unite so in fact the tests are running against a larger cluster not two separate ones ? I think that would explain the behavior from the logs.\r\nI remember @ywelsch suggested we use unique cluster names for tests to make sure this doesn't happen, even trough I don't think it should with the current configuration. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/521626604","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-521626604","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":521626604,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMTYyNjYwNA==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2019-08-15T12:39:47Z","updated_at":"2019-08-15T12:39:47Z","author_association":"CONTRIBUTOR","body":"> Could it be that somehow the clusters unite\r\n\r\nMaybe that has happened in other CI runs, but in the CI run described in the description of this issue the two test clusters did not unite.  The server side logs for the docs cluster show that it only had 1 node (as expected) and the server side logs for the `qa/smoke-test-multinode` cluster show that it only had 2 nodes (as expected).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/521651555","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-521651555","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":521651555,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMTY1MTU1NQ==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2019-08-15T14:00:46Z","updated_at":"2019-08-15T14:00:46Z","author_association":"CONTRIBUTOR","body":"The two examples we have so far where tests for `qa/smoke-test-multinode` and `docs` failed around the same time may be coincidence.\r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+7.x+multijob-windows-compatibility/os=windows-2016/71/console is an example where the failure to delete an index between tests occurs and just a single test fails in the CI run:\r\n\r\n```\r\n  1> [2019-08-15T04:48:49,931][WARN ][o.e.i.IndicesService     ] [node_t1] [test/y12jwHyUSiS70AhJ_iAD3g] failed to delete index\r\n  1> org.elasticsearch.env.ShardLockObtainFailedException: [test][0]: obtaining shard lock timed out after 0ms, previous lock details: [shard creation] trying to lock for [deleting index directory]\r\n```\r\n\r\nThe failure that follows on from that is:\r\n\r\n```\r\n    java.lang.IllegalStateException: Some shards are still open after the threadpool terminated. Something is leaking index readers or store references.\r\n```\r\n\r\nThe repro command for this failure was:\r\n\r\n```\r\ngradlew :server:integTest --tests \"org.elasticsearch.cluster.allocation.ClusterRerouteIT.testRerouteExplain\" \\\r\n  -Dtests.seed=13BCAC0C1240CA7E \\\r\n  -Dtests.security.manager=true \\\r\n  -Dtests.locale=fi \\\r\n  -Dtests.timezone=Canada/Yukon \\\r\n  -Dcompiler.java=12 \\\r\n  -Druntime.java=8\r\n```\r\n\r\nGiven that the problem occurs in the cleanup in between tests I'm not sure individual repro commands are that helpful though.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/521657631","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-521657631","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":521657631,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMTY1NzYzMQ==","user":{"login":"alpar-t","id":2565652,"node_id":"MDQ6VXNlcjI1NjU2NTI=","avatar_url":"https://avatars1.githubusercontent.com/u/2565652?v=4","gravatar_id":"","url":"https://api.github.com/users/alpar-t","html_url":"https://github.com/alpar-t","followers_url":"https://api.github.com/users/alpar-t/followers","following_url":"https://api.github.com/users/alpar-t/following{/other_user}","gists_url":"https://api.github.com/users/alpar-t/gists{/gist_id}","starred_url":"https://api.github.com/users/alpar-t/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alpar-t/subscriptions","organizations_url":"https://api.github.com/users/alpar-t/orgs","repos_url":"https://api.github.com/users/alpar-t/repos","events_url":"https://api.github.com/users/alpar-t/events{/privacy}","received_events_url":"https://api.github.com/users/alpar-t/received_events","type":"User","site_admin":false},"created_at":"2019-08-15T14:18:04Z","updated_at":"2019-08-15T14:18:04Z","author_association":"CONTRIBUTOR","body":"As a side note, it seems that we should fail if the cleanup fails instead of attempting to run the tests","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/523793399","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-523793399","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":523793399,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMzc5MzM5OQ==","user":{"login":"alpar-t","id":2565652,"node_id":"MDQ6VXNlcjI1NjU2NTI=","avatar_url":"https://avatars1.githubusercontent.com/u/2565652?v=4","gravatar_id":"","url":"https://api.github.com/users/alpar-t","html_url":"https://github.com/alpar-t","followers_url":"https://api.github.com/users/alpar-t/followers","following_url":"https://api.github.com/users/alpar-t/following{/other_user}","gists_url":"https://api.github.com/users/alpar-t/gists{/gist_id}","starred_url":"https://api.github.com/users/alpar-t/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alpar-t/subscriptions","organizations_url":"https://api.github.com/users/alpar-t/orgs","repos_url":"https://api.github.com/users/alpar-t/repos","events_url":"https://api.github.com/users/alpar-t/events{/privacy}","received_events_url":"https://api.github.com/users/alpar-t/received_events","type":"User","site_admin":false},"created_at":"2019-08-22T07:54:17Z","updated_at":"2019-08-22T07:54:17Z","author_association":"CONTRIBUTOR","body":"We looked at [this failure](https://gradle-enterprise.elastic.co/s/7idvdpc45pih4) on slack a couple of days ago with @original-brownbear and did not update here. Doing so in case someone finds it useful. \r\n\r\nThere were some CCR test failures that looked similar ( index not found ) that had to do with caching test tasks when we shouldn't have. CCR tests are usually made up of a test that sets CCR up and another one that checks on it, so if part of it get's cached the index will never replicate. This was fixed in #45659 and is unrelated to this issue as here we are dealing with tests that do run with testclusters which can correctly deal with caching. \r\n\r\nIt also seems that this fails on tests that run against a multi node cluster. \r\n\r\nThe following exception shows up after the test when the test framework tries to clean up all indices: \r\n```\r\n[2019-08-19T12:37:44,585][WARN ][o.e.i.IndicesService     ] [integTest-0] [goodbad/29xCvTM8T9GIAsfnTPWONQ] failed to delete index\r\norg.elasticsearch.env.ShardLockObtainFailedException: [goodbad][0]: obtaining shard lock timed out after 0ms, previous lock details: [shard creation] trying to lock for [deleting index directory]\r\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:860) ~[elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:775) ~[elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.env.NodeEnvironment.lockAllForIndex(NodeEnvironment.java:718) ~[elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.env.NodeEnvironment.deleteIndexDirectorySafe(NodeEnvironment.java:670) ~[elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.indices.IndicesService.deleteIndexStoreIfDeletionAllowed(IndicesService.java:800) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.indices.IndicesService.deleteIndexStore(IndicesService.java:787) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.indices.IndicesService.removeIndex(IndicesService.java:691) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.deleteIndices(IndicesClusterStateService.java:375) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyClusterState(IndicesClusterStateService.java:241) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.cluster.service.ClusterApplierService.lambda$callClusterStateAppliers$5(ClusterApplierService.java:508) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at java.lang.Iterable.forEach(Iterable.java:75) [?:?]\r\n    at org.elasticsearch.cluster.service.ClusterApplierService.callClusterStateAppliers(ClusterApplierService.java:505) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.cluster.service.ClusterApplierService.applyChanges(ClusterApplierService.java:482) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.cluster.service.ClusterApplierService.runTask(ClusterApplierService.java:432) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.cluster.service.ClusterApplierService$UpdateTask.run(ClusterApplierService.java:176) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:699) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\r\n    at java.lang.Thread.run(Thread.java:834) [?:?]\r\n```\r\n\r\nIn this case we had 2 tests that used the same index. \r\nThe first one to do so failed, saying the index already exists, and then the cleanup failed ( silently ) again when trying to delete that existing index, yet the next test that used that index name passed. \r\n\r\nThere is no evidence that the index that exists was ever created. No tests that use that index seem to have ran prior to this, and the test __is__ connected to the right cluster, that has the right nodes, excluding cross-talk with other tests. \r\nThis also seems to happen in bwc tests that __do not__ run in parallel.\r\n`checkPart1` seems to fail most often so running `./gradlew checkPart1 --parallel` on a CI worker is likely to reproduce this.\r\n\r\n@dnhatn suggested this could be related to #45409  but we seem to have seen failures after that was merged as well. \r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/523898041","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-523898041","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":523898041,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMzg5ODA0MQ==","user":{"login":"williamrandolph","id":3253644,"node_id":"MDQ6VXNlcjMyNTM2NDQ=","avatar_url":"https://avatars3.githubusercontent.com/u/3253644?v=4","gravatar_id":"","url":"https://api.github.com/users/williamrandolph","html_url":"https://github.com/williamrandolph","followers_url":"https://api.github.com/users/williamrandolph/followers","following_url":"https://api.github.com/users/williamrandolph/following{/other_user}","gists_url":"https://api.github.com/users/williamrandolph/gists{/gist_id}","starred_url":"https://api.github.com/users/williamrandolph/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/williamrandolph/subscriptions","organizations_url":"https://api.github.com/users/williamrandolph/orgs","repos_url":"https://api.github.com/users/williamrandolph/repos","events_url":"https://api.github.com/users/williamrandolph/events{/privacy}","received_events_url":"https://api.github.com/users/williamrandolph/received_events","type":"User","site_admin":false},"created_at":"2019-08-22T13:07:47Z","updated_at":"2019-08-22T13:07:47Z","author_association":"CONTRIBUTOR","body":"On this intake build, OpenCloseIndexIT failed with symptoms that were similar to what @atorok described above: mysterious test failure, then `ShardLockObtainFailedException` at test cleanup time. https://gradle-enterprise.elastic.co/s/t26bxyvqgurkq/tests/kyv2y2z3r4v7m-rg5rz5gnpfyvq","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/525935230","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-525935230","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":525935230,"node_id":"MDEyOklzc3VlQ29tbWVudDUyNTkzNTIzMA==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2019-08-28T21:45:40Z","updated_at":"2019-08-28T21:45:40Z","author_association":"CONTRIBUTOR","body":"As noted in https://github.com/elastic/elasticsearch/issues/45956#issuecomment-525932500 I do not think that `org.elasticsearch.env.ShardLockObtainFailedException: [goodbad][0]: obtaining shard lock timed out after 0ms, previous lock details: [shard creation] trying to lock for [deleting index directory]` is necessarily a problem. But looking at the build failure linked in https://github.com/elastic/elasticsearch/issues/45605#issuecomment-521651555 (i.e. https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+7.x+multijob-windows-compatibility/os=windows-2016/71/consoleText) I see this:\r\n\r\n```\r\n  1> [2019-08-15T04:48:59,949][WARN ][o.e.i.c.IndicesClusterStateService] [node_t0] [[test/y12jwHyUSiS70AhJ_iAD3g]] failed to complete pending deletion for index\r\n  1> org.elasticsearch.env.ShardLockObtainFailedException: [test][0]: thread interrupted while trying to obtain shard lock\r\n  1> \tat org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:775) ~[main/:?]\r\n  1> \tat org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:684) ~[main/:?]\r\n  1> \tat org.elasticsearch.env.NodeEnvironment.lockAllForIndex(NodeEnvironment.java:627) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.IndicesService.processPendingDeletes(IndicesService.java:1077) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.cluster.IndicesClusterStateService$2.doRun(IndicesClusterStateService.java:356) [main/:?]\r\n  1> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:773) [main/:?]\r\n  1> \tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [main/:?]\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_221]\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_221]\r\n  1> \tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_221]\r\n  1> Caused by: java.lang.InterruptedException\r\n  1> \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1039) ~[?:1.8.0_221]\r\n  1> \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328) ~[?:1.8.0_221]\r\n  1> \tat java.util.concurrent.Semaphore.tryAcquire(Semaphore.java:409) ~[?:1.8.0_221]\r\n  1> \tat org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:766) ~[main/:?]\r\n  1> \t... 9 more\r\n```\r\n\r\nInterrupting this wait for the shard lock while the node is closing does seem like it might be problematic.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/528296440","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-528296440","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":528296440,"node_id":"MDEyOklzc3VlQ29tbWVudDUyODI5NjQ0MA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2019-09-05T10:10:55Z","updated_at":"2019-09-05T10:10:55Z","author_association":"CONTRIBUTOR","body":"I've edited the title here. The `ShardLockObtainFailedException` and `IndexNotFoundException` warnings are harmless and also happen on successful runs. I've created #46368 to quiet the `ShardLockObtainFailedException` down as they seem to cause confusion (as any warning that we log).\r\n\r\nWe still need to figure out what's causing the resource_already_exists_exception","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/529939062","html_url":"https://github.com/elastic/elasticsearch/issues/45605#issuecomment-529939062","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45605","id":529939062,"node_id":"MDEyOklzc3VlQ29tbWVudDUyOTkzOTA2Mg==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2019-09-10T13:38:13Z","updated_at":"2019-09-10T13:38:13Z","author_association":"CONTRIBUTOR","body":"Subsequent updates moved to #46091, so closing this one.","performed_via_github_app":null}]