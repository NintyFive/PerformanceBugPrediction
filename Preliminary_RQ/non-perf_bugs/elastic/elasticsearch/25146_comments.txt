[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/307357333","html_url":"https://github.com/elastic/elasticsearch/issues/25146#issuecomment-307357333","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25146","id":307357333,"node_id":"MDEyOklzc3VlQ29tbWVudDMwNzM1NzMzMw==","user":{"login":"javanna","id":832460,"node_id":"MDQ6VXNlcjgzMjQ2MA==","avatar_url":"https://avatars1.githubusercontent.com/u/832460?v=4","gravatar_id":"","url":"https://api.github.com/users/javanna","html_url":"https://github.com/javanna","followers_url":"https://api.github.com/users/javanna/followers","following_url":"https://api.github.com/users/javanna/following{/other_user}","gists_url":"https://api.github.com/users/javanna/gists{/gist_id}","starred_url":"https://api.github.com/users/javanna/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/javanna/subscriptions","organizations_url":"https://api.github.com/users/javanna/orgs","repos_url":"https://api.github.com/users/javanna/repos","events_url":"https://api.github.com/users/javanna/events{/privacy}","received_events_url":"https://api.github.com/users/javanna/received_events","type":"User","site_admin":false},"created_at":"2017-06-09T10:44:10Z","updated_at":"2017-06-09T10:44:10Z","author_association":"MEMBER","body":"Hi @arosenheinrich what is your own client based on? We depend on apache http async client and we can expose pretty much anything that such library supports. Can you expand a bit on what you did and what would help in your case?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/310661029","html_url":"https://github.com/elastic/elasticsearch/issues/25146#issuecomment-310661029","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25146","id":310661029,"node_id":"MDEyOklzc3VlQ29tbWVudDMxMDY2MTAyOQ==","user":{"login":"arosenheinrich","id":5214937,"node_id":"MDQ6VXNlcjUyMTQ5Mzc=","avatar_url":"https://avatars0.githubusercontent.com/u/5214937?v=4","gravatar_id":"","url":"https://api.github.com/users/arosenheinrich","html_url":"https://github.com/arosenheinrich","followers_url":"https://api.github.com/users/arosenheinrich/followers","following_url":"https://api.github.com/users/arosenheinrich/following{/other_user}","gists_url":"https://api.github.com/users/arosenheinrich/gists{/gist_id}","starred_url":"https://api.github.com/users/arosenheinrich/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arosenheinrich/subscriptions","organizations_url":"https://api.github.com/users/arosenheinrich/orgs","repos_url":"https://api.github.com/users/arosenheinrich/repos","events_url":"https://api.github.com/users/arosenheinrich/events{/privacy}","received_events_url":"https://api.github.com/users/arosenheinrich/received_events","type":"User","site_admin":false},"created_at":"2017-06-23T13:09:08Z","updated_at":"2017-06-23T13:09:08Z","author_association":"NONE","body":"Of course, sorry for the late answer. \r\n\r\nThe problem is that we can have quite large (several 100MB) data for a single field in a document. So passing or receiving this data with apache http async client will be a problem because of memory consumption for buffering the object. Apart you could(?) run in limitation like timeouts or body size. Therefore we are using good ol' sync client instead of async and we provide input or output streams as parameter or response so that memory consumption is way lower. Apart, at least in our case this is helpful, we can use libraries that will process such a result stream but couldn't handle a data object of that size. \r\n\r\nSo basicly what would help us would be an option to perform a sync request instead of async and at least get an output stream as result instead of the whole object. Providing data via input stream would be even better ;)\r\n\r\nDoes that make sense to you and would this be useful in a more general context?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/310696028","html_url":"https://github.com/elastic/elasticsearch/issues/25146#issuecomment-310696028","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25146","id":310696028,"node_id":"MDEyOklzc3VlQ29tbWVudDMxMDY5NjAyOA==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-06-23T15:26:21Z","updated_at":"2017-06-23T15:26:21Z","author_association":"CONTRIBUTOR","body":"> The problem is that we can have quite large (several 100MB) data for a single field in a document.\r\n\r\nThis is going to cause Elasticsearch heartburn in general, I think.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/311330133","html_url":"https://github.com/elastic/elasticsearch/issues/25146#issuecomment-311330133","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25146","id":311330133,"node_id":"MDEyOklzc3VlQ29tbWVudDMxMTMzMDEzMw==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-06-27T11:25:37Z","updated_at":"2017-06-27T11:25:37Z","author_association":"CONTRIBUTOR","body":"> The problem is that we can have quite large (several 100MB) data for a single field in a document. So passing or receiving this data with apache http async client will be a problem because of memory consumption for buffering the object. \r\n\r\nelasticsearch is pretty much suffering from the same issue. We have to buffer the documents in memory on all nodes that are involved in the indexing process. There is no streaming of documents in elasticseach itself. From that perspective it might be considered a feature if you don't have a streaming option. I personally think it's good if an API reflects what the software / backend is made for rather than giving the user the impression it has been made for streaming large amounts of data.\r\n\r\n> Apart you could(?) run in limitation like timeouts or body size. \r\n\r\nalso this is a feature here? I mean the body size limitation is already big, I think it's 1GB at this point but I might be mistaken. Timeouts can be configured on the client side and if transferring large documents is running into and issue I guess this is exactly why this option is configurable. \r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/311644617","html_url":"https://github.com/elastic/elasticsearch/issues/25146#issuecomment-311644617","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25146","id":311644617,"node_id":"MDEyOklzc3VlQ29tbWVudDMxMTY0NDYxNw==","user":{"login":"arosenheinrich","id":5214937,"node_id":"MDQ6VXNlcjUyMTQ5Mzc=","avatar_url":"https://avatars0.githubusercontent.com/u/5214937?v=4","gravatar_id":"","url":"https://api.github.com/users/arosenheinrich","html_url":"https://github.com/arosenheinrich","followers_url":"https://api.github.com/users/arosenheinrich/followers","following_url":"https://api.github.com/users/arosenheinrich/following{/other_user}","gists_url":"https://api.github.com/users/arosenheinrich/gists{/gist_id}","starred_url":"https://api.github.com/users/arosenheinrich/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arosenheinrich/subscriptions","organizations_url":"https://api.github.com/users/arosenheinrich/orgs","repos_url":"https://api.github.com/users/arosenheinrich/repos","events_url":"https://api.github.com/users/arosenheinrich/events{/privacy}","received_events_url":"https://api.github.com/users/arosenheinrich/received_events","type":"User","site_admin":false},"created_at":"2017-06-28T12:26:55Z","updated_at":"2017-06-28T12:26:55Z","author_association":"NONE","body":"Hi Simon, thanks for the clarifications. I agree, moving the problem from client side to the server ain't really solving the problem. \r\nI don't like the idea of solving this by configuration, as you pointed out there are good reasons why they are set the way they are and changing them for rarely used special cases might mask other problems with the majority of use cases. \r\n\r\n Maybe it would be interesting to think about supporting streaming in elasticsearch for datatypes like binary where no indexing is done and data can be large, but that is certainly beyond the scope of my request. I will therefore close it here, thanks for your explanations!","performed_via_github_app":null}]