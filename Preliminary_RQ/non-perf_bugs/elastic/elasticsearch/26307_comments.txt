[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/334798014","html_url":"https://github.com/elastic/elasticsearch/issues/26307#issuecomment-334798014","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26307","id":334798014,"node_id":"MDEyOklzc3VlQ29tbWVudDMzNDc5ODAxNA==","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2017-10-06T16:02:28Z","updated_at":"2017-10-06T16:02:28Z","author_association":"MEMBER","body":"I worked on integrating the [CoveringQuery in the percolator](https://github.com/martijnvg/elasticsearch/commit/c714748e856ba4190e5f7ab5c1f25d73e3c41c0a) and it is a good fit! I tested this change out on a percolator query set and the query time was reduced by 3 times.\r\n\r\nI did run into two challenges:\r\n* Currently all the extracted terms from the document to be percolated are being put in a `TermInSetQuery` instance. This is no longer possible with the `CoveringQuery`, because then all terms inside `TermsInSetQuery` count as single match. So currently in the commit mentioned above [each query term is added as a separate`TermQuery` instance](https://github.com/martijnvg/elasticsearch/commit/c714748e856ba4190e5f7ab5c1f25d73e3c41c0a#diff-7a987e4fb3c8a1820bfca74aa3e8b5c0R316) to the `CoveringQuery`. This is not ideal because if a large document is to be percolated then could cause a `TooManyClauses` exception. I need to think about this more on how to solve this.\r\n* In case of range queries each extraction does not simply increment the minimum_should_match for that percolator query like for a term based extraction, so that can lead to more false positives for percolator queries with range queries than term based queries. The is because the way number fields are extracted from the document to be percolated. Per field a single range is extracted and if a percolator query has two or more range queries on the same field than the the minimum should match can be higher than clauses in the CoveringQuery. Therefore right now the minimum should match [is incremented once per number field](https://github.com/martijnvg/elasticsearch/commit/c714748e856ba4190e5f7ab5c1f25d73e3c41c0a#diff-aa342f9a6ef361b8871c6181dcc0ce36R321) when processing the percolator query at index time. I think that this is acceptable, percolator queries with range queries are more prone to false positives than percolator queries with term based queries to begin with, due to how the extracted ranges are stored.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/334799956","html_url":"https://github.com/elastic/elasticsearch/issues/26307#issuecomment-334799956","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26307","id":334799956,"node_id":"MDEyOklzc3VlQ29tbWVudDMzNDc5OTk1Ng==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-10-06T16:09:56Z","updated_at":"2017-10-06T16:09:56Z","author_association":"CONTRIBUTOR","body":"> This is not ideal because if a large document is to be percolated then could cause a TooManyClauses exception\r\n\r\nMaybe we should still use a `TermsQuery` when percolated documents are large?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/334801612","html_url":"https://github.com/elastic/elasticsearch/issues/26307#issuecomment-334801612","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26307","id":334801612,"node_id":"MDEyOklzc3VlQ29tbWVudDMzNDgwMTYxMg==","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2017-10-06T16:16:24Z","updated_at":"2017-10-06T16:16:24Z","author_association":"MEMBER","body":"> Maybe we should still use a TermsQuery when percolated documents are large?\r\n\r\nand than fallback using a constant `LongValuesSource` instance set to 1  as `minimumNumberMatch`?","performed_via_github_app":null}]