{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585/events","html_url":"https://github.com/elastic/elasticsearch/issues/21585","id":189532130,"node_id":"MDU6SXNzdWUxODk1MzIxMzA=","number":21585,"title":"Index Partitioning with Custom Routing","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"labels":[{"id":145572580,"node_id":"MDU6TGFiZWwxNDU1NzI1ODA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/CRUD","name":":Distributed/CRUD","color":"0e8a16","default":false,"description":"A catch all label for issues around indexing, updating and getting a doc by id. Not search."},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":19,"created_at":"2016-11-15T23:00:17Z","updated_at":"2017-01-18T07:51:24Z","closed_at":"2017-01-18T07:51:24Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I'm interested in adding an optimization that would make it possible for a custom routing value go to a subset of shards in an index rather than just a specific shard. This would work by picking the subset of shards using the routing value then pick the shard within that set using the document ID.\r\n\r\nThe primary use case for this is multi-tenant indices with custom routing that suffer from one or both of the following:\r\n\r\n- The big users cannot fit into a single shard or there are so many of them that it is highly likely that they will land on the same shards\r\n- Users occasionally have a surge in write traffic that a single shard cannot process fast enough\r\n\r\nUltimately, you end up with the dreaded hot shard, severely impacting the write throughput and search latency of all requests that visit that node.\r\n\r\nI am familiar with the parts of the Elasticsearch documentation that discuss [faking an index per user with aliases](https://www.elastic.co/guide/en/elasticsearch/guide/current/faking-it.html) and [giving the big users their own index](https://www.elastic.co/guide/en/elasticsearch/guide/current/one-big-user.html). This approach works well initially but what the documentation does not discuss is what happens when you hit 100,000 users, 500,000 users, then 1,000,000 users? Since the [cluster state is finite](https://www.elastic.co/guide/en/elasticsearch/guide/current/finite-scale.html) then having millions of aliases is not good position to be in, especially with it constantly growing for every new user.\r\n\r\nSuppose we had a 100 shard index and we knew our biggest users were each ~3% of the total size. Without being able to use custom routing (users are too big) or aliases per user (too many users) then you would have to settle for one giant index, leading to a sub-optimal configuration since you are not leveraging the spatial locality that the custom routing can give you. As a result, every search request hits all 100 shards in the index.\r\n\r\nIf you could partition this index, let's say 10 ways to be safe, then any given user can only be found on 10 shards. It would not be far fetched to see up to a 10x increase in search throughput on the same amount of hardware when custom routing is provided with this change since you have significantly reduced the work that needs to be done per request.\r\n\r\nIt is certainly possible to achieve this through a client managed hashing scheme such that you have 10 indices of 10 shards each then you just do your own hashing to determine which index you route the request to. However, this would require you to have a special client wrapper for accessing Elasticsearch and if you ever wanted to change your partitioning scheme then you would need to do a coordinated deployment. If this partitioning was managed by Elasticsearch then it would just be a reindex followed by an alias flip.\r\n\r\nI believe this is a generic enough problem that it makes sense to implement this in Elasticsearch, making it easier for other developers in the community to benefit from without having to write their own hashing code and worrying about the complexities that go along with it.\r\n\r\nBefore I got to far along on writing the code I wanted to see if there would be any strong objections to such a feature. For example, one shortcoming is that it would not be possible to guarantee that a grandparent would be in the same shard as its grandchildren (which I believe is a reasonable limitation).\r\n\r\nFor reference, I did manage to dig up a similar feature request from a couple of years ago: https://github.com/elastic/elasticsearch/issues/8472 (hopefully it's ok that I opened a new issue for this)","closed_by":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"performed_via_github_app":null}