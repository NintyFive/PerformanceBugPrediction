[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/152295343","html_url":"https://github.com/elastic/elasticsearch/issues/14299#issuecomment-152295343","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14299","id":152295343,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MjI5NTM0Mw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-10-29T19:31:21Z","updated_at":"2015-10-29T19:31:21Z","author_association":"CONTRIBUTOR","body":"@yiguolei I don't understand why would you want to run two instances on the same disks?  You're splitting the I/O between two instances.  \n\nAlso, a path doesn't have a max amount of space.  Only the disk has that.  \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/293337675","html_url":"https://github.com/elastic/elasticsearch/issues/14299#issuecomment-293337675","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14299","id":293337675,"node_id":"MDEyOklzc3VlQ29tbWVudDI5MzMzNzY3NQ==","user":{"login":"simpleopen","id":17908779,"node_id":"MDQ6VXNlcjE3OTA4Nzc5","avatar_url":"https://avatars0.githubusercontent.com/u/17908779?v=4","gravatar_id":"","url":"https://api.github.com/users/simpleopen","html_url":"https://github.com/simpleopen","followers_url":"https://api.github.com/users/simpleopen/followers","following_url":"https://api.github.com/users/simpleopen/following{/other_user}","gists_url":"https://api.github.com/users/simpleopen/gists{/gist_id}","starred_url":"https://api.github.com/users/simpleopen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/simpleopen/subscriptions","organizations_url":"https://api.github.com/users/simpleopen/orgs","repos_url":"https://api.github.com/users/simpleopen/repos","events_url":"https://api.github.com/users/simpleopen/events{/privacy}","received_events_url":"https://api.github.com/users/simpleopen/received_events","type":"User","site_admin":false},"created_at":"2017-04-11T17:30:02Z","updated_at":"2017-04-11T17:30:02Z","author_association":"NONE","body":"Hi, @clintongormley ,\r\n\r\nTentatively,  I think I have a user case of @yiguolei 's proposal. The feature of disk usage quota per data path can help prevent a single point of issue from causing all-cluster data loss incident. Do you think we can maybe revisit this discussion, and get your feedbacks?\r\n\r\nWe have a self-hosted Elasticsearch cluster, accepting log feeds from multiple sources. Due to a Logstash configuration error, one feeding source suddenly went crazy by sending a huge amount of data within a short period. We have a very big disk buffer, but it was still eaten up. All the nodes got a disk full, and the cluster’s functionality came into a halt. It took some time to fix the problem, and incoming data is lost during the gap.\r\n\r\nFeels like, a single point of issue should not fail the entire cluster, and we want to look into possibly restricting total disk usage of indices from each source.\r\n\r\nOn our cluster, each source creates one index per day, with naming pattern source_name-YYYY-MM-DD, and we can get total disk usage per source by command “du -chs /indices/source_name-* | grep total”.\r\nTentatively thinking, maybe we can have a watchdog script to check it, and close or delete the indices that exceed the quota. I am wondering if there is any existing tool for it?\r\n\r\nI am also wondering how Elasticsearch Cloud handles disk usage issue, assuming there are also multiple clients on their shared host. Is it deploying one virtual machine for each client or something similar?\r\n\r\nFeedbacks and pointers will be highly appreciated.\r\n","performed_via_github_app":null}]