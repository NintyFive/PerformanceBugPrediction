[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387403954","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387403954","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387403954,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzQwMzk1NA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-05-08T13:34:55Z","updated_at":"2018-05-08T13:34:55Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387407072","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387407072","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387407072,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzQwNzA3Mg==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2018-05-08T13:44:42Z","updated_at":"2018-05-08T13:44:42Z","author_association":"MEMBER","body":"These look like ML issues, I am reassigning.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387407108","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387407108","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387407108,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzQwNzEwOA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-05-08T13:44:50Z","updated_at":"2018-05-08T13:44:50Z","author_association":"COLLABORATOR","body":"Pinging @elastic/ml-core","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387666521","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387666521","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387666521,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzY2NjUyMQ==","user":{"login":"davidkyle","id":2353640,"node_id":"MDQ6VXNlcjIzNTM2NDA=","avatar_url":"https://avatars1.githubusercontent.com/u/2353640?v=4","gravatar_id":"","url":"https://api.github.com/users/davidkyle","html_url":"https://github.com/davidkyle","followers_url":"https://api.github.com/users/davidkyle/followers","following_url":"https://api.github.com/users/davidkyle/following{/other_user}","gists_url":"https://api.github.com/users/davidkyle/gists{/gist_id}","starred_url":"https://api.github.com/users/davidkyle/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidkyle/subscriptions","organizations_url":"https://api.github.com/users/davidkyle/orgs","repos_url":"https://api.github.com/users/davidkyle/repos","events_url":"https://api.github.com/users/davidkyle/events{/privacy}","received_events_url":"https://api.github.com/users/davidkyle/received_events","type":"User","site_admin":false},"created_at":"2018-05-09T08:40:14Z","updated_at":"2018-05-09T08:40:14Z","author_association":"MEMBER","body":"Failures have been spotted on the 6.x and master branches. All failures on master occurred upgrading from 6.4 and on 6.x upgrading from 6.2.1, 6.2.3, 6.2.5 and 6.3.0. \r\n\r\nI pulled the logs for one instance a unfortunately they don't provide much insight without debug level logging. \r\n\r\n[mixedcluster.log](https://github.com/elastic/elasticsearch/files/1987005/mixedcluster.log)\r\n[oldcluster.log](https://github.com/elastic/elasticsearch/files/1987006/oldcluster.log)\r\n[upgradedcluster.log](https://github.com/elastic/elasticsearch/files/1987007/upgradedcluster.log)\r\n\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387674203","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387674203","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387674203,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzY3NDIwMw==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2018-05-09T09:06:22Z","updated_at":"2018-05-09T09:06:22Z","author_association":"CONTRIBUTOR","body":"I think the problem is one that we've had intermittently for ages.  I imagine something has recently changed elsewhere in ES or the test infrastructure that makes it more likely that a bug that has existed basically forever to be triggered.\r\n\r\nTo summarise we've had two theories about what might cause this:\r\n\r\n1. Our results index has `async` durability.  We've done testing in the past that shows this dramatically improves performance for our usage pattern.  And it's usually reasonable because in the event of a complete node failure losing a result in the translog is no worse than losing a result that existed in memory but hadn't yet been submitted for indexing.  But what we're doing wrong is that our flush and close endpoints are supposed to guarantee that when they return everything prior to the call is safely persisted.  So ideally our flush and close endpoints should do something that ensures the translog is synced in addition to what they currently do before returning. \r\n2. When we write our results documents we're not waiting for acknowledgement from all replicas, only the primary.  In the case of these rolling upgrade tests it's possible that the primary shard of our results index is on the node that gets taken out of the cluster to be replaced with one running the upgraded version.  If our results document had not made it to the replica then it won't exist in the upgraded cluster.  As with (1), in general this is fine, as a recent result could have been lost in memory if a node was brutally removed from the cluster.  But with flush and close we're not giving the guarantee we should be.  Ideally before returning from a flush or close we'd do something that ensured every document indexed to the results index for the job had been written to both primary and replica.\r\n\r\nI suspect in the case of the tests (2) is more likely, because `fsync` would only have an impact in the case of OS or hardware failure, not abrupt shutdown of a user-space program.  However, to meet the guarantees of ML's flush and close for real-world failures we need to find ways to fix both problems without degrading performance in cases where flush and close are called rarely.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387731644","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387731644","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387731644,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzczMTY0NA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-05-09T13:05:27Z","updated_at":"2018-05-09T13:05:27Z","author_association":"CONTRIBUTOR","body":"> When we write our results documents we're not waiting for acknowledgement from all replicas, only the primary\r\n\r\nHow is that possible? I'm not aware of any option in ES that would allow you to only wait on acknowledgement from the primary.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387731685","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387731685","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387731685,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzczMTY4NQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-05-09T13:05:35Z","updated_at":"2018-05-09T13:05:35Z","author_association":"MEMBER","body":"@droberts195 I'm not sure what you mean with:\r\n\r\n> we're not waiting for acknowledgement from all replicas, only the primary\r\n\r\nES does it all the time. Do you mean you don't ensure all replicas are fully allocated? ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387740738","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387740738","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387740738,"node_id":"MDEyOklzc3VlQ29tbWVudDM4Nzc0MDczOA==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2018-05-09T13:36:24Z","updated_at":"2018-05-09T13:36:24Z","author_association":"CONTRIBUTOR","body":"> Do you mean you don't ensure all replicas are fully allocated?\r\n\r\nYes, sorry, this is what I meant.  I think we need to add a `wait_for_active_shards=all` on some of our indexing operations.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387742199","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387742199","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387742199,"node_id":"MDEyOklzc3VlQ29tbWVudDM4Nzc0MjE5OQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-05-09T13:41:16Z","updated_at":"2018-05-09T13:41:16Z","author_association":"MEMBER","body":"@droberts195 double checking, but you mean in tests only, right? I'm not sure that's the right move for production.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387749475","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387749475","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387749475,"node_id":"MDEyOklzc3VlQ29tbWVudDM4Nzc0OTQ3NQ==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2018-05-09T14:03:42Z","updated_at":"2018-05-09T14:03:42Z","author_association":"CONTRIBUTOR","body":"@bleskes if `wait_for_active_shards=all` is wrong for production then the way we could probably solve this in the tests is to wait for green status after each of the \"old cluster\" tests.  That should guarantee that both the primary and replica for our results index exist before we kill one of the nodes.\r\n\r\nOur results index is created on demand from a template when the first result is indexed, and the job those tests run is tiny - it only receives 2 input documents - so I guess it's quite likely that the replica is not ready at the point where one of the nodes is killed prior to upgrade.  @davidkyle has managed to reproduce this locally often enough to try out possible solutions.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387762428","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-387762428","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":387762428,"node_id":"MDEyOklzc3VlQ29tbWVudDM4Nzc2MjQyOA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-05-09T14:42:04Z","updated_at":"2018-05-09T14:42:04Z","author_association":"MEMBER","body":"> @bleskes if wait_for_active_shards=all is wrong for production then the way we could probably solve this in the tests is to wait for green status after each of the \"old cluster\" tests. That should guarantee that both the primary and replica for our results index exist before we kill one of the nodes.\r\n\r\n+1.  The problem with `wait_for_active_shard=all` in production is that it will block indexing when a node dies and a shard copy is lost until the it is completely rebuilt on another node (which may not exist). I think this is not acceptable. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388304923","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-388304923","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":388304923,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODMwNDkyMw==","user":{"login":"davidkyle","id":2353640,"node_id":"MDQ6VXNlcjIzNTM2NDA=","avatar_url":"https://avatars1.githubusercontent.com/u/2353640?v=4","gravatar_id":"","url":"https://api.github.com/users/davidkyle","html_url":"https://github.com/davidkyle","followers_url":"https://api.github.com/users/davidkyle/followers","following_url":"https://api.github.com/users/davidkyle/following{/other_user}","gists_url":"https://api.github.com/users/davidkyle/gists{/gist_id}","starred_url":"https://api.github.com/users/davidkyle/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidkyle/subscriptions","organizations_url":"https://api.github.com/users/davidkyle/orgs","repos_url":"https://api.github.com/users/davidkyle/repos","events_url":"https://api.github.com/users/davidkyle/events{/privacy}","received_events_url":"https://api.github.com/users/davidkyle/received_events","type":"User","site_admin":false},"created_at":"2018-05-11T08:57:39Z","updated_at":"2018-05-11T08:57:39Z","author_association":"MEMBER","body":"The failures are characterised by missing documents after a node has been upgraded, for example when a job is closed in the old cluster its model state is persisted but often this document cannot be found in the mixed cluster after a single node is upgraded. Here are some of the issues I believe may be the cause\r\n\r\n\r\n1. Translog settings\r\nThe ML indices are configured with [index.translog.durability: async](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/MachineLearning.java#L637). The rolling upgrade tests `kill -9` the node after the suite is run before upgrading, it seems likely that the missing documents are in the translog which does not have an oppourtunity to flush before being brutally killed. However, I have not been able to confirm this hypothesis, defaulting the `index.translog.durability` setting or flushing the indices after the test does help reduce the frequency of failures but failures still occur.\r\n\r\n2. Replica Recovery\r\nIf the problem is not the translog durabilty settting then could it be the missing document are in a shard that is still initialising or recovering? Waiting for no initialising shards in test setup does not help the problem.\r\n\r\n3. Auto Expand Replicas\r\nIndices are created on demand with templates and have `index.auto_expand_replicas: 0-1` set. Could this combination of settings be problematic.\r\n\r\n4. Flush Sync\r\nI can get the tests to pass consistently with a combination of flush and flush sync at the end of tests I believe this enables speedy recovery of replicas after upgrade. I'm not certain what the cause is and I feel like I'm wrapping more and more duct tape (flushes) around the problem until something holds.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388404824","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-388404824","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":388404824,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODQwNDgyNA==","user":{"login":"davidkyle","id":2353640,"node_id":"MDQ6VXNlcjIzNTM2NDA=","avatar_url":"https://avatars1.githubusercontent.com/u/2353640?v=4","gravatar_id":"","url":"https://api.github.com/users/davidkyle","html_url":"https://github.com/davidkyle","followers_url":"https://api.github.com/users/davidkyle/followers","following_url":"https://api.github.com/users/davidkyle/following{/other_user}","gists_url":"https://api.github.com/users/davidkyle/gists{/gist_id}","starred_url":"https://api.github.com/users/davidkyle/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidkyle/subscriptions","organizations_url":"https://api.github.com/users/davidkyle/orgs","repos_url":"https://api.github.com/users/davidkyle/repos","events_url":"https://api.github.com/users/davidkyle/events{/privacy}","received_events_url":"https://api.github.com/users/davidkyle/received_events","type":"User","site_admin":false},"created_at":"2018-05-11T15:51:54Z","updated_at":"2018-05-11T16:09:59Z","author_association":"MEMBER","body":"@ywelsch This is the reproduce command for the master branch it fails fairly frequently for me\r\n\r\n```\r\n./gradlew -q :x-pack:qa:rolling-upgrade:with-system-key:bwcTest -Dtests.class=org.elasticsearch.upgrades.UpgradeClusterClientYamlTestSuiteIT -Dtests.method=\"test {p0=*/30_ml_jobs_crud/*}\" \r\n```\r\n\r\nRemember to unmute the test on line 33  of `REPO_HOME/x-pack/qa/rolling-upgrade/src/test/java/org/elasticsearch/upgrades/UpgradeClusterClientYamlTestSuiteIT.java`","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388553530","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-388553530","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":388553530,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODU1MzUzMA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-05-12T12:57:32Z","updated_at":"2018-05-12T12:57:32Z","author_association":"CONTRIBUTOR","body":"I have reproduced the issue and also have an explanation and fix in the works. The short version is that this is caused by my recent PR #30423 (sorry for that).\r\n\r\nThe steps that lead to this situation are as follows:\r\n- 2 node cluster (minimum_master_nodes = 2), `.ml-state` index that has `durability: async` and uses auto-expand-replicas.\r\n- The document `old-cluster-job_model_state_1526126821#1` is written to both primary and replica of `[.ml-state][4]` and acknowledged:\r\n```\r\n[2018-05-12T14:07:01,544][TRACE][o.e.i.s.IndexShard       ] [node-1] [.ml-state][4] index [doc][old-cluster-job_model_state_1526126821#1] (seq# [-2])\r\n[2018-05-12T14:07:01,546][TRACE][o.e.a.b.TransportShardBulkAction] [node-1] [[.ml-state][4]] op [indices:data/write/bulk[s]] completed on primary for request [BulkShardRequest [[.ml-state][4]] containing [index {[.ml-state][doc][old-cluster-job_model_state_1526126821#1], source[n/a, actual length: [8.8kb], max length: 2kb]}]]\r\n[2018-05-12T14:07:01,547][TRACE][o.e.i.s.IndexShard       ] [node-0] [.ml-state][4] index [doc][old-cluster-job_model_state_1526126821#1] (seq# [0])\r\n[2018-05-12T14:07:01,551][TRACE][o.e.a.b.TransportShardBulkAction] [node-0] action [indices:data/write/bulk[s][r]] completed on shard [[.ml-state][4]] for request [BulkShardRequest [[.ml-state][4]] containing [index {[.ml-state][doc][old-cluster-job_model_state_1526126821#1], source[n/a, actual length: [8.8kb], max length: 2kb]}]]\r\n[2018-05-12T14:07:01,552][TRACE][o.e.a.b.TransportShardBulkAction] [node-1] operation succeeded. action [indices:data/write/bulk[s]],request [BulkShardRequest [[.ml-state][4]] containing [index {[.ml-state][doc][old-cluster-job_model_state_1526126821#1], source[n/a, actual length: [8.8kb], max length: 2kb]}]]\r\n```\r\n- Before restarting `node1`, the cluster state looks as follows:\r\n```\r\n[2018-05-12T14:07:01,777][TRACE][o.e.c.s.MasterService    ] [node-0] cluster state updated, source [finalize_job_execution [old-cluster-job]]\r\ncluster uuid: G7JOOS_BQPqMEI0zw09cWg\r\nversion: 55\r\n   {node-1}{PBVBkxlbQNmFM7-IbxrUqg}{5eo-iP2CSaGdKRaiTP4Gfg}{127.0.0.1}{127.0.0.1:42143}{testattr=test, ml.machine_memory=67423862784, ml.max_open_jobs=20, ml.enabled=true}\r\n   {node-0}{DzJ4MOcvQH6v9pl0gIPByg}{_e0tM49IReiYFX5K_DRT6Q}{127.0.0.1}{127.0.0.1:33498}{ml.machine_memory=67423862784, testattr=test, ml.max_open_jobs=20, ml.enabled=true}, local, master\r\n--------[.ml-state][4], node[PBVBkxlbQNmFM7-IbxrUqg], [P], s[STARTED], a[id=YCQPAECxTUWZnTV_UtxjQQ]\r\n--------[.ml-state][4], node[DzJ4MOcvQH6v9pl0gIPByg], [R], s[STARTED], a[id=ni6u4fegQai4ydnx0pdhYg]\r\n```\r\n- The node to be restarted (`node1`) had the primary for `[.ml-state][4]`.\r\n- After the restart, `node1` now has the name `mixed-node-0`, and the cluster state looks as follows:\r\n```\r\n[2018-05-12T14:07:17,603][TRACE][o.e.c.s.ClusterApplierService] [mixed-node-0] cluster state updated, source [apply cluster state (from master [master {node-0}{DzJ4MOcvQH6v9pl0gIPByg}{_e0tM49IReiYFX5K_DRT6Q}{127.0.0.1}{127.0.0.1:33498}{testattr=test, ml.machine_memory=67423862784, ml.max_open_jobs=20, ml.enabled=true} committed version [56]])]\r\ncluster uuid: G7JOOS_BQPqMEI0zw09cWg\r\nversion: 56\r\nnodes: \r\n   {mixed-node-0}{PBVBkxlbQNmFM7-IbxrUqg}{0KD922s0Q0eLKfbuqOhREg}{127.0.0.1}{127.0.0.1:44336}{ml.machine_memory=67423862784, upgraded=first, testattr=test, ml.max_open_jobs=20, ml.enabled=true}, local\r\n   {node-0}{DzJ4MOcvQH6v9pl0gIPByg}{_e0tM49IReiYFX5K_DRT6Q}{127.0.0.1}{127.0.0.1:33498}{testattr=test, ml.machine_memory=67423862784, ml.max_open_jobs=20, ml.enabled=true}, master\r\n--------[.ml-state][4], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2018-05-12T12:07:17.488Z], delayed=true, details[node_left[PBVBkxlbQNmFM7-IbxrUqg]], allocation_status[fetching_shard_data]]\r\n--------[.ml-state][4], node[null], [R], recovery_source[peer recovery], s[UNASSIGNED], unassigned_info[[reason=REPLICA_ADDED], at[2018-05-12T12:07:17.490Z], delayed=false, allocation_status[no_attempt]]\r\n```\r\n- Both primary and replica are unassigned, and in a subsequent update the primary of `[.ml-state][4]` gets allocated to the restarted node:\r\n```\r\n[2018-05-12T14:07:18,046][DEBUG][o.e.i.e.Engine           ] [mixed-node-0] [.ml-state][4] Safe commit [CommitPoint{segment[segments_2], userData[{history_uuid=kdzLwGGvS9GmFWJBITqyqw, local_checkpoint=-1, max_seq_no=-1, max_unsafe_auto_id_timestamp=-1, translog_generation=1, translog_uuid=zoDotg6hTVmzcUsOphr6Gg}]}], last commit [CommitPoint{segment[segments_2], userData[{history_uuid=kdzLwGGvS9GmFWJBITqyqw, local_checkpoint=-1, max_seq_no=-1, max_unsafe_auto_id_timestamp=-1, translog_generation=1, translog_uuid=zoDotg6hTVmzcUsOphr6Gg}]}]\r\n\r\n[2018-05-12T14:07:18,068][TRACE][o.e.i.s.IndexShard       ] [mixed-node-0] [.ml-state][4] recovery completed from [shard_store], took [92ms]\r\n    index    : files           [1] with total_size [230b], took[2ms]\r\n             : recovered_files [0] with total_size [0b]\r\n             : reusing_files   [1] with total_size [230b]\r\n    verify_index    : took [0s], check_index [0s]\r\n    translog : number_of_operations [0], took [83ms]\r\n```\r\n- As the node was restarted using `kill -9` (that's what we do in our rolling-upgrade tests), the translog was not flushed to disk, and the node therefore has lost the document. The recovery stats above show that no document is recovered.\r\n\r\nDue to the async durability of the index, the test was relying on the fact that the shard on the restarted node would not be the one to be selected as primary after the restart, but that the shard copy on the non-started node would either remain primary or be promoted to primary.\r\n\r\nThis was inadvertently changed by PR #30423 (which I consider a bug), now making it so that when node1 leaves and rejoins the cluster in the same update, that we first auto-expand replicas from 1 to 0, and only then look at failing shards of the removed node, only to auto-expand back from 0 to 1 replicas in the same update. In our situation here, the primary was on the node to be removed and readded. The auto-expand-replicas functionality now removed the replica on node0 (the node that was not restarted), only to notice in a follow-up step that the sole remaining primary now was on the node that was removed. The primary shard would then be failed and moved to unassigned. In the next step we would readd former node1 and add back a replica due to auto-expansion. Now the master would prefer allocating the primary to the node that previously had the primary, which was former node1. As this node was the one that experienced data loss due to the `kill -9`, the replica on node0 would be peer-recovered from that broken copy and the overall cluster would experience data loss, ultimately resulting in \r\n\r\n```\r\n[2018-05-12T14:07:21,683][ERROR][o.e.x.m.j.p.StateStreamer] Expected 1 documents for model state for old-cluster-job snapshot 1526126821 but failed to find old-cluster-job_model_state_1526126821#1\r\n```\r\n\r\nI will be working on a fix for this.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388583147","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-388583147","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":388583147,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODU4MzE0Nw==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2018-05-12T21:11:05Z","updated_at":"2018-05-12T21:11:05Z","author_association":"CONTRIBUTOR","body":"Thanks so much for investigating this @ywelsch. It would have taken us ages to work out what was happening.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/389099285","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-389099285","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":389099285,"node_id":"MDEyOklzc3VlQ29tbWVudDM4OTA5OTI4NQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-05-15T09:13:21Z","updated_at":"2018-05-15T09:16:39Z","author_association":"CONTRIBUTOR","body":"I've verified that #30553 fixes the cause for seeing this issue occurring more often.\r\n\r\n> I think the problem is one that we've had intermittently for ages.\r\n\r\nAs I mentioned in an e-mail, the issue (of losing documents) will sometimes manifest if you don’t wait for the indices with async durability to be fully allocated to both nodes in this test. This should happen rarely in practice when running the test (as replicas should allocate pretty quickly), but could be the cause of the more spurious test failures that you were seeing before this issue got opened.\r\n\r\nI will revert 27429ec to reenable the tests, but would also like to suggest adding an `@After` clause to `UpgradeClusterClientYamlTestSuiteIT` that waits for the `.ml-state` and `.ml-anomalies-shared` indices (which are both `durability:async`) to be fully allocated before killing the node.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/389102782","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-389102782","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":389102782,"node_id":"MDEyOklzc3VlQ29tbWVudDM4OTEwMjc4Mg==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2018-05-15T09:25:30Z","updated_at":"2018-05-15T09:25:30Z","author_association":"CONTRIBUTOR","body":"> but would also like to suggest adding an `@After` clause to `UpgradeClusterClientYamlTestSuiteIT` that waits for the .ml-state and .ml-anomalies-shared indices\r\n\r\nMaybe we could also do this just in the relevant ML tests?  Then we won't cause problems if somebody is trying to just reproduce a problem in, say, the monitoring or watcher tests and is not running the ML ones at all.\r\n\r\nAlso, what is the best way to confirm that the replicas of an index with `auto_expand_replicas` has had its replicas allocated?  Will `wait_for_no_initializing_shards` do it?  Or is there a danger with that that at the instant it's called that the index hasn't yet auto-expanded?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/389111477","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-389111477","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":389111477,"node_id":"MDEyOklzc3VlQ29tbWVudDM4OTExMTQ3Nw==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-05-15T09:55:44Z","updated_at":"2018-05-15T09:55:44Z","author_association":"CONTRIBUTOR","body":"> Maybe we could also do this just in the relevant ML tests? Then we won't cause problems if somebody is trying to just reproduce a problem in, say, the monitoring or watcher tests and is not running the ML ones at all.\r\n\r\nright, I didn't think about that.\r\n\r\n> Also, what is the best way to confirm that the replicas of an index with auto_expand_replicas has had its replicas allocated\r\n\r\nAs the two problematic indices (`.ml-state` and `.ml-anomalies-shared`) are both created in the old cluster with 2 nodes, and the cluster can only function with 2 nodes due to minimum_master_nodes = 2, the auto-expansion functionality (based on #30553) will always have `index.number_of_replicas:1` in this cluster, even with nodes restarting. I think it's therefore sufficient to wait for green health of those two indices at the end of each ML test in `old-cluster` as well as `mixed-cluster`. The mixed-cluster can always allocate the replica, as the primary will always be on the older node (if it was on the node that got restarted, the active replica on the other node is promoted to primary, and if it was on the node that did not get restarted, it's going to stay there as primary) .","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/389446081","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-389446081","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":389446081,"node_id":"MDEyOklzc3VlQ29tbWVudDM4OTQ0NjA4MQ==","user":{"login":"davidkyle","id":2353640,"node_id":"MDQ6VXNlcjIzNTM2NDA=","avatar_url":"https://avatars1.githubusercontent.com/u/2353640?v=4","gravatar_id":"","url":"https://api.github.com/users/davidkyle","html_url":"https://github.com/davidkyle","followers_url":"https://api.github.com/users/davidkyle/followers","following_url":"https://api.github.com/users/davidkyle/following{/other_user}","gists_url":"https://api.github.com/users/davidkyle/gists{/gist_id}","starred_url":"https://api.github.com/users/davidkyle/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidkyle/subscriptions","organizations_url":"https://api.github.com/users/davidkyle/orgs","repos_url":"https://api.github.com/users/davidkyle/repos","events_url":"https://api.github.com/users/davidkyle/events{/privacy}","received_events_url":"https://api.github.com/users/davidkyle/received_events","type":"User","site_admin":false},"created_at":"2018-05-16T08:55:08Z","updated_at":"2018-05-16T08:55:08Z","author_association":"MEMBER","body":"There were 3 issues here the first an actual wire incompatibility problem fixed in #30512, secondly a bug in auto expanding replicas in dead nodes fixed in #30553 and finally because of async durability an extra precaution was added in #30615","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/389530022","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-389530022","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":389530022,"node_id":"MDEyOklzc3VlQ29tbWVudDM4OTUzMDAyMg==","user":{"login":"davidkyle","id":2353640,"node_id":"MDQ6VXNlcjIzNTM2NDA=","avatar_url":"https://avatars1.githubusercontent.com/u/2353640?v=4","gravatar_id":"","url":"https://api.github.com/users/davidkyle","html_url":"https://github.com/davidkyle","followers_url":"https://api.github.com/users/davidkyle/followers","following_url":"https://api.github.com/users/davidkyle/following{/other_user}","gists_url":"https://api.github.com/users/davidkyle/gists{/gist_id}","starred_url":"https://api.github.com/users/davidkyle/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidkyle/subscriptions","organizations_url":"https://api.github.com/users/davidkyle/orgs","repos_url":"https://api.github.com/users/davidkyle/repos","events_url":"https://api.github.com/users/davidkyle/events{/privacy}","received_events_url":"https://api.github.com/users/davidkyle/received_events","type":"User","site_admin":false},"created_at":"2018-05-16T14:03:03Z","updated_at":"2018-05-16T14:03:03Z","author_association":"MEMBER","body":"Argh it's back \r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=ubuntu&&virtual/2439/console\r\n\r\n```\r\njava.lang.AssertionError: Failure at [upgraded_cluster/30_ml_jobs_crud:33]: jobs.0.data_counts.processed_record_count didn't match expected value:\r\njobs.0.data_counts.processed_record_count: expected [2] but was [0]\r\n```\r\n\r\nThis failed on master upgrading from 6.4 I'm not sure which build of 6.4 that was and if it had all the fixes in. I'll keep an eye on this","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/389550706","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-389550706","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":389550706,"node_id":"MDEyOklzc3VlQ29tbWVudDM4OTU1MDcwNg==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-05-16T15:00:17Z","updated_at":"2018-05-16T15:00:17Z","author_association":"CONTRIBUTOR","body":"@davidkyle That build used a last good commit from before you pushed your wait-for-green change:\r\n\r\n> Using last successful commit 9434f25 from the job elastic+elasticsearch+master+intake with the build id 20180516072627-C0E2E615 that was started at 2018-05-16T07:26:34.919Z and finished 57m 1s ago\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/389558983","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-389558983","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":389558983,"node_id":"MDEyOklzc3VlQ29tbWVudDM4OTU1ODk4Mw==","user":{"login":"davidkyle","id":2353640,"node_id":"MDQ6VXNlcjIzNTM2NDA=","avatar_url":"https://avatars1.githubusercontent.com/u/2353640?v=4","gravatar_id":"","url":"https://api.github.com/users/davidkyle","html_url":"https://github.com/davidkyle","followers_url":"https://api.github.com/users/davidkyle/followers","following_url":"https://api.github.com/users/davidkyle/following{/other_user}","gists_url":"https://api.github.com/users/davidkyle/gists{/gist_id}","starred_url":"https://api.github.com/users/davidkyle/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidkyle/subscriptions","organizations_url":"https://api.github.com/users/davidkyle/orgs","repos_url":"https://api.github.com/users/davidkyle/repos","events_url":"https://api.github.com/users/davidkyle/events{/privacy}","received_events_url":"https://api.github.com/users/davidkyle/received_events","type":"User","site_admin":false},"created_at":"2018-05-16T15:22:20Z","updated_at":"2018-05-16T15:22:20Z","author_association":"MEMBER","body":"Closing as the last failure was based on an old commit. \r\n\r\nThanks @ywelsch ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/393298536","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-393298536","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":393298536,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzI5ODUzNg==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2018-05-30T20:01:44Z","updated_at":"2018-05-30T20:01:44Z","author_association":"MEMBER","body":"Fresh failure https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+intake/2003/console","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/393314694","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-393314694","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":393314694,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzMxNDY5NA==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2018-05-30T20:50:24Z","updated_at":"2018-05-30T20:50:24Z","author_association":"MEMBER","body":"And another one https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/6399/console","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/393315917","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-393315917","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":393315917,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzMxNTkxNw==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2018-05-30T20:54:40Z","updated_at":"2018-05-30T22:00:15Z","author_association":"MEMBER","body":"~When it rains it pours https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+g1gc/5918/console~ As @jasontedor correctly pointed out this one is a complelty different issue. I opened #30982 for it. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/393328607","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-393328607","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":393328607,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzMyODYwNw==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2018-05-30T21:39:59Z","updated_at":"2018-05-30T21:39:59Z","author_association":"MEMBER","body":"To me this looks like a different issue and I think that a new issue should be opened.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/393330992","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-393330992","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":393330992,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzMzMDk5Mg==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2018-05-30T21:49:10Z","updated_at":"2018-05-30T21:49:10Z","author_association":"MEMBER","body":"Yes, I think you are right the last one is something completely new. I shouldn't have merged them both here.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/393440985","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-393440985","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":393440985,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzQ0MDk4NQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-05-31T07:45:21Z","updated_at":"2018-05-31T07:45:21Z","author_association":"CONTRIBUTOR","body":"The first failure that @imotov reported is the same as the first failure that was originally reported by @atorok (i.e. `field [jobs.0.node] doesn't have a true value`).\r\n\r\nWhile we fixed the second failure reported by @atorok (`jobs.0.data_counts.processed_record_count didn't match expected value`), I believe the first failure is of a different kind and has a different root cause.\r\nI suspect an issue with persistent task assignment, and the guarantees that the different API calls give us.\r\n@droberts195 / @davidkyle can one of you add an `- match: { \"acknowledged\": true }` after the `open_job` calls? I think it would be interesting to find out if the persistent task associated `job` was indeed properly assigned from the perspective of `open_job`. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/393457148","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-393457148","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":393457148,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzQ1NzE0OA==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2018-05-31T08:35:36Z","updated_at":"2018-05-31T08:35:36Z","author_association":"CONTRIBUTOR","body":"I think the most recent failures are all due to the `autodetect` process core dumping with a SEGV.  The first failure @imotov reported above contains this (and on Linux signal 11 is SEGV):\r\n\r\n> 19:49:03 [2018-05-30T19:48:57,900][ERROR][o.e.x.m.j.p.l.CppLogMessageHandler] [controller/121564] [CDetachedProcessSpawner.cc@184] Child process with PID 121774 was terminated by signal 11\r\n\r\nThese C++ failures are split out into #30982.  As far as I can see all yesterday's failures are caused by this new problem.\r\n\r\nThe full logs for the first failure reported by @atorok are no longer available, but there a total of three problems that were fixed for this issue.  The first was a wire transport format incompatibility between master and 6.x that was fixed in #30512.  @davidkyle found that very quickly during the original investigation of this issue, so I suspect this was the cause of the first reported failure.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/393481578","html_url":"https://github.com/elastic/elasticsearch/issues/30456#issuecomment-393481578","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30456","id":393481578,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzQ4MTU3OA==","user":{"login":"davidkyle","id":2353640,"node_id":"MDQ6VXNlcjIzNTM2NDA=","avatar_url":"https://avatars1.githubusercontent.com/u/2353640?v=4","gravatar_id":"","url":"https://api.github.com/users/davidkyle","html_url":"https://github.com/davidkyle","followers_url":"https://api.github.com/users/davidkyle/followers","following_url":"https://api.github.com/users/davidkyle/following{/other_user}","gists_url":"https://api.github.com/users/davidkyle/gists{/gist_id}","starred_url":"https://api.github.com/users/davidkyle/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidkyle/subscriptions","organizations_url":"https://api.github.com/users/davidkyle/orgs","repos_url":"https://api.github.com/users/davidkyle/repos","events_url":"https://api.github.com/users/davidkyle/events{/privacy}","received_events_url":"https://api.github.com/users/davidkyle/received_events","type":"User","site_admin":false},"created_at":"2018-05-31T10:03:29Z","updated_at":"2018-05-31T10:03:29Z","author_association":"MEMBER","body":"Closing as the investigation has shown that the recent failures are due to a SEGV in autodetect and is tracked in #30982 ","performed_via_github_app":null}]