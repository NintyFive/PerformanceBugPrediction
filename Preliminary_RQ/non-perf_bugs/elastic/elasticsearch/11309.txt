{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/11309","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11309/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11309/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11309/events","html_url":"https://github.com/elastic/elasticsearch/issues/11309","id":79554522,"node_id":"MDU6SXNzdWU3OTU1NDUyMg==","number":11309,"title":"Prevent \"bad\" shard from causing perpetual initialization tasks","user":{"login":"ppf2","id":7216393,"node_id":"MDQ6VXNlcjcyMTYzOTM=","avatar_url":"https://avatars0.githubusercontent.com/u/7216393?v=4","gravatar_id":"","url":"https://api.github.com/users/ppf2","html_url":"https://github.com/ppf2","followers_url":"https://api.github.com/users/ppf2/followers","following_url":"https://api.github.com/users/ppf2/following{/other_user}","gists_url":"https://api.github.com/users/ppf2/gists{/gist_id}","starred_url":"https://api.github.com/users/ppf2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ppf2/subscriptions","organizations_url":"https://api.github.com/users/ppf2/orgs","repos_url":"https://api.github.com/users/ppf2/repos","events_url":"https://api.github.com/users/ppf2/events{/privacy}","received_events_url":"https://api.github.com/users/ppf2/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2015-05-22T18:13:36Z","updated_at":"2015-05-23T08:50:18Z","closed_at":"2015-05-22T18:53:36Z","author_association":"MEMBER","active_lock_reason":null,"body":"Have come across this multiple times in the field.  For whatever reason (still to be determined - different issue), the index files for a shard are gone (for both the primary and replica).  The shard's index directory exists (but is empty) for the shard for both primary and replica.  When this occurs:\n- If the end user checks the cluster health api, it will show that there is a shard initializing, but it will be like this forever as if it is taking a long time to initialize.  But in fact, it is in initializing state because it keeps trying and never stops trying.\n- If you check the cat/shards, you will see that the shard keeps trying to initializing on the 2 nodes, basically, initialize on node 1 -> fails -> initialize on node 2 -> fails, initialize on node 1 -> fails, etc.. and so on.  If you look at head, you will see that the shard is \"bouncing\" between the 2 nodes:\n\n![image](https://cloud.githubusercontent.com/assets/7216393/7776570/a8af3e76-0070-11e5-94c4-79b2b95b8857.png)\n\n![image](https://cloud.githubusercontent.com/assets/7216393/7776564/a136a1a2-0070-11e5-839f-266f5aa9dfd8.png)\n- And corresponding pending tasks keep showing up for each attempt against the 2 nodes:\n\n```\n{\n  \"tasks\" : [ {\n    \"insert_order\" : 14347,\n    \"priority\" : \"HIGH\",\n    \"source\" : \"shard-failed ([default][0], node[5xR9MWlmSDKHjZPIZs4L-w], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[default][0] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[default][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: [.DS_Store]]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/ppf2/ELK/elasticsearch-1.4.2_1/data/elasticsearch_1_4_2/nodes/0/indices/default/0/index), type=MERGE, rate=20.0)]): files: [.DS_Store]]; ]]\",\n    \"executing\" : true,\n    \"time_in_queue_millis\" : 13,\n    \"time_in_queue\" : \"13ms\"\n  } ]\n}\n```\n- And the logs on both nodes will start filling up with a ton of these warning messages:\n\n```\n[2015-05-22 10:56:27,812][WARN ][cluster.action.shard     ] [node2] [default][0] sending failed shard for [default][0], node[ktzJrukQQDux_edYck2tYA], [P], s[INITIALIZING], indexUUID [zdMk9SthR_OzhCJi27QzcQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[default][0] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[default][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: [.DS_Store]]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/ppf2/ELK/elasticsearch-1.4.2_2/data/elasticsearch_1_4_2/nodes/0/indices/default/0/index), type=MERGE, rate=20.0)]): files: [.DS_Store]]; ]]\n[2015-05-22 10:56:27,831][WARN ][indices.cluster          ] [node2] [default][0] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [default][0] failed to fetch index version after copying it over\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [default][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: [.DS_Store]\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\n    ... 4 more\nCaused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/ppf2/ELK/elasticsearch-1.4.2_2/data/elasticsearch_1_4_2/nodes/0/indices/default/0/index), type=MERGE, rate=20.0)]): files: [.DS_Store]\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\n    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\n    ... 4 more\n```\n\nTo reproduce:\n- Have a cluster with 2 node and an index with 1 replica.   Go to the file system and simulate the loss of shard data by deleting all files under a shard's index folder (keep the directory but empty it) from both nodes.\n- Restart the 2 nodes\n- Then you can see the continuous attempts to start the shard on both nodes and it will keep trying and keep failing and doesn't give up.\n\nI suspect that the same behavior will occur if both primary and replica shards have index corruptions.\n\nIt will be nice to:\n- Prevent it from trying perpetually - once it determines that there are no copies of the shard useable/available, stop trying repeatedly on the nodes\n- Leave the problem shard unassigned and write an intuitive message to the master log with a summary of what nodes it tried on and that it is leaving the shard unassigned because it cannot find a usable copy of the shard data, etc..\n","closed_by":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"performed_via_github_app":null}