{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/7430","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7430/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7430/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7430/events","html_url":"https://github.com/elastic/elasticsearch/issues/7430","id":41042082,"node_id":"MDU6SXNzdWU0MTA0MjA4Mg==","number":7430,"title":"Internal: Indexes unuseable after upgrade from 0.2 to 1.3 and cluster restart","user":{"login":"philnate","id":780196,"node_id":"MDQ6VXNlcjc4MDE5Ng==","avatar_url":"https://avatars2.githubusercontent.com/u/780196?v=4","gravatar_id":"","url":"https://api.github.com/users/philnate","html_url":"https://github.com/philnate","followers_url":"https://api.github.com/users/philnate/followers","following_url":"https://api.github.com/users/philnate/following{/other_user}","gists_url":"https://api.github.com/users/philnate/gists{/gist_id}","starred_url":"https://api.github.com/users/philnate/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/philnate/subscriptions","organizations_url":"https://api.github.com/users/philnate/orgs","repos_url":"https://api.github.com/users/philnate/repos","events_url":"https://api.github.com/users/philnate/events{/privacy}","received_events_url":"https://api.github.com/users/philnate/received_events","type":"User","site_admin":false},"labels":[{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null}],"state":"closed","locked":false,"assignee":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"assignees":[{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false}],"milestone":null,"comments":13,"created_at":"2014-08-25T08:27:12Z","updated_at":"2015-06-07T19:02:12Z","closed_at":"2014-09-06T13:24:13Z","author_association":"NONE","active_lock_reason":null,"body":"We recently tried to upgrade a ES cluster from 0.2 to 1.3. The actual upgrade worked out fine, but once we restarted the whole cluster, we saw those warnings for all shards (constantly repeating):\n\n```\nIndexShardGatewayRecoveryException[[maki-log-2014-08-21][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: [write.lock, _checksums-1408609875350, _10f.nrm, segments.gen, _17x.nrm, _10f.tis, _17z.si, _17y.fdt, _17y.tis, _17x.fdx, _10f.frq, _17x.fdt, _17y.tii, _17x.prx, _17y.nrm, _10f.fdx, _10f.fnm, _17x.tis, _10f.tii, _10f.fdt, _17z.cfe, _17x.frq, _17x.tii, segments_j, _10f.prx, _17y.fnm, _17y.fdx, _17y.prx, _17y.frq, _17z.cfs, _17x.fnm]]; nested: FileNotFoundException[No such file [_10f.si]]; ]]\n[2014-08-21 13:44:15,826][WARN ][cluster.action.shard     ] [Ghost Dancer] [maki-log-2014-08-21][3] received shard failed for [maki-log-2014-08-21][3], node[QsfMdS40Qve8PukS4er9oA], [P], s[INITIALIZING], indexUUID [_na_], reason [master [Ghost Dancer][QsfMdS40Qve8PukS4er9oA][gboanea-ThinkPad-W520][inet[/10.200.54.63:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]\n[2014-08-21 13:44:15,841][WARN ][cluster.action.shard     ] [Ghost Dancer] [maki-log-2014-08-21][4] received shard failed for [maki-log-2014-08-21][4], node[QsfMdS40Qve8PukS4er9oA], [P], s[INITIALIZING], indexUUID [_na_], reason [master [Ghost Dancer][QsfMdS40Qve8PukS4er9oA][gboanea-ThinkPad-W520][inet[/10.200.54.63:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]\n[2014-08-21 13:44:15,844][WARN ][indices.cluster          ] [Ghost Dancer] [maki-log-2014-08-21][1] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [maki-log-2014-08-21][1] failed to fetch index version after copying it over\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:152)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [maki-log-2014-08-21][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_17o.nrm, _zv.fdx, write.lock, _zv.fdt, segments.gen, _17o.fdx, _17o.tis, _17o.fdt, _zv.fnm, _17p.cfe, _zv.prx, _zv.frq, _17n.prx, _17n.frq, _17o.tii, _17n.nrm, _17n.tii, _17o.frq, _checksums-1408609865048, _17p.cfs, segments_j, _17o.fnm, _17o.prx, _17n.tis, _17n.fdx, _17n.fdt, _zv.tis, _zv.nrm, _17n.fnm, _zv.tii, _17p.si]\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:131)\n    ... 4 more\nCaused by: java.io.FileNotFoundException: No such file [_zv.si]\n    at org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:173)\n    at org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:144)\n    at org.elasticsearch.index.store.DistributorDirectory.openInput(DistributorDirectory.java:130)\n    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)\n    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:375)\n    at org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfoReader.read(Lucene3xSegmentInfoReader.java:103)\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:361)\n    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:457)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:907)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:753)\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:453)\n    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:122)\n    ... 4 more..\n```\n\nWhen we shutdown the cluster a couple minutes after bringing it up, with the new version, we saw this behavior just for the newest index. After about an hour the behavior would be the same for other indexes after a cluster restart.\n\nWe found out that the indexes are updated and on shutdown nearly all segment info (*.si) files are deleted (those which have a corresponding marker _upgraded.si). Those si files surviving seemed to be not upgraded (at least they don't have those marker files). And there content is like this or this:\n\n```\n?�l\u0017\u0013Lucene3xSegmentInfo3.6.21�\nos.version\u001D2.6.39-300.17.2.el6uek.x86_64\u0002osLinuxlucene.version+3.6.2 1423725 - rmuir - 2012-12-18 19:45:40\u0006sourceflushos.archamd64\n                                                                                                                                  java.versio1.7.0_51\n         java.vendor\u0012Oracle Corporation\n_175.fdt\u0010_175_upgraded.s_175.fdx_175.s_175.fn_175.ti_175.ti_175.nr_175.fr_175.prx%\n```\n\n```\n1408712122907SegmentInfo\u0001\u00034.9�\u0001 timestamp\nos.version\u00103.2.0-67-generic\u0002osLinuxlucene.version+4.9.0 1604085 - rmuir - 2014-06-20 06:22:23\u0006sourceflushos.archamd64\n                                                                                                                     java.versio1.7.0_65\n                                                                                                                                        java.vendor\u0012Oracle Corporation_18n.cf_18n.cfs_18n.si�(����\\%\n```\n\nWhile those updated contain afterwards this kind of information:\n\n```\n?�l\u0017\u0013Lucene3xSegmentInfo3.6.2�2�  \n                                        mergeFactor\u000210\nos.version\u001D2.6.39-300.17.2.el6uek.x86_64\u0002osLinuxlucene.version+3.6.2 1423725 - rmuir - 2012-12-18 19:45:40\u0006sourcemergeos.archamd64\u0013mergeMaxNumSegments\u0002-1\n             java.versio1.7.0_51\n                                java.vendor\u0012Oracle Corporation\n_1mx.ti_1mx.fr_1mx.pr_1mx.fd_1mx.nr_1mx.fdt_1mx.si\u0010_1mx_upgraded.s_1mx.fn_1mx.tis%  \n```\n\nWe could force the same behavior triggering an optimize for a given index. By restarting one node at a time and waiting till it fully integrated into the cluster we were able to restore the deleted si files through other nodes (including the _upgraded.si marker files). Afterwards the si files where safe and didn't got deleted.\n\nTo me it looks like either ES or Lucene is memorizing to delete the _upgraded.si files on VM shutdown but by accident deletes the actual si files as well.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}