{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/42953","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42953/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42953/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42953/events","html_url":"https://github.com/elastic/elasticsearch/issues/42953","id":453109155,"node_id":"MDU6SXNzdWU0NTMxMDkxNTU=","number":42953,"title":"ES crashing multiple times, over 1Billion docs a day, indexing rate falling from 25k/s to 2k/s","user":{"login":"VRYasa","id":29779611,"node_id":"MDQ6VXNlcjI5Nzc5NjEx","avatar_url":"https://avatars2.githubusercontent.com/u/29779611?v=4","gravatar_id":"","url":"https://api.github.com/users/VRYasa","html_url":"https://github.com/VRYasa","followers_url":"https://api.github.com/users/VRYasa/followers","following_url":"https://api.github.com/users/VRYasa/following{/other_user}","gists_url":"https://api.github.com/users/VRYasa/gists{/gist_id}","starred_url":"https://api.github.com/users/VRYasa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/VRYasa/subscriptions","organizations_url":"https://api.github.com/users/VRYasa/orgs","repos_url":"https://api.github.com/users/VRYasa/repos","events_url":"https://api.github.com/users/VRYasa/events{/privacy}","received_events_url":"https://api.github.com/users/VRYasa/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-06-06T15:50:51Z","updated_at":"2019-06-06T16:11:53Z","closed_at":"2019-06-06T16:08:01Z","author_association":"NONE","active_lock_reason":null,"body":"**ES crashing multiple times, over 1Billion logs a day, indexing rate falling from 25k/s to 2k/s**:\r\n\r\n<!-- Issue -->\r\n\r\n**GET /** output:\r\n{\r\n  \"name\": \"es-ingest-3\",\r\n  \"cluster_name\": \"CT\",\r\n  \"cluster_uuid\": \"IEasdfasfsdfaf\",\r\n  \"version\": {\r\n    \"number\": \"6.1.2\",\r\n    \"build_hash\": \"asdfasdf\",\r\n    \"build_date\": \"2018-01-10T02:35:59.208Z\",\r\n    \"build_snapshot\": false,\r\n    \"lucene_version\": \"7.1.0\",\r\n    \"minimum_wire_compatibility_version\": \"5.6.0\",\r\n    \"minimum_index_compatibility_version\": \"5.0.0\"\r\n  },\r\n  \"tagline\": \"You Know, for Search\"\r\n}\r\n\r\n**GET _cat/nodes?v** output:\r\nip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name\r\n10.6.128.4           19          97   4    0.46    0.34     0.36 m         -      es-master-4\r\n10.2.0.2             37          99   9    2.22    1.91     1.56 d         -      es-data-6\r\n10.4.128.2           11          98   3    0.77    0.66     0.46 m         -      es-master-3\r\n10.4.0.7             35          83   2    0.42    0.45     0.43 i         -      es-ingest-1\r\n10.4.0.3             11          90   3    0.13    0.26     0.31 m         -      es-master-1\r\n10.1.192.2           32          99  10    1.49    1.50     1.33 d         -      es-data-0\r\n10.2.128.2           36          99   9    1.09    1.22     1.18 d         -      es-data-5\r\n10.8.0.2             16          99  20    5.02    4.84     4.68 d         -      es-data-8\r\n10.7.128.5           64          66   2    0.01    0.04     0.09 i         -      es-ingest-3\r\n10.9.0.3              26          84   6    0.49    0.48     0.44 m         *      es-master-0\r\n10.7.0.2              14          95  10    1.03    0.95     0.82 m         -      es-master-2\r\n10.6.0.4             60          66   2    0.01    0.07     0.12 i         -      es-ingest-2\r\n10.9.128.2           26          99   6    1.87    1.92     1.66 d         -      es-data-7\r\n10.3.128.2           31          99  10    1.49    1.48     1.35 d         -      es-data-2\r\n10.1.0.2             50          99  10    1.19    1.00     0.91 d         -      es-data-4\r\n10.3.0.9             16          99  10    1.14    1.10     1.01 d         -      es-data-1\r\n10.5.0.2             29          99  22    2.12    2.12     1.77 d         -      es-data-3\r\n10.7.128.6           60          66   4    0.01    0.04     0.09 i         -      es-ingest-0\r\n10.6.128.2           58          99   8    0.10    0.22     0.24 d         -      es-data-9\r\n\r\n**Average indexing rate**: [6k to 10k per sec]\r\n\r\n**When I perform the following steps the indexing rate increased to 25k to 30k per sec**:\r\nPUT /logstash-*/_settings\r\n{\r\n    \"index\" : {\r\n        \"refresh_interval\" : \"-1\"\r\n    }\r\n}\r\n\r\nPUT /logstash-*/_settings\r\n{\r\n    \"index\" : {\r\n        \"number_of_replicas\" : \"0\"\r\n    }\r\n}\r\n\r\n**By performing the above steps it improved the rate and ran very well for about 10days indexing nearly 17Billion documents. But it failed when there was a spike of 1.3 Billion documents per day. Since then, this has not been stable, every time I perform the above steps, the cluster indexing rate increases for a few hours and then crashes again.**\r\n\r\n**Then I have been performing the following steps in the same order**: \r\nPUT /logstash-*/_settings\r\n{\r\n    \"index\" : {\r\n        \"number_of_replicas\" : \"1\"\r\n    }\r\n}\r\nPUT /_cluster/settings\r\n{\r\n    \"transient\" : {\r\n        \"cluster.routing.allocation.enable\" : \"none\"\r\n    }\r\n}\r\n**Waited for all the shards to be reallocated and then did the following steps**:\r\nPUT _cluster/settings\r\n{\r\n  \"persistent\": {\r\n    \"cluster.routing.allocation.enable\": \"primaries\"\r\n  }\r\n}\r\nPUT /_cluster/settings\r\n{\r\n    \"transient\" : {\r\n        \"cluster.routing.allocation.enable\" : \"all\"\r\n    }\r\n}\r\nPUT /logstash-*/_settings\r\n{\r\n    \"index\" : {\r\n        \"refresh_interval\" : \"-1\"\r\n    }\r\n}\r\nPUT /logstash-*/_settings\r\n{\r\n    \"index\" : {\r\n        \"number_of_replicas\" : \"0\"\r\n    }\r\n}\r\n\r\n**I had to restart all Logstash containers and the indexing improved again but failed after few hours. This process keeps repeating with the following logs**:\r\n`[2019-06-06T09:17:50,436][WARN ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] high disk watermark [50gb] exceeded on [M6fzMFiuT_Ks7R0layHpIg][es-data-9][/data/nodes/0] free: 49.9gb[5%], shards will be relocated away from this node\r\n[2019-06-06T09:17:50,436][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] low disk watermark [100gb] exceeded on [q4M5NMS1T5GguZor8fHDSw][es-data-8][/data/nodes/0] free: 51.6gb[5.2%], replicas will not be assigned to this node\r\n[2019-06-06T09:17:50,436][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] rerouting shards: [high disk watermark exceeded on one or more nodes]\r\n[2019-06-06T09:18:50,724][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] low disk watermark [100gb] exceeded on [M6fzMFiuT_Ks7R0layHpIg][es-data-9][/data/nodes/0] free: 50gb[5%], replicas will not be assigned to this node\r\n[2019-06-06T09:18:50,724][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] low disk watermark [100gb] exceeded on [q4M5NMS1T5GguZor8fHDSw][es-data-8][/data/nodes/0] free: 51.2gb[5.2%], replicas will not be assigned to this node\r\n[2019-06-06T09:19:50,941][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] low disk watermark [100gb] exceeded on [q4M5NMS1T5GguZor8fHDSw][es-data-8][/data/nodes/0] free: 51.3gb[5.2%], replicas will not be assigned to this node\r\n[2019-06-06T09:19:50,941][WARN ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] high disk watermark [50gb] exceeded on [M6fzMFiuT_Ks7R0layHpIg][es-data-9][/data/nodes/0] free: 49.5gb[5%], shards will be relocated away from this node\r\n[2019-06-06T09:19:50,941][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] rerouting shards: [high disk watermark exceeded on one or more nodes]\r\n[2019-06-06T09:20:51,070][WARN ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] high disk watermark [50gb] exceeded on [M6fzMFiuT_Ks7R0layHpIg][es-data-9][/data/nodes/0] free: 49.1gb[4.9%], shards will be relocated away from this node\r\n[2019-06-06T09:20:51,070][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] low disk watermark [100gb] exceeded on [q4M5NMS1T5GguZor8fHDSw][es-data-8][/data/nodes/0] free: 50.7gb[5.1%], replicas will not be assigned to this node\r\n[2019-06-06T09:20:51,070][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] rerouting shards: [high disk watermark exceeded on one or more nodes]\r\n[2019-06-06T09:21:51,073][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] low disk watermark [100gb] exceeded on [q4M5NMS1T5GguZor8fHDSw][es-data-8][/data/nodes/0] free: 50.3gb[5.1%], replicas will not be assigned to this node\r\n[2019-06-06T09:21:51,073][WARN ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] high disk watermark [50gb] exceeded on [M6fzMFiuT_Ks7R0layHpIg][es-data-9][/data/nodes/0] free: 48.7gb[4.9%], shards will be relocated away from this node\r\n[2019-06-06T09:21:51,073][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] rerouting shards: [high disk watermark exceeded on one or more nodes]\r\n[2019-06-06T09:22:51,245][WARN ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] high disk watermark [50gb] exceeded on [q4M5NMS1T5GguZor8fHDSw][es-data-8][/data/nodes/0] free: 49.8gb[5%], shards will be relocated away from this node\r\n[2019-06-06T09:22:51,245][WARN ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] high disk watermark [50gb] exceeded on [M6fzMFiuT_Ks7R0layHpIg][es-data-9][/data/nodes/0] free: 49gb[4.9%], shards will be relocated away from this node\r\n[2019-06-06T09:22:51,245][INFO ][o.e.c.r.a.DiskThresholdMonitor] [es-master-0] rerouting shards: [high disk watermark exceeded on one or more nodes]\r\n`\r\n**and**\r\n\r\n`[2019-06-06T11:02:09,629][WARN ][o.e.c.a.s.ShardStateAction] [es-master-0] [logstash-2019.05.31][1] received shard failed for shard id [[logstash-2019.05.31][1]], allocation id [j32CrqOtR9eeqwKxujusNg], primary term [1], message [failed to perform indices:data/write/bulk[s] on replica [logstash-2019.05.31][1], node[y_u8hfExQgeHsnZUiXTgEg], relocating [q4M5NMS1T5GguZor8fHDSw], [P], s[RELOCATING], a[id=j32CrqOtR9eeqwKxujusNg, rId=xQNBvAklQCuyCwADuIOazg], expected_shard_size[34874350916]], failure [RemoteTransportException[[es-data-6][10.2.0.2:9300][indices:data/write/bulk[s][r]]]; nested: IllegalStateException[active primary shard [logstash-2019.05.31][1], node[y_u8hfExQgeHsnZUiXTgEg], relocating [q4M5NMS1T5GguZor8fHDSw], [P], s[RELOCATING], a[id=j32CrqOtR9eeqwKxujusNg, rId=xQNBvAklQCuyCwADuIOazg], expected_shard_size[34874350916] cannot be a replication target before relocation hand off, state is [CLOSED]]; ]\r\norg.elasticsearch.transport.RemoteTransportException: [es-data-6][10.2.0.2:9300][indices:data/write/bulk[s][r]]\r\nCaused by: java.lang.IllegalStateException: active primary shard [logstash-2019.05.31][1], node[y_u8hfExQgeHsnZUiXTgEg], relocating [q4M5NMS1T5GguZor8fHDSw], [P], s[RELOCATING], a[id=j32CrqOtR9eeqwKxujusNg, rId=xQNBvAklQCuyCwADuIOazg], expected_shard_size[34874350916] cannot be a replication target before relocation hand off, state is [CLOSED]\r\n    at org.elasticsearch.index.shard.IndexShard.verifyReplicationTarget(IndexShard.java:1479) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.index.shard.IndexShard.ensureWriteAllowed(IndexShard.java:1462) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:683) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.index.shard.IndexShard.applyIndexOperationOnReplica(IndexShard.java:674) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.performOpOnReplica(TransportShardBulkAction.java:518) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnReplica(TransportShardBulkAction.java:480) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnReplica(TransportShardBulkAction.java:466) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnReplica(TransportShardBulkAction.java:72) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncReplicaAction.onResponse(TransportReplicationAction.java:566) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncReplicaAction.onResponse(TransportReplicationAction.java:529) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.index.shard.IndexShard$2.onResponse(IndexShard.java:2305) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.index.shard.IndexShard$2.onResponse(IndexShard.java:2283) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.index.shard.IndexShardOperationPermits.acquire(IndexShardOperationPermits.java:238) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.index.shard.IndexShard.acquireReplicaOperationPermit(IndexShard.java:2282) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncReplicaAction.doRun(TransportReplicationAction.java:640) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:512) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:492) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:66) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.transport.TcpTransport$RequestHandler.doRun(TcpTransport.java:1554) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:637) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-6.1.2.jar:6.1.2]\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_151]\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_151]\r\n    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]\r\n`\r\n\r\n**We get an average 800 Million documents everyday, sometimes we get spikes of around 1.3 Billion per day. Please suggest the best way to address this. We currently have a 7 days backlog which needs to be cleared.** \r\n\r\n**I am new to managing ES, please guide me.**\r\n","closed_by":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"performed_via_github_app":null}