{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/17267","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17267/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17267/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/17267/events","html_url":"https://github.com/elastic/elasticsearch/issues/17267","id":142876808,"node_id":"MDU6SXNzdWUxNDI4NzY4MDg=","number":17267,"title":"Need help in scaling up my elasticsearch-logstash-graylog setup","user":{"login":"mshar039","id":17826608,"node_id":"MDQ6VXNlcjE3ODI2NjA4","avatar_url":"https://avatars2.githubusercontent.com/u/17826608?v=4","gravatar_id":"","url":"https://api.github.com/users/mshar039","html_url":"https://github.com/mshar039","followers_url":"https://api.github.com/users/mshar039/followers","following_url":"https://api.github.com/users/mshar039/following{/other_user}","gists_url":"https://api.github.com/users/mshar039/gists{/gist_id}","starred_url":"https://api.github.com/users/mshar039/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mshar039/subscriptions","organizations_url":"https://api.github.com/users/mshar039/orgs","repos_url":"https://api.github.com/users/mshar039/repos","events_url":"https://api.github.com/users/mshar039/events{/privacy}","received_events_url":"https://api.github.com/users/mshar039/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2016-03-23T06:48:04Z","updated_at":"2016-03-23T07:01:32Z","closed_at":"2016-03-23T07:01:32Z","author_association":"NONE","active_lock_reason":null,"body":"Hi All,\n\nMy task is to have a centralized log analysis tool that can accommodate 500 GB of log files; and can search for anything within seconds from it. So, had a basic setup of Graylog with elasticsearch and logstash. \nTo start, I tried reading one log file using logstash and stored it in elasticsearch. And am able to visualize them in the Graylog web interface.\n\nNow i need to scale up the setup so that I can index 500 GB of data in elasticsearch. \nIs it possible to read these many files through logstash and index to elasticsearch? \nWill mongodb helpful in this scenario?\nHow should I scale up my architecture in terms of: elasticsearch nodes, RAM, CPU Power, ES-heap size and many more... so that I can meet the requirements of the task.\n\nCurrently have 4 GB RAM in VM\nCentOS 7\nElasticsearch: v 1.7.5\nLogstash: v 2.2.2\nGraylog: v 1.3\n\nKindly help me with my questions. I am very new to this environment.\nThanks.\n","closed_by":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"performed_via_github_app":null}