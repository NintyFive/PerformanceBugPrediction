{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/31801","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31801/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31801/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31801/events","html_url":"https://github.com/elastic/elasticsearch/issues/31801","id":338328662,"node_id":"MDU6SXNzdWUzMzgzMjg2NjI=","number":31801,"title":"ES master re-election algorithm tries electing a non-reachable master.","user":{"login":"itiyama","id":7935367,"node_id":"MDQ6VXNlcjc5MzUzNjc=","avatar_url":"https://avatars3.githubusercontent.com/u/7935367?v=4","gravatar_id":"","url":"https://api.github.com/users/itiyama","html_url":"https://github.com/itiyama","followers_url":"https://api.github.com/users/itiyama/followers","following_url":"https://api.github.com/users/itiyama/following{/other_user}","gists_url":"https://api.github.com/users/itiyama/gists{/gist_id}","starred_url":"https://api.github.com/users/itiyama/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/itiyama/subscriptions","organizations_url":"https://api.github.com/users/itiyama/orgs","repos_url":"https://api.github.com/users/itiyama/repos","events_url":"https://api.github.com/users/itiyama/events{/privacy}","received_events_url":"https://api.github.com/users/itiyama/received_events","type":"User","site_admin":false},"labels":[{"id":881394071,"node_id":"MDU6TGFiZWw4ODEzOTQwNzE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Cluster%20Coordination","name":":Distributed/Cluster Coordination","color":"0e8a16","default":false,"description":"Cluster formation and cluster state publication, including cluster membership and fault detection."}],"state":"closed","locked":false,"assignee":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"assignees":[{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false}],"milestone":null,"comments":8,"created_at":"2018-07-04T16:12:07Z","updated_at":"2018-12-19T10:57:31Z","closed_at":"2018-12-19T10:57:31Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version (bin/elasticsearch --version)**: 5.5\r\n**Plugins installed:** []\r\n\r\n**JVM version (java -version)**: 1.8\r\n**OS version (uname -a if on a Unix-like system)**: 4.9.38-16.35.amzn1.x86_64\r\n**Description of the problem including expected versus actual behavior:**\r\n\r\nThis issue was observed in a cluster with 8 data nodes and 3 dedicated master eligible nodes.\r\nWe have observed that master re-election gets stuck in a loop sometimes even though all master eligible nodes are up and running and only one of them is not reachable. The `UnicastHostsProvider` returns the host, but it is not ping-able from any other node. It is only after `UnicastHostsProvider` stops returning that host that the master re-election is triggered. In this particular case, master re-election was stuck in a loop for the node for more than 1.5 hours because `UnicastHostsProvider` continues to return it. That particular host was the master before the re-election was triggered.\r\n\r\nScenario:\r\n\r\n1. ES on all dedicated masters restart- on current master it restarts in the end.\r\n2. The ‘master left’ is detected by fault detection logic on all data nodes.\r\n3. Master pinger in each node is stopped(as master re-election is going to be triggered) but master is not removed from cluster state.\r\n4. Join logic(Re-election) is triggered but all nodes return the old master in their ping results but joining the old master fails. This happens in a loop.\r\n\r\nI want to understand the following:\r\nWhy does the master node re-appear in the [current nodes](https://github.com/elastic/elasticsearch/blob/5.5/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java#L959) even though [master left](https://github.com/elastic/elasticsearch/blob/5.5/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java#L687). Please note that the master node ephemeral id is old. Also, since master pinger died, we have no way of removing a stale master now.\r\n\r\n**Steps to reproduce:**\r\n\r\nUnfortunately I don't have the steps to reproduce the issue, but have seen this happening more than once. I have the relevant logs and I have looked at the code flow which makes me think that this is a bug.\r\n\r\n**Provide logs (if relevant):**\r\nLog level is info for all the logs\r\n\r\nLogs on one of the data nodes[removed ip addresses]: These log lines appear only once on all data nodes.\r\n```\r\n[2018-06-05T00:34:02,182][INFO ][o.e.d.z.ZenDiscovery     ] [ub38WLW] master_left [{31tudzn}{31tudznpRrOr8iRcg3eqCw}{IchjnSK7SkKsP8u90WgtgA}], reason [transport disconnected]\r\n[2018-06-05T00:34:02,183][WARN ][o.e.d.z.ZenDiscovery     ] [ub38WLW] master left (reason = transport disconnected), current nodes: nodes:\r\n{31tudzn}{31tudznpRrOr8iRcg3eqCw}{IchjnSK7SkKsP8u90WgtgA}, master\r\n   {ub38WLW}{ub38WLW2QrmMUORORsjIaQ}{bzvVWANSTK6svB6wgrbqDQ}, local\r\n   {XCuAlLW}{XCuAlLWhTFixeF-ZfvVZ0g}{mmrE4kn_S2OJp_IMc64CCw}\r\n   {Ostbsj_}{Ostbsj_mRaGzgRzbsMmJCg}{olZOAwrSSX-79FokruKQbQ}\r\n   {K720qk5}{K720qk56TBeULS2GN7J9zA}{QvH5SbF1T0md-ujHPevjUg}\r\n   {UfWI8YH}{UfWI8YHRTyaedIJD0i9UcA}{qcmCT-iPR7KIeZYprntjAA}\r\n   {a4A5voP}{a4A5voPvQzSR7kEh65vNPA}{fVLArUnCRqOzSswQjjiY8w}\r\n   {RFf4YvU}{RFf4YvUPQA-JHGU_NvlPLA}{mWnG3CUiQZqpsHb8U8Bt6Q}\r\n   {0de5xLZ}{0de5xLZDTluZjEBC6SrOHA}{h_fHZguHTr2jPmb_y27EnQ}\r\n   {g6V22Qk}{g6V22QkNREyriF8rmrM8bg}{B98SirMJT6OGjHFV771rwQ}\r\n   {o_yEC-x}{o_yEC-x1S6SeZuswgugMLw}{Twkui3yPQtCNOjEBmPIVtQ}\r\n ```\r\nPost this all I see on all data nodes is a failed attempt to connect to the master - also the master node ephemeral id is old.\r\n```\r\n[2018-06-05T00:34:02,186][WARN ][o.e.c.NodeConnectionsService] [ub38WLW] failed to connect to node {31tudzn}{31tudznpRrOr8iRcg3eqCw}{IchjnSK7SkKsP8u90WgtgA} (tried [1] times)\r\norg.elasticsearch.transport.ConnectTransportException: [31tudzn] connect_timeout[30s]\r\n    at org.elasticsearch.transport.netty4.Netty4Transport.connectToChannels(Netty4Transport.java:361) ~[?:?]\r\n    at org.elasticsearch.transport.TcpTransport.openConnection(TcpTransport.java:548) ~[elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.transport.TcpTransport.connectToNode(TcpTransport.java:472) ~[elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:332) ~[elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:319) ~[elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.cluster.NodeConnectionsService.validateAndConnectIfNeeded(NodeConnectionsService.java:154) [elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.cluster.NodeConnectionsService$1.doRun(NodeConnectionsService.java:107) [elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:638) [elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.5.2.jar:5.5.2]\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]\r\n    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]                      \r\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: ip:9300\r\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_112]  \r\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_112]\r\n    at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:352) ~[?:?]\r\n    at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) ~[?:?]\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) ~[?:?]\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) ~[?:?]\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) ~[?:?]\r\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) ~[?:?]      \r\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) ~[?:?]\r\n    ... 1 more  \r\nCaused by: java.net.ConnectException: Connection refused                        \r\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_112]  \r\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_112]\r\n    at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:352) ~[?:?]\r\n    at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) ~[?:?]\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) ~[?:?]\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) ~[?:?]\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) ~[?:?]\r\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) ~[?:?]      \r\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) ~[?:?]\r\n    ... 1 more \r\n```\r\nLogs on master node- Please note that the master node [ephemeral id](https://github.com/elastic/elasticsearch/blob/5.5/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java#L168) is different here- hence it tries to connect to some itself on a remote connection, instead of [waiting for incoming joins](https://github.com/elastic/elasticsearch/blob/5.5/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java#L389).\r\n```\r\n\r\norg.elasticsearch.transport.ConnectTransportException: [31tudzn] handshake failed. unexpected remote node {31tudzn}{31tudznpRrOr8iRcg3eqCw}{SNZOWSLtQwWsFodR6B0D-w}\r\n    at org.elasticsearch.transport.TransportService.lambda$connectToNode$3(TransportService.java:336) ~[elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.transport.TcpTransport.connectToNode(TcpTransport.java:473) ~[elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:332) ~[elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:319) ~[elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.discovery.zen.ZenDiscovery.joinElectedMaster(ZenDiscovery.java:459) [elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:411) [elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.discovery.zen.ZenDiscovery.access$4100(ZenDiscovery.java:83) [elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1188) [elasticsearch-5.5.2.jar:5.5.2]\r\n    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:569) [elasticsearch-5.5.2.jar:5.5.2]\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]\r\n    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]  \r\n```\r\nMy hypothesis of the issue:\r\n\r\n1. Master node leaves.\r\n2. Due to some concurrency issue- master node comes back again into cluster state(old master node comes back as the ephemeral id is old).\r\n3. Since master fault detection was turned off due to master leaving the cluster, the master is not removed again.\r\n4. The pings from the nodes return the master in their response since master is not removed from cluster state due to concurrency issue + pinger dying(pinger is turned off due to master leaving the cluster). Note that there is no liveness on the master value that is broadcasted during pings. If it is not removed, the same value is always returned once pinger is turned off- irrespective of master being unreachable.\r\n5. Since [activeMaster](https://github.com/elastic/elasticsearch/blob/5.5/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java#L898) is returned from the node, we don't re-elect a master from dedicated master eligible nodes but just do a [tie-break](https://github.com/elastic/elasticsearch/blob/5.5/core/src/main/java/org/elasticsearch/discovery/zen/ElectMasterService.java#L151) leading to the old master being elected as it has the lowest node id and highest cluster state version.\r\n6. We try to connect to the old master but due to some network issue unable to do so. Step 4 starts again and we keep on repeating steps 4-6 until master is removed from ping response.","closed_by":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"performed_via_github_app":null}