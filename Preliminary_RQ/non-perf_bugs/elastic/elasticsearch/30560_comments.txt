[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388743669","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-388743669","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":388743669,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODc0MzY2OQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-05-14T08:51:12Z","updated_at":"2018-05-14T08:51:12Z","author_association":"CONTRIBUTOR","body":"I never did any serious performance testing with these options but my assumption was that they wouldn't impact indexing performance much since compound files are only applied to small segments and the compound file is written right after the segment has been written, so intermediate files don't have time to make it to the disk, they only live in the filesystem cache.\r\n\r\nDo all your 1700 shards receive write operations or are some of them read-only?\r\n\r\nI'm also wondering how much memory you give to the filesystem cache and what your `vm.dirty_background_ratio` is.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388743794","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-388743794","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":388743794,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODc0Mzc5NA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-05-14T08:51:34Z","updated_at":"2018-05-14T08:51:34Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-core-infra","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388744150","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-388744150","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":388744150,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODc0NDE1MA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-05-14T08:52:41Z","updated_at":"2018-05-14T08:52:41Z","author_association":"CONTRIBUTOR","body":"cc @danielmitterdorfer since this is performance/benchmark-related","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388758370","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-388758370","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":388758370,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODc1ODM3MA==","user":{"login":"micoq","id":1741616,"node_id":"MDQ6VXNlcjE3NDE2MTY=","avatar_url":"https://avatars3.githubusercontent.com/u/1741616?v=4","gravatar_id":"","url":"https://api.github.com/users/micoq","html_url":"https://github.com/micoq","followers_url":"https://api.github.com/users/micoq/followers","following_url":"https://api.github.com/users/micoq/following{/other_user}","gists_url":"https://api.github.com/users/micoq/gists{/gist_id}","starred_url":"https://api.github.com/users/micoq/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/micoq/subscriptions","organizations_url":"https://api.github.com/users/micoq/orgs","repos_url":"https://api.github.com/users/micoq/repos","events_url":"https://api.github.com/users/micoq/events{/privacy}","received_events_url":"https://api.github.com/users/micoq/received_events","type":"User","site_admin":false},"created_at":"2018-05-14T09:42:41Z","updated_at":"2018-05-14T09:42:41Z","author_association":"NONE","body":"@jpountz We have only 96 active shards at a time: 3 daily indices with 16 shards and 1 replica for a total of ~700 GB per day (including replicas).\r\n\r\nEach machine have a total of 64 GB (26 GB for the Heap, the rest is for the system and the filesystem cache).\r\n`vm.dirty_background_ratio` is 10%\r\n\r\nFor the Lucene tests and the `index.compound_on_flush` option, I didn't used this cluster but a single machine with Lucene indices and different sizes for the index buffer : 10kb to 256Mb (however, in this case, I didn't measure the %wait CPU).\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/388760374","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-388760374","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":388760374,"node_id":"MDEyOklzc3VlQ29tbWVudDM4ODc2MDM3NA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-05-14T09:49:14Z","updated_at":"2018-05-14T09:49:14Z","author_association":"CONTRIBUTOR","body":"96 active shards on 4 nodes only actually sounds like too many to me, but this wouldn't explain why you can also reproduce this with your standalone Lucene index.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/390126564","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-390126564","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":390126564,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MDEyNjU2NA==","user":{"login":"micoq","id":1741616,"node_id":"MDQ6VXNlcjE3NDE2MTY=","avatar_url":"https://avatars3.githubusercontent.com/u/1741616?v=4","gravatar_id":"","url":"https://api.github.com/users/micoq","html_url":"https://github.com/micoq","followers_url":"https://api.github.com/users/micoq/followers","following_url":"https://api.github.com/users/micoq/following{/other_user}","gists_url":"https://api.github.com/users/micoq/gists{/gist_id}","starred_url":"https://api.github.com/users/micoq/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/micoq/subscriptions","organizations_url":"https://api.github.com/users/micoq/orgs","repos_url":"https://api.github.com/users/micoq/repos","events_url":"https://api.github.com/users/micoq/events{/privacy}","received_events_url":"https://api.github.com/users/micoq/received_events","type":"User","site_admin":false},"created_at":"2018-05-18T07:55:49Z","updated_at":"2018-05-18T07:55:49Z","author_association":"NONE","body":"We have configured 16 shards per index to distribute the searches more efficiently across all the CPUs (each machine has 40 CPUs). It seems to be the right value in our case since less shards increase the response time (and more shards no longer improve the response time).\r\n\r\nI think `index.compound_on_flush` will not influence the storage activity and thus the %wait CPU. Like you explained: the new segments are small and can fit into the filesystem cache (the merged segments are bigger and older, this could explain the influence of `index.compound_format` on the storage activity).\r\n\r\nHowever, the latency could come from the (CPU) time consumed by the bulk threads to create the compound files (`*.cfs`) for re-read and re-write operations.\r\nCorrect me if i'm wrong, the compound files are created synchronously in the indexing process by the bulk threads (and merges are asynchronous).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/391337718","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-391337718","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":391337718,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MTMzNzcxOA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-05-23T13:00:30Z","updated_at":"2018-05-23T13:00:30Z","author_association":"CONTRIBUTOR","body":"@s1monw Do you have an opinion on this one?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/392452394","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-392452394","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":392452394,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MjQ1MjM5NA==","user":{"login":"micoq","id":1741616,"node_id":"MDQ6VXNlcjE3NDE2MTY=","avatar_url":"https://avatars3.githubusercontent.com/u/1741616?v=4","gravatar_id":"","url":"https://api.github.com/users/micoq","html_url":"https://github.com/micoq","followers_url":"https://api.github.com/users/micoq/followers","following_url":"https://api.github.com/users/micoq/following{/other_user}","gists_url":"https://api.github.com/users/micoq/gists{/gist_id}","starred_url":"https://api.github.com/users/micoq/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/micoq/subscriptions","organizations_url":"https://api.github.com/users/micoq/orgs","repos_url":"https://api.github.com/users/micoq/repos","events_url":"https://api.github.com/users/micoq/events{/privacy}","received_events_url":"https://api.github.com/users/micoq/received_events","type":"User","site_admin":false},"created_at":"2018-05-28T08:05:34Z","updated_at":"2018-05-28T08:05:34Z","author_association":"NONE","body":"Well, I launched again my tests on Lucene (7.0.0) with a more precise setup and here are the results for indexing 1 million small documents (~1ko) with various indexing buffer sizes (128kB to 1GB) in a single Lucene index:\r\n\r\nCommit: total time after indexing and a full commit (fsync) without using compound format\r\nCommit+Compound: same using compound format\r\nFlush: total time after indexing and after a flush of the remaining documents but before the full commit without compound format\r\nFlush+Compound: same using compound format\r\n\r\n<a href=\"https://ibb.co/ezuCoJ\"><img src=\"https://preview.ibb.co/cRKdTJ/compound.png\" alt=\"compound\" border=\"0\"></a>\r\n\r\nSo, I wonder if I didn't make a mistake in my first setup:\r\n- We can see the compound format doesn't seem to have a big impact on indexing speed for a sufficient indexing buffer size (yellow and green lines). \r\n- The time consumed by `IndexWriter.commit()` depends on the number of files to write to the filesystem after all operations with `fsync()`. Since the time spent in this syscall seems constant regardless of the file size, the compound format performs better (blue and red lines).\r\n\r\nFor these tests, I totally disabled the segment merges that's why I have a lot of segments for small indexing buffer sizes.\r\n\r\nIn all cases, all the index files fit into the filesystem cache (~200MB) so the storage is used only when `fsync()` is called.\r\nThe 10% drop I measured the first time could be caused by a lack of filesystem cache (these Lucene-only tests are performed on another hardware).\r\n\r\nFinally, enable `index.compound_on_flush` by default seems to be the right choice (at least on a single Lucene index at a time, I didn't perform a concurrent test against multiple threads).\r\n\r\nFor the `index.compound_format` option, the results are still valid thought (less %wait CPU on big segments).\r\n\r\nHere are the raw results (time in seconds):\r\n```\r\nBuffer: 0,128000 MB,    Compound: false, Time: 64295\r\nBuffer: 0,256000 MB,    Compound: false, Time: 32660\r\nBuffer: 0,512000 MB,    Compound: false, Time: 14497\r\nBuffer: 1,000000 MB,    Compound: false, Time: 8451\r\nBuffer: 2,000000 MB,    Compound: false, Time: 5581\r\nBuffer: 4,000000 MB,    Compound: false, Time: 4119\r\nBuffer: 8,000000 MB,    Compound: false, Time: 3437\r\nBuffer: 16,000000 MB,   Compound: false, Time: 3167\r\nBuffer: 32,000000 MB,   Compound: false, Time: 2906\r\nBuffer: 64,000000 MB,   Compound: false, Time: 2970\r\nBuffer: 128,000000 MB,  Compound: false, Time: 3064\r\nBuffer: 256,000000 MB,  Compound: false, Time: 2927\r\nBuffer: 512,000000 MB,  Compound: false, Time: 2952\r\nBuffer: 1024,000000 MB, Compound: false, Time: 2861\r\nBuffer: 0,128000 MB,    Compound: true,  Time: 56452\r\nBuffer: 0,256000 MB,    Compound: true,  Time: 13213\r\nBuffer: 0,512000 MB,    Compound: true,  Time: 6696\r\nBuffer: 1,000000 MB,    Compound: true,  Time: 4728\r\nBuffer: 2,000000 MB,    Compound: true,  Time: 3885\r\nBuffer: 4,000000 MB,    Compound: true,  Time: 3147\r\nBuffer: 8,000000 MB,    Compound: true,  Time: 2984\r\nBuffer: 16,000000 MB,   Compound: true,  Time: 2842\r\nBuffer: 32,000000 MB,   Compound: true,  Time: 2756\r\nBuffer: 64,000000 MB,   Compound: true,  Time: 2749\r\nBuffer: 128,000000 MB,  Compound: true,  Time: 2870\r\nBuffer: 256,000000 MB,  Compound: true,  Time: 2704\r\nBuffer: 512,000000 MB,  Compound: true,  Time: 2789\r\nBuffer: 1024,000000 MB, Compound: true,  Time: 2721\r\n\r\nBuffer: 0,128000 MB,    Compound: false, Time: 35086\r\nBuffer: 0,256000 MB,    Compound: false, Time: 2832\r\nBuffer: 0,512000 MB,    Compound: false, Time: 2390\r\nBuffer: 1,000000 MB,    Compound: false, Time: 2458\r\nBuffer: 2,000000 MB,    Compound: false, Time: 2408\r\nBuffer: 4,000000 MB,    Compound: false, Time: 2413\r\nBuffer: 8,000000 MB,    Compound: false, Time: 2513\r\nBuffer: 16,000000 MB,   Compound: false, Time: 2591\r\nBuffer: 32,000000 MB,   Compound: false, Time: 2559\r\nBuffer: 64,000000 MB,   Compound: false, Time: 2461\r\nBuffer: 128,000000 MB,  Compound: false, Time: 2554\r\nBuffer: 256,000000 MB,  Compound: false, Time: 2527\r\nBuffer: 512,000000 MB,  Compound: false, Time: 2480\r\nBuffer: 1024,000000 MB, Compound: false, Time: 2532\r\nBuffer: 0,128000 MB,    Compound: true,  Time: 16476\r\nBuffer: 0,256000 MB,    Compound: true,  Time: 2885\r\nBuffer: 0,512000 MB,    Compound: true,  Time: 2467\r\nBuffer: 1,000000 MB,    Compound: true,  Time: 2420\r\nBuffer: 2,000000 MB,    Compound: true,  Time: 2458\r\nBuffer: 4,000000 MB,    Compound: true,  Time: 2481\r\nBuffer: 8,000000 MB,    Compound: true,  Time: 2527\r\nBuffer: 16,000000 MB,   Compound: true,  Time: 2638\r\nBuffer: 32,000000 MB,   Compound: true,  Time: 2662\r\nBuffer: 64,000000 MB,   Compound: true,  Time: 2663\r\nBuffer: 128,000000 MB,  Compound: true,  Time: 2665\r\nBuffer: 256,000000 MB,  Compound: true,  Time: 2693\r\nBuffer: 512,000000 MB,  Compound: true,  Time: 2636\r\nBuffer: 1024,000000 MB, Compound: true,  Time: 2658\r\n```\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/392523107","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-392523107","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":392523107,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MjUyMzEwNw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-05-28T13:10:54Z","updated_at":"2018-05-28T13:10:54Z","author_association":"CONTRIBUTOR","body":"2600 seconds for 1M small documents sounds very long to me, it is less than 400 documents per second. Is the code that you used to conduct this benchmark available somewhere?\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/392563262","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-392563262","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":392563262,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MjU2MzI2Mg==","user":{"login":"micoq","id":1741616,"node_id":"MDQ6VXNlcjE3NDE2MTY=","avatar_url":"https://avatars3.githubusercontent.com/u/1741616?v=4","gravatar_id":"","url":"https://api.github.com/users/micoq","html_url":"https://github.com/micoq","followers_url":"https://api.github.com/users/micoq/followers","following_url":"https://api.github.com/users/micoq/following{/other_user}","gists_url":"https://api.github.com/users/micoq/gists{/gist_id}","starred_url":"https://api.github.com/users/micoq/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/micoq/subscriptions","organizations_url":"https://api.github.com/users/micoq/orgs","repos_url":"https://api.github.com/users/micoq/repos","events_url":"https://api.github.com/users/micoq/events{/privacy}","received_events_url":"https://api.github.com/users/micoq/received_events","type":"User","site_admin":false},"created_at":"2018-05-28T16:10:55Z","updated_at":"2018-05-28T16:10:55Z","author_association":"NONE","body":"@jpountz Sorry, the unit was wrong, it's indeed in milliseconds not in seconds !","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/392820110","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-392820110","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":392820110,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MjgyMDExMA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-05-29T15:28:07Z","updated_at":"2018-05-29T15:28:07Z","author_association":"CONTRIBUTOR","body":"Thank you, so it's actually 400K docs per second. How many threads do you use for indexing and how do you generate your documents?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/393273423","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-393273423","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":393273423,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzI3MzQyMw==","user":{"login":"micoq","id":1741616,"node_id":"MDQ6VXNlcjE3NDE2MTY=","avatar_url":"https://avatars3.githubusercontent.com/u/1741616?v=4","gravatar_id":"","url":"https://api.github.com/users/micoq","html_url":"https://github.com/micoq","followers_url":"https://api.github.com/users/micoq/followers","following_url":"https://api.github.com/users/micoq/following{/other_user}","gists_url":"https://api.github.com/users/micoq/gists{/gist_id}","starred_url":"https://api.github.com/users/micoq/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/micoq/subscriptions","organizations_url":"https://api.github.com/users/micoq/orgs","repos_url":"https://api.github.com/users/micoq/repos","events_url":"https://api.github.com/users/micoq/events{/privacy}","received_events_url":"https://api.github.com/users/micoq/received_events","type":"User","site_admin":false},"created_at":"2018-05-30T18:43:18Z","updated_at":"2018-05-30T18:43:55Z","author_association":"NONE","body":"I use a single thread. The documents are generated with constant and variable fields just to fill the inverted index.\r\n\r\nHere is the piece of code I used:\r\n```\r\npublic static void main(String[] args) throws IOException {  \r\n\r\n  double[] segSizes = new double[] {\r\n    0.128,\r\n    0.256,\r\n    0.512,\r\n    1.0,\r\n    2.0,\r\n    4.0,\r\n    8.0,\r\n    16.0,\r\n    32.0,\r\n    64.0,\r\n    128.0,\r\n    256.0,\r\n    512.0,\r\n    1024.0\r\n  };\r\n  \r\n  boolean[] useCompoundList = new boolean[] {\r\n      false,\r\n      true\r\n  };\r\n  \r\n  Directory dir = new MMapDirectory(Paths.get(\"/home/test/test_lucene\"));\r\n  \r\n  StringBuilder blob = new StringBuilder();\r\n  for(int i=0; i<1024; ++i) {\r\n    blob.append('a');\r\n  }\r\n  Field blobField = new StringField(\"blob\",blob.toString(),Field.Store.YES);\r\n  \r\n  for(boolean useCompound : useCompoundList) {\r\n    for(double segSize : segSizes) {\r\n      for(String fileName : dir.listAll()) {\r\n        dir.deleteFile(fileName);\r\n      }\r\n      \r\n      IndexWriterConfig config = new IndexWriterConfig();\r\n      MergePolicy mergePolicy = NoMergePolicy.INSTANCE; // merges are disabled\r\n      config.setMergePolicy(mergePolicy);\r\n      config.setUseCompoundFile(useCompound);\r\n      config.setRAMBufferSizeMB(segSize);\r\n\r\n      long startTime = System.nanoTime();\r\n      long endTime = startTime;\r\n      IndexWriter writer = null;\r\n      try {\r\n        writer = new IndexWriter(dir, config);\r\n        for(int i=0;i<1000000;++i) {\r\n          Document doc = new Document();\r\n          doc.add(blobField);\r\n          doc.add(new StringField(\"field\",Integer.toString(i),Field.Store.YES));\r\n          doc.add(new LongPoint(\"number\",i));\r\n          writer.addDocument(doc);\r\n        }\r\n        writer.flush();\r\n        endTime = System.nanoTime();\r\n        writer.commit(); // can be slow with a lot of files\r\n        writer.close();\r\n      }\r\n      catch (IOException e) {\r\n        e.printStackTrace();\r\n      }\r\n      System.out.println(String.format(\"Buffer: %f MB, Compound: %s, Time: %d\",\r\n          config.getRAMBufferSizeMB(),\r\n          config.getUseCompoundFile() ? \"true\" : \"false\",\r\n          (endTime-startTime)/1000000\r\n          ));\r\n    }\r\n  }\r\n}\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/397231317","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-397231317","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":397231317,"node_id":"MDEyOklzc3VlQ29tbWVudDM5NzIzMTMxNw==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2018-06-14T09:26:06Z","updated_at":"2018-06-14T09:26:06Z","author_association":"CONTRIBUTOR","body":"hey folks, sorry for being late to the party here. You latest benchmark results pretty much match my experience and the benchmarks I did long ago. Yet, I do wonder if you see significant difference depending on the disk you use. In my experience having more than one active shard per spinning disk will cause significant slowdowns and I wonder if your results are coming from the fact that you are using spinning disks? @micoq ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/397247699","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-397247699","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":397247699,"node_id":"MDEyOklzc3VlQ29tbWVudDM5NzI0NzY5OQ==","user":{"login":"micoq","id":1741616,"node_id":"MDQ6VXNlcjE3NDE2MTY=","avatar_url":"https://avatars3.githubusercontent.com/u/1741616?v=4","gravatar_id":"","url":"https://api.github.com/users/micoq","html_url":"https://github.com/micoq","followers_url":"https://api.github.com/users/micoq/followers","following_url":"https://api.github.com/users/micoq/following{/other_user}","gists_url":"https://api.github.com/users/micoq/gists{/gist_id}","starred_url":"https://api.github.com/users/micoq/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/micoq/subscriptions","organizations_url":"https://api.github.com/users/micoq/orgs","repos_url":"https://api.github.com/users/micoq/repos","events_url":"https://api.github.com/users/micoq/events{/privacy}","received_events_url":"https://api.github.com/users/micoq/received_events","type":"User","site_admin":false},"created_at":"2018-06-14T10:26:10Z","updated_at":"2018-06-14T10:26:10Z","author_association":"NONE","body":"@s1monw For this benchmark, I used a single SSD (no RAID). The results are pretty much the same on a spinning disk (for `index.compound_on_flush`).\r\nHowever, in our production environment, we use 10k spinning disks (RAID5 on a battery-backed controller).\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/397341562","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-397341562","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":397341562,"node_id":"MDEyOklzc3VlQ29tbWVudDM5NzM0MTU2Mg==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2018-06-14T15:41:44Z","updated_at":"2018-06-14T15:41:44Z","author_association":"CONTRIBUTOR","body":"I mean we can think about making this a size based setting in lucene ie. everything below 10MB we use CFS and otherwise use plain files. I just need some data to reproduce this in order to work on a fix. @micoq ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/418352545","html_url":"https://github.com/elastic/elasticsearch/issues/30560#issuecomment-418352545","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/30560","id":418352545,"node_id":"MDEyOklzc3VlQ29tbWVudDQxODM1MjU0NQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2018-09-04T12:43:14Z","updated_at":"2018-09-04T12:43:14Z","author_association":"CONTRIBUTOR","body":"we haven't got much feedback I am closing this.","performed_via_github_app":null}]