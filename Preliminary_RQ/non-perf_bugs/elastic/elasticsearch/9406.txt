{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/9406","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9406/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9406/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9406/events","html_url":"https://github.com/elastic/elasticsearch/issues/9406","id":55423453,"node_id":"MDU6SXNzdWU1NTQyMzQ1Mw==","number":9406,"title":"One of the shard couldn't recovery from the rolling upgrade, 1.1.1 -> 1.4.2","user":{"login":"yangou","id":1667326,"node_id":"MDQ6VXNlcjE2NjczMjY=","avatar_url":"https://avatars1.githubusercontent.com/u/1667326?v=4","gravatar_id":"","url":"https://api.github.com/users/yangou","html_url":"https://github.com/yangou","followers_url":"https://api.github.com/users/yangou/followers","following_url":"https://api.github.com/users/yangou/following{/other_user}","gists_url":"https://api.github.com/users/yangou/gists{/gist_id}","starred_url":"https://api.github.com/users/yangou/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yangou/subscriptions","organizations_url":"https://api.github.com/users/yangou/orgs","repos_url":"https://api.github.com/users/yangou/repos","events_url":"https://api.github.com/users/yangou/events{/privacy}","received_events_url":"https://api.github.com/users/yangou/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2015-01-25T18:01:13Z","updated_at":"2015-01-27T09:16:11Z","closed_at":"2015-01-26T19:45:51Z","author_association":"NONE","active_lock_reason":null,"body":"We started upgrading our ES two nodes cluster from 1.1.1 to 1.4.2, using the rolling upgrade.\nAfter restarting one of the upgraded node, we found one of the shards couldn't get itself back. The log out put give this error:\n\n```\nCaused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=x56z8s actual=1h6zri0 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@393b946e)\n    at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)\n    at org.elasticsearch.index.store.Store.verify(Store.java:365)\n    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)\n    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n\n```\n\nAlso, the expected size of that shard should be around 170G, however, the recovering directory grew to more than 650+G.\n\nI checked the hardware, which has no issues at all. But to make sure it wasn't the hard ware issue, I added a brand new node into the cluster, and it seems that particular shard has the reproducible issue on new machine too.\n\nI deleted the directory manually as mentioned in #9302, the cluster didn't automatically create the replica.\nSo I use the reroute API, to try to move that primary shard from the old version node to new version node. It seemed to be promising, because when the move finishes, the size of the directory was correct. However, it turned out that after moving, the old shard on the old version node didn't get removed, and the newly created one on new version node became just a copy of the old shard, not even a replica, because cluster didn't allocate it.\n\nAny idea about how to fix this issue?\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}