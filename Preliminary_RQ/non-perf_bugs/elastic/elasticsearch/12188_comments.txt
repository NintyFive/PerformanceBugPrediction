[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/120540583","html_url":"https://github.com/elastic/elasticsearch/issues/12188#issuecomment-120540583","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12188","id":120540583,"node_id":"MDEyOklzc3VlQ29tbWVudDEyMDU0MDU4Mw==","user":{"login":"jprante","id":635745,"node_id":"MDQ6VXNlcjYzNTc0NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/635745?v=4","gravatar_id":"","url":"https://api.github.com/users/jprante","html_url":"https://github.com/jprante","followers_url":"https://api.github.com/users/jprante/followers","following_url":"https://api.github.com/users/jprante/following{/other_user}","gists_url":"https://api.github.com/users/jprante/gists{/gist_id}","starred_url":"https://api.github.com/users/jprante/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jprante/subscriptions","organizations_url":"https://api.github.com/users/jprante/orgs","repos_url":"https://api.github.com/users/jprante/repos","events_url":"https://api.github.com/users/jprante/events{/privacy}","received_events_url":"https://api.github.com/users/jprante/received_events","type":"User","site_admin":false},"created_at":"2015-07-10T22:16:04Z","updated_at":"2015-07-10T22:16:04Z","author_association":"CONTRIBUTOR","body":"Not sure how blocking chunked transfer can solve challenges like back pressure, but it should be possible to write RxJava https://github.com/ReactiveX/RxJava based code that implements reactive streams http://www.reactive-streams.org/ similar to http://mongodb.github.io/mongo-java-driver-reactivestreams/\nThis would be more easier if Observer pattern for actions and Java 8 lambdas could be used. Example of what can be done is \"HTTP tail\" for JVM https://github.com/myfreeweb/rxjava-http-tail\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/122886413","html_url":"https://github.com/elastic/elasticsearch/issues/12188#issuecomment-122886413","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12188","id":122886413,"node_id":"MDEyOklzc3VlQ29tbWVudDEyMjg4NjQxMw==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2015-07-20T13:36:44Z","updated_at":"2015-07-20T13:36:44Z","author_association":"CONTRIBUTOR","body":"> Not sure how blocking chunked transfer can solve challenges like back pressure\n\nIt _kind of_ can.  There isn't anything that I know of like the triple-ack of tcp but there are buffers and you could in theory check how full they are and only try to fill them when they get below a certain point.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172651816","html_url":"https://github.com/elastic/elasticsearch/issues/12188#issuecomment-172651816","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12188","id":172651816,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjY1MTgxNg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-01-18T20:57:08Z","updated_at":"2016-01-18T20:57:08Z","author_association":"CONTRIBUTOR","body":"@nik9000 is this still something you want to investigate?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172660779","html_url":"https://github.com/elastic/elasticsearch/issues/12188#issuecomment-172660779","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12188","id":172660779,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjY2MDc3OQ==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-01-18T21:37:10Z","updated_at":"2016-01-18T21:37:10Z","author_association":"CONTRIBUTOR","body":"> @nik9000 is this still something you want to investigate?\n\nI think its a neat idea and might be useful for something someday but it just doesn't have the crazy +1 train that some other proposals have accumulated. I'm going to close it. Maybe someone can revive it when they have some super awesome use case.\n\nHonestly the flip side might be more useful: implement bulk indexing using chunked uploads. That has really simple back pressure on the uploading thread and would be simpler to implement. @mikemccand and I talked about it many months ago. The neat thing about it is that Elasticsearch can better manage its memory if the user is uploading using chunks - they can continue sending chunks until they want to make sure the translog has fsynced - then they send the last chunk and we consider the bulk request complete and run the fsync. Rather than having to load the whole bulk request we get to rely on tcp's back pressure to slow the client down so we can have as much of the bulk request \"in flight\" as we think is appropriate.\n\nIts a neat idea but I dunno if its actually worth implementing.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172681576","html_url":"https://github.com/elastic/elasticsearch/issues/12188#issuecomment-172681576","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12188","id":172681576,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjY4MTU3Ng==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2016-01-18T23:33:41Z","updated_at":"2016-01-18T23:33:41Z","author_association":"CONTRIBUTOR","body":"> implement bulk indexing using chunked uploads.\n\n+1\n\n> The neat thing about it is that Elasticsearch can better manage its memory if the user is uploading using chunks\n\nManage its memory and also manage appropriate concurrency to bring to bear.  Plus the client gets much simpler, not having to play games with proper item count per bulk request, how many client threads to use, dealing w/ rejected exceptions, etc.\n\n@HonzaKral recently added some nice sugar to the ES python client APIs that does some of this for the user, so the user feels like they're using a single streaming bulk indexing API, and under the hood the Python ES client breaks it into chunks using N threads ...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/208970272","html_url":"https://github.com/elastic/elasticsearch/issues/12188#issuecomment-208970272","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12188","id":208970272,"node_id":"MDEyOklzc3VlQ29tbWVudDIwODk3MDI3Mg==","user":{"login":"Bargs","id":6239176,"node_id":"MDQ6VXNlcjYyMzkxNzY=","avatar_url":"https://avatars3.githubusercontent.com/u/6239176?v=4","gravatar_id":"","url":"https://api.github.com/users/Bargs","html_url":"https://github.com/Bargs","followers_url":"https://api.github.com/users/Bargs/followers","following_url":"https://api.github.com/users/Bargs/following{/other_user}","gists_url":"https://api.github.com/users/Bargs/gists{/gist_id}","starred_url":"https://api.github.com/users/Bargs/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Bargs/subscriptions","organizations_url":"https://api.github.com/users/Bargs/orgs","repos_url":"https://api.github.com/users/Bargs/repos","events_url":"https://api.github.com/users/Bargs/events{/privacy}","received_events_url":"https://api.github.com/users/Bargs/received_events","type":"User","site_admin":false},"created_at":"2016-04-12T15:45:30Z","updated_at":"2016-04-12T15:48:01Z","author_association":"NONE","body":"+1 on chunked uploads. This would be a huge benefit to the CSV upload functionality I'm building into Kibana. Right now I have to make educated guesses about what bulk size will be the best for the largest number of users, and it just won't be a good experience for some people. If ES supported chunked uploads the entire thing could be implemented as one big stream from the user's browser, to Kibana's node backend, to ES and back.\n\n(https://github.com/elastic/kibana/issues/6541 and https://github.com/elastic/kibana/pull/6844)\n","performed_via_github_app":null}]