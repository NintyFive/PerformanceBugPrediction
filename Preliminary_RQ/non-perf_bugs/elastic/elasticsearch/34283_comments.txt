[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/426940727","html_url":"https://github.com/elastic/elasticsearch/issues/34283#issuecomment-426940727","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34283","id":426940727,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNjk0MDcyNw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-10-04T09:01:04Z","updated_at":"2018-10-04T09:01:04Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-search-aggs","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/426940784","html_url":"https://github.com/elastic/elasticsearch/issues/34283#issuecomment-426940784","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34283","id":426940784,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNjk0MDc4NA==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2018-10-04T09:01:13Z","updated_at":"2018-10-04T09:01:13Z","author_association":"MEMBER","body":"@jimczi could you take a look at this one?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/426943792","html_url":"https://github.com/elastic/elasticsearch/issues/34283#issuecomment-426943792","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34283","id":426943792,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNjk0Mzc5Mg==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2018-10-04T09:10:44Z","updated_at":"2018-10-04T09:10:44Z","author_association":"MEMBER","body":"Thanks @Trey314159 for the detailed report ! This is really appreciated. \r\n\r\n> A. Tokens are split on different character POS types (which seem to not quite line up with Unicode character blocks), which leads to weird results for non-CJK tokens:\r\n\r\nThat's the rule we follow for unknown words. Though we don't have a specific rule for combining diacritical marks, they are detected as `SYMBOL` because it's the default class for unknown characters. We could probably mark them as `ALPHA` in order to group them with latin characters but we also separate Cyrillic from latin chars... I like the idea of adding a character filter to remove them since we don't need precision on foreign languages, maybe we could add this filter automatically in the `analyzer` ?\r\n\r\n> Мoscow (with a Cyrillic М and the rest in Latin) is tokenized as м + oscow\r\n\r\nThat's the rule we follow, we distinguish Latin from Cyrillic and split unknown words that mixes alphabets. I don't know Cyrillic at all but I guess that we could get rid of this rule if it's common to mix these characters class in existing words.\r\n\r\n> The character \"arae-a\" (ㆍ, U+318D) is sometimes used instead of a middle dot (·, U+00B7) for lists. When the arae-a is used, everything after the first one ends up in one giant token. 도로ㆍ지반ㆍ수자원ㆍ건설환경ㆍ건축ㆍ화재설비연구 is tokenized as 도로 + ㆍ지반ㆍ수자원ㆍ건설환경ㆍ건축ㆍ화재설비연구.\r\n\r\nI agree this is a bug,  the dictionary we use doesn't contain the \"arae-a\" character as a separator so it is considered as a simple hangul character. The workaround as you noticed is to remap this character before tokenization but I'll fill a bug report for [mecab-ko-dic]([https://bitbucket.org/eunjeon/mecab-ko-dic).\r\n\r\n> C. Nori splits tokens on soft hyphens (U+00AD) and zero-width non-joiners (U+200C), splitting tokens that should not be split.\r\n\r\nI don't see why it's an issue. Soft hyphens indicates a word boundary (or at least a boundary that can be used to split a very long word) so I don't think we should change the default behavior. You can (as you noticed) remap these characters if you don't want to split.\r\n\r\n> D. Analyzing 그레이맨 generates an extra empty token after it. There may be others, but this is the only one I've found. Work around: at a min length token filter with a minimum length of 1.\r\n\r\n> E. Analyzing 튜토리얼 generates a token with an extra space at the end of it. There may be others, but this is the only one I've found. No work around needed, I guess, since this is only the internal representation of the token. I'm not sure if it has any negative effects.\r\n\r\nThat's a bug, thanks ! The `meca-ko-dic` contains some invalid rules that we should ignore or normalize (we should trim tokens when we read them and remove the empty ones).\r\n\r\nThanks again for reporting and testing this analyzer. However the implementation is in a Lucene module so we should continue this discussion there. Would you mind opening a new issue in [Lucene](https://issues.apache.org/jira/projects/LUCENE/issues) with the same description ? I can do it if you don't want to but if we fix something it will be in Lucene.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/427161238","html_url":"https://github.com/elastic/elasticsearch/issues/34283#issuecomment-427161238","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34283","id":427161238,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNzE2MTIzOA==","user":{"login":"Trey314159","id":13836921,"node_id":"MDQ6VXNlcjEzODM2OTIx","avatar_url":"https://avatars0.githubusercontent.com/u/13836921?v=4","gravatar_id":"","url":"https://api.github.com/users/Trey314159","html_url":"https://github.com/Trey314159","followers_url":"https://api.github.com/users/Trey314159/followers","following_url":"https://api.github.com/users/Trey314159/following{/other_user}","gists_url":"https://api.github.com/users/Trey314159/gists{/gist_id}","starred_url":"https://api.github.com/users/Trey314159/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Trey314159/subscriptions","organizations_url":"https://api.github.com/users/Trey314159/orgs","repos_url":"https://api.github.com/users/Trey314159/repos","events_url":"https://api.github.com/users/Trey314159/events{/privacy}","received_events_url":"https://api.github.com/users/Trey314159/received_events","type":"User","site_admin":false},"created_at":"2018-10-04T20:39:05Z","updated_at":"2018-10-04T20:39:05Z","author_association":"NONE","body":"> I like the idea of adding a character filter to remove them since we don't need precision on foreign languages, maybe we could add this filter automatically in the `analyzer` ?\r\n\r\nFor use on Wikipedia, I usually set up filters to remove most combining diacritics and to normalizer characters with diacritics that are not in the alphabet of the language of the wiki. (Swedish really does need å, ä, and ö, but doesn't really need ß and ł.)\r\n\r\nFor a Korean analyzer, normalizing diacritics for non-Korean text sounds good to me.\r\n\r\n> > Мoscow (with a Cyrillic М and the rest in Latin) is tokenized as м + oscow\r\n> \r\n> That's the rule we follow, we distinguish Latin from Cyrillic and split unknown words that mixes alphabets. I don't know Cyrillic at all but I guess that we could get rid of this rule if it's common to mix these characters class in existing words.\r\n\r\nDepends on your definition of common. There are intentional cases like the band _KoЯn_ and _NGiИX_ software. It's also common enough on Wikipedias when people are transliterating or translating text and they don't change the homoglyphs (i.e., they have _Москва,_ hit delete 4 or 5 times, and then keep typing), or they need a less common character (like Cyrillic і, ј, or ѕ, or accented ѐ), and they substitute the handier Latin versions (i, j, s, è). I see it a lot, but I also go looking for them. `;)`\r\n\r\n> > The character \"arae-a\" (ㆍ, U+318D) is sometimes used instead of a middle dot (·, U+00B7) for lists. When the arae-a is used, everything after the first one ends up in one giant token. 도로ㆍ지반ㆍ수자원ㆍ건설환경ㆍ건축ㆍ화재설비연구 is tokenized as 도로 + ㆍ지반ㆍ수자원ㆍ건설환경ㆍ건축ㆍ화재설비연구.\r\n> \r\n> I agree this is a bug, the dictionary we use doesn't contain the \"arae-a\" character as a separator so it is considered as a simple hangul character. The workaround as you noticed is to remap this character before tokenization but I'll fill a bug report for mecab-ko-dic.\r\n\r\nThanks!\r\n\r\n> > C. Nori splits tokens on soft hyphens (U+00AD) and zero-width non-joiners (U+200C), splitting tokens that should not be split.\r\n> \r\n> I don't see why it's an issue. Soft hyphens indicates a word boundary (or at least a boundary that can be used to split a very long word) so I don't think we should change the default behavior. You can (as you noticed) remap these characters if you don't want to split.\r\n\r\n[Soft hyphens](https://en.wikipedia.org/wiki/Soft_hyphen) and [zero-width non-joiners](https://en.wikipedia.org/wiki/Zero-width_non-joiner) (zwnj) are typographic marks that have no semantic meaning. And since they are invisible, people often don't know they are there. As I mentioned in the other ticket, _hyphenation_ and _hyphen­ation_ look the same, but are not. Zwnj's prevent ligatures that shouldn't occur automatically, like across morpheme boundaries. So hardcore English typography nerds might use it to block rendering of the ffl ligature, ﬄ, in words like _offload_ or _cufflink_; they get used in Arabic a lot more, though.\r\n\r\n> > D. Analyzing 그레이맨 generates an extra empty token after it. There may be others, but this is the only one I've found. Work around: at a min length token filter with a minimum length of 1.\r\n> \r\n> > E. Analyzing 튜토리얼 generates a token with an extra space at the end of it. There may be others, but this is the only one I've found. No work around needed, I guess, since this is only the internal representation of the token. I'm not sure if it has any negative effects.\r\n> \r\n> That's a bug, thanks ! The `meca-ko-dic` contains some invalid rules that we should ignore or normalize (we should trim tokens when we read them and remove the empty ones).\r\n>\r\n> Thanks again for reporting and testing this analyzer. However the implementation is in a Lucene module so we should continue this discussion there. Would you mind opening a new issue in [Lucene](https://issues.apache.org/jira/projects/LUCENE/issues) with the same description ? I can do it if you don't want to but if we fix something it will be in Lucene.\r\n\r\nI've refiled [the ticket](https://issues.apache.org/jira/browse/LUCENE-8524) there. Thanks!\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/443657596","html_url":"https://github.com/elastic/elasticsearch/issues/34283#issuecomment-443657596","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34283","id":443657596,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzY1NzU5Ng==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2018-12-03T10:15:14Z","updated_at":"2018-12-03T10:15:14Z","author_association":"MEMBER","body":"The Lucene [issue](https://issues.apache.org/jira/browse/LUCENE-8548) is now resolved so I am closing this one. The Korean tokenizer will no longer split unknown words on combining diacritics and will detect script boundaries more accurately with java UnicodeScript. \r\nHowever we'll not merge different scripts together (cyrillic and latin for instance), this is not in the specifications and would complicate the handling of unknown words with custom rules. The same applies for special punctuations split (e.g. `don't`) that depends on the type of characters that appear before and after the `'`.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/443744783","html_url":"https://github.com/elastic/elasticsearch/issues/34283#issuecomment-443744783","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34283","id":443744783,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0Mzc0NDc4Mw==","user":{"login":"Trey314159","id":13836921,"node_id":"MDQ6VXNlcjEzODM2OTIx","avatar_url":"https://avatars0.githubusercontent.com/u/13836921?v=4","gravatar_id":"","url":"https://api.github.com/users/Trey314159","html_url":"https://github.com/Trey314159","followers_url":"https://api.github.com/users/Trey314159/followers","following_url":"https://api.github.com/users/Trey314159/following{/other_user}","gists_url":"https://api.github.com/users/Trey314159/gists{/gist_id}","starred_url":"https://api.github.com/users/Trey314159/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Trey314159/subscriptions","organizations_url":"https://api.github.com/users/Trey314159/orgs","repos_url":"https://api.github.com/users/Trey314159/repos","events_url":"https://api.github.com/users/Trey314159/events{/privacy}","received_events_url":"https://api.github.com/users/Trey314159/received_events","type":"User","site_admin":false},"created_at":"2018-12-03T15:14:16Z","updated_at":"2018-12-03T15:14:16Z","author_association":"NONE","body":"Thanks, @jimczi!","performed_via_github_app":null}]