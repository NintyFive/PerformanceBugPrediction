[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330152801","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-330152801","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":330152801,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDE1MjgwMQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-09-18T08:07:07Z","updated_at":"2017-09-18T08:07:07Z","author_association":"MEMBER","body":"@colings86, @polyfractal do have any thoughts on that?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330155045","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-330155045","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":330155045,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDE1NTA0NQ==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2017-09-18T08:19:30Z","updated_at":"2017-09-18T08:19:30Z","author_association":"MEMBER","body":"This is an interesting idea and looks like a good candidate for a new aggregation. However we would need to investigate methods for calculating (or estimating) this in a streaming one-pass fashion. The basic way to calculate this involves first iterating over the values to find the median and then iterating over the values again to calculate the deviation of each value from that median. In the aggregation collect phase we only have one pass over the data so we can do either one of the above steps but not both. If there is a method of calculating or estimating this value in a one-pass fashion then we could explore implementing it.\r\n\r\nNote that you could calculate the MAD today by calculating the median of the values in the normal way in a first request and then by using a script in the percentiles aggregation in a second request to subtract each value from the median obtained form the first request. I appreciate this would be nicer to have in a single request though.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330158006","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-330158006","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":330158006,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDE1ODAwNg==","user":{"login":"carlosvega","id":1414389,"node_id":"MDQ6VXNlcjE0MTQzODk=","avatar_url":"https://avatars1.githubusercontent.com/u/1414389?v=4","gravatar_id":"","url":"https://api.github.com/users/carlosvega","html_url":"https://github.com/carlosvega","followers_url":"https://api.github.com/users/carlosvega/followers","following_url":"https://api.github.com/users/carlosvega/following{/other_user}","gists_url":"https://api.github.com/users/carlosvega/gists{/gist_id}","starred_url":"https://api.github.com/users/carlosvega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/carlosvega/subscriptions","organizations_url":"https://api.github.com/users/carlosvega/orgs","repos_url":"https://api.github.com/users/carlosvega/repos","events_url":"https://api.github.com/users/carlosvega/events{/privacy}","received_events_url":"https://api.github.com/users/carlosvega/received_events","type":"User","site_admin":false},"created_at":"2017-09-18T08:34:44Z","updated_at":"2017-09-18T08:35:17Z","author_association":"NONE","body":"There are [some methods](https://hackage.haskell.org/package/tdigest-0.1/docs/Data-TDigest.html) available, with [a bit of literature about it](https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf). It might be some inaccuracies in the result [[1](https://stackoverflow.com/questions/3903538/online-algorithm-for-calculating-absolute-deviation)][[2](https://stats.stackexchange.com/questions/3377/online-algorithm-for-mean-absolute-deviation-and-large-data-set)] but you could do it online.\r\n\r\nHowever, I think, if you first need a value like the median that requires one-pass is not possible to do the MAD with only one pass over the data. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330158114","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-330158114","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":330158114,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDE1ODExNA==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2017-09-18T08:35:18Z","updated_at":"2017-09-18T08:35:18Z","author_association":"MEMBER","body":"One way to make this algorithm streaming might be to use a binning approach to estimate the median based on the median of previous 'bins' of values so you update the median at the end of every bin and use that value for the deviation of values from the median for the next bin (described for mean absolute deviation in https://stats.stackexchange.com/a/3378). I think this relies on an assumption that the median does not change quickly (where quickly in this context is defined as within a bin) so we can assume that the median for the previous bin is a good estimator for the median in the next bin. This means that the size of the bin we choose will be important and should also probably be configurable (at least to start with)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330158680","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-330158680","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":330158680,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDE1ODY4MA==","user":{"login":"carlosvega","id":1414389,"node_id":"MDQ6VXNlcjE0MTQzODk=","avatar_url":"https://avatars1.githubusercontent.com/u/1414389?v=4","gravatar_id":"","url":"https://api.github.com/users/carlosvega","html_url":"https://github.com/carlosvega","followers_url":"https://api.github.com/users/carlosvega/followers","following_url":"https://api.github.com/users/carlosvega/following{/other_user}","gists_url":"https://api.github.com/users/carlosvega/gists{/gist_id}","starred_url":"https://api.github.com/users/carlosvega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/carlosvega/subscriptions","organizations_url":"https://api.github.com/users/carlosvega/orgs","repos_url":"https://api.github.com/users/carlosvega/repos","events_url":"https://api.github.com/users/carlosvega/events{/privacy}","received_events_url":"https://api.github.com/users/carlosvega/received_events","type":"User","site_admin":false},"created_at":"2017-09-18T08:38:09Z","updated_at":"2017-09-18T08:38:09Z","author_association":"NONE","body":"> One way to make this algorithm streaming might be to use a binning approach to estimate the median based on the median of previous 'bins' of values so you update the median at the end of every bin and use that value for the deviation of values from the median for the next bin\r\n\r\nI think this method would be great. Also, the median should not change that quick, at least, if the bin is relatively small. The median would need 50% of their values to be different in order to change from the previous value.\r\n\r\n> This means that the size of the bin we choose will be important and should also probably be configurable (at least to start with)\r\n\r\nCouldn't the size of the bin be the size of, for example, the data histogram bin?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330160753","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-330160753","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":330160753,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDE2MDc1Mw==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2017-09-18T08:48:42Z","updated_at":"2017-09-18T08:48:42Z","author_association":"MEMBER","body":"Actually I think there is also an assumption that the median does not change significantly over the whole dataset, since the estimates for the median at the start of the stream don't know what the median will be at the end of the stream, so if for example the median value was 10 after 1 million values but 10 million after 5 million values the estimates of the deviation of the median would be quite off since the estimate of the median was off by a factor 10 for at least 20% of the data. We will largely collect in insertion order which for a lot of use-cases translates to timestamp order so I think there is a bit of investigation to be done to find an approach that is not going to be too sensitive to median changes over time (or at least find and document all the assumptions) to make this a useful tool in Elasticsearch. Very interesting subject though ðŸ˜„ \r\n\r\nP.S we actually already use t-digest to estimate percentiles (and hence median) in the percentiles aggregation. We would almost certainly use it here too but unfortunately it doesn't help solve the fact that we need the median (or at least an estimate of the median) for the whole dataset to be able to calculate the deviation from that median of each value. This is something the binning technique might solve but its a fairly naive approach so we may need to be a bit more clever with the implementation","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330232988","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-330232988","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":330232988,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDIzMjk4OA==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2017-09-18T14:05:19Z","updated_at":"2017-09-18T14:05:19Z","author_association":"MEMBER","body":"So amusingly enough, I have some old code that implemented MAD in an aggregation.  Trying to track down the branch that had the agg, but the algo was essentially:\r\n\r\n1. Set `approxMedian` equal to the first point `currentValue`\r\n2. For each point:\r\n   1. Add the point's value to a T-Digest sketch (`values`)\r\n   2. Calculate the deviation as `Math.abs(approxMedian - currentValue)`, add to a secondary T-Digest sketch (`deviations`)\r\n3. Every `n` points, recalculate the approximate median by setting `approxMedian = values.quantile(0.5)`\r\n4. When all data has been visited, the MAD is `deviations.quantile(0.5) * 1.4826`\r\n\r\nIt's very similar to what @colings86 suggested with binning.  One T-Digest is used to record the streaming median, and periodically inspected to set the \"approximate global median\".  This approximate global median is used to calculate an the \"approximate deviation\" for every point.  These deviations are then placed into a T-Digest and used to calculate the median of deviations at the end.\r\n\r\nIt worked well enough in my informal testing at the time. As @colings86 noted, it does assume you have \"enough data\" to smooth out the median, that the cold startup period isn't a big influence on the final MAD, and that the median doesn't change too drastically over the lifetime of the stream.\r\n\r\nAt the time, I also contemplated adding some sort of \"replay\" mechanism so that the first `n%` could be replayed through the algo once the median starts to stabilize.   It'd add complexity and runtime, and potentially skew the MAD itself (since you're re-playing values) but might provide a better estimate.  Never tested it though.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330240719","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-330240719","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":330240719,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDI0MDcxOQ==","user":{"login":"carlosvega","id":1414389,"node_id":"MDQ6VXNlcjE0MTQzODk=","avatar_url":"https://avatars1.githubusercontent.com/u/1414389?v=4","gravatar_id":"","url":"https://api.github.com/users/carlosvega","html_url":"https://github.com/carlosvega","followers_url":"https://api.github.com/users/carlosvega/followers","following_url":"https://api.github.com/users/carlosvega/following{/other_user}","gists_url":"https://api.github.com/users/carlosvega/gists{/gist_id}","starred_url":"https://api.github.com/users/carlosvega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/carlosvega/subscriptions","organizations_url":"https://api.github.com/users/carlosvega/orgs","repos_url":"https://api.github.com/users/carlosvega/repos","events_url":"https://api.github.com/users/carlosvega/events{/privacy}","received_events_url":"https://api.github.com/users/carlosvega/received_events","type":"User","site_admin":false},"created_at":"2017-09-18T14:27:59Z","updated_at":"2017-09-18T14:27:59Z","author_association":"NONE","body":"It would be great, being able to have some index stats and use warmers to recalculate them for this kind of stuff. Like, \"each day, recalc some statistics (like the median) over my previous X days of data and store it in order to be used with these aggregations\". That could boost some kind of visualizations and aggregations. Your stuff sounds great. Thanks for the answers.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/331465416","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-331465416","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":331465416,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMTQ2NTQxNg==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2017-09-22T14:38:12Z","updated_at":"2017-09-22T14:38:12Z","author_association":"MEMBER","body":"We discussed this in FixItFriday and decide that we would like to add this as a pipeline aggregation. It would use the T-digest produced by a percentiles aggregator to get the median and then use the centroids of the t-digest as an approximation of the data as an input to a new t-digest for the deviations from the median.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/375379779","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-375379779","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":375379779,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NTM3OTc3OQ==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2018-03-22T16:57:04Z","updated_at":"2018-03-22T16:57:04Z","author_association":"MEMBER","body":"@elastic/es-search-aggs ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/425574297","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-425574297","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":425574297,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNTU3NDI5Nw==","user":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"created_at":"2018-09-28T21:42:34Z","updated_at":"2018-09-28T21:42:34Z","author_association":"CONTRIBUTOR","body":"Some rally benchmarks for the implementations we've looked at so far - these just cover service performance and don't say anything about accuracy. This is using the [noaa dataset track](https://github.com/andyb-elastic/rally-tracks/tree/57e80bcf7f65333d179c7726d1bc276a36ec042b/noaa) with 4 shards and default compression settings\r\n\r\nFor the method Zach described in [this comment](https://github.com/elastic/elasticsearch/issues/26681#issuecomment-330232988), which is calculating an approximate median and accumulating deviation measurements in the collection phase\r\n\r\n* [rally results](https://gist.github.com/andyb-elastic/b09559db8c44e40aba8806815c96a432#file-collection_median-txt)\r\n* code https://github.com/andyb-elastic/elasticsearch/commit/5d11d7f462c945095a8d017e665c02242da28ef5\r\n\r\nFor the same algorithm as the one above, but recalculating the median less frequently after a threshold of documents has been reached\r\n\r\n* [rally results](https://gist.github.com/andyb-elastic/b09559db8c44e40aba8806815c96a432#file-collection_median_faster-txt)\r\n* code https://github.com/andyb-elastic/elasticsearch/commit/a187e003b49c267f8bbffa5fbe0a6cb06020f11a\r\n\r\nThis method was pretty slow, and recalculating the median less frequently seemed anecdotally to decrease accuracy enough that tests failed. It does seem like the median recalculation is a bottleneck, taking jstacks always showed a thread in the tdigest's quantile calculation method.\r\n\r\nFor another method Zach described elsewhere, which is collecting a tdigest sketch of values, then in reduce using the percentile results of that sketch to construct a similar tdigest sketch of deviations from the median\r\n\r\n* [rally results](https://gist.github.com/andyb-elastic/b09559db8c44e40aba8806815c96a432#file-reduce_percentiles-txt)\r\n* code https://github.com/andyb-elastic/elasticsearch/commit/88d5189ad1f922505ac3fb5f17975926bd327563\r\n\r\nFor the method Colin described in [this comment](https://github.com/elastic/elasticsearch/issues/26681#issuecomment-331465416), which is collecting a tdigest sketch of values, and then in reduce using that tdigest's centroids to construct a similar tdigest sketch of deviations from the median\r\n\r\n* [rally results](https://gist.github.com/andyb-elastic/b09559db8c44e40aba8806815c96a432#file-reduce_centroids-txt)\r\n* code https://github.com/andyb-elastic/elasticsearch/commit/bd2420b368c14b28d76e552d57b919c18047ac07\r\n\r\nAnecdotally from testing the centroids version seems more accurate. In the percentiles version I didn't make how many quantiles are calculcated configurable though. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/425574563","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-425574563","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":425574563,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNTU3NDU2Mw==","user":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"created_at":"2018-09-28T21:43:58Z","updated_at":"2018-09-28T21:43:58Z","author_association":"CONTRIBUTOR","body":"I'll have some accuracy benchmarks next. It's looking like the centroid method is likely going to be the one to use","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/427034794","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-427034794","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":427034794,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNzAzNDc5NA==","user":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"created_at":"2018-10-04T14:13:19Z","updated_at":"2018-10-04T14:13:19Z","author_association":"CONTRIBUTOR","body":"Here's some accuracy benchmarks that I've roughly digested by hand (well, with jq)\r\n\r\n### Results\r\n\r\ntl;dr it looks like the centroids method is most accurate. When using the default compression settings it's within 0.01 relative error in all benchmarking scenarios except for sparsely pareto-distributed data. And for that distribution, increasing compression (higher -> more accuracy, more resource usage) brought it under the 0.01 threshold.\r\n\r\n#### Calculating an approximate median and accumulating deviation measurements in the collection phase\r\n\r\n* [Full results](https://gist.github.com/andyb-elastic/fcb28a76ba571b5e465ab9196ddf3300#file-collection-median-json)\r\n* [Scenarios with at least 0.01 error](https://gist.github.com/andyb-elastic/fcb28a76ba571b5e465ab9196ddf3300#file-collection-median-above-0-01-error-json)\r\n* [Scenarios with at least 0.05 error](https://gist.github.com/andyb-elastic/fcb28a76ba571b5e465ab9196ddf3300#file-collection-median-above-0-05-error-json)\r\n* Code https://github.com/andyb-elastic/elasticsearch/commit/24fc9d911e8f803863995487cd732b1e4d1e9a87\r\n\r\nThis method has many scenarios with high relative error across all benchmarking variables. I'm not aware of any modifications to this algorithm that could make it more accurate. It also was by far the slowest during rally benchmarking, even with optimizations that were not included in these speed benchmarks. We can rule this method out.\r\n\r\n#### Collecting a tdigest sketch of values, then in reduce using the percentile results of that sketch to construct a similar tdigest sketch of deviations from the median\r\n\r\n* [Full results](https://gist.github.com/andyb-elastic/fcb28a76ba571b5e465ab9196ddf3300#file-reduce-percentiles-json)\r\n* [Scenarios with at least 0.01 error](https://gist.github.com/andyb-elastic/fcb28a76ba571b5e465ab9196ddf3300#file-reduce-percentiles-above-0-01-error-json)\r\n* [Scenarios with at least 0.05 error](https://gist.github.com/andyb-elastic/fcb28a76ba571b5e465ab9196ddf3300#file-reduce-percentiles-above-0-05-error-json)\r\n* Code https://github.com/andyb-elastic/elasticsearch/commit/ef33863749c64b308d330aee333b07d8743f1df5\r\n\r\nThis method has similar results to the centroids method at the 0.05 relative error threshold, but has more scenarios above the 0.01 threshold. It seems like it deals with small data sizes poorly. This algorithm can be modified by changing the percentile intervals that we use to build the deviation tdigest sketch, which I did not test here. These tests use [thousandth-quantiles](https://github.com/andyb-elastic/elasticsearch/blob/ef33863749c64b308d330aee333b07d8743f1df5/server/src/main/java/org/elasticsearch/search/aggregations/metrics/mad/MADAggregator.java#L153-L158)\r\n\r\n#### Collecting a tdigest sketch of values, and then in reduce using that tdigest's centroids to construct a similar tdigest sketch of deviations from the median\r\n\r\n* [Full results](https://gist.github.com/andyb-elastic/fcb28a76ba571b5e465ab9196ddf3300#file-reduce-centroids-json)\r\n* [Scenarios with at least 0.01 error](https://gist.github.com/andyb-elastic/fcb28a76ba571b5e465ab9196ddf3300#file-reduce-centroids-above-0-01-error-json)\r\n* [Scenarios with at least 0.05 error](https://gist.github.com/andyb-elastic/fcb28a76ba571b5e465ab9196ddf3300#file-reduce-centroids-above-0-05-error-json)\r\n* Code https://github.com/andyb-elastic/elasticsearch/commit/d0e01425f98f355f8b32b015f5dbf91212cb87ff\r\n\r\nThis method's results are summarized at the top of this comment. This method behaves similarly to the percentiles method but with greater accuracy. I think we can safely choose this method at this point, and it looks like our default compression factor choice is good but could be higher if we want to be safe.\r\n\r\n### Methodology\r\n\r\nThe benchmarking driver code is in the [mad-client-test](https://github.com/andyb-elastic/elasticsearch/blob/d0e01425f98f355f8b32b015f5dbf91212cb87ff/mad-client-test/src/main/java/org/elasticsearch/mad_client_test/Main.java) project. It's the same on all three of the commits mentioned in this comment.\r\n\r\nThe variables tested were dataset size = [100, 1000, 10000], tdigest compression factor = [20, 100, 1000] (100 is the default we have set for this agg and the percentiles agg), data distribution = [uniform, normal, pareto], and data density = [dense, sparse]. If you want to run these benchmarks with other values for these variables, you can set system properties `benchmark.sizes`, `benchmark.compressions`, `benchmark.distributions`, and `benchmark.densities` in `gradle run`.\r\n\r\nEach scenario (that is, a combination of these variables) is run through ten iterations, with each iteration creating a new index and data set. Each iteration takes ten measurements of the aggregation on the same dataset, and then records the median of those measurements. That median of measurements is used as the approximate MAD for that iteration. For a scenario to be listed as exceeding the error thresholds mentioned above, at least one of it's iterations' approximate MAD had to be outside of the error threshold.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/427068371","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-427068371","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":427068371,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNzA2ODM3MQ==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2018-10-04T15:44:02Z","updated_at":"2018-10-04T15:44:02Z","author_association":"MEMBER","body":"I'm happy to go with the centroids method given the above results. @polyfractal do you agree?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/434447050","html_url":"https://github.com/elastic/elasticsearch/issues/26681#issuecomment-434447050","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26681","id":434447050,"node_id":"MDEyOklzc3VlQ29tbWVudDQzNDQ0NzA1MA==","user":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"created_at":"2018-10-30T19:57:47Z","updated_at":"2018-10-30T19:57:47Z","author_association":"CONTRIBUTOR","body":"This was implemented in #34482","performed_via_github_app":null}]