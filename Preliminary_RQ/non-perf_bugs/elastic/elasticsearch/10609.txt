{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/10609","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10609/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10609/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10609/events","html_url":"https://github.com/elastic/elasticsearch/issues/10609","id":68637087,"node_id":"MDU6SXNzdWU2ODYzNzA4Nw==","number":10609,"title":"\"Failed to parse filter for alias/no field mapping can be found\" with field declared in _default_ mapping when using _bulk","user":{"login":"nellicus","id":8770097,"node_id":"MDQ6VXNlcjg3NzAwOTc=","avatar_url":"https://avatars2.githubusercontent.com/u/8770097?v=4","gravatar_id":"","url":"https://api.github.com/users/nellicus","html_url":"https://github.com/nellicus","followers_url":"https://api.github.com/users/nellicus/followers","following_url":"https://api.github.com/users/nellicus/following{/other_user}","gists_url":"https://api.github.com/users/nellicus/gists{/gist_id}","starred_url":"https://api.github.com/users/nellicus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nellicus/subscriptions","organizations_url":"https://api.github.com/users/nellicus/orgs","repos_url":"https://api.github.com/users/nellicus/repos","events_url":"https://api.github.com/users/nellicus/events{/privacy}","received_events_url":"https://api.github.com/users/nellicus/received_events","type":"User","site_admin":false},"labels":[{"id":145572580,"node_id":"MDU6TGFiZWwxNDU1NzI1ODA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/CRUD","name":":Distributed/CRUD","color":"0e8a16","default":false,"description":"A catch all label for issues around indexing, updating and getting a doc by id. Not search."},{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null},{"id":92913658,"node_id":"MDU6TGFiZWw5MjkxMzY1OA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/blocker","name":"blocker","color":"e11d21","default":false,"description":null},{"id":197540995,"node_id":"MDU6TGFiZWwxOTc1NDA5OTU=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/v1.5.2","name":"v1.5.2","color":"dddddd","default":false,"description":null}],"state":"closed","locked":false,"assignee":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"assignees":[{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2015-04-15T10:13:21Z","updated_at":"2018-02-13T19:39:46Z","closed_at":"2015-04-24T07:44:10Z","author_association":"MEMBER","active_lock_reason":null,"body":"This is very closely related to scenario described in #10038:\nthere is an index template with aliases using filter on some fields and a new events come in, failing to created the index when this doesn't yet exists (typical use case is logstash)\n\nThe difference here is that field used in index template to define the alias _is_ declared under default mapping, but still , the problem happens if _bulk endpoint is in use. normal single index request works fine.\n\nRepro using ES 1.5.0 - LS 1.4.2 (same behaviour on ES 1.4.1)\n\n```\n\n# user in _default_ mapping , term filter in alias\n\nPUT _template/template_1\n{\n  \"template\": \"test1*\",\n  \"settings\": {\n    \"number_of_shards\": 1\n  },\n  \"mappings\": {\n    \"_default_\": {\n      \"properties\": {\n        \"user\": {\n          \"type\": \"string\"\n        }\n      }\n    }\n  },\n  \"aliases\": {\n    \"filtered-alias1\": {\n      \"filter\": {\n        \"term\": {\n          \"user\": \"john\"\n        }\n      }\n    }\n  }\n}\n```\n\nUsing logstash 1.4.2 or 1.5.0beta1\n\n```\nabonuccelli@w530 /opt/elk/TEST/logstash-1.4.2 $ cat config/template-filter-alias-stdout.cnf \ninput{ \nstdin{}\n}\nfilter{\n}\noutput{\nstdout{\n    codec=>rubydebug\n}\nelasticsearch {\n        host => \"localhost\"\n        protocol => http\n        manage_template => false\n        index => \"test1-%{+YYYY.MM.dd}\"\n    }\n}\n```\n\nstart logstash parse a doc via logstash\n\n```\nabonuccelli@w530 /opt/elk/TEST/logstash-1.4.2 $ ./bin/logstash -f config/template-filter-alias-stdout.cnf --debug \nReading config file {:file=>\"logstash/agent.rb\", :level=>:debug, :line=>\"301\"}\nCompiled pipeline code:\n@inputs = []\n@filters = []\n@outputs = []\n@input_stdin_1 = plugin(\"input\", \"stdin\")\n\n@inputs << @input_stdin_1\n\n@output_stdout_2 = plugin(\"output\", \"stdout\", LogStash::Util.hash_merge_many({ \"codec\" => (\"rubydebug\".force_encoding(\"UTF-8\")) }))\n\n@outputs << @output_stdout_2\n@output_elasticsearch_3 = plugin(\"output\", \"elasticsearch\", LogStash::Util.hash_merge_many({ \"host\" => (\"localhost\".force_encoding(\"UTF-8\")) }, { \"protocol\" => (\"http\".force_encoding(\"UTF-8\")) }, { \"manage_template\" => (\"false\".force_encoding(\"UTF-8\")) }, { \"index\" => (\"test1-%{+YYYY.MM.dd}\".force_encoding(\"UTF-8\")) }))\n\n@outputs << @output_elasticsearch_3\n  @filter_func = lambda do |event, &block|\n    extra_events = []\n    @logger.debug? && @logger.debug(\"filter received\", :event => event.to_hash)\n\n    extra_events.each(&block)\n  end\n  @output_func = lambda do |event, &block|\n    @logger.debug? && @logger.debug(\"output received\", :event => event.to_hash)\n    @output_stdout_2.handle(event)\n    @output_elasticsearch_3.handle(event)\n\n  end {:level=>:debug, :file=>\"logstash/pipeline.rb\", :line=>\"26\"}\nconfig LogStash::Codecs::Line/@charset = \"UTF-8\" {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Inputs::Stdin/@debug = false {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Inputs::Stdin/@codec = <LogStash::Codecs::Line charset=>\"UTF-8\"> {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Inputs::Stdin/@add_field = {} {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::Stdout/@codec = <LogStash::Codecs::RubyDebug > {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::Stdout/@type = \"\" {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::Stdout/@tags = [] {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::Stdout/@exclude_tags = [] {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::Stdout/@workers = 1 {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Codecs::Plain/@charset = \"UTF-8\" {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@host = \"localhost\" {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@protocol = \"http\" {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@manage_template = false {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@index = \"test1-%{+YYYY.MM.dd}\" {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@type = \"\" {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@tags = [] {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@exclude_tags = [] {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@codec = <LogStash::Codecs::Plain charset=>\"UTF-8\"> {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@workers = 1 {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@template_name = \"logstash\" {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@template_overwrite = false {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@document_id = nil {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@embedded = false {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@embedded_http_port = \"9200-9300\" {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@max_inflight_requests = 50 {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@flush_size = 5000 {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@idle_flush_time = 1 {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nconfig LogStash::Outputs::ElasticSearch/@action = \"index\" {:level=>:debug, :file=>\"logstash/config/mixin.rb\", :line=>\"105\"}\nPipeline started {:level=>:info, :file=>\"logstash/pipeline.rb\", :line=>\"78\"}\nNew Elasticsearch output {:cluster=>nil, :host=>\"localhost\", :port=>\"9200\", :embedded=>false, :protocol=>\"http\", :level=>:info, :file=>\"logstash/outputs/elasticsearch.rb\", :line=>\"252\"}\nasdasdasd\noutput received {:event=>{\"message\"=>\"asdasdasd\", \"@version\"=>\"1\", \"@timestamp\"=>\"2015-04-15T09:51:18.096Z\", \"host\"=>\"w530\"}, :level=>:debug, :file=>\"(eval)\", :line=>\"21\"}\n{\n       \"message\" => \"asdasdasd\",\n      \"@version\" => \"1\",\n    \"@timestamp\" => \"2015-04-15T09:51:18.096Z\",\n          \"host\" => \"w530\"\n}\nFlushing output {:outgoing_count=>1, :time_since_last_flush=>2.154, :outgoing_events=>{nil=>[[\"index\", {:_id=>nil, :_index=>\"test1-2015.04.15\", :_type=>\"logs\"}, {\"message\"=>\"asdasdasd\", \"@version\"=>\"1\", \"@timestamp\"=>\"2015-04-15T09:51:18.096Z\", \"host\"=>\"w530\"}]]}, :batch_timeout=>1, :force=>nil, :final=>nil, :level=>:debug, :file=>\"stud/buffer.rb\", :line=>\"207\"}\n```\n\ntcpdump capture of it\n\n```\n11:52:54.717052 IP (tos 0x0, ttl 64, id 57438, offset 0, flags [DF], proto TCP (6), length 138)\n    127.0.0.1.32768 > 127.0.0.1.9200: Flags [P.], cksum 0xfe7e (incorrect -> 0x05af), seq 245:331, ack 537, win 350, options [nop,nop,TS val 167425274 ecr 167401133], length 86\n..........#.Z....g_-...^.~.....\n    ... .V.POST /_bulk HTTP/1.1\nhost: localhost\nconnection: keep-alive\ncontent-length: 159\n\n\n11:52:54.717234 IP (tos 0x0, ttl 64, id 57439, offset 0, flags [DF], proto TCP (6), length 211)\n    127.0.0.1.32768 > 127.0.0.1.9200: Flags [P.], cksum 0xfec7 (incorrect -> 0xb5b8), seq 331:490, ack 537, win 350, options [nop,nop,TS val 167425274 ecr 167401133], length 159\nE...._@.@.[...........#.Z..=.g_-...^.......\n    ... .V.{\"index\":{\"_id\":null,\"_index\":\"test1-2015.04.15\",\"_type\":\"logs\"}}\n{\"message\":\"asdasdasd\",\"@version\":\"1\",\"@timestamp\":\"2015-04-15T09:52:54.697Z\",\"host\":\"w530\"}\n```\n\nelasticsearch 1.5.0 log\n\n```\n\n[2015-04-15 11:52:54,728][DEBUG][action.admin.indices.create] [nodeM1] [test1-2015.04.15] failed to create\norg.elasticsearch.ElasticsearchIllegalArgumentException: failed to parse filter for alias [filtered-alias1]\n    at org.elasticsearch.cluster.metadata.AliasValidator.validateAliasFilter(AliasValidator.java:142)\n    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:413)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:365)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.index.query.QueryParsingException: [test1-2015.04.15] Strict field resolution and no field mapping can be found for the field with name [user]\n    at org.elasticsearch.index.query.QueryParseContext.failIfFieldMappingNotFound(QueryParseContext.java:422)\n    at org.elasticsearch.index.query.QueryParseContext.smartFieldMappers(QueryParseContext.java:397)\n    at org.elasticsearch.index.query.TermFilterParser.parse(TermFilterParser.java:111)\n    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:368)\n    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:349)\n    at org.elasticsearch.cluster.metadata.AliasValidator.validateAliasFilter(AliasValidator.java:151)\n    at org.elasticsearch.cluster.metadata.AliasValidator.validateAliasFilter(AliasValidator.java:140)\n    ... 7 more\n```\n\ndoing the same via curl - same problem\n\n```\nAntonios-MacBook-Air-2:~ abonuccelli$ curl -XPUT w530:9200/_bulk -d '{\"index\":{\"_id\":null,\"_index\":\"test1-2015.04.15\",\"_type\":\"logs\"}}\n{\"message\":\"asdasdasd\",\"@version\":\"1\",\"@timestamp\":\"2015-04-15T09:52:54.697Z\",\"host\":\"w530\"}\n'\n{\"took\":22,\"errors\":true,\"items\":[{\"index\":{\"_index\":\"test1-2015.04.15\",\"_type\":\"logs\",\"_id\":null,\"status\":400,\"error\":\"RemoteTransportException[[nodeM1][inet[/192.168.0.101:9304]][indices:admin/create]]; nested: ElasticsearchIllegalArgumentException[failed to parse filter for alias [filtered-alias1]]; nested: QueryParsingException[[test1-2015.04.15] Strict field resolution and no field mapping can be found for the field with name [user]]; \"}}]\n```\n\nif using normal single index request\n\n```\nAntonios-MacBook-Air-2:~ abonuccelli$ curl -XPUT w530:9200/test1-2015.04.15/type/1 -d '\n{\"message\":\"asdasdasd\",\"@version\":\"1\",\"@timestamp\":\"2015-04-15T09:52:54.697Z\",\"host\":\"w530\"}'\n```\n\nthere is no problem\n\n```\n[2015-04-15 12:07:34,356][DEBUG][cluster.service          ] [nodeM1] processing [create-index [test1-2015.04.15], cause [auto(index api)]]: execute\n[2015-04-15 12:07:34,357][DEBUG][indices                  ] [nodeM1] creating Index [test1-2015.04.15], shards [1]/[2]\n[2015-04-15 12:07:34,373][DEBUG][index.mapper             ] [nodeM1] [test1-2015.04.15] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[jar:file:/opt/elk/PROD/nodeM1/elasticsearch-1.5.0/lib/elasticsearch-1.5.0.jar!/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null]\n[2015-04-15 12:07:34,373][DEBUG][index.cache.query.parser.resident] [nodeM1] [test1-2015.04.15] using [resident] query cache with max_size [100], expire [null]\n[2015-04-15 12:07:34,374][DEBUG][index.store.fs           ] [nodeM1] [test1-2015.04.15] using index.store.throttle.type [node], with index.store.throttle.max_bytes_per_sec [0b]\n[2015-04-15 12:07:34,377][INFO ][cluster.metadata         ] [nodeM1] [test1-2015.04.15] creating index, cause [auto(index api)], templates [template_1], shards [1]/[2], mappings [_default_, type]\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing ... (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index service (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index cache (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,382][DEBUG][index.cache.filter.weighted] [nodeM1] [test1-2015.04.15] full cache clear, reason [close]\n[2015-04-15 12:07:34,382][DEBUG][index.cache.fixedbitset  ] [nodeM1] [test1-2015.04.15] clearing all bitsets because [close]\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] clearing index field data (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing analysis service (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index engine (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index gateway (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing mapper service (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index query parser service (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index service (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closed... (reason [cleaning up after validating index on master])\n[2015-04-15 12:07:34,383][DEBUG][cluster.service          ] [nodeM1] cluster state updated, version [183], source [create-index [test1-2015.04.15], cause [auto(index api)]]\n[2015-04-15 12:07:34,387][DEBUG][indices.cluster          ] [nodeD2] [test1-2015.04.15] creating index\n[2015-04-15 12:07:34,387][DEBUG][indices                  ] [nodeD2] creating Index [test1-2015.04.15], shards [1]/[2]\n[2015-04-15 12:07:34,398][DEBUG][index.mapper             ] [nodeD2] [test1-2015.04.15] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[jar:file:/opt/elk/PROD/nodeD2/elasticsearch-1.5.0/lib/elasticsearch-1.5.0.jar!/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null]\n[2015-04-15 12:07:34,398][DEBUG][index.cache.query.parser.resident] [nodeD2] [test1-2015.04.15] using [resident] query cache with max_size [100], expire [null]\n[2015-04-15 12:07:34,399][DEBUG][index.store.fs           ] [nodeD2] [test1-2015.04.15] using index.store.throttle.type [node], with index.store.throttle.max_bytes_per_sec [0b]\n[2015-04-15 12:07:34,399][DEBUG][indices.cluster          ] [nodeD2] [test1-2015.04.15] adding mapping [_default_], source [{\"_default_\":{\"properties\":{\"user\":{\"type\":\"string\"}}}}]\n[2015-04-15 12:07:34,399][DEBUG][indices.cluster          ] [nodeD2] [test1-2015.04.15] adding mapping [type], source [{\"type\":{\"properties\":{\"user\":{\"type\":\"string\"}}}}]\n[2015-04-15 12:07:34,400][DEBUG][indices.cluster          ] [nodeD2] [test1-2015.04.15] adding alias [filtered-alias1], filter [{\"term\":{\"user\":\"john\"}}]\n[2015-04-15 12:07:34,400][DEBUG][indices.cluster          ] [nodeD2] [test1-2015.04.15][0] creating shard\n[2015-04-15 12:07:34,400][DEBUG][index                    ] [nodeD2] [test1-2015.04.15] creating shard_id [test1-2015.04.15][0]\n[2015-04-15 12:07:34,402][DEBUG][index.store.fs           ] [nodeD2] [test1-2015.04.15] using [/opt/elk/PROD/FS/data/nodeD2/tony_prod/nodes/0/indices/test1-2015.04.15/0/index] as shard's index location\n[2015-04-15 12:07:34,402][DEBUG][index.store              ] [nodeD2] [test1-2015.04.15][0] store stats are refreshed with refresh_interval [10s]\n[2015-04-15 12:07:34,402][DEBUG][index.merge.scheduler    ] [nodeD2] [test1-2015.04.15][0] using [concurrent] merge scheduler with max_thread_count[3], max_merge_count[5]\n[2015-04-15 12:07:34,402][DEBUG][index.store.fs           ] [nodeD2] [test1-2015.04.15] using [/opt/elk/PROD/FS/data/nodeD2/tony_prod/nodes/0/indices/test1-2015.04.15/0/translog] as shard's translog location\n[2015-04-15 12:07:34,403][DEBUG][index.deletionpolicy     ] [nodeD2] [test1-2015.04.15][0] Using [keep_only_last] deletion policy\n[2015-04-15 12:07:34,403][DEBUG][index.merge.policy       ] [nodeD2] [test1-2015.04.15][0] using [tiered] merge mergePolicy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0]\n[2015-04-15 12:07:34,403][DEBUG][index.shard              ] [nodeD2] [test1-2015.04.15][0] state: [CREATED]\n[2015-04-15 12:07:34,403][DEBUG][index.translog           ] [nodeD2] [test1-2015.04.15][0] interval [5s], flush_threshold_ops [2147483647], flush_threshold_size [512mb], flush_threshold_period [30m]\n[2015-04-15 12:07:34,403][DEBUG][index.shard              ] [nodeD2] [test1-2015.04.15][0] state: [CREATED]->[RECOVERING], reason [from gateway]\n[2015-04-15 12:07:34,403][DEBUG][index.gateway            ] [nodeD2] [test1-2015.04.15][0] starting recovery from local ...\n[2015-04-15 12:07:34,403][DEBUG][index.engine             ] [nodeD2] [test1-2015.04.15][0] no 3.x segments needed upgrading\n[2015-04-15 12:07:34,424][DEBUG][cluster.service          ] [nodeM1] processing [create-index [test1-2015.04.15], cause [auto(index api)]]: done applying updated cluster_state (version: 183)\n[2015-04-15 12:07:34,432][DEBUG][index.shard              ] [nodeD2] [test1-2015.04.15][0] scheduling refresher every 1s\n[2015-04-15 12:07:34,432][DEBUG][index.shard              ] [nodeD2] [test1-2015.04.15][0] scheduling optimizer / merger every 1s\n[2015-04-15 12:07:34,432][DEBUG][index.shard              ] [nodeD2] [test1-2015.04.15][0] state: [RECOVERING]->[POST_RECOVERY], reason [post recovery from gateway, no translog]\n[2015-04-15 12:07:34,432][DEBUG][index.gateway            ] [nodeD2] [test1-2015.04.15][0] recovery completed from [local], took [29ms]\n[2015-04-15 12:07:34,432][DEBUG][cluster.action.shard     ] [nodeD2] sending shard started for [test1-2015.04.15][0], node[ci5IBs99RxmpMNOBvRWAHQ], [P], s[INITIALIZING], indexUUID [Lyn-pvs-QRS_InyMh9bi6w], reason [after recovery from gateway]\n[2015-04-15 12:07:34,432][DEBUG][cluster.action.shard     ] [nodeM1] received shard started for [test1-2015.04.15][0], node[ci5IBs99RxmpMNOBvRWAHQ], [P], s[INITIALIZING], indexUUID [Lyn-pvs-QRS_InyMh9bi6w], reason [after recovery from gateway]\n[2015-04-15 12:07:34,432][DEBUG][cluster.service          ] [nodeM1] processing [shard-started ([test1-2015.04.15][0], node[ci5IBs99RxmpMNOBvRWAHQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: execute\n[2015-04-15 12:07:34,432][DEBUG][cluster.action.shard     ] [nodeM1] [test1-2015.04.15][0] will apply shard started [test1-2015.04.15][0], node[ci5IBs99RxmpMNOBvRWAHQ], [P], s[INITIALIZING], indexUUID [Lyn-pvs-QRS_InyMh9bi6w], reason [after recovery from gateway]\n[2015-04-15 12:07:34,433][DEBUG][indices.store            ] [nodeD2] [test1-2015.04.15][0] loaded store meta data (took [0s])\n[2015-04-15 12:07:34,440][DEBUG][cluster.service          ] [nodeM1] cluster state updated, version [184], source [shard-started ([test1-2015.04.15][0], node[ci5IBs99RxmpMNOBvRWAHQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]\n\n```\n","closed_by":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"performed_via_github_app":null}