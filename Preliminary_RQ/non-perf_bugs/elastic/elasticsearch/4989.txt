{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/4989","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4989/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4989/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4989/events","html_url":"https://github.com/elastic/elasticsearch/issues/4989","id":26762868,"node_id":"MDU6SXNzdWUyNjc2Mjg2OA==","number":4989,"title":"Multicast discovery is broken when network.host is set an IPv6 address","user":{"login":"pieterlexis","id":731232,"node_id":"MDQ6VXNlcjczMTIzMg==","avatar_url":"https://avatars2.githubusercontent.com/u/731232?v=4","gravatar_id":"","url":"https://api.github.com/users/pieterlexis","html_url":"https://github.com/pieterlexis","followers_url":"https://api.github.com/users/pieterlexis/followers","following_url":"https://api.github.com/users/pieterlexis/following{/other_user}","gists_url":"https://api.github.com/users/pieterlexis/gists{/gist_id}","starred_url":"https://api.github.com/users/pieterlexis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pieterlexis/subscriptions","organizations_url":"https://api.github.com/users/pieterlexis/orgs","repos_url":"https://api.github.com/users/pieterlexis/repos","events_url":"https://api.github.com/users/pieterlexis/events{/privacy}","received_events_url":"https://api.github.com/users/pieterlexis/received_events","type":"User","site_admin":false},"labels":[{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2014-02-02T15:28:05Z","updated_at":"2015-08-18T19:36:41Z","closed_at":"2015-08-18T19:36:41Z","author_association":"NONE","active_lock_reason":null,"body":"At FOSDEM, I was fiddling around with ES 1.0RC1 on my laptop. When you bind explicitly to an IPv6 address, multicast discovery is broken.\n\nWhen I start the instances, I get the following logs:\n\n```\nlieter $ ./bin/elasticsearch\n[2014-02-02 16:19:18,501][INFO ][node                     ] [Lorvex] version[1.0.0.RC1], pid[4386], build[c6155c5/2014-01-15T17:02:32Z]\n[2014-02-02 16:19:18,506][INFO ][node                     ] [Lorvex] initializing ...\n[2014-02-02 16:19:18,507][DEBUG][node                     ] [Lorvex] using home [/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1], config [/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/config], data [[/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/data]], logs [/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/logs], work [/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/work], plugins [/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/plugins]\n[2014-02-02 16:19:18,523][INFO ][plugins                  ] [Lorvex] loaded [], sites []\n[2014-02-02 16:19:18,567][DEBUG][common.compress.lzf      ] using [UnsafeChunkDecoder] decoder\n[2014-02-02 16:19:18,592][DEBUG][env                      ] [Lorvex] using node location [[/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/data/elasticsearch/nodes/0]], local_node_id [0]\n[2014-02-02 16:19:21,168][DEBUG][threadpool               ] [Lorvex] creating thread_pool [generic], type [cached], keep_alive [30s]\n[2014-02-02 16:19:21,190][DEBUG][threadpool               ] [Lorvex] creating thread_pool [index], type [fixed], size [2], queue_size [200]\n[2014-02-02 16:19:21,197][DEBUG][threadpool               ] [Lorvex] creating thread_pool [bulk], type [fixed], size [2], queue_size [50]\n[2014-02-02 16:19:21,198][DEBUG][threadpool               ] [Lorvex] creating thread_pool [get], type [fixed], size [2], queue_size [1k]\n[2014-02-02 16:19:21,198][DEBUG][threadpool               ] [Lorvex] creating thread_pool [search], type [fixed], size [6], queue_size [1k]\n[2014-02-02 16:19:21,198][DEBUG][threadpool               ] [Lorvex] creating thread_pool [suggest], type [fixed], size [2], queue_size [1k]\n[2014-02-02 16:19:21,199][DEBUG][threadpool               ] [Lorvex] creating thread_pool [percolate], type [fixed], size [2], queue_size [1k]\n[2014-02-02 16:19:21,199][DEBUG][threadpool               ] [Lorvex] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]\n[2014-02-02 16:19:21,201][DEBUG][threadpool               ] [Lorvex] creating thread_pool [flush], type [scaling], min [1], size [1], keep_alive [5m]\n[2014-02-02 16:19:21,204][DEBUG][threadpool               ] [Lorvex] creating thread_pool [merge], type [scaling], min [1], size [1], keep_alive [5m]\n[2014-02-02 16:19:21,204][DEBUG][threadpool               ] [Lorvex] creating thread_pool [refresh], type [scaling], min [1], size [1], keep_alive [5m]\n[2014-02-02 16:19:21,205][DEBUG][threadpool               ] [Lorvex] creating thread_pool [warmer], type [scaling], min [1], size [1], keep_alive [5m]\n[2014-02-02 16:19:21,205][DEBUG][threadpool               ] [Lorvex] creating thread_pool [snapshot], type [scaling], min [1], size [1], keep_alive [5m]\n[2014-02-02 16:19:21,205][DEBUG][threadpool               ] [Lorvex] creating thread_pool [optimize], type [fixed], size [1], queue_size [null]\n[2014-02-02 16:19:21,267][DEBUG][transport.netty          ] [Lorvex] using worker_count[4], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/3/6/1/1], receive_predictor[512kb->512kb]\n[2014-02-02 16:19:21,290][DEBUG][discovery.zen.ping.multicast] [Lorvex] using group [224.2.2.4], with port [54328], ttl [3], and address [null]\n[2014-02-02 16:19:21,299][DEBUG][discovery.zen.ping.unicast] [Lorvex] using initial hosts [], with concurrent_connects [10]\n[2014-02-02 16:19:21,301][DEBUG][discovery.zen            ] [Lorvex] using ping.timeout [3s], master_election.filter_client [true], master_election.filter_data [false]\n[2014-02-02 16:19:21,310][DEBUG][discovery.zen.elect      ] [Lorvex] using minimum_master_nodes [-1]\n[2014-02-02 16:19:21,315][DEBUG][discovery.zen.fd         ] [Lorvex] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\n[2014-02-02 16:19:21,343][DEBUG][discovery.zen.fd         ] [Lorvex] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\n[2014-02-02 16:19:21,465][DEBUG][monitor.jvm              ] [Lorvex] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}}]\n[2014-02-02 16:19:21,992][DEBUG][monitor.os               ] [Lorvex] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@1d3302a5] with refresh_interval [1s]\n[2014-02-02 16:19:22,003][DEBUG][monitor.process          ] [Lorvex] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@7dc2445f] with refresh_interval [1s]\n[2014-02-02 16:19:22,013][DEBUG][monitor.jvm              ] [Lorvex] Using refresh_interval [1s]\n[2014-02-02 16:19:22,021][DEBUG][monitor.network          ] [Lorvex] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@8a97164] with refresh_interval [5s]\n[2014-02-02 16:19:22,041][DEBUG][monitor.network          ] [Lorvex] net_info\nhost [lieters-klaptop]\nwlan0   display_name [wlan0]\n        address [/2001:67c:1810:f051:222:fbff:fec9:1314%3] [/fe80:0:0:0:222:fbff:fec9:1314%3] \n        mtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]\nlo  display_name [lo]\n        address [/0:0:0:0:0:0:0:1%1] [/127.0.0.1] \n        mtu [65536] multicast [false] ptp [false] loopback [true] up [true] virtual [false]\n\n[2014-02-02 16:19:22,075][DEBUG][monitor.fs               ] [Lorvex] Using probe [org.elasticsearch.monitor.fs.SigarFsProbe@16c0e304] with refresh_interval [1s]\n[2014-02-02 16:19:22,946][DEBUG][indices.store            ] [Lorvex] using indices.store.throttle.type [MERGE], with index.store.throttle.max_bytes_per_sec [20mb]\n[2014-02-02 16:19:22,969][DEBUG][cache.memory             ] [Lorvex] using bytebuffer cache with small_buffer_size [1kb], large_buffer_size [1mb], small_cache_size [10mb], large_cache_size [500mb], direct [true]\n[2014-02-02 16:19:23,005][DEBUG][script                   ] [Lorvex] using script cache with max_size [500], expire [null]\n[2014-02-02 16:19:23,026][DEBUG][cluster.routing.allocation.decider] [Lorvex] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]\n[2014-02-02 16:19:23,027][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]\n[2014-02-02 16:19:23,035][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster_concurrent_rebalance] with [2]\n[2014-02-02 16:19:23,047][DEBUG][gateway.local            ] [Lorvex] using initial_shards [quorum], list_timeout [30s]\n[2014-02-02 16:19:23,353][DEBUG][indices.recovery         ] [Lorvex] using max_bytes_per_sec[20mb], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]\n[2014-02-02 16:19:23,420][DEBUG][http.netty               ] [Lorvex] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb], receive_predictor[512kb->512kb]\n[2014-02-02 16:19:23,437][DEBUG][indices.memory           ] [Lorvex] using index_buffer_size [100.7mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]\n[2014-02-02 16:19:23,444][DEBUG][indices.cache.filter     ] [Lorvex] using [node] weighted filter cache with size [20%], actual_size [201.4mb], expire [null], clean_interval [1m]\n[2014-02-02 16:19:23,453][DEBUG][indices.fielddata.cache  ] [Lorvex] using size [-1] [-1b], expire [null]\n[2014-02-02 16:19:23,492][DEBUG][gateway.local.state.meta ] [Lorvex] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]\n[2014-02-02 16:19:23,512][DEBUG][gateway.local.state.meta ] [Lorvex] took 19ms to load state\n[2014-02-02 16:19:23,513][DEBUG][gateway.local.state.shards] [Lorvex] took 0s to load started shards state\n[2014-02-02 16:19:23,519][DEBUG][bulk.udp                 ] [Lorvex] using enabled [false], host [null], port [9700-9800], bulk_actions [1000], bulk_size [5mb], flush_interval [5s], concurrent_requests [4]\n[2014-02-02 16:19:23,539][DEBUG][cluster.routing.allocation.decider] [Lorvex] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]\n[2014-02-02 16:19:23,542][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]\n[2014-02-02 16:19:23,542][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster_concurrent_rebalance] with [2]\n[2014-02-02 16:19:23,544][DEBUG][cluster.routing.allocation.decider] [Lorvex] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]\n[2014-02-02 16:19:23,544][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]\n[2014-02-02 16:19:23,545][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster_concurrent_rebalance] with [2]\n[2014-02-02 16:19:23,578][INFO ][node                     ] [Lorvex] initialized\n[2014-02-02 16:19:23,579][INFO ][node                     ] [Lorvex] starting ...\n[2014-02-02 16:19:23,612][DEBUG][netty.channel.socket.nio.SelectorUtil] Using select timeout of 500\n[2014-02-02 16:19:23,612][DEBUG][netty.channel.socket.nio.SelectorUtil] Epoll-bug workaround enabled = false\n[2014-02-02 16:19:23,853][DEBUG][transport.netty          ] [Lorvex] Bound to address [/2001:67c:1810:f051:222:fbff:fec9:1314:9300]\n[2014-02-02 16:19:23,854][INFO ][transport                ] [Lorvex] bound_address {inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9300]}, publish_address {inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9300]}\n[2014-02-02 16:19:23,932][DEBUG][discovery.zen.ping.multicast] [Lorvex] failed to send multicast ping request\njava.io.IOException: Network is unreachable\n    at java.net.PlainDatagramSocketImpl.send(Native Method)\n    at java.net.DatagramSocket.send(DatagramSocket.java:676)\n    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.sendPingRequest(MulticastZenPing.java:274)\n    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.ping(MulticastZenPing.java:237)\n    at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:149)\n    at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:127)\n    at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:677)\n    at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:297)\n    at org.elasticsearch.discovery.zen.ZenDiscovery.access$500(ZenDiscovery.java:75)\n    at org.elasticsearch.discovery.zen.ZenDiscovery$1.run(ZenDiscovery.java:282)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n[2014-02-02 16:19:25,446][DEBUG][discovery.zen.ping.multicast] [Lorvex] failed to send multicast ping request\njava.io.IOException: Network is unreachable\n    at java.net.PlainDatagramSocketImpl.send(Native Method)\n    at java.net.DatagramSocket.send(DatagramSocket.java:676)\n    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.sendPingRequest(MulticastZenPing.java:274)\n    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.access$100(MulticastZenPing.java:61)\n    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$3.run(MulticastZenPing.java:244)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n[2014-02-02 16:19:26,958][DEBUG][discovery.zen            ] [Lorvex] filtered ping responses: (filter_client[true], filter_data[false]) {none}\n[2014-02-02 16:19:26,970][DEBUG][cluster.service          ] [Lorvex] processing [zen-disco-join (elected_as_master)]: execute\n[2014-02-02 16:19:26,972][DEBUG][cluster.service          ] [Lorvex] cluster state updated, version [1], source [zen-disco-join (elected_as_master)]\n[2014-02-02 16:19:26,974][INFO ][cluster.service          ] [Lorvex] new_master [Lorvex][m6rOwyS2QKeIhpQNLptJcA][lieters-klaptop][inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9300]], reason: zen-disco-join (elected_as_master)\n[2014-02-02 16:19:27,043][DEBUG][transport.netty          ] [Lorvex] connected to node [[Lorvex][m6rOwyS2QKeIhpQNLptJcA][lieters-klaptop][inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9300]]]\n[2014-02-02 16:19:27,044][DEBUG][cluster.service          ] [Lorvex] publishing cluster state version 1\n[2014-02-02 16:19:27,044][DEBUG][cluster.service          ] [Lorvex] set local cluster state to version 1\n[2014-02-02 16:19:27,047][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: execute\n[2014-02-02 16:19:27,047][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: no change in cluster_state\n[2014-02-02 16:19:27,048][DEBUG][cluster.service          ] [Lorvex] processing [zen-disco-join (elected_as_master)]: done applying updated cluster_state (version: 1)\n[2014-02-02 16:19:27,048][INFO ][discovery                ] [Lorvex] elasticsearch/m6rOwyS2QKeIhpQNLptJcA\n[2014-02-02 16:19:27,066][DEBUG][cluster.service          ] [Lorvex] processing [local-gateway-elected-state]: execute\n[2014-02-02 16:19:27,078][DEBUG][cluster.service          ] [Lorvex] cluster state updated, version [2], source [local-gateway-elected-state]\n[2014-02-02 16:19:27,078][DEBUG][cluster.service          ] [Lorvex] publishing cluster state version 2\n[2014-02-02 16:19:27,079][INFO ][http                     ] [Lorvex] bound_address {inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9200]}, publish_address {inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9200]}\n[2014-02-02 16:19:27,083][DEBUG][cluster.service          ] [Lorvex] set local cluster state to version 2\n[2014-02-02 16:19:27,090][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: execute\n[2014-02-02 16:19:27,091][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: no change in cluster_state\n[2014-02-02 16:19:27,102][INFO ][gateway                  ] [Lorvex] recovered [0] indices into cluster_state\n[2014-02-02 16:19:27,103][DEBUG][cluster.service          ] [Lorvex] processing [local-gateway-elected-state]: done applying updated cluster_state (version: 2)\n[2014-02-02 16:19:27,103][DEBUG][cluster.service          ] [Lorvex] processing [updating local node id]: execute\n[2014-02-02 16:19:27,103][DEBUG][cluster.service          ] [Lorvex] cluster state updated, version [3], source [updating local node id]\n[2014-02-02 16:19:27,103][DEBUG][cluster.service          ] [Lorvex] publishing cluster state version 3\n[2014-02-02 16:19:27,103][DEBUG][cluster.service          ] [Lorvex] set local cluster state to version 3\n[2014-02-02 16:19:27,104][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: execute\n[2014-02-02 16:19:27,104][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: no change in cluster_state\n[2014-02-02 16:19:27,104][DEBUG][cluster.service          ] [Lorvex] processing [updating local node id]: done applying updated cluster_state (version: 3)\n[2014-02-02 16:19:27,104][INFO ][node                     ] [Lorvex] started\n[2014-02-02 16:19:37,047][DEBUG][cluster.service          ] [Lorvex] processing [routing-table-updater]: execute\n[2014-02-02 16:19:37,048][DEBUG][cluster.service          ] [Lorvex] processing [routing-table-updater]: no change in cluster_state\n^C[2014-02-02 16:19:43,925][INFO ][node                     ] [Lorvex] stopping ...\n[2014-02-02 16:19:43,961][INFO ][node                     ] [Lorvex] stopped\n[2014-02-02 16:19:43,961][INFO ][node                     ] [Lorvex] closing ...\n[2014-02-02 16:19:43,971][INFO ][node                     ] [Lorvex] closed\n```\n\nThe configuration is as follows:\n\n```\nlieter $ egrep -v '^#|^$' config/elasticsearch.yml\nnetwork.host: 2001:67c:1810:f051:222:fbff:fec9:1314\n```\n\nBut it also fails if you set `network.host` to `::1`.\n","closed_by":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"performed_via_github_app":null}