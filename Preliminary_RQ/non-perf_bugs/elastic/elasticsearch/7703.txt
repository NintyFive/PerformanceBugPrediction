{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/7703","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7703/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7703/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7703/events","html_url":"https://github.com/elastic/elasticsearch/issues/7703","id":42613640,"node_id":"MDU6SXNzdWU0MjYxMzY0MA==","number":7703,"title":"Running metrics on buckets by doc_count","user":{"login":"jmacmahon","id":1281068,"node_id":"MDQ6VXNlcjEyODEwNjg=","avatar_url":"https://avatars0.githubusercontent.com/u/1281068?v=4","gravatar_id":"","url":"https://api.github.com/users/jmacmahon","html_url":"https://github.com/jmacmahon","followers_url":"https://api.github.com/users/jmacmahon/followers","following_url":"https://api.github.com/users/jmacmahon/following{/other_user}","gists_url":"https://api.github.com/users/jmacmahon/gists{/gist_id}","starred_url":"https://api.github.com/users/jmacmahon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jmacmahon/subscriptions","organizations_url":"https://api.github.com/users/jmacmahon/orgs","repos_url":"https://api.github.com/users/jmacmahon/repos","events_url":"https://api.github.com/users/jmacmahon/events{/privacy}","received_events_url":"https://api.github.com/users/jmacmahon/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2014-09-12T09:58:31Z","updated_at":"2014-09-24T18:00:34Z","closed_at":"2014-09-24T18:00:34Z","author_association":"NONE","active_lock_reason":null,"body":"It would be really useful to run metrics on the doc_count field of buckets returned by aggregations, rather than just on fields of the documents in the buckets themselves.\n\nMy particular use-case is that I have an index of timestamped log entries, and I want to find the distrubution of log rate per unit time.  In particular I might want to find the 95th-percentile logs per second.  It's easy enough to partition the data into second-size buckets using date_histogram, and then get a doc_count for each second in the month, but the only way to split these into percentiles is to sort by doc_count and then download the data to the client and do the percentile calculations there.\n\nMaybe I'm missing some functionality and there is a way to do this, but this forum post in May 2014 says that it's impossible: http://elasticsearch-users.115913.n3.nabble.com/stats-extended-stats-percentiles-for-doc-count-in-aggregations-td4055201.html\n\nSorry if this has been discussed and ruled out.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}