[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/628069480","html_url":"https://github.com/elastic/elasticsearch/issues/56676#issuecomment-628069480","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56676","id":628069480,"node_id":"MDEyOklzc3VlQ29tbWVudDYyODA2OTQ4MA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2020-05-13T15:32:12Z","updated_at":"2020-05-13T15:32:12Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-search (:Search/Analysis)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/628071496","html_url":"https://github.com/elastic/elasticsearch/issues/56676#issuecomment-628071496","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56676","id":628071496,"node_id":"MDEyOklzc3VlQ29tbWVudDYyODA3MTQ5Ng==","user":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"created_at":"2020-05-13T15:35:12Z","updated_at":"2020-05-13T15:35:12Z","author_association":"MEMBER","body":"Hi @ADBalici, thanks for opening this. Since most other tokenizers have a configurable `max_token_length` setting, I think it makes sense to allow it here as well. Out of curiosity though: whats the use case for having such long tokens? Having a good overview of these helps thinking about reasonable defaults and limits in the future.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/628097175","html_url":"https://github.com/elastic/elasticsearch/issues/56676#issuecomment-628097175","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56676","id":628097175,"node_id":"MDEyOklzc3VlQ29tbWVudDYyODA5NzE3NQ==","user":{"login":"ADBalici","id":9438684,"node_id":"MDQ6VXNlcjk0Mzg2ODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/9438684?v=4","gravatar_id":"","url":"https://api.github.com/users/ADBalici","html_url":"https://github.com/ADBalici","followers_url":"https://api.github.com/users/ADBalici/followers","following_url":"https://api.github.com/users/ADBalici/following{/other_user}","gists_url":"https://api.github.com/users/ADBalici/gists{/gist_id}","starred_url":"https://api.github.com/users/ADBalici/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ADBalici/subscriptions","organizations_url":"https://api.github.com/users/ADBalici/orgs","repos_url":"https://api.github.com/users/ADBalici/repos","events_url":"https://api.github.com/users/ADBalici/events{/privacy}","received_events_url":"https://api.github.com/users/ADBalici/received_events","type":"User","site_admin":false},"created_at":"2020-05-13T16:18:57Z","updated_at":"2020-05-13T16:18:57Z","author_association":"CONTRIBUTOR","body":"Hey @cbuescher! Thanks for replying!\r\n\r\nMe and my team are working in a bioinformatics company. The use case is to augment the text in scientific publications with specific units of knowledge, to allow users to retrieve relevant documents.\r\n\r\nOne such KU (knowledge unit) is a biomedical interaction (say between a protein and a chemical).\r\nThese interactions are extracted offline and they have a textual representation that needs to be indexed i.e.:\r\n\r\n```\r\n# represents the fact that chemical with id_1 interacts in a specific way with chemical with id_2\r\nregulates(chemical(id_1), chemical(id_2))\r\n```\r\nWe would like our users to search for relevant documents containing this sort of interactions.\r\n\r\nAs it is in the biomedical industry, some interactions can get quite complex and their textual representation a bit long.\r\n\r\nWe have developed our own plugin to help us index what we need. We are simply supplying all KUs separated by a specific char group, and we have just found out that the tokenizer is truncating the tokens.\r\n\r\nWe have solved the problem by creating our own CharGroupTokenizerFactory alternative that allows us to control the length, but it would still be valuable to have this option out there for other people.\r\n\r\nOur team can even work at this feature if it's accepted.\r\n\r\nHope this helps!\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/628103503","html_url":"https://github.com/elastic/elasticsearch/issues/56676#issuecomment-628103503","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56676","id":628103503,"node_id":"MDEyOklzc3VlQ29tbWVudDYyODEwMzUwMw==","user":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"created_at":"2020-05-13T16:30:11Z","updated_at":"2020-05-13T16:30:11Z","author_association":"MEMBER","body":">  in the biomedical industry, some interactions can get quite complex and their textual representation a bit long.\r\n\r\nThat was my first guess about this use case indeed. \r\n\r\n> Our team can even work at this feature\r\n\r\nGreat, I'll label this as \"help wanted\" to indicate this is a good issue to pick up. Feel free to open a PR, if you need guidance I think looking at e.g. how `WhitespaceTokenizerFactory` handles and passes on the `max_token_length` setting should help a great deal. No need to pick this up yourself though, just if you're interested.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/629679906","html_url":"https://github.com/elastic/elasticsearch/issues/56676#issuecomment-629679906","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/56676","id":629679906,"node_id":"MDEyOklzc3VlQ29tbWVudDYyOTY3OTkwNg==","user":{"login":"ADBalici","id":9438684,"node_id":"MDQ6VXNlcjk0Mzg2ODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/9438684?v=4","gravatar_id":"","url":"https://api.github.com/users/ADBalici","html_url":"https://github.com/ADBalici","followers_url":"https://api.github.com/users/ADBalici/followers","following_url":"https://api.github.com/users/ADBalici/following{/other_user}","gists_url":"https://api.github.com/users/ADBalici/gists{/gist_id}","starred_url":"https://api.github.com/users/ADBalici/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ADBalici/subscriptions","organizations_url":"https://api.github.com/users/ADBalici/orgs","repos_url":"https://api.github.com/users/ADBalici/repos","events_url":"https://api.github.com/users/ADBalici/events{/privacy}","received_events_url":"https://api.github.com/users/ADBalici/received_events","type":"User","site_admin":false},"created_at":"2020-05-16T17:26:04Z","updated_at":"2020-05-16T17:26:04Z","author_association":"CONTRIBUTOR","body":"@cbuescher opened a PR just now :D","performed_via_github_app":null}]