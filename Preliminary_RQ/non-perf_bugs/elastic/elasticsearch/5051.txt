{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/5051","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5051/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5051/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5051/events","html_url":"https://github.com/elastic/elasticsearch/issues/5051","id":27149731,"node_id":"MDU6SXNzdWUyNzE0OTczMQ==","number":5051,"title":"Shard recovery failures due to OutOfMemoryError","user":{"login":"msonnabaum","id":49792,"node_id":"MDQ6VXNlcjQ5Nzky","avatar_url":"https://avatars3.githubusercontent.com/u/49792?v=4","gravatar_id":"","url":"https://api.github.com/users/msonnabaum","html_url":"https://github.com/msonnabaum","followers_url":"https://api.github.com/users/msonnabaum/followers","following_url":"https://api.github.com/users/msonnabaum/following{/other_user}","gists_url":"https://api.github.com/users/msonnabaum/gists{/gist_id}","starred_url":"https://api.github.com/users/msonnabaum/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/msonnabaum/subscriptions","organizations_url":"https://api.github.com/users/msonnabaum/orgs","repos_url":"https://api.github.com/users/msonnabaum/repos","events_url":"https://api.github.com/users/msonnabaum/events{/privacy}","received_events_url":"https://api.github.com/users/msonnabaum/received_events","type":"User","site_admin":false},"labels":[{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2014-02-07T16:30:18Z","updated_at":"2014-11-10T10:55:19Z","closed_at":"2014-11-10T10:55:19Z","author_association":"NONE","active_lock_reason":null,"body":"As the size of my indices started to grow, the frequency of these errors increased:\n\n```\n[2014-02-07 16:04:00,646][WARN ][cluster.action.shard     ] [appname-2.example.com] [appname-2013.11.24][1] sending failed shard for [appname-2013.11.24][1], node[86u1ewhBStSECH5uUkkOFQ], [R], s[INITIALIZING], indexUUID [_na_], reason [Failed to start shard, message [RecoveryFailedException[[appname-2013.11.24][1]: Recovery failed from [appname-1.example.com][8OOCSY-ORReONYEnwq0BNw][inet[/10.80.171.13:9300]]{instance_size=small, max_local_storage_nodes=1} into [appname-2.example.com][86u1ewhBStSECH5uUkkOFQ][inet[/10.28.156.176:9300]]{instance_size=small, max_local_storage_nodes=1}]; nested: RemoteTransportException[[appname-1.example.com][inet[/10.80.171.13:9300]][index/shard/recovery/startRecovery]]; nested: RecoveryEngineException[[appname-2013.11.24][1] Phase[2] Execution failed]; nested: RemoteTransportException[[appname-2.example.com][inet[/10.28.156.176:9300]][index/shard/recovery/prepareTranslog]]; nested: OutOfMemoryError[Java heap space]; ]]\n[2014-02-07 16:04:03,458][WARN ][indices.cluster          ] [appname-2.example.com] [appname-2013.12.29][0] failed to start shard\norg.elasticsearch.indices.recovery.RecoveryFailedException: [appname-2013.12.29][0]: Recovery failed from [appname-1.example.com][8OOCSY-ORReONYEnwq0BNw][inet[/10.80.171.13:9300]]{instance_size=small, max_local_storage_nodes=1} into [appname-2.example.com][86u1ewhBStSECH5uUkkOFQ][inet[/10.28.156.176:9300]]{instance_size=small, max_local_storage_nodes=1}\n        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:303)\n        at org.elasticsearch.indices.recovery.RecoveryTarget.access$300(RecoveryTarget.java:65)\n        at org.elasticsearch.indices.recovery.RecoveryTarget$2.run(RecoveryTarget.java:171)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:724)\nCaused by: org.elasticsearch.transport.RemoteTransportException: [appname-1.example.com][inet[/10.80.171.13:9300]][index/shard/recovery/startRecovery]\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [appname-2013.12.29][0] Phase[2] Execution failed\n        at org.elasticsearch.index.engine.robin.RobinEngine.recover(RobinEngine.java:1156)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:589)\n        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)\n        at org.elasticsearch.indices.recovery.RecoverySource.access$1600(RecoverySource.java:61)\n        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:337)\n        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:323)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:724)\nCaused by: org.elasticsearch.transport.RemoteTransportException: [appname-2.example.com][inet[/10.28.156.176:9300]][index/shard/recovery/prepareTranslog]\nCaused by: java.lang.OutOfMemoryError: Java heap space\n```\n\nThe errors occur on the two smaller nodes in the cluster with a 4.4G heap, and the larger nodes with 9G recover ok. It's to the point now where the cluster comes up yellow or red after a restart, depending on which shards the small nodes fail to recover.\n\nThis is on ES 0.90.11, and the two indices in the log entry are each around 60G with 2 shards.\n\nAlthough I could just replace those nodes with higher memory instances, I wanted to confirm that this is the expected behavior, or if there's an upper limit to how large a shard should be to be able to recover it given a heap size. I expect the indices to continue to grow, so I'm afraid of reaching the point where a 9G heap isn't enough to recover.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}