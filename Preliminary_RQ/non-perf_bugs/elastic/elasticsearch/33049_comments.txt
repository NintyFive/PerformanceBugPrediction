[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/414990048","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-414990048","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":414990048,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNDk5MDA0OA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-08-22T10:41:32Z","updated_at":"2018-08-22T10:41:32Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-search-aggs","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/414990548","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-414990548","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":414990548,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNDk5MDU0OA==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2018-08-22T10:43:41Z","updated_at":"2018-08-22T10:43:41Z","author_association":"MEMBER","body":"> As using autogenerated IDs give a speed advantage at indexing time, I think it would be great to make the format/.structure of autogenerated IDs configurable per index so the user can choose between optimizing for heap or disk usage. \r\n\r\nIt would be useful to know the indexing speed for each of these options too since we would want to be clear with the user whether choosing a particular option also influences the indexing speed as well as heap usage and index size","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415094390","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415094390","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415094390,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTA5NDM5MA==","user":{"login":"cdahlqvist","id":2804455,"node_id":"MDQ6VXNlcjI4MDQ0NTU=","avatar_url":"https://avatars0.githubusercontent.com/u/2804455?v=4","gravatar_id":"","url":"https://api.github.com/users/cdahlqvist","html_url":"https://github.com/cdahlqvist","followers_url":"https://api.github.com/users/cdahlqvist/followers","following_url":"https://api.github.com/users/cdahlqvist/following{/other_user}","gists_url":"https://api.github.com/users/cdahlqvist/gists{/gist_id}","starred_url":"https://api.github.com/users/cdahlqvist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdahlqvist/subscriptions","organizations_url":"https://api.github.com/users/cdahlqvist/orgs","repos_url":"https://api.github.com/users/cdahlqvist/repos","events_url":"https://api.github.com/users/cdahlqvist/events{/privacy}","received_events_url":"https://api.github.com/users/cdahlqvist/received_events","type":"User","site_admin":false},"created_at":"2018-08-22T16:28:47Z","updated_at":"2018-08-22T19:06:09Z","author_association":"NONE","body":"@colings86 I can set up and run [this challenge](https://github.com/elastic/rally-eventdata-track/blob/master/eventdata/challenges/document_id_benchmark.json) against an Elastic Cloud cluster for comparison. It indexes the same amount of data using the following 3 main ID schemes:\r\n\r\n* Autogenerated IDs\r\n* UUIDs (uuid4)\r\n* UUID (uuid4) prefixed by hex-encoded epoch\r\n\r\nDetails about the ID generation can be found [here](https://github.com/elastic/rally-eventdata-track/blob/master/eventdata/parameter_sources/elasticlogs_bulk_source.py#L118).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415296769","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415296769","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415296769,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTI5Njc2OQ==","user":{"login":"cdahlqvist","id":2804455,"node_id":"MDQ6VXNlcjI4MDQ0NTU=","avatar_url":"https://avatars0.githubusercontent.com/u/2804455?v=4","gravatar_id":"","url":"https://api.github.com/users/cdahlqvist","html_url":"https://github.com/cdahlqvist","followers_url":"https://api.github.com/users/cdahlqvist/followers","following_url":"https://api.github.com/users/cdahlqvist/following{/other_user}","gists_url":"https://api.github.com/users/cdahlqvist/gists{/gist_id}","starred_url":"https://api.github.com/users/cdahlqvist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdahlqvist/subscriptions","organizations_url":"https://api.github.com/users/cdahlqvist/orgs","repos_url":"https://api.github.com/users/cdahlqvist/repos","events_url":"https://api.github.com/users/cdahlqvist/events{/privacy}","received_events_url":"https://api.github.com/users/cdahlqvist/received_events","type":"User","site_admin":false},"created_at":"2018-08-23T05:34:07Z","updated_at":"2018-08-23T05:34:35Z","author_association":"NONE","body":"@colings86 I ran the challenge overnight and got the following average indexing throughout when indexing 100 million documents (~20GB shard size) against a single 32GB Elastic Cloud node:\r\n\r\n* Autogenerated IDs: **25.5k EPS**\r\n* UUIDs (uuid4): **18.9k EPS**\r\n* UUID (uuid4) prefixed by hex-encoded epoch: **22.2k EPS**\r\n\r\nI used these document ID types as an example and am not suggesting we necessarily use one of them. The ideal solution would be if we could create a variation of the currently used scheme that has similar heap usage profile as these alternative ID types while still retaining the characteristics that make autogenerated IDs so fast to index.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415300935","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415300935","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415300935,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTMwMDkzNQ==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-08-23T05:59:18Z","updated_at":"2018-08-23T05:59:18Z","author_association":"CONTRIBUTOR","body":"> the characteristics that make autogenerated IDs so fast to index\r\n\r\nOne of these characteristics is that we know the provenance of an autogenerated ID, so we know whether we can skip the checks for the existence of another document with the same ID. With externally-provided IDs we always have to check. Importantly, this is independent of the format of the ID.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415303490","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415303490","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415303490,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTMwMzQ5MA==","user":{"login":"cdahlqvist","id":2804455,"node_id":"MDQ6VXNlcjI4MDQ0NTU=","avatar_url":"https://avatars0.githubusercontent.com/u/2804455?v=4","gravatar_id":"","url":"https://api.github.com/users/cdahlqvist","html_url":"https://github.com/cdahlqvist","followers_url":"https://api.github.com/users/cdahlqvist/followers","following_url":"https://api.github.com/users/cdahlqvist/following{/other_user}","gists_url":"https://api.github.com/users/cdahlqvist/gists{/gist_id}","starred_url":"https://api.github.com/users/cdahlqvist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdahlqvist/subscriptions","organizations_url":"https://api.github.com/users/cdahlqvist/orgs","repos_url":"https://api.github.com/users/cdahlqvist/repos","events_url":"https://api.github.com/users/cdahlqvist/events{/privacy}","received_events_url":"https://api.github.com/users/cdahlqvist/received_events","type":"User","site_admin":false},"created_at":"2018-08-23T06:12:27Z","updated_at":"2018-08-23T06:22:08Z","author_association":"NONE","body":"@DaveCTurner Excellent. I remember that but was not sure if it was still in place or not. I assume that means that if we created an alternate autogenerated ID format that has a different heap usage profile, it would by default then also be faster than my benchmarks as we also for this format would be able to skip the checks? ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415305442","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415305442","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415305442,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTMwNTQ0Mg==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-08-23T06:23:12Z","updated_at":"2018-08-23T06:26:25Z","author_association":"CONTRIBUTOR","body":"In theory yes, although nothing is certain in these situations without measurement.\r\n\r\nHowever, it puzzles me that autogenerated IDs take up approximately twice the terms memory of the other formats. A nice round multiple like that makes me wonder if we are perhaps double-counting something, or mistakenly generating twice as many terms or something like that?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415321877","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415321877","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415321877,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTMyMTg3Nw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-08-23T07:39:25Z","updated_at":"2018-08-23T07:39:25Z","author_association":"CONTRIBUTOR","body":"The 2x factor is probably a coincidence. However it's expected that patterns that compress well also require more memory. Memory usage is due to the FST that records shared prefixes of terms per block of 25-40 terms, so the number of prefixes that are recorded in the FST doesn't depend too much on the pattern, but the length of the recorded prefixes does.\r\n\r\nEven in the worst case, these numbers are pretty good in my opinion: 40MB of terms dictionary still gives a memory/disk ratio of 1.6%. Though I agree it is a bit disappointing to spend most memory on the `_id` field for an append-only use-case.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415725672","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415725672","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415725672,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTcyNTY3Mg==","user":{"login":"cdahlqvist","id":2804455,"node_id":"MDQ6VXNlcjI4MDQ0NTU=","avatar_url":"https://avatars0.githubusercontent.com/u/2804455?v=4","gravatar_id":"","url":"https://api.github.com/users/cdahlqvist","html_url":"https://github.com/cdahlqvist","followers_url":"https://api.github.com/users/cdahlqvist/followers","following_url":"https://api.github.com/users/cdahlqvist/following{/other_user}","gists_url":"https://api.github.com/users/cdahlqvist/gists{/gist_id}","starred_url":"https://api.github.com/users/cdahlqvist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdahlqvist/subscriptions","organizations_url":"https://api.github.com/users/cdahlqvist/orgs","repos_url":"https://api.github.com/users/cdahlqvist/repos","events_url":"https://api.github.com/users/cdahlqvist/events{/privacy}","received_events_url":"https://api.github.com/users/cdahlqvist/received_events","type":"User","site_admin":false},"created_at":"2018-08-24T10:57:12Z","updated_at":"2018-08-24T11:47:13Z","author_association":"NONE","body":"@DaveCTurner helped me set up a build where I could test a few different permutations of the current autogenerated ID format. Of the 6 variations I tested (`auto-v2` - `auto-v7`), a few seemed to perform significantly better than the existing scheme. For each ID type I indexed 100 million documents from the rally-eventdata-track into a single shard index (Rally ran on the same host as Elasticsearch):\r\n\r\n![screen shot 2018-08-24 at 09 55 28](https://user-images.githubusercontent.com/2804455/44580936-cad83c00-a793-11e8-94d1-56bb089367f8.png)\r\n\r\nThe `auto-v4` version (code below) was a bit faster (~2.5%, which could be due to cloud variability), took up 13% less space on disk (not forcemerged, so could be down to the test ending at a different stage in the merging cycle) and used almost 40% less heap compared to the default `auto` format.\r\n\r\nThis type of ID was generated by simply rearranging the order of the various components that make up the current ID:\r\n\r\n```\r\n// We start with the MAC address\r\nbyte[] macAddress = macAddress();\r\nassert macAddress.length == 6;\r\nSystem.arraycopy(macAddress, 0, uuidBytes, i, macAddress.length); \r\ni += macAddress.length;\r\n\r\n// Then we put the remaining bytes, which will likely not be compressed at all.\r\nuuidBytes[i++] = (byte) (timestamp >>> 8);\r\nuuidBytes[i++] = (byte) (sequenceId >>> 8);\r\nuuidBytes[i++] = (byte) timestamp;\r\n\r\nuuidBytes[i++] = (byte) sequenceId;\r\n// changes every 65k docs, so potentially every second if you have a steady indexing rate\r\nuuidBytes[i++] = (byte) (sequenceId >>> 16);\r\n\r\n// Now we start focusing on compression and put bytes that should not change too often.\r\nuuidBytes[i++] = (byte) (timestamp >>> 16); // changes every ~65 secs\r\nuuidBytes[i++] = (byte) (timestamp >>> 24); // changes every ~4.5h\r\nuuidBytes[i++] = (byte) (timestamp >>> 32); // changes every ~50 days\r\nuuidBytes[i++] = (byte) (timestamp >>> 40); // changes every 35 years\r\n\r\nassert i == uuidBytes.length;\r\n```\r\n\r\nAnother version that also performed quite well was `auto-v2`:\r\n\r\n```\r\n// Initially we put the remaining bytes, which will likely not be compressed at all.\r\nuuidBytes[i++] = (byte) (timestamp >>> 8);\r\nuuidBytes[i++] = (byte) (sequenceId >>> 8);\r\nuuidBytes[i++] = (byte) timestamp;\r\n\r\nuuidBytes[i++] = (byte) sequenceId;\r\n// changes every 65k docs, so potentially every second if you have a steady indexing rate\r\nuuidBytes[i++] = (byte) (sequenceId >>> 16);\r\n\r\n// Now we start focusing on compression and put bytes that should not change too often.\r\nuuidBytes[i++] = (byte) (timestamp >>> 16); // changes every ~65 secs\r\nuuidBytes[i++] = (byte) (timestamp >>> 24); // changes every ~4.5h\r\nuuidBytes[i++] = (byte) (timestamp >>> 32); // changes every ~50 days\r\nuuidBytes[i++] = (byte) (timestamp >>> 40); // changes every 35 years\r\nbyte[] macAddress = macAddress();\r\nassert macAddress.length == 6;\r\nSystem.arraycopy(macAddress, 0, uuidBytes, i, macAddress.length);\r\ni += macAddress.length;\r\n\r\nassert i == uuidBytes.length;\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415959777","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415959777","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415959777,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTk1OTc3Nw==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2018-08-25T10:32:07Z","updated_at":"2018-08-25T10:32:07Z","author_association":"CONTRIBUTOR","body":"I have a general question, are you comparing the memory consumption of optimized indices? If not I think the numbers are misleading. It would be good to clarify this.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415960825","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415960825","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415960825,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTk2MDgyNQ==","user":{"login":"cdahlqvist","id":2804455,"node_id":"MDQ6VXNlcjI4MDQ0NTU=","avatar_url":"https://avatars0.githubusercontent.com/u/2804455?v=4","gravatar_id":"","url":"https://api.github.com/users/cdahlqvist","html_url":"https://github.com/cdahlqvist","followers_url":"https://api.github.com/users/cdahlqvist/followers","following_url":"https://api.github.com/users/cdahlqvist/following{/other_user}","gists_url":"https://api.github.com/users/cdahlqvist/gists{/gist_id}","starred_url":"https://api.github.com/users/cdahlqvist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdahlqvist/subscriptions","organizations_url":"https://api.github.com/users/cdahlqvist/orgs","repos_url":"https://api.github.com/users/cdahlqvist/repos","events_url":"https://api.github.com/users/cdahlqvist/events{/privacy}","received_events_url":"https://api.github.com/users/cdahlqvist/received_events","type":"User","site_admin":false},"created_at":"2018-08-25T10:51:08Z","updated_at":"2018-08-25T10:51:08Z","author_association":"NONE","body":"No, these numbers are not for optimized indices. I have just indexed ~25GB of data and refreshed before recording statistics. I agree this is one of the tests we should run, but also feel forcemerging is not always a realistic option when you are running with quite large shards as per our best practices for logging and metrics use cases, so gains without forcemerging are also of interest even if they can vary more.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415960944","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415960944","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415960944,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTk2MDk0NA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2018-08-25T10:53:27Z","updated_at":"2018-08-25T10:53:27Z","author_association":"CONTRIBUTOR","body":"> No, these numbers are not for optimized indices. I have just indexed ~25GB of data and refreshed before recording statistics. I agree this is one of the tests we should run, but also feel forcemerging is not always a realistic option when you are running with quite large shards as per our best practices for logging and metrics use cases, so gains without forcemerging are also of interest even if they can vary more.\r\n\r\nthis is a misconception IMO. We try to compare the memory consumption of a certain ID format. If you end up with a lot more segments in a certain case due to some indexing patterns and your other ID has  only half the segments you might think it takes 1/2 the memory but in fact it's the same or even more. If you want to compare the numbers you have to compare apples against apples and therefor force merge to a single segment. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415961164","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-415961164","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":415961164,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTk2MTE2NA==","user":{"login":"cdahlqvist","id":2804455,"node_id":"MDQ6VXNlcjI4MDQ0NTU=","avatar_url":"https://avatars0.githubusercontent.com/u/2804455?v=4","gravatar_id":"","url":"https://api.github.com/users/cdahlqvist","html_url":"https://github.com/cdahlqvist","followers_url":"https://api.github.com/users/cdahlqvist/followers","following_url":"https://api.github.com/users/cdahlqvist/following{/other_user}","gists_url":"https://api.github.com/users/cdahlqvist/gists{/gist_id}","starred_url":"https://api.github.com/users/cdahlqvist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdahlqvist/subscriptions","organizations_url":"https://api.github.com/users/cdahlqvist/orgs","repos_url":"https://api.github.com/users/cdahlqvist/repos","events_url":"https://api.github.com/users/cdahlqvist/events{/privacy}","received_events_url":"https://api.github.com/users/cdahlqvist/received_events","type":"User","site_admin":false},"created_at":"2018-08-25T10:57:36Z","updated_at":"2018-08-25T10:57:36Z","author_association":"NONE","body":"Agreed. I will rerun it and record stats before and after a forcemerge.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/416126280","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-416126280","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":416126280,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNjEyNjI4MA==","user":{"login":"cdahlqvist","id":2804455,"node_id":"MDQ6VXNlcjI4MDQ0NTU=","avatar_url":"https://avatars0.githubusercontent.com/u/2804455?v=4","gravatar_id":"","url":"https://api.github.com/users/cdahlqvist","html_url":"https://github.com/cdahlqvist","followers_url":"https://api.github.com/users/cdahlqvist/followers","following_url":"https://api.github.com/users/cdahlqvist/following{/other_user}","gists_url":"https://api.github.com/users/cdahlqvist/gists{/gist_id}","starred_url":"https://api.github.com/users/cdahlqvist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdahlqvist/subscriptions","organizations_url":"https://api.github.com/users/cdahlqvist/orgs","repos_url":"https://api.github.com/users/cdahlqvist/repos","events_url":"https://api.github.com/users/cdahlqvist/events{/privacy}","received_events_url":"https://api.github.com/users/cdahlqvist/received_events","type":"User","site_admin":false},"created_at":"2018-08-27T06:21:33Z","updated_at":"2018-08-27T09:27:30Z","author_association":"NONE","body":"I ran it again for some of the ID types and recorded stats before and after forcemerge. I also recorded the number of segments before the forcemerge for comparison:\r\n\r\n![screen shot 2018-08-27 at 07 05 48](https://user-images.githubusercontent.com/2804455/44643275-d3b15380-a9c7-11e8-9a57-cf82419337fc.png)\r\n\r\nThe results seem to largely correlate with the previous run, so I think it is worth investigating a bit further. If it is consistent, we could save a good amount of heap. If it on the other hand is indeed a red herring and not consistently reproducible, the spread of values itself suggests to me that we hight have an area where some level of optimization to reduce the spread potentially could bring significant gains.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/417244389","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-417244389","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":417244389,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNzI0NDM4OQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-08-30T08:59:55Z","updated_at":"2018-08-30T09:11:36Z","author_association":"CONTRIBUTOR","body":"I tweaked UUIDTests to run some simulations with various patterns for `_id`, indexing rates and numbers of documents. `auto-v2` and `auto-v4` are described above. `original` is the original flake id format which puts the timestamp first, then mac address, then sequence id. `current` is the current pattern that we use for `_id`. `common bytes first` optimizes for disk usage by putting shared bytes first, see below:\r\n\r\n```java\r\n// common bytes first\r\nuuidBytes[i++] = (byte) (timestamp >>> 40); // changes every 35 years\r\nuuidBytes[i++] = (byte) (timestamp >>> 32); // changes every ~50 days\r\nassert macAddress.length == 6;\r\nSystem.arraycopy(macAddress, 0, uuidBytes, i, macAddress.length);\r\ni += macAddress.length;\r\nuuidBytes[i++] = (byte) (timestamp >>> 24); // changes every ~4.5h\r\nuuidBytes[i++] = (byte) (timestamp >>> 16); // changes every ~65 secs\r\nuuidBytes[i++] = (byte) (sequenceId >>> 16); // changes every ~65k docs\r\nuuidBytes[i++] = (byte) (timestamp >>> 8);\r\nuuidBytes[i++] = (byte) (sequenceId >>> 8);\r\nuuidBytes[i++] = (byte) timestamp;\r\nuuidBytes[i++] = (byte) sequenceId;\r\n```\r\n\r\nIndices were forced-merged before computing disk and memory usage.\r\n\r\n**50,000,000 at 8k docs/s**\r\n\r\n| Format        | Disk per doc (bytes) | Memory per doc (bytes) |\r\n| ------------- |-------------:| -----:|\r\n| original | 20.4 | 0.14 |\r\n| current | 11.2 | 0.40 |\r\n| common bytes first | 9.52 | 0.20 |\r\n| auto-v2 | 24.5 | 0.26 |\r\n| auto-v4 | 16.5| 0.24 |\r\n\r\n**43,200,000 documents indexed at 500 docs/s** (simulates an entire day of indexing since 24\\*60\\*60\\*500=43,200,000):\r\n\r\n| Format        | Disk per doc (bytes) | Memory per doc (bytes) |\r\n| ------------- |-------------:| -----:|\r\n| original | 20.6 | 0.26 |\r\n| current | 11.9 | 0.27 |\r\n| common bytes first | 10.5 | 0.20 |\r\n| auto-v2 | 24.5 | 0.28 |\r\n| auto-v4 | 16.5| 0.27 |\r\n\r\nIt's interesting to notice how differently these patterns perform depending on the configuration. For instance the current format looks bad in the first test regarding memory usage, but is on par with most other formats in the second test while providing better disk usage than all but one.\r\n\r\nThe current pattern for `_id` tried to optimize for disk usage and indexing speed. Maybe we should reconsider this and optimize for disk usage and memory usage. We had previous discussions around changing the format that were not applied because they hurt indexing speed (see eg. #18209) but that should be much less of an issue nowadays that we almost always call addDocument rather than updateDocument which helps skip costly lookups. Shared common prefixes still have some indexing cost due to the fact that flushes and merges need to compare more bytes, but that might not be an issue in practice.\r\n\r\nI'm happy to test more formats if needed.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/417255317","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-417255317","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":417255317,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNzI1NTMxNw==","user":{"login":"cdahlqvist","id":2804455,"node_id":"MDQ6VXNlcjI4MDQ0NTU=","avatar_url":"https://avatars0.githubusercontent.com/u/2804455?v=4","gravatar_id":"","url":"https://api.github.com/users/cdahlqvist","html_url":"https://github.com/cdahlqvist","followers_url":"https://api.github.com/users/cdahlqvist/followers","following_url":"https://api.github.com/users/cdahlqvist/following{/other_user}","gists_url":"https://api.github.com/users/cdahlqvist/gists{/gist_id}","starred_url":"https://api.github.com/users/cdahlqvist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdahlqvist/subscriptions","organizations_url":"https://api.github.com/users/cdahlqvist/orgs","repos_url":"https://api.github.com/users/cdahlqvist/repos","events_url":"https://api.github.com/users/cdahlqvist/events{/privacy}","received_events_url":"https://api.github.com/users/cdahlqvist/received_events","type":"User","site_admin":false},"created_at":"2018-08-30T09:33:25Z","updated_at":"2018-08-30T09:46:37Z","author_association":"NONE","body":"@jpountz That is very interesting. The `common bytes first` approach seems very promising, especially as gains seem to improve with increased indexing rate. It would be useful to see the numbers for the same number of documents indexed at a variety of indexing speeds, e.g. 1k, 2k, and 4k EPS, for comparison.\r\n\r\nIt would also be interesting to see if/how changing the MAC address (primary shard relocation) during the run affects the results.\r\n\r\nThe difference in size on disk looks large, but as this only covers ID storage (only small part of shard disk usage), would it be correct to assume the difference as a percentage would be much smaller in a real use-case?\r\n\r\nThe `common bytes first` seem to have properties in line with the best I observed, so I do not necessarily have any suggestions for other formats worth testing.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/417312667","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-417312667","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":417312667,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNzMxMjY2Nw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-08-30T13:07:00Z","updated_at":"2018-08-30T13:07:00Z","author_association":"CONTRIBUTOR","body":"Here are some more numbers. 50M documents are indexed in every case. I also ran the test with 5 different mac addresses, which gives numbers for a shrinked index and probably a worst-case scenario for a shard that gets relocated while indexing.\r\n\r\n| Format    | # mac addresses | indexing rate (EPS) | Disk per doc (bytes) | Memory per doc (bytes) |\r\n| ------------- | ------------- | ------------- |-------------| -----|\r\n| original | 1 | 1k | 20.5 | 0.23 |\r\n|  |  | 2k | 20.5 | 0.17 |\r\n|  |  | 4k | 20.4 | 0.15 |\r\n|  |  | 8k | 20.4 | 0.14 |\r\n|  |  | 16k | 20.3 | 0.13 |\r\n|  | 5 | 1k | 20.5 | 0.23 |\r\n|  |  | 2k | 20.5 | 0.17 |\r\n|  |  | 4k | 20.4 | 0.15 |\r\n|  |  | 8k | 20.4 | 0.14 |\r\n|  |  | 16k | 20.3 | 0.13 |\r\n| current | 1 | 1k | 11.7 | 0.21 |\r\n|  |  | 2k | 11.6 | 0.20 |\r\n|  |  | 4k | 11.5 | 0.16 |\r\n|  |  | 8k | 11.2 | 0.40 |\r\n|  |  | 16k | 11.3 | 0.33 |\r\n|  | 5 | 1k | 13.9 | 0.34 |\r\n|  |  | 2k | 12.9 | 0.32 |\r\n|  |  | 4k | 12.4 | 0.31 |\r\n|  |  | 8k | 12.1 | 0.30 |\r\n|  |  | 16k | 12.0 | 0.30 |\r\n| common bytes first | 1 | 1k | 10.4 | 0.16 |\r\n|  |  | 2k | 9.8 | 0.28 |\r\n|  |  | 4k | 9.6 | 0.23 |\r\n|  |  | 8k | 9.5 | 0.20 |\r\n|  |  | 16k | 9.4 | 0.15 |\r\n|  | 5 | 1k | 10.7 | 0.29 |\r\n|  |  | 2k | 10.6 | 0.26 |\r\n|  |  | 4k | 10.5 | 0.23 |\r\n|  |  | 8k | 10.2 | 0.25 |\r\n|  |  | 16k | 9.8 | 0.25 |\r\n| auto-v2 | 1 | 1k | 24.6 | 0.33 |\r\n|  |  | 2k | 24.5 | 0.28 |\r\n|  |  | 4k | 24.5| 0.28 |\r\n|  |  | 8k | 24.5 | 0.26 |\r\n|  |  | 16k | 24.5 | 0.26 |\r\n|  | 5 | 1k | 24.6 | 0.28 |\r\n|  |  | 2k | 24.6 | 0.27 |\r\n|  |  | 4k | 24.6 | 0.28 |\r\n|  |  | 8k | 24.5  | 0.26  |\r\n|  |  | 16k | 24.4 | 0.23 |\r\n| auto-v4 | 1 | 1k | 16.6 | 0.29 |\r\n|  |  | 2k | 16.5 | 0.28 |\r\n|  |  | 4k | 16.6| 0.27 |\r\n|  |  | 8k | 16.5 | 0.24 |\r\n|  |  | 16k | 16.4 | 0.22 |\r\n|  | 5 | 1k | 16.8 | 0.33 |\r\n|  |  | 2k | 16.7 | 0.32 |\r\n|  |  | 4k | 16.7 | 0.32 |\r\n|  |  | 8k | 16.6 | 0.31 |\r\n|  |  | 16k | 16.6 | 0.30 |\r\n\r\n\r\n> The difference in size on disk looks large, but as this only covers ID storage (only small part of shard disk usage), would it be correct to assume the difference as a percentage would be much smaller in a real use-case?\r\n\r\nYes.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/417349542","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-417349542","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":417349542,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNzM0OTU0Mg==","user":{"login":"cdahlqvist","id":2804455,"node_id":"MDQ6VXNlcjI4MDQ0NTU=","avatar_url":"https://avatars0.githubusercontent.com/u/2804455?v=4","gravatar_id":"","url":"https://api.github.com/users/cdahlqvist","html_url":"https://github.com/cdahlqvist","followers_url":"https://api.github.com/users/cdahlqvist/followers","following_url":"https://api.github.com/users/cdahlqvist/following{/other_user}","gists_url":"https://api.github.com/users/cdahlqvist/gists{/gist_id}","starred_url":"https://api.github.com/users/cdahlqvist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdahlqvist/subscriptions","organizations_url":"https://api.github.com/users/cdahlqvist/orgs","repos_url":"https://api.github.com/users/cdahlqvist/repos","events_url":"https://api.github.com/users/cdahlqvist/events{/privacy}","received_events_url":"https://api.github.com/users/cdahlqvist/received_events","type":"User","site_admin":false},"created_at":"2018-08-30T14:55:59Z","updated_at":"2018-08-30T14:57:39Z","author_association":"NONE","body":"Based on this, the `original` scheme actually looks superior (roughly on par with `current` at lower indexing rates and significantly better as it increases), at least as long as additional disk usage is acceptable. It is also interesting to see that it is not sensitive to MAC address changes. I think trading a bit of additional disk usage for reduced heap usage is the right trade-off though, as heap space is precious and in limited supply. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/417864206","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-417864206","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":417864206,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNzg2NDIwNg==","user":{"login":"cdahlqvist","id":2804455,"node_id":"MDQ6VXNlcjI4MDQ0NTU=","avatar_url":"https://avatars0.githubusercontent.com/u/2804455?v=4","gravatar_id":"","url":"https://api.github.com/users/cdahlqvist","html_url":"https://github.com/cdahlqvist","followers_url":"https://api.github.com/users/cdahlqvist/followers","following_url":"https://api.github.com/users/cdahlqvist/following{/other_user}","gists_url":"https://api.github.com/users/cdahlqvist/gists{/gist_id}","starred_url":"https://api.github.com/users/cdahlqvist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdahlqvist/subscriptions","organizations_url":"https://api.github.com/users/cdahlqvist/orgs","repos_url":"https://api.github.com/users/cdahlqvist/repos","events_url":"https://api.github.com/users/cdahlqvist/events{/privacy}","received_events_url":"https://api.github.com/users/cdahlqvist/received_events","type":"User","site_admin":false},"created_at":"2018-09-01T14:42:59Z","updated_at":"2018-09-01T14:42:59Z","author_association":"NONE","body":"One additional use case that would be useful to simulate is when we have a long retention period and overshard during indexing (low indexing rate and multiple MAC addresses active during the same time period) and then use the shrink index API to merge these into fewer shards. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/418667384","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-418667384","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":418667384,"node_id":"MDEyOklzc3VlQ29tbWVudDQxODY2NzM4NA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-09-05T09:38:12Z","updated_at":"2018-09-05T09:38:12Z","author_association":"CONTRIBUTOR","body":"One observation: given that the terms index (the part that is in memory) is built for each prefix that is shared by 25 to 40 terms, its memory usage per term tends to increase upon merging when ids are interleaved (average shared prefix length increases) and to remain the same if ids are not interleaved at all (average prefix length remains the same), eg. if all ids of one segment are greater that all ids of the other segment. So if we want an ID pattern whose memory usage doesn't increase as more documents are added, we should try to pick a pattern that produces increasing ids.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/419389096","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-419389096","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":419389096,"node_id":"MDEyOklzc3VlQ29tbWVudDQxOTM4OTA5Ng==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-09-07T09:59:27Z","updated_at":"2018-09-07T09:59:41Z","author_association":"CONTRIBUTOR","body":"This program may be used to simulate indexing/disk usage. It only needs Lucene on the classpath.\r\n\r\n```java\r\nimport java.io.IOException;\r\nimport java.io.UncheckedIOException;\r\nimport java.nio.file.Files;\r\nimport java.nio.file.Path;\r\nimport java.util.Random;\r\nimport java.util.concurrent.ArrayBlockingQueue;\r\nimport java.util.concurrent.ThreadLocalRandom;\r\nimport java.util.concurrent.ThreadPoolExecutor;\r\nimport java.util.concurrent.ThreadPoolExecutor.CallerRunsPolicy;\r\nimport java.util.concurrent.TimeUnit;\r\nimport java.util.function.BiConsumer;\r\nimport java.util.function.LongSupplier;\r\n\r\nimport org.apache.lucene.document.Document;\r\nimport org.apache.lucene.document.Field.Store;\r\nimport org.apache.lucene.document.StringField;\r\nimport org.apache.lucene.index.DirectoryReader;\r\nimport org.apache.lucene.index.IndexReader;\r\nimport org.apache.lucene.index.IndexWriter;\r\nimport org.apache.lucene.index.IndexWriterConfig;\r\nimport org.apache.lucene.index.SegmentReader;\r\nimport org.apache.lucene.store.Directory;\r\nimport org.apache.lucene.store.FSDirectory;\r\nimport org.apache.lucene.util.BytesRef;\r\nimport org.apache.lucene.util.IOUtils;\r\n\r\npublic class DiskMemIdUsage {\r\n\r\n  private static final int NUM_INDEXING_THREADS = 8;\r\n  private static final int MAC_ADDRESS_LENGTH = 6;\r\n  private static final int ID_LENGTH = 15;\r\n  private static final int BATCH_SIZE = 10_000;\r\n\r\n  private static interface IDFormat {\r\n    public void setId(long timestamp, byte[] macAddress, int sequenceId, byte[] idBytes);\r\n  }\r\n\r\n  private static final IDFormat FLAKE_IDS = new IDFormat() {\r\n\r\n    @Override\r\n    public void setId(long timestamp, byte[] macAddress, int sequenceId, byte[] idBytes) {\r\n      int i = 0;\r\n      idBytes[i++] = (byte) (timestamp >>> 40); // changes every 35 years\r\n      idBytes[i++] = (byte) (timestamp >>> 32); // changes every ~50 days\r\n      idBytes[i++] = (byte) (timestamp >>> 24); // changes every ~4.5h\r\n      idBytes[i++] = (byte) (timestamp >>> 16); // changes every ~65 secs\r\n      idBytes[i++] = (byte) (timestamp >>> 8);\r\n      idBytes[i++] = (byte) timestamp;\r\n      assert macAddress.length == 6;\r\n      System.arraycopy(macAddress, 0, idBytes, i, macAddress.length);\r\n      i += macAddress.length;\r\n      idBytes[i++] = (byte) (sequenceId >>> 16); // changes every ~65k docs\r\n      idBytes[i++] = (byte) (sequenceId >>> 8);\r\n      idBytes[i++] = (byte) sequenceId;\r\n      assert i == ID_LENGTH;\r\n    }\r\n\r\n    @Override\r\n    public String toString() {\r\n      return \"FLAKE\";\r\n    }\r\n  };\r\n\r\n  private static final IDFormat CURRENT_IDS = new IDFormat() {\r\n\r\n    @Override\r\n    public void setId(long timestamp, byte[] macAddress, int sequenceId, byte[] idBytes) {\r\n      int i = 0;\r\n      idBytes[i++] = (byte) sequenceId;\r\n      // changes every 65k docs, so potentially every second if you have a steady indexing rate\r\n      idBytes[i++] = (byte) (sequenceId >>> 16);\r\n\r\n      // Now we start focusing on compression and put bytes that should not change too often.\r\n      idBytes[i++] = (byte) (timestamp >>> 16); // changes every ~65 secs\r\n      idBytes[i++] = (byte) (timestamp >>> 24); // changes every ~4.5h\r\n      idBytes[i++] = (byte) (timestamp >>> 32); // changes every ~50 days\r\n      idBytes[i++] = (byte) (timestamp >>> 40); // changes every 35 years\r\n      assert macAddress.length == 6;\r\n      System.arraycopy(macAddress, 0, idBytes, i, macAddress.length);\r\n      i += macAddress.length;\r\n\r\n      // Finally we put the remaining bytes, which will likely not be compressed at all.\r\n      idBytes[i++] = (byte) (timestamp >>> 8);\r\n      idBytes[i++] = (byte) (sequenceId >>> 8);\r\n      idBytes[i++] = (byte) timestamp;\r\n      assert i == ID_LENGTH;\r\n    }\r\n\r\n    @Override\r\n    public String toString() {\r\n      return \"CURRENT\";\r\n    }\r\n  };\r\n\r\n  private static LongSupplier timestampSupplier(Random random, double docsPerSecond) {\r\n    final double averageIntervalBetweenDocsMs = 1000 / docsPerSecond;\r\n    return new LongSupplier() {\r\n\r\n      long initialTimestamp = System.currentTimeMillis();\r\n      double addend = 0;\r\n\r\n      @Override\r\n      public long getAsLong() {\r\n        long value = initialTimestamp + (long) addend;\r\n        addend += random.nextDouble() * 2 * averageIntervalBetweenDocsMs;\r\n        return value;\r\n      }\r\n    };\r\n  }\r\n\r\n  private static class ResourceUsage {\r\n    final double diskUsagePerDoc;\r\n    final double memoryUsagePerDoc;\r\n\r\n    ResourceUsage(double diskUsagePerDoc, double memoryUsagePerDoc) {\r\n      this.diskUsagePerDoc = diskUsagePerDoc;\r\n      this.memoryUsagePerDoc = memoryUsagePerDoc;\r\n    }\r\n  }\r\n\r\n  private static BiConsumer<byte[], byte[]> idGenerator(IDFormat format, double docsPerSecond) {\r\n    final Random random = new Random(0);\r\n    return new BiConsumer<byte[], byte[]>() {\r\n\r\n      final LongSupplier timestampSupplier = timestampSupplier(random, docsPerSecond);\r\n      long maxTimestamp = Long.MIN_VALUE;\r\n      int sequenceId = random.nextInt();\r\n\r\n      @Override\r\n      public synchronized void accept(byte[] macAddress, byte[] idBytes) {\r\n        assert macAddress.length == 6;\r\n        assert idBytes.length == ID_LENGTH;\r\n        long timestamp = Math.max(maxTimestamp, timestampSupplier.getAsLong());\r\n        if (++sequenceId == 0) {\r\n          timestamp++;\r\n        }\r\n        maxTimestamp = timestamp;\r\n        format.setId(timestamp, macAddress, sequenceId, idBytes);\r\n      }\r\n    };\r\n  }\r\n\r\n  private static ResourceUsage simulateResourceUsage(int numDocs, BiConsumer<byte[], byte[]> idGenerator, int numNodes) throws IOException, InterruptedException {\r\n    final byte[][] macAddresses = new byte[numNodes][];\r\n    for (int i = 0; i < macAddresses.length; ++i) {\r\n      macAddresses[i] = new byte[MAC_ADDRESS_LENGTH];\r\n      ThreadLocalRandom.current().nextBytes(macAddresses[i]);\r\n    }\r\n    Path path = Files.createTempDirectory(\"bench_id\");\r\n    IndexWriterConfig config = new IndexWriterConfig()\r\n        .setRAMBufferSizeMB(512);\r\n    ThreadPoolExecutor threadPool = new ThreadPoolExecutor(NUM_INDEXING_THREADS - 1, NUM_INDEXING_THREADS - 1,\r\n        10, TimeUnit.MINUTES, new ArrayBlockingQueue<>(NUM_INDEXING_THREADS), new CallerRunsPolicy());\r\n    try (Directory dir = FSDirectory.open(path); IndexWriter w = new IndexWriter(dir, config)) {\r\n      for (int remainingDocs = numDocs; remainingDocs > 0; ) {\r\n        final int batchSize = Math.min(remainingDocs, BATCH_SIZE);\r\n        final byte[] macAddress = macAddresses[ThreadLocalRandom.current().nextInt(numNodes)];\r\n        threadPool.submit(new Runnable() {\r\n          @Override\r\n          public void run() {\r\n            byte[] id = new byte[ID_LENGTH];\r\n            StringField field = new StringField(\"_id\", new BytesRef(id), Store.NO);\r\n            Document doc = new Document();\r\n            doc.add(field);\r\n            try {\r\n              for (int i = 0; i < batchSize; ++i) {\r\n                idGenerator.accept(macAddress, id);\r\n                w.addDocument(doc);\r\n              }\r\n            } catch (IOException e) {\r\n              throw new UncheckedIOException(e);\r\n            }\r\n          }\r\n        });\r\n        remainingDocs -= batchSize;\r\n      }\r\n      threadPool.shutdown();\r\n      threadPool.awaitTermination(10, TimeUnit.MINUTES);\r\n      w.forceMerge(1);\r\n      long diskSize = 0;\r\n      for (String file : dir.listAll()) {\r\n        diskSize += dir.fileLength(file);\r\n      }\r\n      final long memorySize;\r\n      try (IndexReader reader = DirectoryReader.open(w)) {\r\n        if (reader.numDocs() != numDocs || reader.leaves().size() != 1) {\r\n          throw new Error();\r\n        }\r\n        SegmentReader onlySegmentReader = (SegmentReader) reader.leaves().get(0).reader();\r\n        memorySize = onlySegmentReader.ramBytesUsed();\r\n      }\r\n      return new ResourceUsage((double) diskSize / numDocs, (double) memorySize / numDocs);\r\n    } finally {\r\n      try {\r\n        IOUtils.rm(path);\r\n      } finally {\r\n        threadPool.shutdownNow();\r\n      }\r\n    }\r\n  }\r\n\r\n  public static void main(String[] args) throws Exception {\r\n    System.out.println(\"num_docs\\tnum_nodes\\tdocs/s\\tformat\\tdisk/doc\\tmemory/doc\");\r\n    for (int numDocs : new int[] { 50_000_000 }) {\r\n      for (int numNodes : new int[] { 1, 5 }) {\r\n        for (int docsPerSecond: new int[] { 100, 1000, 10000 }) {\r\n          for (IDFormat idFormat : new IDFormat[] { FLAKE_IDS, CURRENT_IDS }) {\r\n            BiConsumer<byte[], byte[]> idGenerator = idGenerator(idFormat, docsPerSecond);\r\n            ResourceUsage resourceUsage = simulateResourceUsage(numDocs, idGenerator, numNodes);\r\n            System.out.println(String.format(\"%d\\t%d\\t%d\\t%s\\t%2f\\t%2f\",\r\n                numDocs, numNodes, docsPerSecond, idFormat.toString(),\r\n                resourceUsage.diskUsagePerDoc, resourceUsage.memoryUsagePerDoc));\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/450665184","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-450665184","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":450665184,"node_id":"MDEyOklzc3VlQ29tbWVudDQ1MDY2NTE4NA==","user":{"login":"micoq","id":1741616,"node_id":"MDQ6VXNlcjE3NDE2MTY=","avatar_url":"https://avatars3.githubusercontent.com/u/1741616?v=4","gravatar_id":"","url":"https://api.github.com/users/micoq","html_url":"https://github.com/micoq","followers_url":"https://api.github.com/users/micoq/followers","following_url":"https://api.github.com/users/micoq/following{/other_user}","gists_url":"https://api.github.com/users/micoq/gists{/gist_id}","starred_url":"https://api.github.com/users/micoq/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/micoq/subscriptions","organizations_url":"https://api.github.com/users/micoq/orgs","repos_url":"https://api.github.com/users/micoq/repos","events_url":"https://api.github.com/users/micoq/events{/privacy}","received_events_url":"https://api.github.com/users/micoq/received_events","type":"User","site_admin":false},"created_at":"2018-12-31T16:26:57Z","updated_at":"2018-12-31T16:26:57Z","author_association":"NONE","body":"A smaller memory footprint for the `_id` field would be a great improvement especially for indices containing  time series.\r\n\r\nHere is a small program which directly uses the finite state transducers classes from Lucene and returns their memory size for 100000 identifiers with different formats and sequences.\r\n\r\nWithout surprise, shared prefixes are efficiently compressed (344 bytes for the padded format with pure incremental ids) whereas high cardinality prefixes take a lot of memory (3MB with random ids !)\r\n\r\n```\r\nimport java.io.IOException;\r\nimport java.nio.file.Paths;\r\nimport java.util.Random;\r\n\r\nimport org.apache.lucene.util.BytesRef;\r\nimport org.apache.lucene.util.BytesRefBuilder;\r\nimport org.apache.lucene.util.IntsRefBuilder;\r\nimport org.apache.lucene.util.fst.Builder;\r\nimport org.apache.lucene.util.fst.ByteSequenceOutputs;\r\nimport org.apache.lucene.util.fst.FST;\r\nimport org.apache.lucene.util.fst.FST.INPUT_TYPE;\r\nimport org.apache.lucene.util.fst.Outputs;\r\nimport org.apache.lucene.util.fst.Util;\r\n\r\npublic class TestFST {\r\n  \r\n  public static interface IDSequence {\r\n    public long nextId();\r\n  }\r\n  \r\n  public static final IDSequence RandomIncrementalSequence = new IDSequence() {\r\n    private long current = 0;\r\n    private Random random = new Random(0);\r\n\r\n    @Override\r\n    public long nextId() {\r\n      long n = current;\r\n      current += random.nextInt(1000)+1;\r\n      return n;\r\n    }\r\n    \r\n    @Override\r\n    public String toString() {\r\n      return \"RandomIncrementalSequence\";\r\n    }\r\n  };\r\n  \r\n  public static final IDSequence IncrementalSequence = new IDSequence() {\r\n    private long current = 0;\r\n\r\n    @Override\r\n    public long nextId() {\r\n      return current++;\r\n    }\r\n    \r\n    @Override\r\n    public String toString() {\r\n      return \"IncrementalSequence\";\r\n    }\r\n  };\r\n  \r\n  public static final IDSequence RandomSequence = new IDSequence() {\r\n    private Random random = new Random(0);\r\n\r\n    @Override\r\n    public long nextId() {\r\n      return random.nextLong();\r\n    }\r\n    \r\n    @Override\r\n    public String toString() {\r\n      return \"RandomSequence\";\r\n    }\r\n  };\r\n\r\n  public static interface IDFormat {\r\n    public void setId(BytesRefBuilder builder, long id);\r\n  }\r\n  \r\n  public static final IDFormat NoPaddedId = new IDFormat() {\r\n    @Override\r\n    public void setId(BytesRefBuilder builder, long id) {\r\n      builder.copyChars(String.format(\"%d\",id));\r\n    }\r\n    \r\n    @Override\r\n    public String toString() {\r\n      return \"NoPaddedId\";\r\n    }\r\n  };\r\n  \r\n  public static final IDFormat PaddedId = new IDFormat() {\r\n    @Override\r\n    public void setId(BytesRefBuilder builder, long id) {\r\n      builder.copyChars(String.format(\"%020d\",id));\r\n    }\r\n    \r\n    @Override\r\n    public String toString() {\r\n      return \"PaddedId\";\r\n    }\r\n  };\r\n  \r\n  public static final IDFormat PaddedInvertedId = new IDFormat() {\r\n    @Override\r\n    public void setId(BytesRefBuilder builder, long id) {\r\n      builder.copyChars(new StringBuilder(String.format(\"%020d\",id)).reverse());\r\n    }\r\n    \r\n    @Override\r\n    public String toString() {\r\n      return \"InvertedId\";\r\n    }\r\n  };\r\n  \r\n  public static final IDFormat PaddedAndConstantSuffixId = new IDFormat() {\r\n    @Override\r\n    public void setId(BytesRefBuilder builder, long id) {\r\n      builder.copyChars(String.format(\"%020dABCDEFGHIJ\",id));\r\n    }\r\n    \r\n    @Override\r\n    public String toString() {\r\n      return \"PaddedAndConstantSuffixId\";\r\n    }\r\n  };\r\n  \r\n  public static void main(String[] args) throws IOException {\r\n    \r\n    IDFormat[] formats = new IDFormat[] {\r\n        PaddedId,NoPaddedId,PaddedInvertedId,PaddedAndConstantSuffixId\r\n    };\r\n    \r\n    IDSequence[] sequences = new IDSequence[] {\r\n        RandomSequence,IncrementalSequence,RandomIncrementalSequence\r\n    };\r\n    \r\n    Outputs<BytesRef> outputs = ByteSequenceOutputs.getSingleton();\r\n\r\n    for(IDSequence sequence : sequences) {\r\n      for(IDFormat format : formats) {\r\n        \r\n        BytesRefBuilder idBuilder = new BytesRefBuilder();\r\n        \r\n        Builder<BytesRef> FSTbuilder = new Builder<>(INPUT_TYPE.BYTE1, outputs);\r\n        IntsRefBuilder scratch = new IntsRefBuilder();\r\n        \r\n        BytesRef dummyOutput = new BytesRef();\r\n        \r\n        for (int i = 0; i < 100000; i++) {\r\n          format.setId(idBuilder, sequence.nextId());\r\n          try {\r\n            FSTbuilder.add(Util.toIntsRef(idBuilder.get(), scratch), dummyOutput);\r\n          }\r\n          catch (UnsupportedOperationException ex) {\r\n            // Cannot add the same value two times\r\n            System.out.println(\"Oh NO\");\r\n          }\r\n          idBuilder.clear();\r\n        }\r\n        \r\n        FST fst = FSTbuilder.finish(); // to compress the FST (reduce the terms number to zero)\r\n        //fst.save(Paths.get(\"/tmp/fst.bin\")); // to save the FST in a file\r\n        System.out.println(String.format(\"[%s,%s] Terms: %d, Nodes: %d, States: %d, Size: %d\",\r\n            sequence,\r\n            format,\r\n            FSTbuilder.getTermCount(),\r\n            FSTbuilder.getNodeCount(),\r\n            FSTbuilder.getMappedStateCount(),\r\n            FSTbuilder.fstRamBytesUsed()));\r\n      }\r\n    }\r\n  }\r\n}\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/498439643","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-498439643","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":498439643,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5ODQzOTY0Mw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-06-03T21:50:10Z","updated_at":"2019-06-03T21:50:10Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-core-infra","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/588859246","html_url":"https://github.com/elastic/elasticsearch/issues/33049#issuecomment-588859246","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33049","id":588859246,"node_id":"MDEyOklzc3VlQ29tbWVudDU4ODg1OTI0Ng==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2020-02-20T09:42:42Z","updated_at":"2020-02-20T09:42:42Z","author_association":"CONTRIBUTOR","body":"Closed via #52405.","performed_via_github_app":null}]