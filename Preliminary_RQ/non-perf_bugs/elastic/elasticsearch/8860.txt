{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/8860","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8860/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8860/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8860/events","html_url":"https://github.com/elastic/elasticsearch/issues/8860","id":51501091,"node_id":"MDU6SXNzdWU1MTUwMTA5MQ==","number":8860,"title":"\"IMMEDIATE\" tasks getting queued up in pending tasks","user":{"login":"ppf2","id":7216393,"node_id":"MDQ6VXNlcjcyMTYzOTM=","avatar_url":"https://avatars0.githubusercontent.com/u/7216393?v=4","gravatar_id":"","url":"https://api.github.com/users/ppf2","html_url":"https://github.com/ppf2","followers_url":"https://api.github.com/users/ppf2/followers","following_url":"https://api.github.com/users/ppf2/following{/other_user}","gists_url":"https://api.github.com/users/ppf2/gists{/gist_id}","starred_url":"https://api.github.com/users/ppf2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ppf2/subscriptions","organizations_url":"https://api.github.com/users/ppf2/orgs","repos_url":"https://api.github.com/users/ppf2/repos","events_url":"https://api.github.com/users/ppf2/events{/privacy}","received_events_url":"https://api.github.com/users/ppf2/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2014-12-10T00:11:09Z","updated_at":"2015-11-21T22:26:51Z","closed_at":"2015-11-21T22:26:51Z","author_association":"MEMBER","active_lock_reason":null,"body":"Have a situation where the cluster had to be restarted.  Upon restarting, there was a ton of recovery activity (at times, we observed >100 `EMERGENCY` tasks in pending_tasks).  As a result, attempts to update the cluster (eg. to increase the concurrency setting for recovery) started failing with \n`ProcessClusterEventTimeoutException` errors.\n\n```\nRequest:\n\n{\"transient\":{\"cluster.routing.allocation.node_concurrent_recoveries\": 10}}\n\nResponse:\n\nHTTP/1.1 503 Service Unavailable\n\n{\"error\":\"RemoteTransportException[[Rush][inet[/IP:9300]][cluster/settings/update]]; nested: ProcessClusterEventTimeoutException[failed to process cluster event (cluster_update_settings) within 30s]; \",\"status\":503}\n```\n\nhttps://github.com/elasticsearch/elasticsearch/blob/1816951b6b0320e7a011436c7c7519ec2bfabc6e/src/main/java/org/elasticsearch/common/Priority.java#L45 seems to indicate that `IMMEDIATE` tasks are of higher priority of `EMERGENCY` tasks.  But for some reason, these update cluster setting calls are being queued up still.\n\n```\nPartial pending_tasks output:\n\n{ \n\"insert_order\" : 2125, \n\"priority\" : \"IMMEDIATE\", \n\"source\" : \"cluster_update_settings\", \n\"executing\" : false, \n\"time_in_queue_millis\" : 5908, \n\"time_in_queue\" : \"5.9s\" \n}\n\n{ \n\"insert_order\" : 1949, \n\"priority\" : \"URGENT\", \n\"source\" : \"shard-started ([index20141116][8], node[nodeID], [P], s[INITIALIZING]), reason [master [Rush][nodeID][node][inet[/172.16.0.6:9300]]{http=false, data=false, master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]\", \n\"executing\" : true, \n\"time_in_queue_millis\" : 2463692, \n\"time_in_queue\" : \"41m\" \n}\n```\n\nIs the cluster too busy to even go and re-prioritize its running tasks?  Or is it because once an `EMERGENCY` task starts to run, even if an `IMMEDIATE` task comes in, it will still have to wait till these running `EMERGENCY` tasks have completed?  In other words, if the `IMMEDIATE` task comes in at the same time as an `EMERGENCY` task, it will get prioritized higher, but it will not go and suspend any running `EMERGENCY` task to allow for the `IMMEDIATE` task to run first, etc..?\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}