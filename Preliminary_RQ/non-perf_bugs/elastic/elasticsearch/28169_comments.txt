[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/357174635","html_url":"https://github.com/elastic/elasticsearch/issues/28169#issuecomment-357174635","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28169","id":357174635,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NzE3NDYzNQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-01-12T08:31:24Z","updated_at":"2018-01-12T08:31:24Z","author_association":"CONTRIBUTOR","body":"@tlrx I think I know what happened. Even though the logs don't provide evidence for the whole scenario, what's in the logs makes the following quite plausible. The test deliberately causes a `FailedToCommitException` in `ZenDiscovery.publish(...)`:\r\n\r\n```\r\n  1> [2018-01-10T05:23:40,406][WARN ][o.e.t.d.TestZenDiscovery ] [node_tm1] zen-disco-failed-to-publish, current nodes: nodes: \r\n  1>    {node_tm1}{05tuxEPVT5ypY7Mz6o_VRA}{p_hm4MVNRoOvl-n281Co2A}{127.0.0.1}{127.0.0.1:30101}, local, master\r\n  1>    {node_td3}{Pvyfq38HSLqL04-F3jFIMw}{hnOj7wrPQNyK8PobpXXwIg}{127.0.0.1}{127.0.0.1:30103}\r\n  1>    {node_tm2}{nPk7Z6bvRI-MufskXAhGTw}{-NZTnhd1SrSolB7FjkDwcg}{127.0.0.1}{127.0.0.1:30102}\r\n  1>    {node_tm0}{JvhDBfzERq6UD13DUGBUJg}{Grp3AjfZR4m4Wb4Re-p5MA}{127.0.0.1}{127.0.0.1:30100}\r\n\r\n  1> [2018-01-10T05:23:40,407][WARN ][o.e.c.s.MasterService    ] [node_tm1] failing [update_snapshot [test-repo:test-snap-2/vyIDc0GFSIKVtGd--HP_hQ]]: failed to commit cluster state version [25]\r\n  1> org.elasticsearch.discovery.Discovery$FailedToCommitClusterStateException: timed out while waiting for enough masters to ack sent cluster state. [1] left\r\n  1> \tat org.elasticsearch.discovery.zen.PublishClusterStateAction$SendingController.waitForCommit(PublishClusterStateAction.java:532) ~[main/:?]\r\n  1> \tat org.elasticsearch.discovery.zen.PublishClusterStateAction.innerPublish(PublishClusterStateAction.java:196) ~[main/:?]\r\n  1> \tat org.elasticsearch.discovery.zen.PublishClusterStateAction.publish(PublishClusterStateAction.java:161) ~[main/:?]\r\n  1> \tat org.elasticsearch.discovery.zen.ZenDiscovery.publish(ZenDiscovery.java:338) ~[main/:?]\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:226) [main/:?]\r\n  1> \tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:133) [main/:?]\r\n```\r\n\r\nWhat happens in that case in `ZenDiscovery.publish(...)` is that the `rejoin` method is called. This in turn tells `ClusterApplierService` to submit an update task to apply the last cluster state from ZenDiscovery (`clusterApplier.onNewClusterState(...)`), the cluster state which has the master set to null. `ZenDiscovery` / `MasterService` does not wait for the result of that, but returns to the caller. The test has meanwhile executed the command `ensureStableCluster(4, masterNode1);` which sends a cluster health request to the node that just failed to commit. This is handled by `TransportClusterHealthAction`, a subclass of `TransportMasterNodeAction` for which we see the stackoverflow error. The request that was sent has `request.waitForEvents() != null` and `request.local() == false`, hence it is queued on the MasterService with the call `clusterService.submitStateUpdateTask(...)`. The node is not master anymore according to `ZenDiscovery`, hence `MasterService` will call `onNoLongerMaster`, which in turn calls `doExecute` on the same `ClusterHealthAction`. Let's see what happens there. This method is implemented by `TransportMasterNodeAction`. The node gets the latest state from the `ClusterApplierService` to determine if the current node is a master node or not. If the `ClusterApplierService` has not applied the cluster state yet that was submitted above by the `rejoin` method in `ZenDiscovery`, then it still looks like the node is a master. It will therefore wrap the current listener in a delegate and then call `masterOperation` with that delegate (in the same thread, as `TransportClusterHealthAction.executor() == ThreadPool.Names.SAME`. This in turn will submit an update task on the `MasterService`, which will call `onNoLongerMaster` again on it's master state update thread. The same cycle continues and continues. There is no stackoverflow yet as we're switching threads each time we submit an update task to `MasterService`, even though each time we wrap the response listener. At some point `ClusterApplierService` has successfully applied the cluster state that has the info that the node is no longer master. `TransportMasterNodeAction` would wait up to 30 seconds for a master to appear. Note that `node_tm1` has seen and joined the master after 5 seconds (at the time the stackoverflow happens):\r\n\r\n```\r\n[2018-01-10T05:23:45,696][INFO ][o.e.c.s.ClusterApplierService] [node_tm1] detected_master {node_tm0}{JvhDBfzERq6UD13DUGBUJg}{Grp3AjfZR4m4Wb4Re-p5MA}{127.0.0.1}{127.0.0.1:30100}, reason: apply cluster state (from master [master {node_tm0}{JvhDBfzERq6UD13DUGBUJg}{Grp3AjfZR4m4Wb4Re-p5MA}{127.0.0.1}{127.0.0.1:30100} committed version [33]])\r\n```\r\n\r\n`TransportMasterNodeAction` reroutes the health request to the new master, where it gets a successful response (all nodes have rejoined the cluster). Now it calls `listener.onResponse` on `node_tm1` which is this object which has many times wrapped the original listener, each object calling `listener.onResponse` on the inner object that it wraps. This leads to the stackoverflow.\r\n\r\nNow to the question what could have caused it or why we're not seeing this more often. What's odd is that it took such a long time for `ClusterApplierService` to apply the cluster state that just contains the info that the master is not master anymore. Unfortunately the logs don't show when that cluster state was applied. The only indication is that there are some GC logs showing all nodes in distress and also the log line above could indicate that the previous cluster just finished being applied. Also note that when we apply a cluster state, we validate connectivity to the nodes in the cluster state, independent of whether that cluster state has a master or not (see `nodeConnectionsService.connectToNodes(...)` in `ClusterApplierService`). We do this blockingly in `NodeConnectionsService` and this might have caused the cluster state from taking a while to be applied.\r\n\r\nWhat fixes should we do? The first thing we should change is for `TransportClusterHealthAction` not to directly call `doExecute` but leave the retry logic to its superclass `TransportMasterNodeAction` by calling `listener.onFailure` instead. We could also possibly not validate connections when applying a cluster state that has no master (i.e. this would ensure that the cluster state is timely applied). Another option (I'm less excited by this one) would be to wait in `ZenDiscovery.rejoin` for the cluster state to be fully applied before returning control to the `MasterService`. I'll look further into those options and open PRs.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/357190053","html_url":"https://github.com/elastic/elasticsearch/issues/28169#issuecomment-357190053","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28169","id":357190053,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NzE5MDA1Mw==","user":{"login":"tlrx","id":642733,"node_id":"MDQ6VXNlcjY0MjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/642733?v=4","gravatar_id":"","url":"https://api.github.com/users/tlrx","html_url":"https://github.com/tlrx","followers_url":"https://api.github.com/users/tlrx/followers","following_url":"https://api.github.com/users/tlrx/following{/other_user}","gists_url":"https://api.github.com/users/tlrx/gists{/gist_id}","starred_url":"https://api.github.com/users/tlrx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tlrx/subscriptions","organizations_url":"https://api.github.com/users/tlrx/orgs","repos_url":"https://api.github.com/users/tlrx/repos","events_url":"https://api.github.com/users/tlrx/events{/privacy}","received_events_url":"https://api.github.com/users/tlrx/received_events","type":"User","site_admin":false},"created_at":"2018-01-12T09:39:37Z","updated_at":"2018-01-12T09:39:37Z","author_association":"MEMBER","body":"@ywelsch Thanks for the detailed comment, I understand the execution flow and I agree that it can explain the StackOverflowError.\r\n\r\n> I'll look further into those options and open PRs.\r\n\r\nPlease let me know if/how I can help. Also, feel free to reassign this issue to you if you want.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/360775428","html_url":"https://github.com/elastic/elasticsearch/issues/28169#issuecomment-360775428","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28169","id":360775428,"node_id":"MDEyOklzc3VlQ29tbWVudDM2MDc3NTQyOA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-01-26T12:45:05Z","updated_at":"2018-01-26T12:45:05Z","author_association":"CONTRIBUTOR","body":"I think we can close this. The StackOverflowError should be addressed by #28195","performed_via_github_app":null}]