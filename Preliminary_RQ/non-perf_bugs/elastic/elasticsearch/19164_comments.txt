[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/230617036","html_url":"https://github.com/elastic/elasticsearch/issues/19164#issuecomment-230617036","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19164","id":230617036,"node_id":"MDEyOklzc3VlQ29tbWVudDIzMDYxNzAzNg==","user":{"login":"krystalcode","id":301364,"node_id":"MDQ6VXNlcjMwMTM2NA==","avatar_url":"https://avatars2.githubusercontent.com/u/301364?v=4","gravatar_id":"","url":"https://api.github.com/users/krystalcode","html_url":"https://github.com/krystalcode","followers_url":"https://api.github.com/users/krystalcode/followers","following_url":"https://api.github.com/users/krystalcode/following{/other_user}","gists_url":"https://api.github.com/users/krystalcode/gists{/gist_id}","starred_url":"https://api.github.com/users/krystalcode/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krystalcode/subscriptions","organizations_url":"https://api.github.com/users/krystalcode/orgs","repos_url":"https://api.github.com/users/krystalcode/repos","events_url":"https://api.github.com/users/krystalcode/events{/privacy}","received_events_url":"https://api.github.com/users/krystalcode/received_events","type":"User","site_admin":false},"created_at":"2016-07-05T22:05:57Z","updated_at":"2016-07-05T22:05:57Z","author_association":"NONE","body":"The situation corresponding to the second log entry can be reproduced as described here: https://github.com/elastic/elasticsearch/issues/19275\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/230748919","html_url":"https://github.com/elastic/elasticsearch/issues/19164#issuecomment-230748919","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19164","id":230748919,"node_id":"MDEyOklzc3VlQ29tbWVudDIzMDc0ODkxOQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-07-06T11:42:56Z","updated_at":"2016-07-06T11:42:56Z","author_association":"CONTRIBUTOR","body":"I think the disk watermark message was logged every time it tried to assign a shard.  Given that the shard is corrupt, this happens very frequently.  @dakrone is this the case?\n\nNote: https://github.com/elastic/elasticsearch/pull/18467 should have stopped trying to reallocate the shard after five attempts, which would have stopped this excessive logging i think?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/230753966","html_url":"https://github.com/elastic/elasticsearch/issues/19164#issuecomment-230753966","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19164","id":230753966,"node_id":"MDEyOklzc3VlQ29tbWVudDIzMDc1Mzk2Ng==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-07-06T12:11:35Z","updated_at":"2016-07-06T12:11:35Z","author_association":"CONTRIBUTOR","body":"The reason that #18467 does not stop reallocating this shard (failed_attempts[14]) is because it's a primary shard. For primary shards we currently force allocation if there are only shard copies on nodes where the deciders say no. The reason for this is to ensure for example that a primary is still allocated even if the node is above the low watermark. This does not play nicely with the allocation decider that checks the number of allocation attempts, however. We should rethink how allocation deciders work in case of primary shard allocation.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/230922132","html_url":"https://github.com/elastic/elasticsearch/issues/19164#issuecomment-230922132","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19164","id":230922132,"node_id":"MDEyOklzc3VlQ29tbWVudDIzMDkyMjEzMg==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2016-07-06T22:08:21Z","updated_at":"2016-07-06T22:08:21Z","author_association":"MEMBER","body":"> I think the disk watermark message was logged every time it tried to assign a shard. @dakrone is this the case?\n\nThis message (the exceeded message) should only occur when a new cluster info is retrieved and the node is still over the limit. By default this is every 30 seconds, or whenever a new data node has joined. I just tested this locally and I get the message every 30 seconds, as expected.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/230948134","html_url":"https://github.com/elastic/elasticsearch/issues/19164#issuecomment-230948134","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19164","id":230948134,"node_id":"MDEyOklzc3VlQ29tbWVudDIzMDk0ODEzNA==","user":{"login":"krystalcode","id":301364,"node_id":"MDQ6VXNlcjMwMTM2NA==","avatar_url":"https://avatars2.githubusercontent.com/u/301364?v=4","gravatar_id":"","url":"https://api.github.com/users/krystalcode","html_url":"https://github.com/krystalcode","followers_url":"https://api.github.com/users/krystalcode/followers","following_url":"https://api.github.com/users/krystalcode/following{/other_user}","gists_url":"https://api.github.com/users/krystalcode/gists{/gist_id}","starred_url":"https://api.github.com/users/krystalcode/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krystalcode/subscriptions","organizations_url":"https://api.github.com/users/krystalcode/orgs","repos_url":"https://api.github.com/users/krystalcode/repos","events_url":"https://api.github.com/users/krystalcode/events{/privacy}","received_events_url":"https://api.github.com/users/krystalcode/received_events","type":"User","site_admin":false},"created_at":"2016-07-07T00:38:11Z","updated_at":"2016-07-07T00:38:11Z","author_association":"NONE","body":"@clintongormley unfortunately I have not kept these logs. However, from what I remember, the disk watermark message was present without the failed shard recovery message and could be unrelated. The failed shard recovery message appeared later on the log, due to https://github.com/elastic/elasticsearch/issues/19275, after a restart. By that time, I had probably cleaned up some disk space and I believe I was not getting the high watermark message any more.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/372883914","html_url":"https://github.com/elastic/elasticsearch/issues/19164#issuecomment-372883914","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19164","id":372883914,"node_id":"MDEyOklzc3VlQ29tbWVudDM3Mjg4MzkxNA==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2018-03-14T02:27:59Z","updated_at":"2018-03-14T02:27:59Z","author_association":"MEMBER","body":"We handle disk-full events more gracefully now since #25541. I am closing this issue, please feel free to open a new issue if an actionable issue persists.","performed_via_github_app":null}]