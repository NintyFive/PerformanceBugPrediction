[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/152726558","html_url":"https://github.com/elastic/elasticsearch/issues/14399#issuecomment-152726558","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14399","id":152726558,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MjcyNjU1OA==","user":{"login":"monotek","id":1069919,"node_id":"MDQ6VXNlcjEwNjk5MTk=","avatar_url":"https://avatars1.githubusercontent.com/u/1069919?v=4","gravatar_id":"","url":"https://api.github.com/users/monotek","html_url":"https://github.com/monotek","followers_url":"https://api.github.com/users/monotek/followers","following_url":"https://api.github.com/users/monotek/following{/other_user}","gists_url":"https://api.github.com/users/monotek/gists{/gist_id}","starred_url":"https://api.github.com/users/monotek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/monotek/subscriptions","organizations_url":"https://api.github.com/users/monotek/orgs","repos_url":"https://api.github.com/users/monotek/repos","events_url":"https://api.github.com/users/monotek/events{/privacy}","received_events_url":"https://api.github.com/users/monotek/received_events","type":"User","site_admin":false},"created_at":"2015-10-31T11:47:04Z","updated_at":"2015-10-31T12:21:17Z","author_association":"NONE","body":"After some time one of the nodes has even higher load (> 20), the other 2 nodes get load 0.1 and elasticsearch does not work anymore. If i try to stop elasticsearch on the node with the high load, it becomeas a zombie.\n\nSyslog for the node with the high load says:\n\nOct 30 21:13:03 es1 kernel: [22200.884124] INFO: task jbd2/vdb1-8:307 blocked for more than 120 seconds.\nOct 30 21:13:03 es1 kernel: [22200.892344]       Not tainted 3.13.0-66-generic #108-Ubuntu\nOct 30 21:13:03 es1 kernel: [22200.898723] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nOct 30 21:13:03 es1 kernel: [22200.904917] jbd2/vdb1-8     D ffff88023fd13180     0   307      2 0x00000000\nOct 30 21:13:03 es1 kernel: [22200.904922]  ffff8802322bdcb0 0000000000000046 ffff8802322ab000 ffff8802322bdfd8\nOct 30 21:13:03 es1 kernel: [22200.904925]  0000000000013180 0000000000013180 ffff8802322ab000 ffff8802322bdd98\nOct 30 21:13:03 es1 kernel: [22200.904931]  ffff8800370480b8 ffff8802322ab000 ffff88013eef2800 ffff8802322bdd80\nOct 30 21:13:03 es1 kernel: [22200.904934] Call Trace:\nOct 30 21:13:03 es1 kernel: [22200.904969]  [<ffffffff81728499>] schedule+0x29/0x70\nOct 30 21:13:03 es1 kernel: [22200.9ies completly and the oth04988]  [<ffffffff8128ac67>] jbd2_journal_commit_transaction+0x287/0x1ab0\nOct 30 21:13:03 es1 kernel: [22200.905001]  [<ffffffff810a3234>] ? update_curr+0xe4/0x180\nOct 30 21:13:03 es1 kernel: [22200.905007]  [<ffffffff810ab390>] ? prepare_to_wait_event+0x100/0x100\nOct 30 21:13:03 es1 kernel: [22200.905014]  [<ffffffff8107484b>] ? lock_timer_base.isra.35+0x2b/0x50\nOct 30 21:13:03 es1 kernel: [22200.905017]  [<ffffffff810756ff>] ? try_to_del_timer_sync+0x4f/0x70\nOct 30 21:13:03 es1 kernel: [22200.905020]  [<ffffffff812905fd>] kjournald2+0xbd/0x250\nOct 30 21:13:03 es1 kernel: [22200.905023]  [<ffffffff810ab390>] ? prepare_to_wait_event+0x100/0x100\nOct 30 21:13:03 es1 kernel: [22200.905025]  [<ffffffff81290540>] ? commit_timeout+0x10/0x10\nOct 30 21:13:03 es1 kernel: [22200.905033]  [<ffffffff8108b7d2>] kthread+0xd2/0xf0\nOct 30 21:13:03 es1 kernel: [22200.905036]  [<ffffffff8108b700>] ? kthread_create_on_node+0x1c0/0x1c0\nOct 30 21:13:03 es1 kernel: [22200.905041]  [<ffffffff81734ba8>] ret_from_fork+0x58/0x90\nOct 30 21:13:03 es1 kernel: [22200.905044]  [<ffffffff8108b700>] ? kthread_create_on_node+0x1c0/0x1c0\n\nNothing for this time in elasticsearch.log\n\nThe elasticsearch log of the other nodes say things like:\n\n[2015-10-31 03:50:02,299][INFO ][rest.suppressed          ] //otrs-2015.10/tickets/19088107 Params: {index=otrs-2015.10, id=19088107, type=tickets}\nRemoteTransportException[[es1][192.168.10.61:9300][indices:data/write/index]]; nested: EsRejectedExecutionException[rejected execution of org.elasticsearch.action.support.replication.TransportRep\nlicationAction$PrimaryPhase$1@134e54bc on EsThreadPoolExecutor[index, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@1205638d[Running, pool size = 2, active t\nhreads = 2, queued tasks = 200, completed tasks = 42298]]];\nCaused by: EsRejectedExecutionException[rejected execution of org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1@134e54bc on EsThreadPoolExecutor[index, queue \ncapacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@1205638d[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 42298]]]\n\n[2015-10-31 12:51:03,567][WARN ][discovery.zen.ping.unicast] [es3] [12] failed send ping to {es1}{egOv1fGiRvmxV8BrvXfpnQ}{192.168.10.61}{192.168.10.61:9300}{max_local_storage_nodes=1}\njava.lang.IllegalStateException: can't add nodes to a stopped transport\n        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:833)\n        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:828)\n        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:243)\n        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:379)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/152744759","html_url":"https://github.com/elastic/elasticsearch/issues/14399#issuecomment-152744759","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14399","id":152744759,"node_id":"MDEyOklzc3VlQ29tbWVudDE1Mjc0NDc1OQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-10-31T15:49:46Z","updated_at":"2015-10-31T15:49:46Z","author_association":"MEMBER","body":"In 2.0 we change the default behavior of the translog to fsync on every operation. Before it was fsync-ing every 5 seconds (by default), in order to strike a good balance between performance and durability (i.e., data loss when all shard copies loose power). We recently measured the impact fsync has on bulk operation and have discovered that it was greatly reduced in the years since the 5 second decision was made. As such, we decided to increase safety change the default. Note that we do this smartly and try to share fsyncs among as many operations as we can.\n\nFrom your rejection exception I see you are using the single doc indexing operation, which means that the fsync cost is not well amortized. I suggest you move to the _bulk api if possible. If not you can disable the new behavior by setting `index.translog.durability` to `async` (see https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html#_translog_settings  )\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/152765742","html_url":"https://github.com/elastic/elasticsearch/issues/14399#issuecomment-152765742","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14399","id":152765742,"node_id":"MDEyOklzc3VlQ29tbWVudDE1Mjc2NTc0Mg==","user":{"login":"monotek","id":1069919,"node_id":"MDQ6VXNlcjEwNjk5MTk=","avatar_url":"https://avatars1.githubusercontent.com/u/1069919?v=4","gravatar_id":"","url":"https://api.github.com/users/monotek","html_url":"https://github.com/monotek","followers_url":"https://api.github.com/users/monotek/followers","following_url":"https://api.github.com/users/monotek/following{/other_user}","gists_url":"https://api.github.com/users/monotek/gists{/gist_id}","starred_url":"https://api.github.com/users/monotek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/monotek/subscriptions","organizations_url":"https://api.github.com/users/monotek/orgs","repos_url":"https://api.github.com/users/monotek/repos","events_url":"https://api.github.com/users/monotek/events{/privacy}","received_events_url":"https://api.github.com/users/monotek/received_events","type":"User","site_admin":false},"created_at":"2015-10-31T19:33:37Z","updated_at":"2015-10-31T19:47:03Z","author_association":"NONE","body":"thanks for the hint. i will have a look into bulk indexing of the php library i use.\n\ni also had some help from the forum:\nhttps://discuss.elastic.co/t/elasticsearch-crashes-after-update-to-2-0/33437/8\n\nindeed it seems that it was an io problem.\n\nmy path.data dir was already an own partition which mounts a glusterfs volume via libgfapi to prevent context switches of fuse mounts. it looks like this was still not fast enough for the new fsync implementation in 2.0.\n\nthe mount options \"data=writeback,relatime,nobarrier\" took a lot of io pressure from the system now.\nload seems to be normal again.\n\nclosing this bug for now...\n","performed_via_github_app":null}]