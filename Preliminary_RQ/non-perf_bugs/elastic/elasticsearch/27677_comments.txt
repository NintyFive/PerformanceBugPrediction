[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/349437042","html_url":"https://github.com/elastic/elasticsearch/issues/27677#issuecomment-349437042","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27677","id":349437042,"node_id":"MDEyOklzc3VlQ29tbWVudDM0OTQzNzA0Mg==","user":{"login":"msimos","id":11743255,"node_id":"MDQ6VXNlcjExNzQzMjU1","avatar_url":"https://avatars3.githubusercontent.com/u/11743255?v=4","gravatar_id":"","url":"https://api.github.com/users/msimos","html_url":"https://github.com/msimos","followers_url":"https://api.github.com/users/msimos/followers","following_url":"https://api.github.com/users/msimos/following{/other_user}","gists_url":"https://api.github.com/users/msimos/gists{/gist_id}","starred_url":"https://api.github.com/users/msimos/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/msimos/subscriptions","organizations_url":"https://api.github.com/users/msimos/orgs","repos_url":"https://api.github.com/users/msimos/repos","events_url":"https://api.github.com/users/msimos/events{/privacy}","received_events_url":"https://api.github.com/users/msimos/received_events","type":"User","site_admin":false},"created_at":"2017-12-05T20:49:42Z","updated_at":"2017-12-05T20:49:42Z","author_association":"MEMBER","body":"Also if you already have the transient settings set to all, then setting persistent in Step 1 I assume has no effect because transient takes precedent.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/356593608","html_url":"https://github.com/elastic/elasticsearch/issues/27677#issuecomment-356593608","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27677","id":356593608,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NjU5MzYwOA==","user":{"login":"voegelas","id":1150045,"node_id":"MDQ6VXNlcjExNTAwNDU=","avatar_url":"https://avatars0.githubusercontent.com/u/1150045?v=4","gravatar_id":"","url":"https://api.github.com/users/voegelas","html_url":"https://github.com/voegelas","followers_url":"https://api.github.com/users/voegelas/followers","following_url":"https://api.github.com/users/voegelas/following{/other_user}","gists_url":"https://api.github.com/users/voegelas/gists{/gist_id}","starred_url":"https://api.github.com/users/voegelas/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/voegelas/subscriptions","organizations_url":"https://api.github.com/users/voegelas/orgs","repos_url":"https://api.github.com/users/voegelas/repos","events_url":"https://api.github.com/users/voegelas/events{/privacy}","received_events_url":"https://api.github.com/users/voegelas/received_events","type":"User","site_admin":false},"created_at":"2018-01-10T12:52:03Z","updated_at":"2018-01-10T12:52:03Z","author_association":"NONE","body":"We suffered from this documentation mistake too. After a system reboot \"cluster.routing.allocation.enable\" was set to \"none\" and newly created indices didn't become available.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/367082588","html_url":"https://github.com/elastic/elasticsearch/issues/27677#issuecomment-367082588","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27677","id":367082588,"node_id":"MDEyOklzc3VlQ29tbWVudDM2NzA4MjU4OA==","user":{"login":"pickypg","id":1501235,"node_id":"MDQ6VXNlcjE1MDEyMzU=","avatar_url":"https://avatars2.githubusercontent.com/u/1501235?v=4","gravatar_id":"","url":"https://api.github.com/users/pickypg","html_url":"https://github.com/pickypg","followers_url":"https://api.github.com/users/pickypg/followers","following_url":"https://api.github.com/users/pickypg/following{/other_user}","gists_url":"https://api.github.com/users/pickypg/gists{/gist_id}","starred_url":"https://api.github.com/users/pickypg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pickypg/subscriptions","organizations_url":"https://api.github.com/users/pickypg/orgs","repos_url":"https://api.github.com/users/pickypg/repos","events_url":"https://api.github.com/users/pickypg/events{/privacy}","received_events_url":"https://api.github.com/users/pickypg/received_events","type":"User","site_admin":false},"created_at":"2018-02-20T19:01:30Z","updated_at":"2018-02-20T19:01:30Z","author_association":"MEMBER","body":"I think the bug here is not that the last setting is set transiently, rather than the full cluster restart and rolling restart documentation are in disagreement.\r\n\r\nRolling restart, step 7 shows:\r\n\r\n```http\r\nPUT _cluster/settings\r\n{\r\n  \"transient\": {\r\n    \"cluster.routing.allocation.enable\": \"all\"\r\n  }\r\n}\r\n```\r\n\r\nFull cluster restart, step 8 shows:\r\n\r\n```http\r\nPUT _cluster/settings\r\n{\r\n  \"persistent\": {\r\n    \"cluster.routing.allocation.enable\": \"all\"\r\n  }\r\n}\r\n```\r\n\r\nAs long as you follow either guide, you _should_ end up with allocation enabled. The only issue here is that a transient setting of `\"all\"` is lost following a full cluster restart, but that is a _good_ thing as it's already the first step in a full cluster restart -- so setting it transiently protects you from forgetting to set it to `\"none\"` as the first step.\r\n\r\nI think the fix should be to change the full cluster restart docs to transiently set the setting in step 8 and then close out this issue.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/367209808","html_url":"https://github.com/elastic/elasticsearch/issues/27677#issuecomment-367209808","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27677","id":367209808,"node_id":"MDEyOklzc3VlQ29tbWVudDM2NzIwOTgwOA==","user":{"login":"astefan","id":893749,"node_id":"MDQ6VXNlcjg5Mzc0OQ==","avatar_url":"https://avatars2.githubusercontent.com/u/893749?v=4","gravatar_id":"","url":"https://api.github.com/users/astefan","html_url":"https://github.com/astefan","followers_url":"https://api.github.com/users/astefan/followers","following_url":"https://api.github.com/users/astefan/following{/other_user}","gists_url":"https://api.github.com/users/astefan/gists{/gist_id}","starred_url":"https://api.github.com/users/astefan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/astefan/subscriptions","organizations_url":"https://api.github.com/users/astefan/orgs","repos_url":"https://api.github.com/users/astefan/repos","events_url":"https://api.github.com/users/astefan/events{/privacy}","received_events_url":"https://api.github.com/users/astefan/received_events","type":"User","site_admin":false},"created_at":"2018-02-21T04:26:51Z","updated_at":"2018-02-21T04:26:51Z","author_association":"CONTRIBUTOR","body":"I agree with [this comment](https://github.com/elastic/elasticsearch/issues/28747#issuecomment-367092132), the cluster needs to be brought back to the initial state. Not many users are following the procedure when it comes to a full cluster restart. Usually, they are doing it when they actually upgrade.\r\n\r\nIn other cases, the cluster is just restarted without any allocation being disabled. When this happens and the cluster comes back no new indices will be allocated and then it is realized that allocation was mysteriously disabled sometime in the past. I am in favor of having  \r\n```\r\n\"transient\": {\r\n\"cluster.routing.allocation.enable\": \"none\"\r\n}\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/367227917","html_url":"https://github.com/elastic/elasticsearch/issues/27677#issuecomment-367227917","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27677","id":367227917,"node_id":"MDEyOklzc3VlQ29tbWVudDM2NzIyNzkxNw==","user":{"login":"pickypg","id":1501235,"node_id":"MDQ6VXNlcjE1MDEyMzU=","avatar_url":"https://avatars2.githubusercontent.com/u/1501235?v=4","gravatar_id":"","url":"https://api.github.com/users/pickypg","html_url":"https://github.com/pickypg","followers_url":"https://api.github.com/users/pickypg/followers","following_url":"https://api.github.com/users/pickypg/following{/other_user}","gists_url":"https://api.github.com/users/pickypg/gists{/gist_id}","starred_url":"https://api.github.com/users/pickypg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pickypg/subscriptions","organizations_url":"https://api.github.com/users/pickypg/orgs","repos_url":"https://api.github.com/users/pickypg/repos","events_url":"https://api.github.com/users/pickypg/events{/privacy}","received_events_url":"https://api.github.com/users/pickypg/received_events","type":"User","site_admin":false},"created_at":"2018-02-21T06:35:20Z","updated_at":"2018-02-21T06:35:20Z","author_association":"MEMBER","body":"But when you restart a cluster accidentally, you are in _better_ shape having disabled allocation rather than allowing allocations in an unexpected situation.\r\n\r\nIt's true that it is not the default, but that's because the default when you startup is to let you get to work. Leaving it set as `none` leaves you in a safer state whether you intentionally or accidentally restart a node: no allocations happen without you giving permission.\r\n\r\nAs to transiently setting  `none`, you cannot do this for full cluster restarts because the master disappears and thus allocation happens immediately upon cluster reformation (which is the problem that setting it persistently, and `all` transiently addresses).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/367247906","html_url":"https://github.com/elastic/elasticsearch/issues/27677#issuecomment-367247906","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27677","id":367247906,"node_id":"MDEyOklzc3VlQ29tbWVudDM2NzI0NzkwNg==","user":{"login":"astefan","id":893749,"node_id":"MDQ6VXNlcjg5Mzc0OQ==","avatar_url":"https://avatars2.githubusercontent.com/u/893749?v=4","gravatar_id":"","url":"https://api.github.com/users/astefan","html_url":"https://github.com/astefan","followers_url":"https://api.github.com/users/astefan/followers","following_url":"https://api.github.com/users/astefan/following{/other_user}","gists_url":"https://api.github.com/users/astefan/gists{/gist_id}","starred_url":"https://api.github.com/users/astefan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/astefan/subscriptions","organizations_url":"https://api.github.com/users/astefan/orgs","repos_url":"https://api.github.com/users/astefan/repos","events_url":"https://api.github.com/users/astefan/events{/privacy}","received_events_url":"https://api.github.com/users/astefan/received_events","type":"User","site_admin":false},"created_at":"2018-02-21T08:24:38Z","updated_at":"2018-02-21T08:24:38Z","author_association":"CONTRIBUTOR","body":"This gh issue was about rolling upgrades and my comment regarding transiently setting it to none applies to rolling upgrades.\r\n\r\nI agree it is safer to do it persistently and then reverting it with transiently, but you have the downside of not being aware of the persistent one having been applied at some time and assuming that the cluster should get back to \"normal\" after restart. Which is not happening until the next day (the logging use case with daily indices) when the user notices that day's indices were not created, Kibana visualizations are not working and all the ingesting flows (LS or not) are accumulating events or, worse, dropping them.\r\n\r\nTo me, it looks like we are dropping predictability. And safer it may be, but not always (see the logging use case scenario I described above).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/367590939","html_url":"https://github.com/elastic/elasticsearch/issues/27677#issuecomment-367590939","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27677","id":367590939,"node_id":"MDEyOklzc3VlQ29tbWVudDM2NzU5MDkzOQ==","user":{"login":"jarro2783","id":1456242,"node_id":"MDQ6VXNlcjE0NTYyNDI=","avatar_url":"https://avatars2.githubusercontent.com/u/1456242?v=4","gravatar_id":"","url":"https://api.github.com/users/jarro2783","html_url":"https://github.com/jarro2783","followers_url":"https://api.github.com/users/jarro2783/followers","following_url":"https://api.github.com/users/jarro2783/following{/other_user}","gists_url":"https://api.github.com/users/jarro2783/gists{/gist_id}","starred_url":"https://api.github.com/users/jarro2783/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jarro2783/subscriptions","organizations_url":"https://api.github.com/users/jarro2783/orgs","repos_url":"https://api.github.com/users/jarro2783/repos","events_url":"https://api.github.com/users/jarro2783/events{/privacy}","received_events_url":"https://api.github.com/users/jarro2783/received_events","type":"User","site_admin":false},"created_at":"2018-02-22T07:28:02Z","updated_at":"2018-02-22T07:28:02Z","author_association":"NONE","body":"I think the docs are still wrong for a rolling restart. If I use *transient* to re-enable, then repeat the procedure for the next node, I am now in the wrong state. If I follow the first step and disable it for *persistent*, then nothing happens, because it is already disabled, and *transient* is already enabled. So the docs should either say not to use *persistent* for a _rolling_ restart at all, or to point out that you need to disable the *transient* setting for each node.\r\n\r\nThen you probably also want it back to the normal state after finishing, meaning that *persistent* should be set to enabled.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/367789231","html_url":"https://github.com/elastic/elasticsearch/issues/27677#issuecomment-367789231","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27677","id":367789231,"node_id":"MDEyOklzc3VlQ29tbWVudDM2Nzc4OTIzMQ==","user":{"login":"pickypg","id":1501235,"node_id":"MDQ6VXNlcjE1MDEyMzU=","avatar_url":"https://avatars2.githubusercontent.com/u/1501235?v=4","gravatar_id":"","url":"https://api.github.com/users/pickypg","html_url":"https://github.com/pickypg","followers_url":"https://api.github.com/users/pickypg/followers","following_url":"https://api.github.com/users/pickypg/following{/other_user}","gists_url":"https://api.github.com/users/pickypg/gists{/gist_id}","starred_url":"https://api.github.com/users/pickypg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pickypg/subscriptions","organizations_url":"https://api.github.com/users/pickypg/orgs","repos_url":"https://api.github.com/users/pickypg/repos","events_url":"https://api.github.com/users/pickypg/events{/privacy}","received_events_url":"https://api.github.com/users/pickypg/received_events","type":"User","site_admin":false},"created_at":"2018-02-22T19:15:44Z","updated_at":"2018-02-22T19:15:44Z","author_association":"MEMBER","body":"@jarro2783 \r\n\r\nThat's a great point. Rolling restarts should probably specify `none` as both persistent and transient (or, for simple matters, just one or the other). At the very least, we must specify in the docs to remove the transient setting between node changes.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/383904826","html_url":"https://github.com/elastic/elasticsearch/issues/27677#issuecomment-383904826","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27677","id":383904826,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MzkwNDgyNg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-04-24T11:59:49Z","updated_at":"2018-04-24T11:59:49Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/415784363","html_url":"https://github.com/elastic/elasticsearch/issues/27677#issuecomment-415784363","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27677","id":415784363,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNTc4NDM2Mw==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-08-24T14:55:19Z","updated_at":"2018-08-24T14:55:19Z","author_association":"CONTRIBUTOR","body":"We updated the docs to recommend using `persistent` throughout the full cluster restart docs in #29670, and in the rolling upgrade docs in #29671, so I think this issue is resolved.","performed_via_github_app":null}]