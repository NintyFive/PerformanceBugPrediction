{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/61959","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/61959/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/61959/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/61959/events","html_url":"https://github.com/elastic/elasticsearch/issues/61959","id":692832052,"node_id":"MDU6SXNzdWU2OTI4MzIwNTI=","number":61959,"title":"InternalEngine doesn't throttle when indexing buffer limit is reached","user":{"login":"sohami","id":22159459,"node_id":"MDQ6VXNlcjIyMTU5NDU5","avatar_url":"https://avatars1.githubusercontent.com/u/22159459?v=4","gravatar_id":"","url":"https://api.github.com/users/sohami","html_url":"https://github.com/sohami","followers_url":"https://api.github.com/users/sohami/followers","following_url":"https://api.github.com/users/sohami/following{/other_user}","gists_url":"https://api.github.com/users/sohami/gists{/gist_id}","starred_url":"https://api.github.com/users/sohami/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sohami/subscriptions","organizations_url":"https://api.github.com/users/sohami/orgs","repos_url":"https://api.github.com/users/sohami/repos","events_url":"https://api.github.com/users/sohami/events{/privacy}","received_events_url":"https://api.github.com/users/sohami/received_events","type":"User","site_admin":false},"labels":[{"id":836542781,"node_id":"MDU6TGFiZWw4MzY1NDI3ODE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Engine","name":":Distributed/Engine","color":"0e8a16","default":false,"description":"Anything around managing Lucene and the Translog in an open shard."},{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null},{"id":1967496670,"node_id":"MDU6TGFiZWwxOTY3NDk2Njcw","url":"https://api.github.com/repos/elastic/elasticsearch/labels/Team:Distributed","name":"Team:Distributed","color":"fef2c0","default":false,"description":"Meta label for distributed team"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2020-09-04T07:34:17Z","updated_at":"2020-10-02T13:10:40Z","closed_at":"2020-10-02T13:10:40Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version** (7.7):\r\n\r\n**JVM version** (jdk-11):\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n1) I was seeing an issue where the JVM usage of the ES process went up very high to 97% and was analyzing the heap dump for it. In this case, one shard of an index end up consuming around 25GB of JVM out of 32GB. The index was receiving very high ingestion workload. I was surprised to see that and was trying to understand why indexingBuffer based throttling didn't kicked in. I can see from the dump, that the indexingBuffer limit is breached for one ES shard (or Lucene index) and IndexingMemoryController detects that and activates the throttling mode for the shard. But I was expecting that with throttling mode activated the InternalEngine will start throttling any new indexing requests and also initiates the flush process to relieve the memory pressure. However, while looking into the code it looks like InternalEngine doesn't throttle the new indexing requests if it's not because of recovery. It looks to me even though the indexingBuffer limit is breached by one of the Lucene index, it can still keep consuming the memory by the new indexing requests and increase the overall memory usage by the Lucene index or ES shard. If the ingestion workload is very high this can consume memory very fast.\r\n\r\nRef: https://github.com/elastic/elasticsearch/pull/22721/commits/bd1c30ffa5c775981c7678432609b669147b9202#diff-74a25d57bbb52202a04fe43d8632172eR547\r\n\r\nIt looks like with this commit (https://github.com/elastic/elasticsearch/pull/22721)  the check was reversed to apply throttling only in recovery scenarios. \r\n\r\nPlease let me know if my understanding is correct. I can open a PR with the fix.","closed_by":{"login":"henningandersen","id":33268011,"node_id":"MDQ6VXNlcjMzMjY4MDEx","avatar_url":"https://avatars2.githubusercontent.com/u/33268011?v=4","gravatar_id":"","url":"https://api.github.com/users/henningandersen","html_url":"https://github.com/henningandersen","followers_url":"https://api.github.com/users/henningandersen/followers","following_url":"https://api.github.com/users/henningandersen/following{/other_user}","gists_url":"https://api.github.com/users/henningandersen/gists{/gist_id}","starred_url":"https://api.github.com/users/henningandersen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/henningandersen/subscriptions","organizations_url":"https://api.github.com/users/henningandersen/orgs","repos_url":"https://api.github.com/users/henningandersen/repos","events_url":"https://api.github.com/users/henningandersen/events{/privacy}","received_events_url":"https://api.github.com/users/henningandersen/received_events","type":"User","site_admin":false},"performed_via_github_app":null}