{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/55240","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/55240/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/55240/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/55240/events","html_url":"https://github.com/elastic/elasticsearch/issues/55240","id":600351378,"node_id":"MDU6SXNzdWU2MDAzNTEzNzg=","number":55240,"title":"Heap memory issue during cardinality request","user":{"login":"Yv-o-rL","id":42407721,"node_id":"MDQ6VXNlcjQyNDA3NzIx","avatar_url":"https://avatars0.githubusercontent.com/u/42407721?v=4","gravatar_id":"","url":"https://api.github.com/users/Yv-o-rL","html_url":"https://github.com/Yv-o-rL","followers_url":"https://api.github.com/users/Yv-o-rL/followers","following_url":"https://api.github.com/users/Yv-o-rL/following{/other_user}","gists_url":"https://api.github.com/users/Yv-o-rL/gists{/gist_id}","starred_url":"https://api.github.com/users/Yv-o-rL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Yv-o-rL/subscriptions","organizations_url":"https://api.github.com/users/Yv-o-rL/orgs","repos_url":"https://api.github.com/users/Yv-o-rL/repos","events_url":"https://api.github.com/users/Yv-o-rL/events{/privacy}","received_events_url":"https://api.github.com/users/Yv-o-rL/received_events","type":"User","site_admin":false},"labels":[{"id":141141324,"node_id":"MDU6TGFiZWwxNDExNDEzMjQ=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Analytics/Aggregations","name":":Analytics/Aggregations","color":"0e8a16","default":false,"description":"Aggregations"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2020-04-15T14:42:43Z","updated_at":"2020-04-17T14:13:45Z","closed_at":"2020-04-17T14:13:45Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\r\n\r\n** Please read the guidelines below. **\r\n\r\nIssues that do not follow these guidelines are likely to be closed.\r\n\r\n1.  GitHub is reserved for bug reports and feature requests. The best place to\r\n    ask a general question is at the Elastic [forums](https://discuss.elastic.co).\r\n    GitHub is not the place for general questions.\r\n\r\n2.  Is this bug report or feature request for a supported OS? If not, it\r\n    is likely to be closed.  See https://www.elastic.co/support/matrix#show_os\r\n\r\n3.  Please fill out EITHER the feature request block or the bug report block\r\n    below, and delete the other block.\r\n\r\n-->\r\n\r\n<!-- Bug report -->\r\n\r\n**Elasticsearch version** (`bin/elasticsearch --version`): 7.2.1\r\n\r\n**Plugins installed**: []\r\n\r\n**JVM version** (`java -version`): OpenJDK 11\r\n\r\n**OS version** (`uname -a` if on a Unix-like system): Ubuntu 18.04.3 LTS\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\nI've got a large daily index where sometimes I need to perform an unfiltered cardinality query. I already noticed that the filtered ones are a lot slower than before. When I load a line visualization in Kibana, check the past 30 minutes for the unique values, aggregate by time (auto), and add the top 5 instances as sub-buckets, the cluster crashes for 5-10 minutes. When I check the graphs, I see a couple of nodes hitting the max heap memory. 6 out of 8 nodes had a heap memory jump 4-9 GB (a total of ~46GB of heap memory spike). While I'm far from being an expert, I would say that if there were more memory available the 3 nodes which hit the heap limit would try to hog more.\r\nI understand that cardinality isn't that easy but I didn't experience this issue before and I'd expect that this much HW resources can handle that one query without crashing.\r\n\r\n**Steps to reproduce**:\r\n\r\n 1. Index dimensions (at query time):\r\n- 300/150GB (primary/store)\r\n- 20 shards (10 primary and 10 replica)\r\n- ~277 million documents\r\n- indexing rate 15300/7650 (primary/total)\r\n- unique values of the field: ~20-25M\r\n- unique instances: ~40K\r\n- 500K documents per minute\r\n\r\n 2. Cluster dimensions:\r\n- 8 nodes\r\n- 32 vCPU and 120GB memory per node\r\n- all in the same DC\r\n\r\n 3. Elasticsearch settings:\r\n- JVM: G1GC \r\n- heap memory set to 30GB\r\n\r\n 4. Perform the high cardinality query:\r\n- unique count of high cardinality field\r\n- past 30 minutes\r\n- buckets: date histogram (30-second buckets); sub-bucket: terms: TOP 5 instances\r\n\r\n**Provide logs (if relevant)**:\r\nFirst, the Logstash instances are reporting 429:\r\n\r\n> retrying failed action with response code: 429 ({\"type\"=>\"circuit_breaking_exception\", \"reason\"=>\"[parent] Data too large, data for [<transport_request>] would be [30618130228/28.5gb], which is larger than the limit of [30601641984/28.5gb], real usage: [30618128352/28.5gb], new bytes reserved: [1876/1.8kb]\", \"bytes_wanted\"=>30618130228, \"bytes_limit\"=>30601641984, \"durability\"=>\"PERMANENT\"})\r\n\r\nThan ES errors pile up:\r\n\r\n> [myelastic-1] failed to execute on node [k2pz-cp-XXXXXXXXXXXXX]\r\n> org.elasticsearch.transport.RemoteTransportException: [myelastic-3][10.0.0.3:9300][cluster:monitor/nodes/info[n]]\r\n> Caused by: org.elasticsearch.common.breaker.CircuitBreakingException: [parent] Data too large, data for [<transport_request>] would be [30651686876/28.5gb], which is larger than the limit of [30601641984/28.5gb], real usage: [30651682792/28.5gb], new bytes reserved: [4084/3.9kb]\r\n> \tat org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService.checkParentLimit(HierarchyCircuitBreakerService.java:343) ~[elasticsearch-7.2.1.jar:7.2.1]\r\n> \tat org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreaker.java:128) ~[elasticsearch-7.2.1.jar:7.2.1]\r\n> \tat org.elasticsearch.transport.InboundHandler.handleRequest(InboundHandler.java:173) [elasticsearch-7.2.1.jar:7.2.1]\r\n> \tat org.elasticsearch.transport.InboundHandler.messageReceived(InboundHandler.java:121) [elasticsearch-7.2.1.jar:7.2.1]\r\n> \tat org.elasticsearch.transport.InboundHandler.inboundMessage(InboundHandler.java:105) [elasticsearch-7.2.1.jar:7.2.1]\r\n> \tat org.elasticsearch.transport.TcpTransport.inboundMessage(TcpTransport.java:660) [elasticsearch-7.2.1.jar:7.2.1]\r\n> \tat org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:62) [transport-netty4-client-7.2.1.jar:7.2.1]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:323) [netty-codec-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:297) [netty-codec-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:241) [netty-handler-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1478) [netty-handler-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1227) [netty-handler-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1274) [netty-handler-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502) [netty-codec-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:441) [netty-codec-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) [netty-codec-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1408) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:682) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:582) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:536) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496) [netty-transport-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:906) [netty-common-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.35.Final.jar:4.1.35.Final]\r\n> \tat java.lang.Thread.run(Thread.java:835) [?:?]\r\n","closed_by":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"performed_via_github_app":null}