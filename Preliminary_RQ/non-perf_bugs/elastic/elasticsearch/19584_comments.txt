[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/235074891","html_url":"https://github.com/elastic/elasticsearch/issues/19584#issuecomment-235074891","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19584","id":235074891,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNTA3NDg5MQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-07-25T20:30:15Z","updated_at":"2016-07-25T20:30:15Z","author_association":"MEMBER","body":"answering in reverse order:\n\nB) we went to a great pain to actually make it so, in order to limit the scope of such a failure (and disk loss) to a few shards on node rather then all of them.\n\nA is our chosen strategy, assuming that by \"other shards\" you mean that the shard on the full disk will be failed and other other shards will carry the load. Note that all active shards copies are indexing all operations in ES and that each document is mapped to single set of shard copies (and this can not be changed).  \n\nIt may however take some time for the master to process the shard failed, remove it and publish to the node in question which in turn will clean it up. \n\nFrom you exception it seem you are talking about multiple attempts to assign a primary to that node and it seems you have no other valid copy for ES to use. Is that the case? if so, this is a duplicate of #19446\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/235083912","html_url":"https://github.com/elastic/elasticsearch/issues/19584#issuecomment-235083912","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19584","id":235083912,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNTA4MzkxMg==","user":{"login":"neuroticnetworks","id":3965137,"node_id":"MDQ6VXNlcjM5NjUxMzc=","avatar_url":"https://avatars0.githubusercontent.com/u/3965137?v=4","gravatar_id":"","url":"https://api.github.com/users/neuroticnetworks","html_url":"https://github.com/neuroticnetworks","followers_url":"https://api.github.com/users/neuroticnetworks/followers","following_url":"https://api.github.com/users/neuroticnetworks/following{/other_user}","gists_url":"https://api.github.com/users/neuroticnetworks/gists{/gist_id}","starred_url":"https://api.github.com/users/neuroticnetworks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/neuroticnetworks/subscriptions","organizations_url":"https://api.github.com/users/neuroticnetworks/orgs","repos_url":"https://api.github.com/users/neuroticnetworks/repos","events_url":"https://api.github.com/users/neuroticnetworks/events{/privacy}","received_events_url":"https://api.github.com/users/neuroticnetworks/received_events","type":"User","site_admin":false},"created_at":"2016-07-25T21:01:30Z","updated_at":"2016-07-25T21:02:32Z","author_association":"NONE","body":"Ah, interesting. So this is a staging cluster where we run without replicas. In retrospect I definitely should have mentioned that. My apologies.\n\nIf I follow you correctly, the failover mechanics you are describing require that a promotable replica exists... which I could see leading to some bizarre edge cases like a a node failure on node \"Foo\" taking down replica copies of indices \"A\" and \"B\". The cluster might rightfully attempt to reallocate replica copies of \"A\" and \"B to nodes \"Bar\" and \" Baz.\" Then if node Bar had the only primary copy of a shard of index A (the replica having been lost when node Foo died, and currently in the process of being streamed from node Bar to node Baz) and one of Bar's write paths filled up, the cluster might enter the sort of deadlock state I observed where the cluster was stuck for hours trying to write data to a disk location that doesn't have any space available on one of its write paths...\n\nwhich is an edge case that might best be subtitled \"If many bad things happen at the same time, Elastic might enter a state from which it can't recover on its own.\"\n\nThe behavior you are describing makes a lot of sense and the scenarios in which I can see that strategy being inadequate are esoteric. This may very well be such an edge case that it's not worth worrying about. \n\nAs far as https://github.com/elastic/elasticsearch/issues/19446, that looks related to me but not a duplicate of this ticket. But I may be overlooking something.  \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/235308075","html_url":"https://github.com/elastic/elasticsearch/issues/19584#issuecomment-235308075","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19584","id":235308075,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNTMwODA3NQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-07-26T15:38:10Z","updated_at":"2016-07-26T15:38:10Z","author_association":"MEMBER","body":"> If many bad things happen at the same time, Elastic might enter a state from which it can't recover on its own\n\nFor what it's worth, if many bad things happen Elasticsearch may very well end up not being able to recover. It's about documenting those scenarios - for example - if you loose all copies of a shard, data will be lost.\n\n>  in which I can see that strategy being inadequate are esoteric.\n\nWhat are they?\n\n> As far as #19446, that looks related to me but not a duplicate of this ticket. But I may be overlooking something.\n\nAs far as I can ES does the only thing it can do which is try to salvage the only copy of the primary it has with the hope that some space was freed (but a user/completed merge). The problem is it never gives up and waits for a human intervention which is what #19446  is about. \n\nI'm going to close this for now as a duplicate. if it turns out to be something else we can reopen.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/235320372","html_url":"https://github.com/elastic/elasticsearch/issues/19584#issuecomment-235320372","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19584","id":235320372,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNTMyMDM3Mg==","user":{"login":"neuroticnetworks","id":3965137,"node_id":"MDQ6VXNlcjM5NjUxMzc=","avatar_url":"https://avatars0.githubusercontent.com/u/3965137?v=4","gravatar_id":"","url":"https://api.github.com/users/neuroticnetworks","html_url":"https://github.com/neuroticnetworks","followers_url":"https://api.github.com/users/neuroticnetworks/followers","following_url":"https://api.github.com/users/neuroticnetworks/following{/other_user}","gists_url":"https://api.github.com/users/neuroticnetworks/gists{/gist_id}","starred_url":"https://api.github.com/users/neuroticnetworks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/neuroticnetworks/subscriptions","organizations_url":"https://api.github.com/users/neuroticnetworks/orgs","repos_url":"https://api.github.com/users/neuroticnetworks/repos","events_url":"https://api.github.com/users/neuroticnetworks/events{/privacy}","received_events_url":"https://api.github.com/users/neuroticnetworks/received_events","type":"User","site_admin":false},"created_at":"2016-07-26T16:15:06Z","updated_at":"2016-07-26T16:15:06Z","author_association":"NONE","body":"> in which I can see that strategy being inadequate are esoteric.\n\nThe scenarios I can think of basically all boil down to a node failing and a disk containing a primary shard filling up while ES tries to replicate the shards that were lost when a node failed. And with 2 replica copies instead of 1, the odds of these kinds of failures happening seem extremely unlikely to me. \n\nAs an example of what I have in mind, imagine a cluster with one replica of all its indices and a server dies. It seems possible to me that recovery could fill up one of several disks on a node and, assuming no watermarks are exceeded overall (it's worth noting that _cat/allocation was reporting 60% used space on my node that entered a coma), you could have a cluster lock up and freeze. Am I mistaken that this could happen? Suppose you have disks A, B, C, and D. Suppose A is 90% full and the others are 50% full. Is there anything preventing recovery from placing a shard on disk A? \n\nInre \"For what it's worth, if many bad things happen Elasticsearch may very well end up not being able to recover. It's about documenting those scenarios - for example - if you loose all copies of a shard, data will be lost.\" \n\nI agree. But if you loose all copies of a shard, you should expect to lose data. If you have 6 write paths with TBs of free space, node-level disk allocation significantly under the high and low water marks, and a single disk that fills up, I would argue that ES freezing for hours on end until someone clears it up manually is a bit strange to see and probably not what most people would expect. But I could be wrong. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/235333196","html_url":"https://github.com/elastic/elasticsearch/issues/19584#issuecomment-235333196","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19584","id":235333196,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNTMzMzE5Ng==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-07-26T16:57:33Z","updated_at":"2016-07-26T16:57:33Z","author_association":"MEMBER","body":"I'm not 100% sure I understand what you mean exactly, but I think what you mean is whether ES will transfer shard data from one path to another on the same node (remember each shard is always completely on one path), if one is getting full and the other is still empty. Sadly, this is not yet the case - there are existing issues to track this one- for example #16763\n","performed_via_github_app":null}]