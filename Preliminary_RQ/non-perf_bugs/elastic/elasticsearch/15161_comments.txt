[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/161083856","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-161083856","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":161083856,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MTA4Mzg1Ng==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2015-12-01T20:19:23Z","updated_at":"2015-12-01T20:19:23Z","author_association":"MEMBER","body":"> `indices.recovery.concurrent_streams` / `indices.recovery.concurrent_small_file_streams` why do we have this and why can't this come from some ThreadPool instead. I also wonder if the small / large differentiation is worth the trouble?\n\nIt was [originally added](https://github.com/elastic/elasticsearch/pull/3584) because large relocations can block things like the recovery of newly created indices, so you end up waiting a long time before the newly created index is available.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/161084611","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-161084611","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":161084611,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MTA4NDYxMQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-01T20:22:26Z","updated_at":"2015-12-01T20:22:26Z","author_association":"CONTRIBUTOR","body":"> It was originally added because large relocations can block things like the recovery of newly created indices, so you end up waiting a long time before the newly created index is available.\n\nfor stuff like this I think we should just use the coordinating thread and do it sequentially instead of in-parallel. But thanks for the pointer @dakrone !!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/161223017","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-161223017","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":161223017,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MTIyMzAxNw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2015-12-02T08:48:02Z","updated_at":"2015-12-02T08:48:02Z","author_association":"CONTRIBUTOR","body":"I'm not familiar enough with the recovery process to comment but I'm all for having fewer low-level settings. If we don't fully trust some components, we can have an escape hatch using system properties like you suggested.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/161227518","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-161227518","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":161227518,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MTIyNzUxOA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-12-02T09:04:10Z","updated_at":"2015-12-02T09:04:10Z","author_association":"MEMBER","body":"> indices.recovery.compress \n\n+1 on removing ^^ never heard about a case where it was used.\n\n> indices.recovery.concurrent_streams / indices.recovery.concurrent_small_file_streams\n\nWhat Lee said re that we need a solution to let new indices get to green quickly, even if a big file is being copied. Doesn't have to this solution as Simon said.\n\nAlso, these setting protect a node holding many primaries from being overloaded by outbound recoveries. I think this is simpler to implement on the master with a decision allocator (like we do for target relocation). The was an e-mail thread about this where @kimchy thought at the time that this will be complicated (the thread at the time was about the tension between max_bytes_per_sec and these settings).\n\n> index.shard.recovery.translog_size / index.shard.recovery.translog_ops I think we can drop both and just hide sending behind a stream and just keep on writing to the stream and flush when necessary (8kb) or just use 8kb as a fixed size?\n\nWe already read ops from the translog one by one, so writing to a blocking output stream will easy to implement here.\n\n> I would be ok with moving some of these settings to system properties to signal how expert they are\n\n+1 to simplify and remove escape hatches which we haven't used for a while. Even better if we can simplify the mechanics to have one setting do more (like the 8k buffer which holds for many transmissions). One note about system properties - our current settings are dynamically updatable so in case of trouble you can tweak them without going down. system properties aren't. As these settings are relevant for all nodes of the cluster, having them updatable from code means they can be changed in one command rather than a full cluster restart.  To me this is a case by case basis judgment call. \n\nIf we move to system properties, we need some kind of mechanism to report they were used and changed. Right now asking for the elasticsearch.yml file + cluster settings gives you a complete picture. In general we are moving towards using more and more cluster settings and less the yml file. I'm worried moving to system properties will go agains this.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/161228722","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-161228722","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":161228722,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MTIyODcyMg==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-02T09:10:55Z","updated_at":"2015-12-02T09:10:55Z","author_association":"CONTRIBUTOR","body":"@kimchy can you tell us where the `512kb` came from vs. `8k` buffer. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/161755836","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-161755836","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":161755836,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MTc1NTgzNg==","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2015-12-03T19:32:28Z","updated_at":"2015-12-03T19:32:28Z","author_association":"MEMBER","body":"- compress: I agree that we can remove this setting, I don't think someone really sets it. My suggestion is actually not to have it on when transferring segments (they are already compressed), but still do compression on translog entries as those tend to be highly compressed json documents.\n- Regarding concurrent streams, I agree that if we can make better decision when allocating a shard, for example, prefer new indices to allocate almost ignoring deciders that throttle, will help. We will still need to somehow prioritize \"the streams\" of segments, though in that case, a single recovery thread would be enough. I do think that it is still good to have concurrent streams for beefy shard recovery as it will speed it up compared to single thread streaming data.\n- To do blocking streaming, we would simulate it over evented networking, deciding when to \"flush\". We should have higher than 8k buffer to flush, as network buffers can transfer in one go much more, and the 8k one we do in Lucene for files doesn't apply in this case (copy to native direct buffer). I am +1 on removing the setting, since 512k is a good default and I doubt someone would want to change it. Note that higher flush buffer starts to hurt you when doing evented networking.\n- Regarding translog size, I agree that we can remove this setting as well, and just flush based on bytw size, and use the same flush size we decide to use for segments here as well (512k).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/161916319","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-161916319","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":161916319,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MTkxNjMxOQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-04T09:15:49Z","updated_at":"2015-12-04T09:15:49Z","author_association":"CONTRIBUTOR","body":"> Regarding concurrent streams, I agree that if we can make better decision when allocating a shard, for example, prefer new indices to allocate almost ignoring deciders that throttle, will help. We will still need to somehow prioritize \"the streams\" of segments, though in that case, a single recovery thread would be enough. I do think that it is still good to have concurrent streams for beefy shard recovery as it will speed it up compared to single thread streaming data.\n\n@kimchy I think we should just use the recovery thread itself to send such small files. That way it's simplified and we can rely on throttling to not overload the node. The recovery thread is blocking anyway and sending small files should be very very quick anyhow.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163693288","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-163693288","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":163693288,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzY5MzI4OA==","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2015-12-10T17:20:44Z","updated_at":"2015-12-10T17:20:44Z","author_association":"MEMBER","body":"@s1monw right, we might still want to then not throttle recoveries for freshly created indices? otherwise they might backlog?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163693669","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-163693669","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":163693669,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzY5MzY2OQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-10T17:21:36Z","updated_at":"2015-12-10T17:21:36Z","author_association":"CONTRIBUTOR","body":"> @s1monw right, we might still want to then not throttle recoveries for freshly created indices? otherwise they might backlog?\n\nI don't see that really but that has to be a sep change since all our tests will fail on that one I tried it\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/183944984","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-183944984","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":183944984,"node_id":"MDEyOklzc3VlQ29tbWVudDE4Mzk0NDk4NA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-02-14T18:27:09Z","updated_at":"2016-02-14T18:27:09Z","author_association":"CONTRIBUTOR","body":"@s1monw is this issue closed by https://github.com/elastic/elasticsearch/pull/15235 or is there more to do?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/191391430","html_url":"https://github.com/elastic/elasticsearch/issues/15161#issuecomment-191391430","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15161","id":191391430,"node_id":"MDEyOklzc3VlQ29tbWVudDE5MTM5MTQzMA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-03-02T19:38:43Z","updated_at":"2016-03-02T19:38:43Z","author_association":"CONTRIBUTOR","body":"@clintongormley you are right I missed that... closing\n","performed_via_github_app":null}]