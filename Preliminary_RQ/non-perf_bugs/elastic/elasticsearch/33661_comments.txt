[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/420920099","html_url":"https://github.com/elastic/elasticsearch/issues/33661#issuecomment-420920099","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33661","id":420920099,"node_id":"MDEyOklzc3VlQ29tbWVudDQyMDkyMDA5OQ==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-09-13T08:04:50Z","updated_at":"2018-09-13T08:04:50Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/421574482","html_url":"https://github.com/elastic/elasticsearch/issues/33661#issuecomment-421574482","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33661","id":421574482,"node_id":"MDEyOklzc3VlQ29tbWVudDQyMTU3NDQ4Mg==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-09-15T13:56:34Z","updated_at":"2018-09-15T13:56:34Z","author_association":"MEMBER","body":"Also pinging @elastic/es-search-aggs to raise awareness with Lucene devs.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/421923215","html_url":"https://github.com/elastic/elasticsearch/issues/33661#issuecomment-421923215","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33661","id":421923215,"node_id":"MDEyOklzc3VlQ29tbWVudDQyMTkyMzIxNQ==","user":{"login":"gna582","id":2534537,"node_id":"MDQ6VXNlcjI1MzQ1Mzc=","avatar_url":"https://avatars0.githubusercontent.com/u/2534537?v=4","gravatar_id":"","url":"https://api.github.com/users/gna582","html_url":"https://github.com/gna582","followers_url":"https://api.github.com/users/gna582/followers","following_url":"https://api.github.com/users/gna582/following{/other_user}","gists_url":"https://api.github.com/users/gna582/gists{/gist_id}","starred_url":"https://api.github.com/users/gna582/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gna582/subscriptions","organizations_url":"https://api.github.com/users/gna582/orgs","repos_url":"https://api.github.com/users/gna582/repos","events_url":"https://api.github.com/users/gna582/events{/privacy}","received_events_url":"https://api.github.com/users/gna582/received_events","type":"User","site_admin":false},"created_at":"2018-09-17T08:14:32Z","updated_at":"2018-09-17T11:33:17Z","author_association":"NONE","body":"After trying ext4, then XFS I am currently testing btrfs partitions in the hope that this FS would have improved logging capacity in case the hardware is misbehaving. After running this over the weekend the two nodes I tested it on seem to avoid this issue. \r\n\r\nJust had a look at the history and this is how the partitions were created (just wanted to doublecheck whether sane defaults were used):\r\n```\r\nmkfs.ext4 -L es-data01 /dev/sdb\r\nmkfs.xfs -L es-data07 /dev/sdh -f\r\nmkfs.btrfs -L es-data06 /dev/sdg -f\r\n```\r\n\r\nI will now try to migrate the whole cluster to btrfs single partitions and hopefully it will \"just work\". Will comment when this is done and running for at least a day.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/422294741","html_url":"https://github.com/elastic/elasticsearch/issues/33661#issuecomment-422294741","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33661","id":422294741,"node_id":"MDEyOklzc3VlQ29tbWVudDQyMjI5NDc0MQ==","user":{"login":"gna582","id":2534537,"node_id":"MDQ6VXNlcjI1MzQ1Mzc=","avatar_url":"https://avatars0.githubusercontent.com/u/2534537?v=4","gravatar_id":"","url":"https://api.github.com/users/gna582","html_url":"https://github.com/gna582","followers_url":"https://api.github.com/users/gna582/followers","following_url":"https://api.github.com/users/gna582/following{/other_user}","gists_url":"https://api.github.com/users/gna582/gists{/gist_id}","starred_url":"https://api.github.com/users/gna582/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gna582/subscriptions","organizations_url":"https://api.github.com/users/gna582/orgs","repos_url":"https://api.github.com/users/gna582/repos","events_url":"https://api.github.com/users/gna582/events{/privacy}","received_events_url":"https://api.github.com/users/gna582/received_events","type":"User","site_admin":false},"created_at":"2018-09-18T08:00:37Z","updated_at":"2018-09-18T08:12:09Z","author_association":"NONE","body":"I have migrated the whole cluster. Sadly the issue is NOT solved but almost circumvented. Btrfs is truly the better file system xD.\r\n\r\nI still had a few sharderrors today but only on master and in relation they got rarer by factor 100.\r\n```\r\n[2018-09-18T05:00:02,752][WARN ][o.e.g.GatewayAllocator$InternalReplicaShardAllocator] [mes-any-log004-mes_any_log1] [koopa-app-logs-2018.09.18-03:00][2]: failed to list shard for shard_store on node [AaSyf61E\r\nSMOVB2ZiLNbs3w]\r\norg.elasticsearch.action.FailedNodeException: Failed node [AaSyf61ESMOVB2ZiLNbs3w]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.onFailure(TransportNodesAction.java:237) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$200(TransportNodesAction.java:153) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction$1.handleException(TransportNodesAction.java:211) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at com.floragunn.searchguard.transport.SearchGuardInterceptor$RestoringTransportResponseHandler.handleException(SearchGuardInterceptor.java:194) ~[?:?]\r\n        at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1095) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport.lambda$handleException$34(TcpTransport.java:1510) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.common.util.concurrent.EsExecutors$1.execute(EsExecutors.java:135) [elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport.handleException(TcpTransport.java:1508) [elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport.handlerResponseError(TcpTransport.java:1500) [elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1430) [elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:64) [transport-netty4-client-6.3.0.jar:6.3.0]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:297) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:413) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:241) [netty-handler-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1336) [netty-handler-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1127) [netty-handler-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1162) [netty-handler-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:545) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.16.Final.jar:4.1.16.Final]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]\r\nCaused by: org.elasticsearch.transport.RemoteTransportException: [mes-any-log006-mes_any_log1][172.19.126.35:9300][internal:cluster/nodes/indices/shard/store[n]]\r\nCaused by: org.elasticsearch.ElasticsearchException: Failed to list store metadata for shard [[koopa-app-logs-2018.09.18-03:00][2]]\r\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:111) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:61) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction.nodeOperation(TransportNodesAction.java:140) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:260) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:256) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at com.floragunn.searchguard.ssl.transport.SearchGuardSSLRequestHandler.messageReceivedDecorate(SearchGuardSSLRequestHandler.java:170) ~[?:?]\r\n        at com.floragunn.searchguard.transport.SearchGuardRequestHandler.messageReceivedDecorate(SearchGuardRequestHandler.java:235) ~[?:?]\r\n        at com.floragunn.searchguard.ssl.transport.SearchGuardSSLRequestHandler.messageReceived(SearchGuardSSLRequestHandler.java:142) ~[?:?]\r\n        at com.floragunn.searchguard.SearchGuardPlugin$7$1.messageReceived(SearchGuardPlugin.java:601) ~[?:?]\r\n        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:66) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport$RequestHandler.doRun(TcpTransport.java:1592) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:724) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_121]\r\nCaused by: java.io.FileNotFoundException: no segments* file found in store(MMapDirectory@/opt/mes-any-log1-data03/nodes/0/indices/Xfw504T0SMi9cmFgyjvDIA/2/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@74ce4571): files: [write.lock]\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:670) ~[lucene-core-7.3.1.jar:7.3.1 ae0705edb59eaa567fe13ed3a222fdadc7153680 - caomanhdat - 2018-05-09 09:27:24]\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:627) ~[lucene-core-7.3.1.jar:7.3.1 ae0705edb59eaa567fe13ed3a222fdadc7153680 - caomanhdat - 2018-05-09 09:27:24]\r\n        at org.apache.lucene.index.SegmentInfos.readLatestCommit(SegmentInfos.java:434) ~[lucene-core-7.3.1.jar:7.3.1 ae0705edb59eaa567fe13ed3a222fdadc7153680 - caomanhdat - 2018-05-09 09:27:24]\r\n        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:122) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:207) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.store.Store.access$200(Store.java:134) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.store.Store$MetadataSnapshot.loadMetadata(Store.java:864) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.store.Store$MetadataSnapshot.<init>(Store.java:797) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.store.Store.getMetadata(Store.java:293) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.shard.IndexShard.snapshotStoreMetadata(IndexShard.java:1138) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.listStoreMetaData(TransportNodesListShardStoreMetaData.java:125) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:109) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:61) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction.nodeOperation(TransportNodesAction.java:140) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:260) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:256) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at com.floragunn.searchguard.ssl.transport.SearchGuardSSLRequestHandler.messageReceivedDecorate(SearchGuardSSLRequestHandler.java:170) ~[?:?]\r\n        at com.floragunn.searchguard.transport.SearchGuardRequestHandler.messageReceivedDecorate(SearchGuardRequestHandler.java:235) ~[?:?]\r\n        at com.floragunn.searchguard.ssl.transport.SearchGuardSSLRequestHandler.messageReceived(SearchGuardSSLRequestHandler.java:142) ~[?:?]\r\n        at com.floragunn.searchguard.SearchGuardPlugin$7$1.messageReceived(SearchGuardPlugin.java:601) ~[?:?]\r\n        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:66) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport$RequestHandler.doRun(TcpTransport.java:1592) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:724) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_121]\r\n        ... 1 more\r\n```\r\n\r\n```\r\n[2018-09-18T08:00:03,107][WARN ][o.e.g.GatewayAllocator$InternalReplicaShardAllocator] [mes-any-log004-mes_any_log1] [koopa-app-logs-2018.09.18-06:00][1]: failed to list shard for shard_store on node [5dgwU1uK\r\nTwKhDFxmx5DaKQ]\r\norg.elasticsearch.action.FailedNodeException: Failed node [5dgwU1uKTwKhDFxmx5DaKQ]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.onFailure(TransportNodesAction.java:237) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$200(TransportNodesAction.java:153) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction$1.handleException(TransportNodesAction.java:211) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at com.floragunn.searchguard.transport.SearchGuardInterceptor$RestoringTransportResponseHandler.handleException(SearchGuardInterceptor.java:194) ~[?:?]\r\n        at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1095) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport.lambda$handleException$34(TcpTransport.java:1510) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.common.util.concurrent.EsExecutors$1.execute(EsExecutors.java:135) [elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport.handleException(TcpTransport.java:1508) [elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport.handlerResponseError(TcpTransport.java:1500) [elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1430) [elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:64) [transport-netty4-client-6.3.0.jar:6.3.0]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:297) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:413) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:241) [netty-handler-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1336) [netty-handler-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1127) [netty-handler-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1162) [netty-handler-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) [netty-codec-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:545) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) [netty-transport-4.1.16.Final.jar:4.1.16.Final]\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.16.Final.jar:4.1.16.Final]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]\r\nCaused by: org.elasticsearch.transport.RemoteTransportException: [mes-any-log007-mes_any_log1][172.19.126.54:9300][internal:cluster/nodes/indices/shard/store[n]]\r\nCaused by: org.elasticsearch.ElasticsearchException: Failed to list store metadata for shard [[koopa-app-logs-2018.09.18-06:00][1]]\r\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:111) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:61) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction.nodeOperation(TransportNodesAction.java:140) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:260) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:256) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at com.floragunn.searchguard.ssl.transport.SearchGuardSSLRequestHandler.messageReceivedDecorate(SearchGuardSSLRequestHandler.java:170) ~[?:?]\r\n        at com.floragunn.searchguard.transport.SearchGuardRequestHandler.messageReceivedDecorate(SearchGuardRequestHandler.java:235) ~[?:?]\r\n        at com.floragunn.searchguard.ssl.transport.SearchGuardSSLRequestHandler.messageReceived(SearchGuardSSLRequestHandler.java:142) ~[?:?]\r\n        at com.floragunn.searchguard.SearchGuardPlugin$7$1.messageReceived(SearchGuardPlugin.java:601) ~[?:?]\r\n        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:66) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport$RequestHandler.doRun(TcpTransport.java:1592) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:724) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_121]\r\n        ... 1 more\r\nCaused by: java.io.FileNotFoundException: no segments* file found in store(MMapDirectory@/opt/mes-any-log1-data04/nodes/0/indices/urYrB3giTMWgflBm0TVPsw/1/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@4fca4b16): files: [write.lock]\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:670) ~[lucene-core-7.3.1.jar:7.3.1 ae0705edb59eaa567fe13ed3a222fdadc7153680 - caomanhdat - 2018-05-09 09:27:24]\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:627) ~[lucene-core-7.3.1.jar:7.3.1 ae0705edb59eaa567fe13ed3a222fdadc7153680 - caomanhdat - 2018-05-09 09:27:24]\r\n        at org.apache.lucene.index.SegmentInfos.readLatestCommit(SegmentInfos.java:434) ~[lucene-core-7.3.1.jar:7.3.1 ae0705edb59eaa567fe13ed3a222fdadc7153680 - caomanhdat - 2018-05-09 09:27:24]\r\n        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:122) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:207) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.store.Store.access$200(Store.java:134) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.store.Store$MetadataSnapshot.loadMetadata(Store.java:864) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.store.Store$MetadataSnapshot.<init>(Store.java:797) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.store.Store.getMetadata(Store.java:293) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.index.shard.IndexShard.snapshotStoreMetadata(IndexShard.java:1138) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.listStoreMetaData(TransportNodesListShardStoreMetaData.java:125) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:109) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:61) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction.nodeOperation(TransportNodesAction.java:140) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:260) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:256) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at com.floragunn.searchguard.ssl.transport.SearchGuardSSLRequestHandler.messageReceivedDecorate(SearchGuardSSLRequestHandler.java:170) ~[?:?]\r\n        at com.floragunn.searchguard.transport.SearchGuardRequestHandler.messageReceivedDecorate(SearchGuardRequestHandler.java:235) ~[?:?]\r\n        at com.floragunn.searchguard.ssl.transport.SearchGuardSSLRequestHandler.messageReceived(SearchGuardSSLRequestHandler.java:142) ~[?:?]\r\n        at com.floragunn.searchguard.SearchGuardPlugin$7$1.messageReceived(SearchGuardPlugin.java:601) ~[?:?]\r\n        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:66) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.transport.TcpTransport$RequestHandler.doRun(TcpTransport.java:1592) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:724) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_121]\r\n        ... 1 more\r\n```\r\nAt the very least this setup stopped losing indices with a replication of 1.\r\n\r\n\r\nThe btrfs devices don't have any errs on all of the nodes.\r\n```for i in b1 c1 d1 e1 f1 g1; do btrfs device stats /dev/sd$i; done\r\n[/dev/sdb1].write_io_errs   0\r\n[/dev/sdb1].read_io_errs    0\r\n[/dev/sdb1].flush_io_errs   0\r\n[/dev/sdb1].corruption_errs 0\r\n[/dev/sdb1].generation_errs 0\r\n[/dev/sdc1].write_io_errs   0\r\n[/dev/sdc1].read_io_errs    0\r\n[/dev/sdc1].flush_io_errs   0\r\n[/dev/sdc1].corruption_errs 0\r\n[/dev/sdc1].generation_errs 0\r\n[/dev/sdd1].write_io_errs   0\r\n[/dev/sdd1].read_io_errs    0\r\n[/dev/sdd1].flush_io_errs   0\r\n[/dev/sdd1].corruption_errs 0\r\n[/dev/sdd1].generation_errs 0\r\n[/dev/sde1].write_io_errs   0\r\n[/dev/sde1].read_io_errs    0\r\n[/dev/sde1].flush_io_errs   0\r\n[/dev/sde1].corruption_errs 0\r\n[/dev/sde1].generation_errs 0\r\n[/dev/sdf1].write_io_errs   0\r\n[/dev/sdf1].read_io_errs    0\r\n[/dev/sdf1].flush_io_errs   0\r\n[/dev/sdf1].corruption_errs 0\r\n[/dev/sdf1].generation_errs 0\r\n[/dev/sdg1].write_io_errs   0\r\n[/dev/sdg1].read_io_errs    0\r\n[/dev/sdg1].flush_io_errs   0\r\n[/dev/sdg1].corruption_errs 0\r\n[/dev/sdg1].generation_errs 0\r\n```\r\nI really think this is caused by Elasticsearch.\r\n\r\nPlease have a look!","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/423555038","html_url":"https://github.com/elastic/elasticsearch/issues/33661#issuecomment-423555038","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33661","id":423555038,"node_id":"MDEyOklzc3VlQ29tbWVudDQyMzU1NTAzOA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-09-21T14:39:15Z","updated_at":"2018-09-21T14:39:15Z","author_association":"CONTRIBUTOR","body":"Hi @gna582,\r\n\r\nThis corruption exception doesn't look like corruptions that we occasionally see that are due to faulty hardware. Instead it looks like the issue occurs on data tha was freshly written. The exception suggests that the expected file exists and contains some data, but trailing bytes can't be read for some reason. The fact that KVM reimplements some functionality via software might explain why you can't reproduce this issue with KVM.\r\n\r\nFYI the last logs that you shared 3 days ago are not a corruption and actually pretty harmless. This has to do with asynchronously fetching information about the shard stores on a node while other things are going on\r\n\r\nCould you clarify the following points:\r\n - When trying to reproduce with KVM, did you run the exact same hardware?\r\n - Did you really see messages indicating corruption on _every_ node, or were you seeing exceptions logged on every node because of how we propagate remote transport exceptions back to the caller?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/423937447","html_url":"https://github.com/elastic/elasticsearch/issues/33661#issuecomment-423937447","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33661","id":423937447,"node_id":"MDEyOklzc3VlQ29tbWVudDQyMzkzNzQ0Nw==","user":{"login":"gna582","id":2534537,"node_id":"MDQ6VXNlcjI1MzQ1Mzc=","avatar_url":"https://avatars0.githubusercontent.com/u/2534537?v=4","gravatar_id":"","url":"https://api.github.com/users/gna582","html_url":"https://github.com/gna582","followers_url":"https://api.github.com/users/gna582/followers","following_url":"https://api.github.com/users/gna582/following{/other_user}","gists_url":"https://api.github.com/users/gna582/gists{/gist_id}","starred_url":"https://api.github.com/users/gna582/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gna582/subscriptions","organizations_url":"https://api.github.com/users/gna582/orgs","repos_url":"https://api.github.com/users/gna582/repos","events_url":"https://api.github.com/users/gna582/events{/privacy}","received_events_url":"https://api.github.com/users/gna582/received_events","type":"User","site_admin":false},"created_at":"2018-09-24T10:54:06Z","updated_at":"2018-09-24T10:56:17Z","author_association":"NONE","body":"Dear @jpountz ,\r\n\r\nThank you for replying!\r\n\r\n- I didn't run the KVM on the same hardware. I run a QA environment on a general KVM cluster with the same config where the problem does not occur even though it hold's a few hundred GB of QA environment logdata and it shows up in live with far less data involved. It even happened with the .monitoring indices.\r\n- I doublechecked whether it occurs on _every_ node. You guessed correctly. It didn't happen on two machines but on all 5 others.\r\n\r\n```\r\nzgrep \"shard failed\" mes_any_log-2018-09-14-1.log.gz on all machines:\r\n\r\n[2018-09-14T02:01:47,420][WARN ][o.e.i.c.IndicesClusterStateService] [mes-any-log001-mes_any_log1] [[.monitoring-es-6-2018.09.14][0]] marking and sending shard failed due to [shard failure, reason [refresh failed source[schedule]]]\r\n=> 42 overall\r\n\r\n002:\r\n=> 0 overall\r\n\r\n[2018-09-14T17:40:07,521][WARN ][o.e.i.c.IndicesClusterStateService] [mes-any-log003-mes_any_log1] [[koopa-app-logs-2018.09.14-14:00][0]] marking and sending shard failed due to [failed recovery]\r\n=> 1 overall\r\n\r\n[2018-09-14T23:30:24,302][WARN ][o.e.i.c.IndicesClusterStateService] [mes-any-log004-mes_any_log1] [[koopa-app-logs-2018.09.14-21:00][1]] marking and sending shard failed due to [shard failure, reason [already closed by tragic event on the index writer]]\r\n=> 28 overall\r\n\r\n[2018-09-14T16:40:07,335][WARN ][o.e.i.c.IndicesClusterStateService] [mes-any-log005-mes_any_log1] [[koopa-app-logs-2018.09.14-13:00][1]] marking and sending shard failed due to [failed recovery]\r\n=> 23 overall\r\n\r\n006:\r\n=> 0 overall\r\n\r\n[2018-09-14T21:26:03,867][WARN ][o.e.i.c.IndicesClusterStateService] [mes-any-log007-mes_any_log1] [[koopa-app-logs-2018.09.14-19:00][0]] marking and sending shard failed due to [shard failure, reason [merge failed]]\r\n=> 22 overall\r\n```\r\n\r\nThe btrfs error counters are still all 0 on all machines\r\n```\r\nfor i in b1 c1 d1 e1 f1 g1; do btrfs device stats /dev/sd$i; done\r\n\r\n[/dev/sdb1].write_io_errs   0\r\n[/dev/sdb1].read_io_errs    0\r\n[/dev/sdb1].flush_io_errs   0\r\n[/dev/sdb1].corruption_errs 0\r\n[/dev/sdb1].generation_errs 0\r\n[/dev/sdc1].write_io_errs   0\r\n[/dev/sdc1].read_io_errs    0\r\n[/dev/sdc1].flush_io_errs   0\r\n[/dev/sdc1].corruption_errs 0\r\n[/dev/sdc1].generation_errs 0\r\n[/dev/sdd1].write_io_errs   0\r\n[/dev/sdd1].read_io_errs    0\r\n[/dev/sdd1].flush_io_errs   0\r\n[/dev/sdd1].corruption_errs 0\r\n[/dev/sdd1].generation_errs 0\r\n[/dev/sde1].write_io_errs   0\r\n[/dev/sde1].read_io_errs    0\r\n[/dev/sde1].flush_io_errs   0\r\n[/dev/sde1].corruption_errs 0\r\n[/dev/sde1].generation_errs 0\r\n[/dev/sdf1].write_io_errs   0\r\n[/dev/sdf1].read_io_errs    0\r\n[/dev/sdf1].flush_io_errs   0\r\n[/dev/sdf1].corruption_errs 0\r\n[/dev/sdf1].generation_errs 0\r\n[/dev/sdg1].write_io_errs   0\r\n[/dev/sdg1].read_io_errs    0\r\n[/dev/sdg1].flush_io_errs   0\r\n[/dev/sdg1].corruption_errs 0\r\n[/dev/sdg1].generation_errs 0\r\n```\r\n\r\nThank you for clarifying that the current state is harmless. This helps me going forward with the upgrade of another 5.3.1 cluster.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/473388799","html_url":"https://github.com/elastic/elasticsearch/issues/33661#issuecomment-473388799","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/33661","id":473388799,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3MzM4ODc5OQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2019-03-15T18:06:26Z","updated_at":"2019-03-15T18:06:26Z","author_association":"CONTRIBUTOR","body":"There is very little we can investigate on our end here, which is why I'm closing this one out. @gna582 do let us know if you have more news here. Thank you.","performed_via_github_app":null}]