[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73059024","html_url":"https://github.com/elastic/elasticsearch/issues/9498#issuecomment-73059024","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9498","id":73059024,"node_id":"MDEyOklzc3VlQ29tbWVudDczMDU5MDI0","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2015-02-05T14:56:26Z","updated_at":"2015-02-05T14:56:26Z","author_association":"MEMBER","body":"+1 on this change, my suggestion to upgrade is to upgrade on node startup automatically. We could scan the data directory, and rebalance shards data paths during startup, and only then make the node available. If we can use MOVE of files it would be great, and if MOVE fails, we can resort to copying it over so it will be more lightweight.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/76695853","html_url":"https://github.com/elastic/elasticsearch/issues/9498#issuecomment-76695853","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9498","id":76695853,"node_id":"MDEyOklzc3VlQ29tbWVudDc2Njk1ODUz","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-03-02T11:22:21Z","updated_at":"2015-03-02T11:22:21Z","author_association":"CONTRIBUTOR","body":"moved out to 2.0 for now\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/83123119","html_url":"https://github.com/elastic/elasticsearch/issues/9498#issuecomment-83123119","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9498","id":83123119,"node_id":"MDEyOklzc3VlQ29tbWVudDgzMTIzMTE5","user":{"login":"joealex","id":115545,"node_id":"MDQ6VXNlcjExNTU0NQ==","avatar_url":"https://avatars3.githubusercontent.com/u/115545?v=4","gravatar_id":"","url":"https://api.github.com/users/joealex","html_url":"https://github.com/joealex","followers_url":"https://api.github.com/users/joealex/followers","following_url":"https://api.github.com/users/joealex/following{/other_user}","gists_url":"https://api.github.com/users/joealex/gists{/gist_id}","starred_url":"https://api.github.com/users/joealex/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/joealex/subscriptions","organizations_url":"https://api.github.com/users/joealex/orgs","repos_url":"https://api.github.com/users/joealex/repos","events_url":"https://api.github.com/users/joealex/events{/privacy}","received_events_url":"https://api.github.com/users/joealex/received_events","type":"User","site_admin":false},"created_at":"2015-03-18T19:02:55Z","updated_at":"2015-03-18T20:24:34Z","author_association":"NONE","body":"This is a very common scenario when you have very large clusters and multiple disks on each node. Some disk is bound to fail every few days. We had a bunch of disk failures across some days and took some time to figure out the cause was due to shards striped across multiple disks. When it runs it is perfect since the load is balanced and the multiple disk spins are utilized. We had to shutdown ES on the issue node, remove the disk from ES config and bring ES back up after sometime. This forced the Node refresh of data from other Nodes.\n\nWith the all shard data on one disk, balancing/utilizing all disks may have to be given a good look, ie if this will cause some disks to be over-utilized due to that index living there getting more data. Also about reading performance since now all from one single disk.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/83150195","html_url":"https://github.com/elastic/elasticsearch/issues/9498#issuecomment-83150195","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9498","id":83150195,"node_id":"MDEyOklzc3VlQ29tbWVudDgzMTUwMTk1","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2015-03-18T20:12:24Z","updated_at":"2015-03-18T20:12:24Z","author_association":"CONTRIBUTOR","body":"+1 on doing this.\n\n> likely in the background such that the upgrade process for users is smooth\n\nI'm not sure how this can be nicely automated.....  I think the optimal process involves slowly moving away from this by moving shards off of a node (a few nodes?), taking it out of the cluster, reformatting its multiple disks to proper raid, adding it back to the cluster, and letting the cluster heal.  I suppose in many respects its similar to replacing the disks entirely.  We did that about a year ago with great success, btw. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/90260965","html_url":"https://github.com/elastic/elasticsearch/issues/9498#issuecomment-90260965","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9498","id":90260965,"node_id":"MDEyOklzc3VlQ29tbWVudDkwMjYwOTY1","user":{"login":"josephglanville","id":605591,"node_id":"MDQ6VXNlcjYwNTU5MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/605591?v=4","gravatar_id":"","url":"https://api.github.com/users/josephglanville","html_url":"https://github.com/josephglanville","followers_url":"https://api.github.com/users/josephglanville/followers","following_url":"https://api.github.com/users/josephglanville/following{/other_user}","gists_url":"https://api.github.com/users/josephglanville/gists{/gist_id}","starred_url":"https://api.github.com/users/josephglanville/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/josephglanville/subscriptions","organizations_url":"https://api.github.com/users/josephglanville/orgs","repos_url":"https://api.github.com/users/josephglanville/repos","events_url":"https://api.github.com/users/josephglanville/events{/privacy}","received_events_url":"https://api.github.com/users/josephglanville/received_events","type":"User","site_admin":false},"created_at":"2015-04-06T22:12:03Z","updated_at":"2015-04-06T22:13:12Z","author_association":"NONE","body":"Overall +1 to this change for increasing durability of shards.\n\nHowever once this change goes though one will need to either resort to RAID of some description of more shards to make up for the loss of spindles.\n\nIn light of this change having multiple shards allocated to the same node would actually be beneficial in terms of IO throughput but could anyone comment on any drawbacks of this? It seems to be the most elegant solution to restore the lost performance without resorting to external systems.\n\nIf the increase in shards is indeed feasible it would also be nice as a latter optimisation to attempt to allocate shards of the same index to different disks.\n","performed_via_github_app":null}]