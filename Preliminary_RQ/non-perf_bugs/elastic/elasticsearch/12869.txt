{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/12869","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12869/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12869/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12869/events","html_url":"https://github.com/elastic/elasticsearch/issues/12869","id":100953454,"node_id":"MDU6SXNzdWUxMDA5NTM0NTQ=","number":12869,"title":"Socket appender logging to unavailable Logstash node kills Elasticsearch node","user":{"login":"HenrikOssipoff","id":4737224,"node_id":"MDQ6VXNlcjQ3MzcyMjQ=","avatar_url":"https://avatars2.githubusercontent.com/u/4737224?v=4","gravatar_id":"","url":"https://api.github.com/users/HenrikOssipoff","html_url":"https://github.com/HenrikOssipoff","followers_url":"https://api.github.com/users/HenrikOssipoff/followers","following_url":"https://api.github.com/users/HenrikOssipoff/following{/other_user}","gists_url":"https://api.github.com/users/HenrikOssipoff/gists{/gist_id}","starred_url":"https://api.github.com/users/HenrikOssipoff/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HenrikOssipoff/subscriptions","organizations_url":"https://api.github.com/users/HenrikOssipoff/orgs","repos_url":"https://api.github.com/users/HenrikOssipoff/repos","events_url":"https://api.github.com/users/HenrikOssipoff/events{/privacy}","received_events_url":"https://api.github.com/users/HenrikOssipoff/received_events","type":"User","site_admin":false},"labels":[{"id":151561891,"node_id":"MDU6TGFiZWwxNTE1NjE4OTE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Infra/Logging","name":":Core/Infra/Logging","color":"0e8a16","default":false,"description":"Log management and logging utilities"},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2015-08-14T07:39:04Z","updated_at":"2015-10-16T09:40:28Z","closed_at":"2015-10-16T09:40:28Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"We experienced an issue today in our production cluster, after a cluster upgrade from 1.6.x to 1.7.1.\n\n(In all of the below, our \"Elasticsearch cluster\" is a 3 node setup running production data for our e-commerce platform, whereas our \"Logstash cluster\" is a different setup with other Elasticsearch nodes and the Logstash service)\n\nOur whole Elasticsearch cluster is set to have a socket appender log option, to our Logstash cluster. Previously in 1.6.x when the Logstash service was unavailable on the cluster, nothing bad would happen (except missing log entries obviously), but after our upgrade to 1.7.x we yesterday experienced some weird behaviour.\n\nIt is as if about 5-10 minutes pass if the Logstash service dies, after which exactly one node in our Elasticsearch cluster will start to exhibit the following:\n- Most of the times it will act as it's still part of the whole cluster, and will give cluster status and correct master response\n- Some times, above _cat operations will give the 30sec timeout\n- All queries, GETs, deleted, index operations and even a _cat/indices will result in a request that seems to never terminate\n\nAll other nodes in the cluster _seems_ to work. Not entirely sure.\n\nBefore we identified the missing Logstash service as the culprit, we attempted a full cluster restart. Bringing node 1 and node 2 up, those worked for a while, but when about 5 minutes passed and we brought up node 3 (and it started recovery), node 1 started it's weird behaviour again -- resulting in the recovery on node 3 to hang, because it attempted to read data from node 1.\n\nWe also have file logging in place, and nothing seems to be in the logs around this issue, on either node.\n\nI'm not entirely sure about all of this, but wanted to report it just in case. It seems weird that a missing log source should be able to bring the whole cluster down - and since we didn't see the same behaviour when we were running 1.6.x, it's tempting to believe this is something that is caused by the upgrade to 1.7.1, made only a few days ahead of all of this.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}