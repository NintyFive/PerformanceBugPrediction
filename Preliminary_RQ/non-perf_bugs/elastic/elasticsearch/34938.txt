{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/34938","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34938/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34938/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34938/events","html_url":"https://github.com/elastic/elasticsearch/issues/34938","id":374574795,"node_id":"MDU6SXNzdWUzNzQ1NzQ3OTU=","number":34938,"title":"ILM shrink action runs when shards aren't allocated on the same node","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"labels":[{"id":912828538,"node_id":"MDU6TGFiZWw5MTI4Mjg1Mzg=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Features/ILM+SLM","name":":Core/Features/ILM+SLM","color":"0e8a16","default":false,"description":"Index and Snapshot lifecycle management"},{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null},{"id":92913658,"node_id":"MDU6TGFiZWw5MjkxMzY1OA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/blocker","name":"blocker","color":"e11d21","default":false,"description":null}],"state":"closed","locked":false,"assignee":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"assignees":[{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2018-10-26T23:03:34Z","updated_at":"2018-11-05T19:03:00Z","closed_at":"2018-11-05T19:03:00Z","author_association":"MEMBER","active_lock_reason":null,"body":"For ILM, we have a step that allocates an index on a single machine so that we can then call the shrink/resize action, however, in some cases, the shrink can be run after allocating the index to a single node, but still error out related to the shards not being on the same node:\r\n\r\n```\r\n    \"test-000019\" : {\r\n      \"step\" : \"ERROR\",\r\n      \"step_time\" : 1540588519429,\r\n      \"step_info\" : {\r\n        \"type\" : \"illegal_state_exception\",\r\n        \"reason\" : \"index test-000019 must have all shards allocated on the same node to shrink index\"\r\n      }\r\n    },\r\n```\r\n\r\nI was able to reproduce this with the following configuration:\r\n\r\n- 2 nodes with the \"hot\" type\r\n- 3 nodes with the \"cold\" type\r\n- 1 node with the \"other\" type\r\n\r\nUsing a 1 second poll interval:\r\n\r\n```\r\nPUT /_cluster/settings\r\n{\r\n  \"transient\": {\r\n    \"logger.org.elasticsearch.xpack.core.indexlifecycle\": \"TRACE\",\r\n    \"logger.org.elasticsearch.xpack.indexlifecycle\": \"TRACE\",\r\n    \"indices.lifecycle.poll_interval\": \"1s\"\r\n  }\r\n}\r\n```\r\n\r\nThe following policy:\r\n\r\n```\r\nPUT _ilm/my_lifecycle3\r\n{\r\n  \"policy\": {\r\n    \"phases\": {\r\n      \"hot\": {\r\n        \"actions\": {\r\n          \"rollover\": {\r\n            \"max_age\": \"5s\"\r\n          }\r\n        }\r\n      },\r\n      \"warm\": {\r\n        \"minimum_age\": \"30s\",\r\n        \"actions\": {\r\n          \"forcemerge\": {\r\n            \"max_num_segments\": 1\r\n          },\r\n          \"shrink\": {\r\n            \"number_of_shards\": 1\r\n          },\r\n          \"allocate\": {\r\n            \"include\": {\r\n              \"type\": \"\"\r\n            },\r\n            \"exclude\": {},\r\n            \"require\": {}\r\n          }\r\n        }\r\n      },\r\n      \"cold\": {\r\n        \"minimum_age\": \"1m\",\r\n        \"actions\": {\r\n          \"allocate\": {\r\n            \"number_of_replicas\": 2,\r\n            \"include\": {\r\n              \"type\": \"cold\"\r\n            },\r\n            \"exclude\": {},\r\n            \"require\": {}\r\n          }\r\n        }\r\n      },\r\n      \"delete\": {\r\n        \"minimum_age\": \"2m\",\r\n        \"actions\": {\r\n          \"delete\": {}\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nIndex template:\r\n\r\n```\r\nPUT _template/my_template\r\n{\r\n  \"index_patterns\": [\"test-*\"],\r\n  \"settings\": {\r\n    \"number_of_shards\": 2,\r\n    \"number_of_replicas\": 1,\r\n    \"index.lifecycle.name\": \"my_lifecycle3\",\r\n    \"index.lifecycle.rollover_alias\": \"test-alias\",\r\n    \"index.routing.allocation.include.type\": \"hot\"\r\n  }\r\n}\r\n```\r\n\r\nThen, I created an index:\r\n\r\n```\r\nPUT test-000001\r\n{\r\n  \"aliases\": {\r\n    \"test-alias\":{\r\n      \"is_write_index\": true\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nAnd then continually ran:\r\n\r\n```\r\nGET /*/_ilm/explain?filter_path=indices.*.step*\r\n```\r\n\r\nUntil I saw a failure similar to the one above (took 1-30 minutes to reproduce).\r\n\r\nI've added additional logging to see what's going on with the node:\r\n\r\n```\r\n[2018-10-26T15:15:19,399][TRACE][o.e.x.i.ExecuteStepsUpdateTask] [hot1] [test-000019] waiting for cluster state step condition (AllocationRoutedStep) [{\"phase\":\"warm\",\"action\":\"shrink\",\"name\":\"check-allocation\"}], next: [{\"phase\":\"warm\",\"action\":\"shrink\",\"name\":\"shrink\"}]\r\n[2018-10-26T15:15:19,399][DEBUG][o.e.x.c.i.AllocationRoutedStep] [hot1] --> SHRINK checking whether [test-000019] has enough shards allocated\r\n[2018-10-26T15:15:19,399][DEBUG][o.e.x.c.i.AllocationRoutedStep] [hot1] --> shard [test-000019][1], node[Mi73iCROTT2dM4We9oQIgA], [P], s[STARTED], a[id=IXX6Ix8EQdmsvhNT-7BQug] cannot remain on Mi73iCROTT2dM4We9oQIgA, allocPendingThisShard: 1\r\n[2018-10-26T15:15:19,399][DEBUG][o.e.x.c.i.AllocationRoutedStep] [hot1] --> SHRINK shardCopiesThisShard(2) - allocationPendingThisShard(1) == 0 ? 1 \r\n[2018-10-26T15:15:19,399][DEBUG][o.e.x.c.i.AllocationRoutedStep] [hot1] --> shard [test-000019][0], node[RiSQ1bfhSkS_G90VZH-BLA], [R], s[STARTED], a[id=iCGSUFcYRXWl8yvDtcuhHg] cannot remain on RiSQ1bfhSkS_G90VZH-BLA, allocPendingThisShard: 1\r\n[2018-10-26T15:15:19,399][DEBUG][o.e.x.c.i.AllocationRoutedStep] [hot1] --> SHRINK shardCopiesThisShard(2) - allocationPendingThisShard(1) == 0 ? 1 \r\n[2018-10-26T15:15:19,399][DEBUG][o.e.x.c.i.AllocationRoutedStep] [hot1] SHRINK [shrink] lifecycle action for index [[test-000019/pIKgUp5bTpCxZhJMOAWRxg]] complete\r\n[2018-10-26T15:15:19,399][DEBUG][o.e.x.c.i.AllocationRoutedStep] [hot1] --> test-000019 SUCCESS allocationPendingAllShards: 0\r\n[2018-10-26T15:15:19,399][TRACE][o.e.x.i.ExecuteStepsUpdateTask] [hot1] [test-000019] cluster state step condition met successfully (AllocationRoutedStep) [{\"phase\":\"warm\",\"action\":\"shrink\",\"name\":\"check-allocation\"}], moving to next step {\"phase\":\"warm\",\"action\":\"shrink\",\"name\":\"shrink\"}\r\n```\r\n\r\nAnd then a bit further down:\r\n\r\n```\r\n[2018-10-26T15:15:19,428][ERROR][o.e.x.i.IndexLifecycleRunner] [hot1] policy [my_lifecycle3] for index [test-000019] failed on step [{\"phase\":\"warm\",\"action\":\"shrink\",\"name\":\"shrink\"}]. Moving to ERROR step\r\njava.lang.IllegalStateException: index test-000019 must have all shards allocated on the same node to shrink index\r\n\tat org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateShrinkIndex(MetaDataCreateIndexService.java:679) ~[elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.prepareResizeIndexSettings(MetaDataCreateIndexService.java:740) ~[elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$IndexCreationTask.execute(MetaDataCreateIndexService.java:406) ~[elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45) ~[elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:639) ~[elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:268) ~[elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:198) [elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:133) [elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) [elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188) [elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:624) [elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:244) [elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:207) [elasticsearch-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\r\n\tat java.lang.Thread.run(Thread.java:834) [?:?]\r\n```\r\n\r\nIt *looks* like the check succeeds and that the shards are in the right place, but then the shrink fails nonetheless.\r\n\r\nIt's worth noting I could only reproduce this with a 1 second poll interval, so it may be a timing issue. Also, it does appear that the shard is correctly allocated from /_cat/shards output (`hot2` is the node that ILM set as the `_name` allocation filtering target):\r\n\r\n```\r\ntest-000019        1     r      STARTED    0  261b 127.0.0.1 hot2\r\ntest-000019        1     p      STARTED    0  261b 127.0.0.1 hot1\r\ntest-000019        0     p      STARTED    0  261b 127.0.0.1 hot2\r\ntest-000019        0     r      STARTED    0  261b 127.0.0.1 other\r\n```","closed_by":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"performed_via_github_app":null}