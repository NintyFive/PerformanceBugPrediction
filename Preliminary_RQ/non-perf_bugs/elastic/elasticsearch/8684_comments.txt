[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/64792496","html_url":"https://github.com/elastic/elasticsearch/issues/8684#issuecomment-64792496","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684","id":64792496,"node_id":"MDEyOklzc3VlQ29tbWVudDY0NzkyNDk2","user":{"login":"jillesvangurp","id":819187,"node_id":"MDQ6VXNlcjgxOTE4Nw==","avatar_url":"https://avatars2.githubusercontent.com/u/819187?v=4","gravatar_id":"","url":"https://api.github.com/users/jillesvangurp","html_url":"https://github.com/jillesvangurp","followers_url":"https://api.github.com/users/jillesvangurp/followers","following_url":"https://api.github.com/users/jillesvangurp/following{/other_user}","gists_url":"https://api.github.com/users/jillesvangurp/gists{/gist_id}","starred_url":"https://api.github.com/users/jillesvangurp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jillesvangurp/subscriptions","organizations_url":"https://api.github.com/users/jillesvangurp/orgs","repos_url":"https://api.github.com/users/jillesvangurp/repos","events_url":"https://api.github.com/users/jillesvangurp/events{/privacy}","received_events_url":"https://api.github.com/users/jillesvangurp/received_events","type":"User","site_admin":false},"created_at":"2014-11-27T13:51:00Z","updated_at":"2014-11-27T13:51:00Z","author_association":"CONTRIBUTOR","body":"some more info from the log\n\n```\n[2014-11-27 00:48:48,050][DEBUG][action.bulk              ] [192.168.1.15] [logstash-2014.11.26][3] failed to execute bulk item (index) index {[logstash-2014.11.26][scheduler][AUnugds1mlBs-FhUr0lx], source[{\"@version\":1,\"@timestamp\":\"2014-11-27T00:48:47.322+01:00\",\"level\":\"INFO\",\"type\":\"scheduler\",\"logger_name\":\"dispatcher\",\"job\":{\"id\":\"q4JDAm8mvtE\",\"worker\":\"exchange\",\"task\":\"ts_sync\",\"status\":\"failed\",\"meta\":{},\"service\":\"provider\",\"node\":{\"name\":\"exchange\",\"host\":\"https://api.inbot.io\",\"running\":0,\"jobs\":[]},\"account\":{\"provider\":\"exchange\",\"human_uid\":\"jouni.hannula@vauraus.fi\",\"credentials\":{\"password_encrypted\":\"3D4E96CAE509453E59AEC4FEDB4FDA7F$JAY9bnLFEfkDRtdos9mhEBkkLb-9BHlNTXGNI-5vpmU\",\"username\":\"jouni.hannula@vauraus.fi\",\"password\":\"xlYuPyC+nDuT/WwLlzLRnKIcAxcnnIyZdn+BBbwSYBA=\"}},\"user\":{\"id\":\"IJ77m8NgLVr5J1Nrha_pSg\"},\"parameters\":{},\"result\":{\"ingestedObjects\":0},\"error\":{\"name\":\"UnexpectedWorkerError\",\"message\":\"Unexpected error from inbot server: null\",\"error_handle\":\"error\",\"original_error\":{\"cause\":{\"suppressed\":[]},\"suppressed\":[]},\"error\":\"io.linko.worker.exceptions.ExchangeException: Unexpected error from inbot server: null\\n\\tat io.linko.worker.services.InbotConnector.ingestData(InbotConnector.java:62)\\n\\tat io.linko.worker.services.ExchangeService.queryFolder(ExchangeService.java:425)\\n\\tat io.linko.worker.services.ExchangeService.tsSync(ExchangeService.java:258)\\n\\tat io.linko.worker.resources.WorkerResource.lambda$postTsSync$155(WorkerResource.java:102)\\n\\tat io.linko.worker.resources.WorkerResource$$Lambda$77/63591602.apply(Unknown Source)\\n\\tat io.linko.worker.resources.WorkerResource.execExchange(WorkerResource.java:178)\\n\\tat io.linko.worker.resources.WorkerResource.postTsSync(WorkerResource.java:97)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:483)\\n\\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:151)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:171)\\n\\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:152)\\n\\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:104)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:384)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:342)\\n\\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:101)\\n\\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:271)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)\\n\\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:315)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:297)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:267)\\n\\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:297)\\n\\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:254)\\n\\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:1030)\\n\\tat org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:378)\\n\\tat io.linko.ng.server.InbotHttpContainer.service(InbotHttpContainer.java:83)\\n\\tat org.glassfish.grizzly.http.server.HttpHandler.runService(HttpHandler.java:204)\\n\\tat org.glassfish.grizzly.http.server.HttpHandler.doHandle(HttpHandler.java:178)\\n\\tat org.glassfish.grizzly.http.server.HttpHandlerChain.doHandle(HttpHandlerChain.java:197)\\n\\tat org.glassfish.grizzly.http.server.HttpServerFilter.handleRead(HttpServerFilter.java:235)\\n\\tat org.glassfish.grizzly.filterchain.ExecutorResolver$9.execute(ExecutorResolver.java:119)\\n\\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.executeFilter(DefaultFilterChain.java:284)\\n\\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.executeChainPart(DefaultFilterChain.java:201)\\n\\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.execute(DefaultFilterChain.java:133)\\n\\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.process(DefaultFilterChain.java:112)\\n\\tat org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:77)\\n\\tat org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:561)\\n\\tat org.glassfish.grizzly.strategies.AbstractIOStrategy.fireIOEvent(AbstractIOStrategy.java:112)\\n\\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.run0(WorkerThreadIOStrategy.java:117)\\n\\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.access$100(WorkerThreadIOStrategy.java:56)\\n\\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy$WorkerThreadRunnable.run(WorkerThreadIOStrategy.java:137)\\n\\tat org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)\\n\\tat org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)\\n\\tat java.lang.Thread.run(Thread.java:745)\\nCaused by: javax.ws.rs.ProcessingException\\n\\tat org.glassfish.jersey.grizzly.connector.GrizzlyConnector.apply(GrizzlyConnector.java:259)\\n\\tat org.glassfish.jersey.client.ClientRuntime.invoke(ClientRuntime.java:246)\\n\\tat org.glassfish.jersey.client.JerseyInvocation$1.call(JerseyInvocation.java:667)\\n\\tat org.glassfish.jersey.client.JerseyInvocation$1.call(JerseyInvocation.java:664)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:315)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:297)\\n\\tat org.glassfish.jersey.internal.Errors.process(Errors.java:228)\\n\\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:424)\\n\\tat org.glassfish.jersey.client.JerseyInvocation.invoke(JerseyInvocation.java:664)\\n\\tat org.glassfish.jersey.client.JerseyInvocation$Builder.method(JerseyInvocation.java:424)\\n\\tat org.glassfish.jersey.client.JerseyInvocation$Builder.post(JerseyInvocation.java:333)\\n\\tat io.linko.worker.utils.RestClient.POST(RestClient.java:81)\\n\\tat io.linko.worker.services.InbotConnector.ingestData(InbotConnector.java:51)\\n\\t... 47 more\\nCaused by: java.lang.NullPointerException\\n\\tat io.linko.ng.server.JsonArrayProvider.writeTo(JsonArrayProvider.java:74)\\n\\tat io.linko.ng.server.JsonArrayProvider.writeTo(JsonArrayProvider.java:35)\\n\\tat org.glassfish.jersey.message.internal.WriterInterceptorExecutor$TerminalWriterInterceptor.invokeWriteTo(WriterInterceptorExecutor.java:265)\\n\\tat org.glassfish.jersey.message.internal.WriterInterceptorExecutor$TerminalWriterInterceptor.aroundWriteTo(WriterInterceptorExecutor.java:250)\\n\\tat org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:162)\\n\\tat org.glassfish.jersey.message.internal.MessageBodyFactory.writeTo(MessageBodyFactory.java:1154)\\n\\tat org.glassfish.jersey.client.ClientRequest.writeEntity(ClientRequest.java:503)\\n\\tat org.glassfish.jersey.grizzly.connector.GrizzlyConnector$7.writeEntity(GrizzlyConnector.java:500)\\n\\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$EntityWriterBodyHandler.doHandle(GrizzlyAsyncHttpProvider.java:2117)\\n\\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider.sendRequest(GrizzlyAsyncHttpProvider.java:560)\\n\\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$AsyncHttpClientFilter.sendAsGrizzlyRequest(GrizzlyAsyncHttpProvider.java:943)\\n\\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$AsyncHttpClientFilter.handleWrite(GrizzlyAsyncHttpProvider.java:811)\\n\\tat org.glassfish.grizzly.filterchain.ExecutorResolver$8.execute(ExecutorResolver.java:111)\\n\\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.executeFilter(DefaultFilterChain.java:284)\\n\\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.executeChainPart(DefaultFilterChain.java:201)\\n\\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.execute(DefaultFilterChain.java:133)\\n\\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.process(DefaultFilterChain.java:112)\\n\\tat org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:77)\\n\\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.write(DefaultFilterChain.java:413)\\n\\tat org.glassfish.grizzly.nio.NIOConnection.write(NIOConnection.java:407)\\n\\tat org.glassfish.grizzly.nio.NIOConnection.write(NIOConnection.java:381)\\n\\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider.execute(GrizzlyAsyncHttpProvider.java:322)\\n\\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$1.completed(GrizzlyAsyncHttpProvider.java:238)\\n\\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$1.completed(GrizzlyAsyncHttpProvider.java:224)\\n\\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$ConnectionManager$1.completed(GrizzlyAsyncHttpProvider.java:2572)\\n\\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$ConnectionManager$1.completed(GrizzlyAsyncHttpProvider.java:2550)\\n\\tat org.glassfish.grizzly.nio.transport.TCPNIOConnectorHandler$EnableReadHandler.onComplete(TCPNIOConnectorHandler.java:342)\\n\\tat org.glassfish.grizzly.ProcessorExecutor.complete(ProcessorExecutor.java:115)\\n\\tat org.glassfish.grizzly.ProcessorExecutor.complete0(ProcessorExecutor.java:204)\\n\\tat org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:86)\\n\\tat org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:561)\\n\\tat org.glassfish.grizzly.nio.transport.TCPNIOConnectorHandler.onConnectedAsync(TCPNIOConnectorHandler.java:221)\\n\\tat org.glassfish.grizzly.nio.transport.TCPNIOConnectorHandler$1.connected(TCPNIOConnectorHandler.java:154)\\n\\tat org.glassfish.grizzly.nio.transport.TCPNIOConnection.onConnect(TCPNIOConnection.java:258)\\n\\tat org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:552)\\n\\tat org.glassfish.grizzly.strategies.AbstractIOStrategy.fireIOEvent(AbstractIOStrategy.java:112)\\n\\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.run0(WorkerThreadIOStrategy.java:117)\\n\\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.executeIoEvent(WorkerThreadIOStrategy.java:103)\\n\\tat org.glassfish.grizzly.strategies.AbstractIOStrategy.executeIoEvent(AbstractIOStrategy.java:89)\\n\\tat org.glassfish.grizzly.nio.SelectorRunner.iterateKeyEvents(SelectorRunner.java:414)\\n\\tat org.glassfish.grizzly.nio.SelectorRunner.iterateKeys(SelectorRunner.java:383)\\n\\tat org.glassfish.grizzly.nio.SelectorRunner.doSelect(SelectorRunner.java:347)\\n\\tat org.glassfish.grizzly.nio.SelectorRunner.run(SelectorRunner.java:278)\\n\\t... 3 more\\n\",\"code\":500},\"stats\":{},\"updated_at\":\"2014-11-26T23:48:47.321Z\",\"started_at\":\"2014-11-26T23:47:57.673Z\",\"self\":\"http://localhost:3200/scheduler/status/q4JDAm8mvtE\"},\"source\":\"dispatcher.finished\",\"message\":\"Job finished with status failed\",\"host\":\"app2.inbot.io\"}]}\norg.elasticsearch.index.mapper.MapperParsingException: object mapping for [scheduler] tried to parse as object, but got EOF, has a concrete value been provided to it?\n        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:498)\n        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:541)\n        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:490)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:392)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:444)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:150)\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n[2014-11-27 01:00:00,747][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] creating index, cause [auto(bulk api)], shards [5]/[2], mappings [_default_]\n[2014-11-27 01:00:01,862][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:00:02,082][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [collectd] (dynamic)\n[2014-11-27 01:00:04,522][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [inbot_metrics] (dynamic)\n[2014-11-27 01:00:07,358][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [nginx_access] (dynamic)\n[2014-11-27 01:00:07,527][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:00:08,095][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [inbot_api] (dynamic)\n[2014-11-27 01:00:08,218][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:00:08,363][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:00:08,533][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:00:10,832][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:00:11,375][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:00:45,149][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:00:45,963][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:02:18,291][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [inbot_api] (dynamic)\n[2014-11-27 01:02:28,603][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:02:57,662][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:02:58,641][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:03:48,477][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [nginx_access] (dynamic)\n[2014-11-27 01:03:55,143][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:03:55,497][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:03:55,745][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:03:56,145][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [inbot_api] (dynamic)\n[2014-11-27 01:03:57,799][WARN ][cluster.action.shard     ] [192.168.1.15] [logstash-2014.11.27][1] received shard failed for [logstash-2014.11.27][1], node[o9vhU4BhSCuQ4BmLJjPtfA], [R], s[STARTED], indexUUID [-mMLqYjAQuCUDcczYf5SHA], reason [Failed to perform [indices:data/write/bulk[s]] on replica, message [RemoteTransportException[[192.168.1.13][inet[/192.168.1.13:9300]][indices:data/write/bulk[s][r]]]; nested: NumberFormatException[For input string: \"finished\"]; ]]\n[2014-11-27 01:03:57,879][WARN ][cluster.action.shard     ] [192.168.1.15] [logstash-2014.11.27][1] received shard failed for [logstash-2014.11.27][1], node[o9vhU4BhSCuQ4BmLJjPtfA], [R], s[STARTED], indexUUID [-mMLqYjAQuCUDcczYf5SHA], reason [engine failure, message [indices:data/write/bulk[s] failed on replica][NumberFormatException[For input string: \"finished\"]]]\n[2014-11-27 01:03:58,152][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)\n[2014-11-27 01:03:58,252][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [inbot_api] (dynamic)\n[2014-11-27 01:04:22,901][WARN ][cluster.action.shard     ] [192.168.1.15] [logstash-2014.11.27][3] received shard failed for [logstash-2014.11.27][3], node[o9vhU4BhSCuQ4BmLJjPtfA], [R], s[STARTED], indexUUID [-mMLqYjAQuCUDcczYf5SHA], reason [engine failure, message [indices:data/write/bulk[s] failed on replica][NumberFormatExceptio:\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/64797640","html_url":"https://github.com/elastic/elasticsearch/issues/8684#issuecomment-64797640","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684","id":64797640,"node_id":"MDEyOklzc3VlQ29tbWVudDY0Nzk3NjQw","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-11-27T14:39:48Z","updated_at":"2014-11-27T14:39:48Z","author_association":"CONTRIBUTOR","body":"Hi @jillesvangurp \n\nIt sounds like you have inconsistent mapping between shards, so the same field was added to different shards at the same time: once as a string and once as a number.  Only one of those mappings made it into the master's version of the mapping (the number won, by the sounds of it). (Note: we're planning on fixing this so that we don't get into these situations any more).\n\nNow, you're trying to recover a replica (which uses the master's version of the mapping) from a primary which thinks that that field is a string.  When it tries to replay the translog, it is getting these number format exceptions.\n\nSo: how to recover from this situation?  I think you have a few options, but either way that field is useless. You can't reliably search on it. You definitely can't load fielddata for it (ie aggs or sorting or scripts), and if you have doc values, you are probably going to have merge failures later on.\n\nFirst step:  update the field mapping to set `ignore_malformed: true`.  Hopefully this will allow you to recover the shard and to continue indexing.\n\nIf that isn't sufficient, you may have to:\n- stop indexing\n- flush the index\n- close the index\n- open the index\n\nThat should allow shards to recover correctly, using the mapping from the master.\n\nIf that still doesn't work, then you may have to delete that shard and use the cluster reroute API to force assign an empty primary shard.\n\nThe way to avoid this going forward is to map this field explicitly.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/64810736","html_url":"https://github.com/elastic/elasticsearch/issues/8684#issuecomment-64810736","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684","id":64810736,"node_id":"MDEyOklzc3VlQ29tbWVudDY0ODEwNzM2","user":{"login":"jillesvangurp","id":819187,"node_id":"MDQ6VXNlcjgxOTE4Nw==","avatar_url":"https://avatars2.githubusercontent.com/u/819187?v=4","gravatar_id":"","url":"https://api.github.com/users/jillesvangurp","html_url":"https://github.com/jillesvangurp","followers_url":"https://api.github.com/users/jillesvangurp/followers","following_url":"https://api.github.com/users/jillesvangurp/following{/other_user}","gists_url":"https://api.github.com/users/jillesvangurp/gists{/gist_id}","starred_url":"https://api.github.com/users/jillesvangurp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jillesvangurp/subscriptions","organizations_url":"https://api.github.com/users/jillesvangurp/orgs","repos_url":"https://api.github.com/users/jillesvangurp/repos","events_url":"https://api.github.com/users/jillesvangurp/events{/privacy}","received_events_url":"https://api.github.com/users/jillesvangurp/received_events","type":"User","site_admin":false},"created_at":"2014-11-27T16:35:29Z","updated_at":"2014-11-27T16:35:29Z","author_association":"CONTRIBUTOR","body":"Thanks for the detailed explanation. Makes sense. I've deleted the index (easiest) and we are going to fix our logstash mapping to map most fields to strings explicitly (except those we care about being numbers). I think this should probably go into the logstash documentation in some form. The out of the box mapping will get you in trouble.\n\nThe ignore_malformed option also sounds like a good idea.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/64890608","html_url":"https://github.com/elastic/elasticsearch/issues/8684#issuecomment-64890608","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684","id":64890608,"node_id":"MDEyOklzc3VlQ29tbWVudDY0ODkwNjA4","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-11-28T12:48:55Z","updated_at":"2014-11-28T12:48:55Z","author_association":"CONTRIBUTOR","body":"Closing this in favour of #8688\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/69809901","html_url":"https://github.com/elastic/elasticsearch/issues/8684#issuecomment-69809901","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684","id":69809901,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA5OTAx","user":{"login":"eugenp","id":1022859,"node_id":"MDQ6VXNlcjEwMjI4NTk=","avatar_url":"https://avatars1.githubusercontent.com/u/1022859?v=4","gravatar_id":"","url":"https://api.github.com/users/eugenp","html_url":"https://github.com/eugenp","followers_url":"https://api.github.com/users/eugenp/followers","following_url":"https://api.github.com/users/eugenp/following{/other_user}","gists_url":"https://api.github.com/users/eugenp/gists{/gist_id}","starred_url":"https://api.github.com/users/eugenp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eugenp/subscriptions","organizations_url":"https://api.github.com/users/eugenp/orgs","repos_url":"https://api.github.com/users/eugenp/repos","events_url":"https://api.github.com/users/eugenp/events{/privacy}","received_events_url":"https://api.github.com/users/eugenp/received_events","type":"User","site_admin":false},"created_at":"2015-01-13T20:04:22Z","updated_at":"2015-01-13T20:04:34Z","author_association":"NONE","body":"I'm also seeing something very similar - using 1.4.1: \n\n```\n[2015-01-13 19:54:33,762][WARN ][indices.cluster          ] [Caregiver] [prod_defaultorg_event][8] failed to start shard\norg.elasticsearch.indices.recovery.RecoveryFailedException: [prod_defaultorg_event][8]: Recovery failed from [Gronk][AWrt4ZBVQi2Ad8U7DK7dlA][ip-172-31-20-162][inet[/172.31.20.162:9300]]{master=false} into [Caregiver][GGL-OXkdSQC0Ro5vPQN3fg][ip-172-31-17-177][inet[ip-172-31-17-177.ec2.internal/172.31.17.177:9300]]{master=false}\n        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:308)\n        at org.elasticsearch.indices.recovery.RecoveryTarget.access$200(RecoveryTarget.java:65)\n        at org.elasticsearch.indices.recovery.RecoveryTarget$2.run(RecoveryTarget.java:177)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.transport.RemoteTransportException: [Gronk][inet[/172.31.20.162:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [prod_defaultorg_event][8] Phase[2] Execution failed\n        at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1136)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:654)\n        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:137)\n        at org.elasticsearch.indices.recovery.RecoverySource.access$2600(RecoverySource.java:74)\n        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:464)\n        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:450)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.transport.RemoteTransportException: [Caregiver][inet[/172.31.17.177:9300]][internal:index/shard/recovery/translog_ops]\nCaused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse [details.dataPack.1.wsStat]\n        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:415)\n        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:707)\n        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:500)\n        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:555)\n        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:490)\n        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:555)\n        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:490)\n        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:555)\n        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:490)\n        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:541)\n        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:490)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:392)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryOperation(InternalIndexShard.java:775)\n        at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:433)\n        at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:412)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NumberFormatException: For input string: \"IDLE\"\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n        at java.lang.Double.parseDouble(Double.java:538)\n        at org.elasticsearch.common.xcontent.support.AbstractXContentParser.doubleValue(AbstractXContentParser.java:182)\n        at org.elasticsearch.index.mapper.core.DoubleFieldMapper.innerParseCreateField(DoubleFieldMapper.java:310)\n        at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:235)\n        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:405)\n        ... 18 more\n```\n- now I'm also seeing: \n\n```\n[2015-01-13 19:54:33,788][WARN ][cluster.action.shard     ] [Caregiver] [prod_defaultorg_event][8] sending failed shard for [prod_defaultorg_event][8], node[GGL-OXkdSQC0Ro5vPQN3fg], [R], s[INITIALIZING], indexUUID [sNPkCODUS9WA4J4_L0x5qQ], reason [Failed to start shard, message [RecoveryFailedException[[prod_defaultorg_event][8]: Recovery failed from [Gronk][AWrt4ZBVQi2Ad8U7DK7dlA][ip-172-31-20-162][inet[/172.31.20.162:9300]]{master=false} into [Caregiver][GGL-OXkdSQC0Ro5vPQN3fg][ip-172-31-17-177][inet[ip-172-31-17-177.ec2.internal/172.31.17.177:9300]]{master=false}]; nested: RemoteTransportException[[Gronk][inet[/172.31.20.162:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[prod_defaultorg_event][8] Phase[2] Execution failed]; nested: RemoteTransportException[[Caregiver][inet[/172.31.17.177:9300]][internal:index/shard/recovery/translog_ops]]; nested: MapperParsingException[failed to parse [details.dataPack.1.wsStat]]; nested: NumberFormatException[For input string: \"IDLE\"]; ]]\n```\n\nAs a result, 3 replica shards keep moving back and forth between nodes in a constant INITIALIZING state. \n\nIs it the same issue or potentially something else?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70148366","html_url":"https://github.com/elastic/elasticsearch/issues/8684#issuecomment-70148366","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684","id":70148366,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQ4MzY2","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-01-15T19:43:28Z","updated_at":"2015-01-15T19:43:28Z","author_association":"CONTRIBUTOR","body":"@eugenp looks like the same problem\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/220083435","html_url":"https://github.com/elastic/elasticsearch/issues/8684#issuecomment-220083435","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684","id":220083435,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMDA4MzQzNQ==","user":{"login":"Mannoj87","id":14368618,"node_id":"MDQ6VXNlcjE0MzY4NjE4","avatar_url":"https://avatars3.githubusercontent.com/u/14368618?v=4","gravatar_id":"","url":"https://api.github.com/users/Mannoj87","html_url":"https://github.com/Mannoj87","followers_url":"https://api.github.com/users/Mannoj87/followers","following_url":"https://api.github.com/users/Mannoj87/following{/other_user}","gists_url":"https://api.github.com/users/Mannoj87/gists{/gist_id}","starred_url":"https://api.github.com/users/Mannoj87/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Mannoj87/subscriptions","organizations_url":"https://api.github.com/users/Mannoj87/orgs","repos_url":"https://api.github.com/users/Mannoj87/repos","events_url":"https://api.github.com/users/Mannoj87/events{/privacy}","received_events_url":"https://api.github.com/users/Mannoj87/received_events","type":"User","site_admin":false},"created_at":"2016-05-18T16:31:32Z","updated_at":"2016-05-18T16:31:32Z","author_association":"NONE","body":"**_\"It sounds like you have inconsistent mapping between shards\"_**\n@clintongormley  - You mean to say the mapping/structure of an index can be different node wise? Or is it something internally it had hit a bug which accepted the data that it is not supposed to? Please clarify.\n","performed_via_github_app":null}]