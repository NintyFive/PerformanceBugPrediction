{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/39259","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/39259/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/39259/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/39259/events","html_url":"https://github.com/elastic/elasticsearch/issues/39259","id":413032149,"node_id":"MDU6SXNzdWU0MTMwMzIxNDk=","number":39259,"title":"Unstable ES 6.3.0 cluster due to persistent shard lock acquisition exceptions","user":{"login":"andrejbl","id":36857192,"node_id":"MDQ6VXNlcjM2ODU3MTky","avatar_url":"https://avatars1.githubusercontent.com/u/36857192?v=4","gravatar_id":"","url":"https://api.github.com/users/andrejbl","html_url":"https://github.com/andrejbl","followers_url":"https://api.github.com/users/andrejbl/followers","following_url":"https://api.github.com/users/andrejbl/following{/other_user}","gists_url":"https://api.github.com/users/andrejbl/gists{/gist_id}","starred_url":"https://api.github.com/users/andrejbl/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andrejbl/subscriptions","organizations_url":"https://api.github.com/users/andrejbl/orgs","repos_url":"https://api.github.com/users/andrejbl/repos","events_url":"https://api.github.com/users/andrejbl/events{/privacy}","received_events_url":"https://api.github.com/users/andrejbl/received_events","type":"User","site_admin":false},"labels":[{"id":836504707,"node_id":"MDU6TGFiZWw4MzY1MDQ3MDc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Distributed","name":":Distributed/Distributed","color":"0e8a16","default":false,"description":"A catch all label for anything in the Distributed Area. If you aren't sure, use this one."},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2019-02-21T17:10:00Z","updated_at":"2019-03-28T10:33:10Z","closed_at":"2019-03-20T13:26:44Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version** (`bin/elasticsearch --version`):\r\n```\r\ncurl -s -XGET localhost:9200\r\n{\r\n  \"name\" : \"redacted\",\r\n  \"cluster_name\" : \"redacted\",\r\n  \"cluster_uuid\" : \"vqMCItjLT5--emXo9fjQlA\",\r\n  \"version\" : {\r\n    \"number\" : \"6.3.0\",\r\n    \"build_flavor\" : \"default\",\r\n    \"build_type\" : \"rpm\",\r\n    \"build_hash\" : \"424e937\",\r\n    \"build_date\" : \"2018-06-11T23:38:03.357887Z\",\r\n    \"build_snapshot\" : false,\r\n    \"lucene_version\" : \"7.3.1\",\r\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\r\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\r\n  },\r\n  \"tagline\" : \"You Know, for Search\"\r\n}\r\n```\r\n**Plugins installed**: [repository-s3, discovery-ec2]\r\n\r\n**JVM version** (`java -version`):\r\n```\r\njava version \"1.8.0_171\"\r\nJava(TM) SE Runtime Environment (build 1.8.0_171-b11)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)\r\n```\r\n\r\n**OS version** (`uname -a` if on a Unix-like system):\r\n```\r\nLinux 4.14.97-74.72.amzn1.x86_64 #1 SMP Tue Feb 5 20:59:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\nRolling upgrade (https://www.elastic.co/guide/en/elasticsearch/reference/6.3/rolling-upgrades.html) of a large cluster (40 data nodes, 240 shards, approx. 200GB of data per node) is causing cluster instability issues (yellow/green state flip-flopping for several hours) with lots of shard rebalancing operations going on. Shard lock failure exceptions are being logged on the master node during that time:\r\n```\r\njava.io.IOException: failed to obtain in-memory shard lock\r\nat org.elasticsearch.index.IndexService.createShard(IndexService.java:392) ~[elasticsearch-6.3.0.jar:6.3.0]\r\nat org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:515) ~[elasticsearch-6.3.0.jar:6.3.0]\r\nat org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:144) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n...\r\nCaused by: org.elasticsearch.env.ShardLockObtainFailedException: [redacted][2]: obtaining shard lock timed out after 5000ms\r\nat org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:678) ~[elasticsearch-6.3.0.jar:6.3.0]\r\nat org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:597) ~[elasticsearch-6.3.0.jar:6.3.0]\r\nat org.elasticsearch.index.IndexService.createShard(IndexService.java:329) ~[elasticsearch-6.3.0.jar:6.3.0]\r\n```\r\n\r\nWe are also seeing a large number of node disconnected exceptions across multiple nodes logged by master at the same time:\r\n```\r\n[2019-02-21T14:53:16,707][WARN ][o.e.c.r.a.AllocationService] [redacted] failing shard [failed shard, shard [redacted][10], node[eyYZXHsETW-lBHB6y0SfzQ], [R], s[STARTED], a[id=PFHSM67dT2e1QSSfAR_LIg], message [failed to perform indices:data/write/bulk[s] on replica [redacted][10], node[eyYZXHsETW-lBHB6y0SfzQ], [R], s[STARTED], a[id=PFHSM67dT2e1QSSfAR_LIg]], failure [NodeNotConnectedException[[redacted][10.0.64.221:9300] Node not connected]], markAsStale [true]]\r\n```\r\n\r\n**Steps to reproduce**:\r\n\r\nWe are performing the following sequence of automated operations during each node maintenance:\r\n1. Disable shard allocation:\r\n```\r\ncurl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d'\r\n{\r\n  \"persistent\": {\r\n    \"cluster.routing.allocation.enable\": \"none\"\r\n  }\r\n}\r\n'\r\n```\r\n2. stop elasticsearch service (using `sudo -i service elasticsearch stop`)\r\n3. system reboot the node for maintenance (takes around 1-2 minutes)\r\n4. start elasticsearch service (using `sudo -i service elasticsearch start`)\r\n5. Enable shard allocation:\r\n```\r\ncurl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d'\r\n{\r\n  \"persistent\": {\r\n    \"cluster.routing.allocation.enable\": null\r\n  }\r\n}\r\n'\r\n```\r\n6. wait for the cluster to get into green state\r\n7. wait 3 minutes before moving onto next node\r\n\r\nThe issue happens consistently after the sequence above goes through 6-7 nodes. We have ingestion and search production load running against the cluster at all times, as we are not in a position to disable that during the rolling upgrade. Search load contains a significant amount of search scroll requests (https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html).\r\n\r\nHow can we prevent this from happening? Are there any optimisations we can make to the process or settings we can tweak during the node updates?\r\n","closed_by":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"performed_via_github_app":null}