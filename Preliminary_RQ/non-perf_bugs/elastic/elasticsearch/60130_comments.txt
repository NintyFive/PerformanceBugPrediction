[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/663045841","html_url":"https://github.com/elastic/elasticsearch/issues/60130#issuecomment-663045841","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/60130","id":663045841,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzA0NTg0MQ==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2020-07-23T14:40:02Z","updated_at":"2020-07-23T14:40:02Z","author_association":"COLLABORATOR","body":"Pinging @elastic/ml-core (:ml)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/664336316","html_url":"https://github.com/elastic/elasticsearch/issues/60130#issuecomment-664336316","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/60130","id":664336316,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDMzNjMxNg==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2020-07-27T11:18:05Z","updated_at":"2020-07-27T11:18:05Z","author_association":"CONTRIBUTOR","body":"> When the data node holding this index leaves the cluster these indexing requests on the ML node will fail.\r\n\r\nThis is not actually true.  When I tried to reproduce this problem locally to fix it I found that if the data node leaves the cluster then the ML node continues to retry.\r\n\r\nLooking more closely at the failure messages in the log extract, the irrecoverable exceptions were `NodeClosedException`.  This is thrown when the cluster service on the local node running the request is closed.\r\n\r\nSo, what actually happened was that the ML job was trying to write results in one thread while Elasticsearch shutdown was progressing in a different thread.  As more Elasticsearch internal services shut down the results persister started suffering exceptions.  These exceptions _must not_ cause the job to go into the `failed` state.\r\n\r\nIt will be impossible to reproduce this problem on a single node cluster, because obviously if the cluster service is shut down on the single node then the persistent tasks service cannot update the cluster state to set the job to `failed` status.  On a multi-node cluster it is possible, if the request to fail the job is sent to the master node after the local cluster service on the ML node is shut down but before networking is shut down.\r\n\r\nThe problem is that the work of shutting down the cluster applier service is done in `ClusterApplierService.doStop()`.  It would be better for ML if it was done in `ClusterApplierService.doClose()`.  The shutdown sequence for a node is:\r\n\r\n1. Stop everything that is interested in lifecycle events\r\n2. Close everything that is interested in lifecycle events\r\n\r\nML has a stop handler that marks all jobs as killed, which will prevent them failing if they suffer exceptions.  However, the cluster service is stopped before plugins so the ML jobs are not marked as killed at the point they can receive a `NodeClosedException` if they are in the middle of a request when the node is shut down.\r\n\r\nIt would be better if there was a principle that \"stop\" just prepared for shutdown and then \"close\" actually did the shutdown.\r\n\r\nIn the short term, to fix the problem of spurious failures in Cloud, ML threads will have to have special handling for `NodeClosedException` and stop what they are doing but without failing the job.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/668058580","html_url":"https://github.com/elastic/elasticsearch/issues/60130#issuecomment-668058580","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/60130","id":668058580,"node_id":"MDEyOklzc3VlQ29tbWVudDY2ODA1ODU4MA==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2020-08-03T14:36:45Z","updated_at":"2020-08-03T16:45:23Z","author_association":"CONTRIBUTOR","body":"Looking at the log more closely, I think the problem was caused by the Cloud infrastructure sending `SIGTERM` to the native processes before the ES JVM.  The order of the messages in the screenshot of the log in the original issue description is misleading.  It is sorted by a timestamp that is slightly later than the original message timestamp (both are visible in the screenshot).  In the order of the original log timestamps the `autodetect process stopped unexpectedly` messages come before `stopping ...`.  One of the side effects of `autodetect process stopped unexpectedly` is that the entry for the process in the `AutodetectCommunicator` `processByAllocation` map is removed.  The effect of this is that the flag that indicates node shutdown in progress cannot be set by the `MlLifecycleListener`, because it sets the flags by iterating this map.  So we are back to needing Cloud to send `SIGTERM` only to the ES JVM initially instead of sending `SIGTERM` to all proceses simultaneously including the ML native processes.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/707621332","html_url":"https://github.com/elastic/elasticsearch/issues/60130#issuecomment-707621332","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/60130","id":707621332,"node_id":"MDEyOklzc3VlQ29tbWVudDcwNzYyMTMzMg==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2020-10-13T09:37:36Z","updated_at":"2020-10-13T09:37:36Z","author_association":"CONTRIBUTOR","body":"As the previous comment says, this issue was opened on a misunderstanding - closing","performed_via_github_app":null}]