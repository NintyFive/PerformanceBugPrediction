{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/11202","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11202/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11202/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11202/events","html_url":"https://github.com/elastic/elasticsearch/issues/11202","id":77650429,"node_id":"MDU6SXNzdWU3NzY1MDQyOQ==","number":11202,"title":"Node restart causes ClusterBlockException when Transport Client is used","user":{"login":"bigwheel","id":826646,"node_id":"MDQ6VXNlcjgyNjY0Ng==","avatar_url":"https://avatars2.githubusercontent.com/u/826646?v=4","gravatar_id":"","url":"https://api.github.com/users/bigwheel","html_url":"https://github.com/bigwheel","followers_url":"https://api.github.com/users/bigwheel/followers","following_url":"https://api.github.com/users/bigwheel/following{/other_user}","gists_url":"https://api.github.com/users/bigwheel/gists{/gist_id}","starred_url":"https://api.github.com/users/bigwheel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bigwheel/subscriptions","organizations_url":"https://api.github.com/users/bigwheel/orgs","repos_url":"https://api.github.com/users/bigwheel/repos","events_url":"https://api.github.com/users/bigwheel/events{/privacy}","received_events_url":"https://api.github.com/users/bigwheel/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2015-05-18T13:07:43Z","updated_at":"2016-08-16T10:44:26Z","closed_at":"2015-06-19T02:37:02Z","author_association":"NONE","active_lock_reason":null,"body":"## Abstract\n\nTransport Client can work correctly when shutdown a connecting node, however when the node starts ClusterBlockException raised.\n## Environment\n- cluster\n  - version: 1.4.4\n  - 3 nodes(phyical server)\n  - Index\n    - big index data(20gb, i couldnt reproduce this issue with no index data)\n    - number_of_replicas: 1\n    - number_of_shards: 9\n- application\n  - version: 1.4.4\n  - Java using Transport Client\n  - specify all IPs of 3 nodes\n## Reproduce process\n1. query through Transport Client continuosly, repeatedly(any query is ok. ex: search, get, index...)\n2. shutdown a node (shutdown API or initd script)\n   - In this phase sometime searching method returns a result that contains failed shards. But this is NOT fatail. I retry search and can get collect result.\n3. start the node\n4. Transport Client sometime causes following Exception. We cannot catch it bacause it happens in Elasticsearch Client Thread pool maybe.\n\n```\n[error] (run-main-0) org.elasticsearch.transport.RemoteTransportException: [es-node01][inet[/192.168.10.42:9300]][indices:data/read/search]\norg.elasticsearch.transport.RemoteTransportException: [es-node01][inet[/192.168.10.42:9300]][indices:data/read/search]\nCaused by: org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];\n        at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:151)\n        at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:141)\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.<init>(TransportSearchTypeAction.java:111)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.<init>(TransportSearchQueryThenFetchAction.java:70)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.<init>(TransportSearchQueryThenFetchAction.java:64)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:61)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:51)\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)\n        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:100)\n        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:43)\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)\n        at org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:63)\n        at org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:51)\n        at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)\n        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n```\n## IMO\n\nhttps://groups.google.com/d/msg/elasticsearch/qBw6mgCO_Nk/N77Oe9X2YdIJ is right.\nTransport Client accesses starting client before it connects cluster.\nBut, I cannot find any settings to solve this.\nI tried [Sniff Setting](https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.x/client.html#transport-client) but the exception is still caused.\n\nBy deleting TransportAddress of restarting node, exception didn't happen finally.\nHowever it needs application restart.\nI want to do rolling update elasticsearch cluster without application server shutdown.\n\nCould anyone help us?\n## Other Info\n- I couldnt reproduce this issue using Node Client\n- We cannot use Node Client by production enviroinment restriction (at least now, im trying to solve this)\n- Almost same issue: https://groups.google.com/forum/#!msg/elasticsearch/qBw6mgCO_Nk/BLuikJhaiBsJ\n","closed_by":{"login":"bigwheel","id":826646,"node_id":"MDQ6VXNlcjgyNjY0Ng==","avatar_url":"https://avatars2.githubusercontent.com/u/826646?v=4","gravatar_id":"","url":"https://api.github.com/users/bigwheel","html_url":"https://github.com/bigwheel","followers_url":"https://api.github.com/users/bigwheel/followers","following_url":"https://api.github.com/users/bigwheel/following{/other_user}","gists_url":"https://api.github.com/users/bigwheel/gists{/gist_id}","starred_url":"https://api.github.com/users/bigwheel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bigwheel/subscriptions","organizations_url":"https://api.github.com/users/bigwheel/orgs","repos_url":"https://api.github.com/users/bigwheel/repos","events_url":"https://api.github.com/users/bigwheel/events{/privacy}","received_events_url":"https://api.github.com/users/bigwheel/received_events","type":"User","site_admin":false},"performed_via_github_app":null}