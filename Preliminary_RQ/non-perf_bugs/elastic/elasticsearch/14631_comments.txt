[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/155976556","html_url":"https://github.com/elastic/elasticsearch/issues/14631#issuecomment-155976556","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14631","id":155976556,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NTk3NjU1Ng==","user":{"login":"jolynch","id":1420460,"node_id":"MDQ6VXNlcjE0MjA0NjA=","avatar_url":"https://avatars1.githubusercontent.com/u/1420460?v=4","gravatar_id":"","url":"https://api.github.com/users/jolynch","html_url":"https://github.com/jolynch","followers_url":"https://api.github.com/users/jolynch/followers","following_url":"https://api.github.com/users/jolynch/following{/other_user}","gists_url":"https://api.github.com/users/jolynch/gists{/gist_id}","starred_url":"https://api.github.com/users/jolynch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jolynch/subscriptions","organizations_url":"https://api.github.com/users/jolynch/orgs","repos_url":"https://api.github.com/users/jolynch/repos","events_url":"https://api.github.com/users/jolynch/events{/privacy}","received_events_url":"https://api.github.com/users/jolynch/received_events","type":"User","site_admin":false},"created_at":"2015-11-12T02:20:55Z","updated_at":"2015-11-12T02:20:55Z","author_association":"NONE","body":"Even with reroutes on all node join, leaves, and master changes I still run into situations where shards hang out permanently unassigned. This usually happens when a node leaves and never comes back. I have confirmed that if I manually call reroute or schedule a task on the threadpool that just calls reroute whenever there are unassigned shards, my cluster is able to recover.\n\nI'll keep working on trying to come up with a reproduction in a test, but before I spend too much time is there a strong opposition to periodically checking that all shards are assigned and rerouting if not? \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/157149424","html_url":"https://github.com/elastic/elasticsearch/issues/14631#issuecomment-157149424","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14631","id":157149424,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NzE0OTQyNA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-11-16T19:49:21Z","updated_at":"2015-11-16T19:49:21Z","author_association":"MEMBER","body":"As you said, we tried the route of periodic reroutes. It was our experience that it made the code more complex and was hiding bugs in other places - but not completely fixing them. \n\nNote that we typically do the re-route inlined with another change in order to save on cluster state publishing and do adapt allocation immediately.\n\nThat said I agree that the current implementation is  tricky and have proven to have bugs. @ywelsch has started working what would hopefully be a simpler approach.\n\n> I'll keep working on trying to come up with a reproduction in a test, but before I spend too much time is there a strong opposition to periodically checking that all shards are assigned and rerouting if not?\n\nI don't think periodic reroute is coming back. It would be great if you can reproduce or give some more info as to what went wrong (assuming it's not related to your custom discovery). I would love to know what the bug exactly was so we can make sure it's fixed.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/157169395","html_url":"https://github.com/elastic/elasticsearch/issues/14631#issuecomment-157169395","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14631","id":157169395,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NzE2OTM5NQ==","user":{"login":"jolynch","id":1420460,"node_id":"MDQ6VXNlcjE0MjA0NjA=","avatar_url":"https://avatars1.githubusercontent.com/u/1420460?v=4","gravatar_id":"","url":"https://api.github.com/users/jolynch","html_url":"https://github.com/jolynch","followers_url":"https://api.github.com/users/jolynch/followers","following_url":"https://api.github.com/users/jolynch/following{/other_user}","gists_url":"https://api.github.com/users/jolynch/gists{/gist_id}","starred_url":"https://api.github.com/users/jolynch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jolynch/subscriptions","organizations_url":"https://api.github.com/users/jolynch/orgs","repos_url":"https://api.github.com/users/jolynch/repos","events_url":"https://api.github.com/users/jolynch/events{/privacy}","received_events_url":"https://api.github.com/users/jolynch/received_events","type":"User","site_admin":false},"created_at":"2015-11-16T21:03:11Z","updated_at":"2015-11-16T21:03:11Z","author_association":"NONE","body":"> It was our experience that it made the code more complex and was hiding bugs in other places - but not completely fixing them.\n\nFair enough, thank you for the context!\n\n> I don't think periodic reroute is coming back. It would be great if you can reproduce or give some more info as to what went wrong (assuming it's not related to your custom discovery). I would love to know what the bug exactly was so we can make sure it's fixed.\n\nBummer, ok well I'll do it from our discovery plugin as a way to get moving to 1.7.3 while I find a reproduction using Zen. When we were first having issues I tried switching to Zen for a bit and still observed the behaviour so I switched back to our disco plugin to continue debugging other issues. I remember being able to fairly consistently reproduce with Zen by doing the following:\n1. Start a 5 node cluster\n2. Create ~10 indices with ~5 shards a piece\n3. Pick a node Y, stop it and wait 60s\n4. Wait X seconds past 60s for the shards to be re-assigned.\n\nStrangely reliably, the _first_ time I would follow this procedure X would be ~=0, but if I tried stopping the same node again I would often see the shards stay unassigned. It was a while since I ran the experiment but I think that the following was a snippet from the logs at that time, maybe the throttle is a clue (I'll work on getting a cleaner repro)\n\n```\n[2015-11-06 18:05:47,755] [TRACE] [host_redacted] Assigned shard [[test_index_v1][2], node[lwzhmdi-SAaI8ADYj2oJYg], [R], s[STARTED]] to node [lwzhmdi-SAaI8ADYj2oJYg]\n[2015-11-06 18:05:47,755] [TRACE] [host_redacted] Assigned shard [[test_index_v12][12], node[lwzhmdi-SAaI8ADYj2oJYg], [R], s[INITIALIZING], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] to node [lwzhmdi-SAaI8ADYj2oJYg]\n[2015-11-06 18:05:47,755] [TRACE] [host_redacted] Assigned shard [[test_index_v12][2], node[lwzhmdi-SAaI8ADYj2oJYg], [R], s[STARTED]] to node [lwzhmdi-SAaI8ADYj2oJYg]\n[2015-11-06 18:05:47,755] [TRACE] [host_redacted] Assigned shard [[test_index_v4][2], node[lwzhmdi-SAaI8ADYj2oJYg], [R], s[STARTED]] to node [lwzhmdi-SAaI8ADYj2oJYg]\n[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Assigned shard [[test_index_v12][14], node[lwzhmdi-SAaI8ADYj2oJYg], [R], s[INITIALIZING], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] to node [lwzhmdi-SAaI8ADYj2oJYg]\n[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Start allocating unassigned shards\n[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Can not allocate [[test_index_v12][0], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] on node [hBAmZ5DmStmQXKl_9F-TEw] due to [SameShardAllocationDecider]\n[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Can not allocate [[test_index_v12][0], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] on node [pI7Ow3A8Sc6DdNC1VMFhOg] due to [SameShardAllocationDecider]\n[2015-11-06 18:05:47,756] [TRACE] [host_redacted] usage without relocations: [lwzhmdi-SAaI8ADYj2oJYg][10-40-26-46-uswest1cdevc.dev.yelpcorp.com] free: 304.9gb[99.9%]\n[2015-11-06 18:05:47,756] [TRACE] [host_redacted] usage with relocations: [0 bytes] [lwzhmdi-SAaI8ADYj2oJYg][10-40-26-46-uswest1cdevc.dev.yelpcorp.com] free: 304.9gb[99.9%]\n[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Node [lwzhmdi-SAaI8ADYj2oJYg] has 0.010636288173571984% used disk\n[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Can not allocate on node [routingNode ([10-40-26-46-uswest1cdevc.dev.yelpcorp.com][lwzhmdi-SAaI8ADYj2oJYg][10-40-26-46-uswest1cdevc][10.40.26.46], [5 assigned shards])] remove from round decision [THROTTLE]\n[2015-11-06 18:05:47,756] [TRACE] [host_redacted] No eligable node found to assign shard [[test_index_v12][0], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] decision [THROTTLE]\n[2015-11-06 18:05:47,757] [TRACE] [host_redacted] Can not allocate [[test_index_v12][1], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] on node [hBAmZ5DmStmQXKl_9F-TEw] due to [SameShardAllocationDecider]\n[2015-11-06 18:05:47,757] [TRACE] [host_redacted] Can not allocate [[test_index_v12][1], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] on node [pI7Ow3A8Sc6DdNC1VMFhOg] due to [SameShardAllocationDecider]\n[2015-11-06 18:05:47,757] [TRACE] [host_redacted] No Node found to assign shard [[test_index_v12][1], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]]\n[2015-11-06 18:05:47,757] [TRACE] [host_redacted] Can not allocate [[test_index_v12][3], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:55:34.871Z], details[node_left[It-kO-ryT8iELRml2uaCyw]]]] on node [hBAmZ5DmStmQXKl_9F-TEw] due to [SameShardAllocationDecider]\n[2015-11-06 18:05:47,757] [TRACE] [host_redacted] Can not allocate [[test_index_v12][3], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:55:34.871Z], details[node_left[It-kO-ryT8iELRml2uaCyw]]]] on node [pI7Ow3A8Sc6DdNC1VMFhOg] due to [SameShardAllocationDecider]\n[2015-11-06 18:05:47,757] [TRACE] [host_redacted] No Node found to assign shard [[test_index_v12][3], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:55:34.871Z], details[node_left[It-kO-ryT8iELRml2uaCyw]]]]\n```\n\nTesting configuration:\n\n```\naction:\n  auto_create_index: false\n  disable_delete_all_indices: true\nbootstrap:\n  mlockall: true\ncluster:\n  name: <cluster name>\ndiscovery.zen.fd.ping_timeout: 5s\ndiscovery.zen.minimum_master_nodes: 2\ndiscovery.zen.ping.multicast.enabled: false\ndiscovery.zen.ping.unicast.hosts: [<list of hosts>]\ngateway:\n  expected_nodes: 1\n  recover_after_nodes: 1\n  recover_after_time: 1m\nhttp:\n  compression: true\n  port: <port>\nindex:\n  auto_expand_replicas: false\n  number_of_replicas: 2\n  number_of_shards: 5\nnode:\n  data: true\n  rack: <rack name>\n  master: true\n  name: <hostname>\n```\n\nSince periodic re-routes are out in core I'll add them in our discovery plugin as a temporary workaround and then dig into this and see if I can reproduce it asap.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/157307856","html_url":"https://github.com/elastic/elasticsearch/issues/14631#issuecomment-157307856","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14631","id":157307856,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NzMwNzg1Ng==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-11-17T08:30:04Z","updated_at":"2015-11-17T08:30:04Z","author_association":"MEMBER","body":"> maybe the throttle is a clue \n\nI think this might be indeed one of the issues we discovered with the current approach. Hopefully it will be solved with the new one.\n\n> (I'll work on getting a cleaner repro)\n\nThat would be great.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/183907796","html_url":"https://github.com/elastic/elasticsearch/issues/14631#issuecomment-183907796","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14631","id":183907796,"node_id":"MDEyOklzc3VlQ29tbWVudDE4MzkwNzc5Ng==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-02-14T15:59:17Z","updated_at":"2016-02-14T15:59:17Z","author_association":"CONTRIBUTOR","body":"No further feedback. Closing\n","performed_via_github_app":null}]