[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/353068388","html_url":"https://github.com/elastic/elasticsearch/issues/27922#issuecomment-353068388","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27922","id":353068388,"node_id":"MDEyOklzc3VlQ29tbWVudDM1MzA2ODM4OA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-12-20T13:52:09Z","updated_at":"2017-12-20T13:52:09Z","author_association":"CONTRIBUTOR","body":"can you give us more information about the system in general. ie do you have massive mappings, large number of indices. massive synonym files etc. Can you try to dig where these massive hashmaps are hold?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/353238709","html_url":"https://github.com/elastic/elasticsearch/issues/27922#issuecomment-353238709","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27922","id":353238709,"node_id":"MDEyOklzc3VlQ29tbWVudDM1MzIzODcwOQ==","user":{"login":"junkainiu","id":12933695,"node_id":"MDQ6VXNlcjEyOTMzNjk1","avatar_url":"https://avatars0.githubusercontent.com/u/12933695?v=4","gravatar_id":"","url":"https://api.github.com/users/junkainiu","html_url":"https://github.com/junkainiu","followers_url":"https://api.github.com/users/junkainiu/followers","following_url":"https://api.github.com/users/junkainiu/following{/other_user}","gists_url":"https://api.github.com/users/junkainiu/gists{/gist_id}","starred_url":"https://api.github.com/users/junkainiu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/junkainiu/subscriptions","organizations_url":"https://api.github.com/users/junkainiu/orgs","repos_url":"https://api.github.com/users/junkainiu/repos","events_url":"https://api.github.com/users/junkainiu/events{/privacy}","received_events_url":"https://api.github.com/users/junkainiu/received_events","type":"User","site_admin":false},"created_at":"2017-12-21T02:14:14Z","updated_at":"2017-12-21T02:14:14Z","author_association":"NONE","body":"@s1monw We do have about 10000 indices in the elasticsearch node,  and have a mapping containing about 60 feilds, most of which are keyword type or long type.  Not sure about the hashmaps. \r\n![424275953695910711](https://user-images.githubusercontent.com/12933695/34237438-a8c6ce24-e637-11e7-9493-f53b278dfff7.jpg)\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/353355839","html_url":"https://github.com/elastic/elasticsearch/issues/27922#issuecomment-353355839","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27922","id":353355839,"node_id":"MDEyOklzc3VlQ29tbWVudDM1MzM1NTgzOQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-12-21T13:52:43Z","updated_at":"2017-12-21T13:52:43Z","author_association":"CONTRIBUTOR","body":"60 fields is fine, but 10k indices is too much. This correlates with your heap dump which says there are about 5M segments (SegmentCoreReaders). The reason is that each segment needs to maintain its own metadata, so you should rather have fewer larger segments, and the easiest way to do this is to reduce your number of shards per node, which in turn is probably most easily done by reducing the number of indices. Maybe some of these indices could be merged together? If you use time-based indices, maybe some daily indices could be made monthly indices,etc.","performed_via_github_app":null}]