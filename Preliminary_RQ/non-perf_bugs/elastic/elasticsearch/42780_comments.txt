[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/498062842","html_url":"https://github.com/elastic/elasticsearch/issues/42780#issuecomment-498062842","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42780","id":498062842,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5ODA2Mjg0Mg==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2019-06-02T20:28:58Z","updated_at":"2019-06-02T20:28:58Z","author_association":"CONTRIBUTOR","body":"Hi Eran,\r\n>When setting \"filter_duplicate_text\": true in significant_text aggregation, it changes the count only for the doc_count and not for the bg_count.\r\n\r\nYes, ideally filtering duplicate text sequences the reported stats would take effect on foreground and background texts but in reality this is not practical. When sampling the top 200 or so docs for a foreground it is practical to spot duplicate text sequences as similar docs rank similarly and are likely to be bunched together in the relatively small sample. In the background (_all_ docs in the index) similar docs may be nowhere near each other - possibly on different machines, so it is not practical to find duplicate sequences of text in a space this large.\r\n\r\nSo in summary, this is working-as-designed, subject to some known limitations for which there are no good workarounds.\r\nClosing, but if you have any brilliant ideas, we are more than happy to take another look :) ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/498062862","html_url":"https://github.com/elastic/elasticsearch/issues/42780#issuecomment-498062862","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42780","id":498062862,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5ODA2Mjg2Mg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-06-02T20:29:18Z","updated_at":"2019-06-02T20:29:18Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-analytics-geo","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/498143103","html_url":"https://github.com/elastic/elasticsearch/issues/42780#issuecomment-498143103","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42780","id":498143103,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5ODE0MzEwMw==","user":{"login":"eranhirs","id":3372820,"node_id":"MDQ6VXNlcjMzNzI4MjA=","avatar_url":"https://avatars1.githubusercontent.com/u/3372820?v=4","gravatar_id":"","url":"https://api.github.com/users/eranhirs","html_url":"https://github.com/eranhirs","followers_url":"https://api.github.com/users/eranhirs/followers","following_url":"https://api.github.com/users/eranhirs/following{/other_user}","gists_url":"https://api.github.com/users/eranhirs/gists{/gist_id}","starred_url":"https://api.github.com/users/eranhirs/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eranhirs/subscriptions","organizations_url":"https://api.github.com/users/eranhirs/orgs","repos_url":"https://api.github.com/users/eranhirs/repos","events_url":"https://api.github.com/users/eranhirs/events{/privacy}","received_events_url":"https://api.github.com/users/eranhirs/received_events","type":"User","site_admin":false},"created_at":"2019-06-03T07:24:20Z","updated_at":"2019-06-03T07:24:20Z","author_association":"NONE","body":"@markharwood thanks for answering so quickly .\r\nI think solving this issue requires a smaller solution than the ideal solution that you've described.\r\n\r\nThis issue is specifically about the documents that were already filtered out from the foreground but not from the background.\r\nWe've already identified them as duplicated, why can't we just remove from the background the same count of documents that were removed from the foreground?\r\nIt would already improve the accuracy a lot.\r\n\r\nWhat are your thoughts?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/498160073","html_url":"https://github.com/elastic/elasticsearch/issues/42780#issuecomment-498160073","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42780","id":498160073,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5ODE2MDA3Mw==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2019-06-03T08:20:32Z","updated_at":"2019-06-03T08:20:32Z","author_association":"CONTRIBUTOR","body":">This issue is specifically about the documents that were already filtered out\r\n\r\nThat's perhaps where there's a disconnect. We filter _sections of documents_, not whole documents.\r\nSee https://youtu.be/zH7bizwjj20?t=236","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/498171602","html_url":"https://github.com/elastic/elasticsearch/issues/42780#issuecomment-498171602","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42780","id":498171602,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5ODE3MTYwMg==","user":{"login":"eranhirs","id":3372820,"node_id":"MDQ6VXNlcjMzNzI4MjA=","avatar_url":"https://avatars1.githubusercontent.com/u/3372820?v=4","gravatar_id":"","url":"https://api.github.com/users/eranhirs","html_url":"https://github.com/eranhirs","followers_url":"https://api.github.com/users/eranhirs/followers","following_url":"https://api.github.com/users/eranhirs/following{/other_user}","gists_url":"https://api.github.com/users/eranhirs/gists{/gist_id}","starred_url":"https://api.github.com/users/eranhirs/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eranhirs/subscriptions","organizations_url":"https://api.github.com/users/eranhirs/orgs","repos_url":"https://api.github.com/users/eranhirs/repos","events_url":"https://api.github.com/users/eranhirs/events{/privacy}","received_events_url":"https://api.github.com/users/eranhirs/received_events","type":"User","site_admin":false},"created_at":"2019-06-03T08:55:10Z","updated_at":"2019-06-03T08:56:23Z","author_association":"NONE","body":"Thank you that was very helpful.\r\n\r\nIt still sounds possible to implement a simple solution - my thoughts are an algorithm that counts the `doc_count` before and the `doc_count` after the dedup, calculates a diff and removes this diff from `bg_count`.\r\n\r\nQuickly adding the motivation for this issue:\r\nThere are websites with `\"About bird flu\"` sections which is duplicated through different `Bird flu` articles.\r\nThis is a problem because it has good words like `h5n1`, but they are getting penalized and then don't count as significant.\r\nOn the other hand, you don't want these words to take over all the significant results.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/498185639","html_url":"https://github.com/elastic/elasticsearch/issues/42780#issuecomment-498185639","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42780","id":498185639,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5ODE4NTYzOQ==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2019-06-03T09:36:46Z","updated_at":"2019-06-03T09:36:46Z","author_association":"CONTRIBUTOR","body":">It still sounds possible to implement a simple solution\r\n\r\nI'm not sure. Let's imagine a doc with `content` and `sidebar` sections - the content is the unique stuff, the `sidebar` is your `\"About bird flu\"` example which has useful word `h5n1` and crappy word `foobar.com`\r\nSignificant text (with filtering on) only considers words that survive the de-dup - otherwise we'd tune into the `foobar.com` in the sidebar.  So we'd consider `h5n1` and `avian` from the content.\r\nLet's say the sidebar did not include the word `avian` - that would get a \"true\" background count (assuming there's no other copy-and-paste sidebars bumping up the numbers).\r\nIn contrast, the background-count for `h5n1` though, you suspect might need adjustment. Well the only adjustment we can make is the number of documents we saw in the foreground where `h5n1` was in the sidebar but not in the content.  \r\nImplementing that will be challenging - right now filtering happens in the text-analysis phase before we get to the aggregation of terms and counts. This would be increased complexity, response times and RAM cost for what I suspect will be a small increase in usefulness in some cases. There are several other reasons why counts are incorrect and they are all performance trade-offs of this nature.\r\n\r\n","performed_via_github_app":null}]