{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/11822","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11822/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11822/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11822/events","html_url":"https://github.com/elastic/elasticsearch/issues/11822","id":90351675,"node_id":"MDU6SXNzdWU5MDM1MTY3NQ==","number":11822,"title":"High CPU and Memory usage by elasticsearch","user":{"login":"darshanmehta10","id":12479117,"node_id":"MDQ6VXNlcjEyNDc5MTE3","avatar_url":"https://avatars3.githubusercontent.com/u/12479117?v=4","gravatar_id":"","url":"https://api.github.com/users/darshanmehta10","html_url":"https://github.com/darshanmehta10","followers_url":"https://api.github.com/users/darshanmehta10/followers","following_url":"https://api.github.com/users/darshanmehta10/following{/other_user}","gists_url":"https://api.github.com/users/darshanmehta10/gists{/gist_id}","starred_url":"https://api.github.com/users/darshanmehta10/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/darshanmehta10/subscriptions","organizations_url":"https://api.github.com/users/darshanmehta10/orgs","repos_url":"https://api.github.com/users/darshanmehta10/repos","events_url":"https://api.github.com/users/darshanmehta10/events{/privacy}","received_events_url":"https://api.github.com/users/darshanmehta10/received_events","type":"User","site_admin":false},"labels":[{"id":146829143,"node_id":"MDU6TGFiZWwxNDY4MjkxNDM=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Infra/Transport%20API","name":":Core/Infra/Transport API","color":"0e8a16","default":false,"description":"Transport client API"},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2015-06-23T10:41:40Z","updated_at":"2016-01-13T17:04:57Z","closed_at":"2016-01-13T17:04:57Z","author_association":"NONE","active_lock_reason":null,"body":"We are using java client to communicate with Elasticsearch (search requests only). Once started, CPU and Memory usage keeps going up and doesn't come down unless the process is restarted.\n\nThreaddump shows around 120 blocked threads with below stack trace:\n\n```\nThread 26356: (state = BLOCKED)\n - java.lang.Thread.sleep(long) @bci=0 (Compiled frame; information may be imprecise)\n - org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick() @bci=81, line=445 (Compiled frame)\n - org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run() @bci=43, line=364 (Compiled frame)\n - org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=108 (Interpreted frame)\n - java.lang.Thread.run() @bci=11, line=724 (Interpreted frame)\n```\n\nWe are using TranspostClient object and close it once the search is done. Following is the hot_threads output:\n\n```\n0.2% (795.3micros out of 500ms) cpu usage by thread 'elasticsearch[es1][scheduler][T#1]'\n 10/10 snapshots sharing following 9 elements\n   sun.misc.Unsafe.park(Native Method)\n   java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)\n   java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)\n   java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)\n   java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)\n   java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)\n   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n   java.lang.Thread.run(Thread.java:745)\n\n0.1% (647.2micros out of 500ms) cpu usage by thread 'elasticsearch[es1][management][T#4]'\n 10/10 snapshots sharing following 9 elements\n   sun.misc.Unsafe.park(Native Method)\n   java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)\n   java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:731)\n   java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:644)\n   java.util.concurrent.LinkedTransferQueue.poll(LinkedTransferQueue.java:1145)\n   java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)\n   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n   java.lang.Thread.run(Thread.java:745)\n\n0.1% (489.6micros out of 500ms) cpu usage by thread 'elasticsearch[es1][[transport_server_worker.default]][T#5]{New I/O worker #14}'\n 10/10 snapshots sharing following 15 elements\n   sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n   sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n   sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)\n   sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)\n   sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)\n   org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\n   org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)\n   org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\n   org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n   org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n   org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n   org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n   java.lang.Thread.run(Thread.java:745)\n```\n\nBefore closing TransportClient, we are also shutting down associated threadpool (shutdownNow()) to kill the threads, still there are too many blocked threads in thread dump.\n\nLet me know if anything else is required.\n\nThanks\n","closed_by":{"login":"eskibars","id":2246002,"node_id":"MDQ6VXNlcjIyNDYwMDI=","avatar_url":"https://avatars0.githubusercontent.com/u/2246002?v=4","gravatar_id":"","url":"https://api.github.com/users/eskibars","html_url":"https://github.com/eskibars","followers_url":"https://api.github.com/users/eskibars/followers","following_url":"https://api.github.com/users/eskibars/following{/other_user}","gists_url":"https://api.github.com/users/eskibars/gists{/gist_id}","starred_url":"https://api.github.com/users/eskibars/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eskibars/subscriptions","organizations_url":"https://api.github.com/users/eskibars/orgs","repos_url":"https://api.github.com/users/eskibars/repos","events_url":"https://api.github.com/users/eskibars/events{/privacy}","received_events_url":"https://api.github.com/users/eskibars/received_events","type":"User","site_admin":false},"performed_via_github_app":null}