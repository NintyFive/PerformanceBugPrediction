[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261200513","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-261200513","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":261200513,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTIwMDUxMw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T09:43:11Z","updated_at":"2016-11-17T09:43:11Z","author_association":"CONTRIBUTOR","body":"This looks bad. When this happens, does the cluster eventually heal, or do you need to restart it to get back to normal?\n\nGiven that we do not seem to leak search contexts (the number remains reasonable), it looks as if when reaching some threshold in terms of search load, then things get worse and worse exponentially. I'd be very curious what the search threads are busy doing when that problem occurs, would you be able to get the [node hot threads](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html) while your queue is filling up?\n\nIn #21612 you suspect the fact that we use the `mmap` store to be related to this, we could easily check since you mention the problem occurs every day by temporarily setting [`index.store.type=niofs`](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-store.html#file-system) in the index settings. Note that this setting requires the index to be reopen in order to take effect.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261203881","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-261203881","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":261203881,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTIwMzg4MQ==","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T09:57:38Z","updated_at":"2016-11-17T09:57:38Z","author_association":"NONE","body":"@jpountz When this happens, I am not able to tell if it could heal by itself as this is a production cluster and there are constant queries from the users.  I had to restart the nodes in problem in trying to bring service back to normal ASAP. \n\nI also tried to get hot threads when this happens but the api call seems not returning anything. Some other api call such as node stats takes very long time to respond.  I'll try get hot threads again when it reoccur.\n\nI'll try `index.store.type=niofs` index setting and see if this is related.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261207440","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-261207440","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":261207440,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTIwNzQ0MA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T10:12:44Z","updated_at":"2016-11-17T10:12:44Z","author_association":"CONTRIBUTOR","body":"The fact the stats APIs take a long time to return is concerning too, these APIs are supposed to be cheap. Out of curiosity, is garbage collection activity still normal when your cluster is having trouble?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261212633","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-261212633","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":261212633,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTIxMjYzMw==","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T10:34:40Z","updated_at":"2016-11-17T10:34:40Z","author_association":"NONE","body":"Heap usage remains constant during that period. There is no old GC and even no young GC for some time. Below are the monitoring stats(I restarted the node at around 11:00):\n\n![image](https://cloud.githubusercontent.com/assets/10510416/20385763/b35d98be-acf3-11e6-8464-85e5f511027d.png)\n![image](https://cloud.githubusercontent.com/assets/10510416/20385781/c286ab28-acf3-11e6-9e28-38ef07eba4d7.png)\n![image](https://cloud.githubusercontent.com/assets/10510416/20385792/d3e5d632-acf3-11e6-8ef4-12a0cdb91885.png)\n![image](https://cloud.githubusercontent.com/assets/10510416/20385805/e51c2af0-acf3-11e6-83b4-60be4183be83.png)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261289159","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-261289159","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":261289159,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI4OTE1OQ==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T16:08:19Z","updated_at":"2016-11-17T16:08:19Z","author_association":"CONTRIBUTOR","body":"Can you use `jstat` to get a snapshot of the running threads both when it is ok and when it is behavior badly? That will work regardless of the APIs.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261297882","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-261297882","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":261297882,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI5Nzg4Mg==","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T16:38:13Z","updated_at":"2016-11-17T16:38:13Z","author_association":"NONE","body":"@nik9000 I've already updated indices to using niofs and tried some very expensive queries with high concurrency and the result seems positive.\n\n![image](https://cloud.githubusercontent.com/assets/10510416/20398026/addaaea8-ad26-11e6-910d-bbeffde714a2.png)\n![image](https://cloud.githubusercontent.com/assets/10510416/20398031/b393f5c0-ad26-11e6-8dfc-740a22a0bf2c.png)\n![image](https://cloud.githubusercontent.com/assets/10510416/20398038/bcd8af22-ad26-11e6-85af-ecd6d5e21aec.png)\n![image](https://cloud.githubusercontent.com/assets/10510416/20398044/c211c01e-ad26-11e6-9028-4087a860a00a.png)\n![image](https://cloud.githubusercontent.com/assets/10510416/20398051/c8ab1bfa-ad26-11e6-917f-33508dc4af54.png)\n\nThe performance looks better but I'll wait to see if this solves the issue tomorrow during business hours.\n\nIf the issue happens again, I'll capture a snapshot of the running threads with jstat as you suggested.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261305844","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-261305844","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":261305844,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTMwNTg0NA==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T17:06:04Z","updated_at":"2016-11-17T17:06:04Z","author_association":"CONTRIBUTOR","body":"> If the issue happens again, I'll capture a snapshot of the running threads with jstat as you suggested.\n\nIt'd probably be useful to have a before/after as well. I'd take a snapshot when you next have a chance just to have a picture of it. Also so you are practiced at whatever hoops you have to go through to do it. I know when things are going sideways I don't want to try new things.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261403072","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-261403072","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":261403072,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTQwMzA3Mg==","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T23:28:49Z","updated_at":"2016-11-17T23:28:49Z","author_association":"NONE","body":"I'll closely make minitor the cluster status today during business peak time to see if the change makes big difference.\n\nIf the issue doesn't re-occure, i'll arrange some time to revert back index store back to mmap, stress test to trigger the problem so to capture threads info before/after.\n\n? 2016?11?18??01:08?Nik Everett <notifications@github.com<mailto:notifications@github.com>> ???\n\nIf the issue happens again, I'll capture a snapshot of the running threads with jstat as you suggested.\n\nIt'd probably be useful to have a before/after as well. I'd take a snapshot when you next have a chance just to have a picture of it. Also so you are practiced at whatever hoops you have to go through to do it. I know when things are going sideways I don't want to try new things.\n\n## \n\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/elastic/elasticsearch/issues/21611#issuecomment-261305844, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AKBgUMwW5ls2WpSqctJfuD4Yu8hd434Xks5q_ImbgaJpZM4K02l3.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261764104","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-261764104","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":261764104,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTc2NDEwNA==","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"created_at":"2016-11-20T07:45:56Z","updated_at":"2016-11-20T07:57:24Z","author_association":"NONE","body":"Since I changed index store to niofs last Thursday night, the cluster survived the Friday business peak.  \n\nI managed to find some time today (Sunday) to carry on some tests.\n\nFirst I changed index store of one type of big index back to default, ie, mmapfs, and send some queries to 3 days of indices using Kibana.  All the queries are with timeout of 35 seconds set. Not long after I send these queries, a few nodes appeared struggling in searching again.  It took quite a  long time for active threads to drop down.  I then changed the store to niofs  and did more expensive queries to compare the threads stats. Below screenshot shows the result (stats from all nodes were stacked).\n![image](https://cloud.githubusercontent.com/assets/10510416/20461266/2e03031a-af33-11e6-9f14-e674b04301f3.png)\n\n The first bunch of queries were sent at 13:00 and it took a long time for active threads to drop down. Several search threads not terminated even after an hour, so I have to restart some nodes . The second bunches were sent at about 14:40 and with more days of indices included in the request.  This time search active peaked for a short time and search completed are much higher than previous test.\n\nFor the first test, i captured more details on a particular node in problem. Here's what its thread pool stats looked like:\n![image](https://cloud.githubusercontent.com/assets/10510416/20461283/cc455db6-af33-11e6-9522-0134967ac7a2.png)\n\nInitially, open context stayed the same number as that of active threads. When I reverted index settings back by closing it at 14:08, I noticed open context dropped to 0  but active threads remains at 2 (The time difference may not be that obvious on screenshot I attached). I am not sure if it will eventually drop to 0 , but I am impatient to wait and so restarted the node at 14:18 after when the last 2 active thread is gone.\n\n![image](https://cloud.githubusercontent.com/assets/10510416/20461288/e236c57e-af33-11e6-92ee-3aaf06a54a9c.png)\n\nFollowed @nik9000 's advice, I captured threads dump with jstack at different time point (Suffixed with HH:MM) on node in problem.  Please refer to below attached file. \n[threads_dump.zip](https://github.com/elastic/elasticsearch/files/601899/threads_dump.zip)\n\nAt 14:13, one search thread looks to be building global ordinals.\n`\"elasticsearch[oy_data_weak_1_10.15.118.11_B][search][T#4]\" #135 daemon prio=5 os_prio=0 tid=0x00007f1e1c093000 nid=0x34cf runnable [0x00007f1e45be8000]\n   java.lang.Thread.State: RUNNABLE\n        at org.apache.lucene.index.MultiDocValues$OrdinalMap.<init>(MultiDocValues.java:531)\n        at org.apache.lucene.index.MultiDocValues$OrdinalMap.build(MultiDocValues.java:484)\n        at org.apache.lucene.index.MultiDocValues$OrdinalMap.build(MultiDocValues.java:463)\n        at org.elasticsearch.index.fielddata.ordinals.GlobalOrdinalsBuilder.build(GlobalOrdinalsBuilder.java:61)\n        at org.elasticsearch.index.fielddata.plain.SortedSetDVOrdinalsIndexFieldData.localGlobalDirect(SortedSetDVOrdinalsIndexFieldData.java:103)\n        at org.elasticsearch.index.fielddata.plain.SortedSetDVOrdinalsIndexFieldData.localGlobalDirect(SortedSetDVOrdinalsIndexFieldData.java:38)\n        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.lambda$load$1(IndicesFieldDataCache.java:159)\n        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$$Lambda$1280/790193266.load(Unknown Source)\n        at org.elasticsearch.common.cache.Cache.computeIfAbsent(Cache.java:385)\n        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:154)\n        at org.elasticsearch.index.fielddata.plain.SortedSetDVOrdinalsIndexFieldData.loadGlobal(SortedSetDVOrdinalsIndexFieldData.java:91)\n        at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$WithOrdinals$FieldData.globalOrdinalsValues(ValuesSource.java:150)\n        at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$WithOrdinals.globalMaxOrd(ValuesSource.java:123)\n        at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.doCreateInternal(TermsAggregatorFactory.java:120)\n        at org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.createInternal(ValuesSourceAggregatorFactory.java:55)\n        at org.elasticsearch.search.aggregations.AggregatorFactory.create(AggregatorFactory.java:225)\n        at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:102)\n        at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:61)\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:104)\n        at org.elasticsearch.indices.IndicesService.lambda$loadIntoContext$17(IndicesService.java:1109)\n        at org.elasticsearch.indices.IndicesService$$Lambda$1410/1877142574.load(Unknown Source)\n        at org.elasticsearch.indices.AbstractIndexShardCacheEntity.loadValue(AbstractIndexShardCacheEntity.java:73)\n        at org.elasticsearch.indices.IndicesRequestCache$Loader.load(IndicesRequestCache.java:148)\n        at org.elasticsearch.indices.IndicesRequestCache$Loader.load(IndicesRequestCache.java:133)\n        at org.elasticsearch.common.cache.Cache.computeIfAbsent(Cache.java:385)\n        at org.elasticsearch.indices.IndicesRequestCache.getOrCompute(IndicesRequestCache.java:116)\n        at org.elasticsearch.indices.IndicesService.loadIntoContext(IndicesService.java:1113)\n        at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:236)\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:251)\n        at org.elasticsearch.action.search.SearchTransportService.lambda$registerRequestHandler$6(SearchTransportService.java:276)\n        at org.elasticsearch.action.search.SearchTransportService$$Lambda$1029/773765215.messageReceived(Unknown Source)\n        at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)\n        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69)\n        at org.elasticsearch.transport.TcpTransport$RequestHandler.doRun(TcpTransport.java:1348)\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:504)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n`\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262204033","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-262204033","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":262204033,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjIwNDAzMw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T10:29:16Z","updated_at":"2016-11-22T10:29:16Z","author_association":"CONTRIBUTOR","body":"@xgwu Can you clarify which time range are using the default and which time range is using `niofs`?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262204727","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-262204727","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":262204727,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjIwNDcyNw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T10:31:36Z","updated_at":"2016-11-22T10:41:58Z","author_association":"CONTRIBUTOR","body":"Also how many nodes, data (in GB) and shards do you have in total? Also, how much RAM do you give to the JVM and to the filesystem cache on each node?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262210708","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-262210708","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":262210708,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjIxMDcwOA==","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T10:58:57Z","updated_at":"2016-11-22T10:58:57Z","author_association":"NONE","body":"It was mmapfs between 13:00 to 14:15 and niofs since 14:30.\r\n\r\nWe have 120 data nodes, 17000 shardsï¼Œ800billion docs , 900TB data in total including one replica. \r\n\r\n60 out of the 120 nodes are backup nodes without write. They are hosted by physical servers of 32 core, 128GB RAM & 12 4TB spin disks with raid0.  Each physical host contains 3 es instances with 31GB heap each, ie,35GB left for filesystem cache.\r\n\r\nOther nodes are mainly for write and only run 1 instance on each host. They only hold  ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262210852","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-262210852","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":262210852,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjIxMDg1Mg==","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T10:59:38Z","updated_at":"2016-11-22T10:59:38Z","author_association":"NONE","body":"Sorry closed the issue by making mistake. Reopening...","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262211299","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-262211299","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":262211299,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjIxMTI5OQ==","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T11:01:39Z","updated_at":"2016-11-22T11:01:39Z","author_association":"NONE","body":"Write nodes only hold one day worth data. The problem is likely to happen only on some read only backup nodes.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262314021","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-262314021","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":262314021,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjMxNDAyMQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T17:52:00Z","updated_at":"2016-11-22T17:52:00Z","author_association":"CONTRIBUTOR","body":"To clarify: when you say `node`, you mean elatsticsearch process rather than physical machine right?\r\n\r\nThe most immediate issue here is the sizing of the filesystem cache. Out of 128GB per physical server, 93GB go to the JVM and only 35GB go to the filesystem cache. There should be at least 64GB for the filesystem, probably even more. Does your JVM make use of all the available memory? If yes, do you know why? For instance are you using the completion suggester? You might want to give less memory to your JVMs and/or start fewer nodes per physical machine in order to give more memory to the filesystem cache. Note that decreasing the amount of memory per JVM also has the side benefit of making garbage collection more efficient since smaller heaps can be collected more quickly.\r\n\r\nThe jstack traces show that an `_optimize` call is running. This is not a bad thing since you seem to have time-based data, but this might also explain why building global ordinals is so slow. Are all your readonly indices expected to be merged to a single segment (optimized)? Since you seem to be running terms aggregations, this actually helps since global ordinals come for free once a shard has been merged to a single segment.\r\n\r\nSince your nodes spend a lot of time building global ordinals, you might also want to look into increasing the `refresh_interval` (eg maybe 10s or 30s would be fine for your use case?) as well as building global ordinals on refresh rather than in search requests. In order to do that, just set `eager_global_ordinals: true` on the fields that you use in `terms` aggregations.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262420630","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-262420630","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":262420630,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjQyMDYzMA==","user":{"login":"xgwu","id":10510416,"node_id":"MDQ6VXNlcjEwNTEwNDE2","avatar_url":"https://avatars2.githubusercontent.com/u/10510416?v=4","gravatar_id":"","url":"https://api.github.com/users/xgwu","html_url":"https://github.com/xgwu","followers_url":"https://api.github.com/users/xgwu/followers","following_url":"https://api.github.com/users/xgwu/following{/other_user}","gists_url":"https://api.github.com/users/xgwu/gists{/gist_id}","starred_url":"https://api.github.com/users/xgwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xgwu/subscriptions","organizations_url":"https://api.github.com/users/xgwu/orgs","repos_url":"https://api.github.com/users/xgwu/repos","events_url":"https://api.github.com/users/xgwu/events{/privacy}","received_events_url":"https://api.github.com/users/xgwu/received_events","type":"User","site_admin":false},"created_at":"2016-11-23T02:16:02Z","updated_at":"2016-11-23T02:16:02Z","author_association":"NONE","body":"@jpountz  Yes, the node is meant an elasticsearch process.\r\n\r\nOne more point to clarify that this issue also happened on a write node that have only one elasticsearch process with 31GB JVM on a physical machine. In that case, 97GB ram is available for filesystem cache.  But it only happened once compared to several times a day on those cold data nodes.\r\n\r\nThis cluster is for application/business logs search and analysis.  The ops/business require a fairly long history data, for example last 3 months,  be immediately available for search whenever they need it. To keep so huge cold indices open in elasticsearch, it would require a lot of space in JVM for segment memory.  In 2.4 era, I found it a good balance to put 3 instances on 1 physical box for this kind of use case, as it allows us to hold about 30TB indices open on a single physical machine and the search performance is acceptable even only leaving 35GB memory to filesystem cache.\r\n\r\nAnd you are right, that we force merge each shard to 1 segment every night on these cold data nodes to reduce segment memory footprint. The refresh_interval is already 60s globally on our cluster and worked well for a long time. I am not sure whether setting `eager_global_ordinals: true` would alleviate this particular problem as these are cold data never been written anymore.\r\n\r\nWe don't use completion suggester on this cluster. The mostly used features are queries, filters plus terms, data histogram and percentiles aggregation.  For some terms aggregation, the cardinality could be very high, such as client ip field or  url stem field. \r\n\r\nPer your comment, it feels like when reading every part of an indices via `mmapfs`, the available filesystem cache becomes very critical. When the search work load crosses a certain threshold, the performance degraded very quickly.  One other thing worrying me is that why search timeout takes no effect in this situation? Every query was sent with 35 seconds timeout but a search thread could last for an hour without terminating itself.  That eventually saturated all search threads making the node not responding.  \r\n\r\nFor now I'll stick to niofs as it looked performant enough in our situation and I am not seeing long running search threads anymore.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262520966","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-262520966","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":262520966,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjUyMDk2Ng==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-11-23T14:06:39Z","updated_at":"2016-11-23T14:06:39Z","author_association":"CONTRIBUTOR","body":"> I am not sure whether setting eager_global_ordinals: true would alleviate this particular problem as these are cold data never been written anymore.\r\n\r\nThe reason that made me think about it is that the jstack traces show that some search threads are building global ordinals, which would never happen if global ordinals were built at refresh time.\r\n\r\n>  One other thing worrying me is that why search timeout takes no effect in this situation? \r\n\r\nThe way that the timeout is implemented, Elasticsearch has some checkpoints during the execution of a search request when it checks the timeout. Unfortunately, some parts of query execution do not check the timeout while they are running, such as when rewriting multi-term queries (eg. ranges, wildcard or fuzzy queries) or building global ordinals.\r\n\r\n> For now I'll stick to niofs\r\n\r\nThis sounds like a good idea indeed since niofs seems to perform better in your case.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/263225595","html_url":"https://github.com/elastic/elasticsearch/issues/21611#issuecomment-263225595","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21611","id":263225595,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MzIyNTU5NQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-11-28T09:41:08Z","updated_at":"2016-11-28T09:41:08Z","author_association":"CONTRIBUTOR","body":"Since it looks like using `niofs` fixes your issue, I'll close.","performed_via_github_app":null}]