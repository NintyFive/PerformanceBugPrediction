[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/318446033","html_url":"https://github.com/elastic/elasticsearch/issues/25939#issuecomment-318446033","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25939","id":318446033,"node_id":"MDEyOklzc3VlQ29tbWVudDMxODQ0NjAzMw==","user":{"login":"hub-cap","id":613352,"node_id":"MDQ6VXNlcjYxMzM1Mg==","avatar_url":"https://avatars2.githubusercontent.com/u/613352?v=4","gravatar_id":"","url":"https://api.github.com/users/hub-cap","html_url":"https://github.com/hub-cap","followers_url":"https://api.github.com/users/hub-cap/followers","following_url":"https://api.github.com/users/hub-cap/following{/other_user}","gists_url":"https://api.github.com/users/hub-cap/gists{/gist_id}","starred_url":"https://api.github.com/users/hub-cap/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hub-cap/subscriptions","organizations_url":"https://api.github.com/users/hub-cap/orgs","repos_url":"https://api.github.com/users/hub-cap/repos","events_url":"https://api.github.com/users/hub-cap/events{/privacy}","received_events_url":"https://api.github.com/users/hub-cap/received_events","type":"User","site_admin":false},"created_at":"2017-07-27T18:24:24Z","updated_at":"2017-07-27T18:24:24Z","author_association":"CONTRIBUTOR","body":"Hi, \r\n\r\nAre you breaking the pdf up into chunks and encoding them in parallel? I just tested with a quick java application and was able to produce a valid base64 encoded file reading in 1000*3 byte chunks (Base64 likes 3 byte increments so it does not need to pad). Im sure you can write some code that will parallelize that operation to speed up the encoding.\r\n\r\nAlso, questions like this are better served in the [community forums](https://discuss.elastic.co/c/elasticsearch), as we reserve github issue for verified bug reports and feature requests.","performed_via_github_app":null}]