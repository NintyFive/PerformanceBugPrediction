[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/247784894","html_url":"https://github.com/elastic/elasticsearch/issues/20510#issuecomment-247784894","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20510","id":247784894,"node_id":"MDEyOklzc3VlQ29tbWVudDI0Nzc4NDg5NA==","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2016-09-17T16:05:47Z","updated_at":"2016-09-17T16:05:47Z","author_association":"MEMBER","body":"The mapper attachment plugin does not support \"crawling\" inside ZIP files.\n[The doc](https://www.elastic.co/guide/en/elasticsearch/plugins/current/mapper-attachments.html) says:\n\n> The mapper attachments plugin lets Elasticsearch index file attachments in common formats (such as PPT, XLS, PDF)\n\nBTW mapper attachments has been deprecated so we are not going to implement such a feature in it.\nI don't believe we will do it in ingest-attachment. This is something you should deal with in your application layer.\n\nUnzip, then for each file, send it to elasticsearch.\n\nAs the developer of the community project [FSCrawler](https://github.com/dadoonet/fscrawler), I think this is something which could be supported while crawling the filesystem. I opened https://github.com/dadoonet/fscrawler/issues/230 for this.\n\nClosing this one as this won't be implemented.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/248251555","html_url":"https://github.com/elastic/elasticsearch/issues/20510#issuecomment-248251555","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20510","id":248251555,"node_id":"MDEyOklzc3VlQ29tbWVudDI0ODI1MTU1NQ==","user":{"login":"chindhuhari","id":15011828,"node_id":"MDQ6VXNlcjE1MDExODI4","avatar_url":"https://avatars1.githubusercontent.com/u/15011828?v=4","gravatar_id":"","url":"https://api.github.com/users/chindhuhari","html_url":"https://github.com/chindhuhari","followers_url":"https://api.github.com/users/chindhuhari/followers","following_url":"https://api.github.com/users/chindhuhari/following{/other_user}","gists_url":"https://api.github.com/users/chindhuhari/gists{/gist_id}","starred_url":"https://api.github.com/users/chindhuhari/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/chindhuhari/subscriptions","organizations_url":"https://api.github.com/users/chindhuhari/orgs","repos_url":"https://api.github.com/users/chindhuhari/repos","events_url":"https://api.github.com/users/chindhuhari/events{/privacy}","received_events_url":"https://api.github.com/users/chindhuhari/received_events","type":"User","site_admin":false},"created_at":"2016-09-20T09:32:07Z","updated_at":"2016-10-17T08:58:52Z","author_association":"NONE","body":"Thank you @dadoonet !\nWhen we don't pass the subset of PARSERS to the AutoDetectParser, it works fine for zip files as \nwell (in TikaImpl.java):\n _private static final AutoDetectParser PARSER_INSTANCE = new AutoDetectParser();_ \n\nTried explicitly adding RecursiveParserWrapper in the list of PARSERS, that didn't work though.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/263052096","html_url":"https://github.com/elastic/elasticsearch/issues/20510#issuecomment-263052096","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20510","id":263052096,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MzA1MjA5Ng==","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2016-11-26T08:49:32Z","updated_at":"2016-11-26T08:49:32Z","author_association":"MEMBER","body":"Sorry for the late reply.\r\n\r\nYes. I believe we decided to support the most common patterns with the least cost in term of memory usage, CPU usage on elasticsearch side because overloading the index process (analyze part, mapping part) is definitely something we don't want to have. Plus security concerns\r\nFurthermore, we decided to add dedicated ingest nodes which are responsible for doing that.\r\nBut, ingest-attachment does not support ZIP files as well and IMO it's good we don't support it.\r\nImagine you have one single ZIP containing 100 of PDF documents. If you index the content of 100 PDF documents inside a single Elasticsearch document, how the end user will know where to find the exact document?\r\n\r\nI think this is something which must be solved on the client side. So on client side:\r\n\r\n* Unzip the file\r\n* For each file of the ZIP file create a JSON doc like\r\n```json\r\n{\r\n  \"parent_filename\": \"name of the zip.zip\",\r\n  \"filename\": \"the internal file name within the ZIP\",\r\n  \"content\": \"the BASE64 content\"\r\n}\r\n```\r\n* Send this JSON doc to elasticsearch (using mapper-attachments) or better, send it to an ingest pipeline which uses ingest-attachment\r\n\r\nI hope this helps","performed_via_github_app":null}]