{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/49754","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/49754/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/49754/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/49754/events","html_url":"https://github.com/elastic/elasticsearch/issues/49754","id":531049550,"node_id":"MDU6SXNzdWU1MzEwNDk1NTA=","number":49754,"title":"Filebeat 6.8.1 Sporadically stops Sending Logs","user":{"login":"danil-lavrentyev","id":36068271,"node_id":"MDQ6VXNlcjM2MDY4Mjcx","avatar_url":"https://avatars1.githubusercontent.com/u/36068271?v=4","gravatar_id":"","url":"https://api.github.com/users/danil-lavrentyev","html_url":"https://github.com/danil-lavrentyev","followers_url":"https://api.github.com/users/danil-lavrentyev/followers","following_url":"https://api.github.com/users/danil-lavrentyev/following{/other_user}","gists_url":"https://api.github.com/users/danil-lavrentyev/gists{/gist_id}","starred_url":"https://api.github.com/users/danil-lavrentyev/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danil-lavrentyev/subscriptions","organizations_url":"https://api.github.com/users/danil-lavrentyev/orgs","repos_url":"https://api.github.com/users/danil-lavrentyev/repos","events_url":"https://api.github.com/users/danil-lavrentyev/events{/privacy}","received_events_url":"https://api.github.com/users/danil-lavrentyev/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-12-02T10:56:02Z","updated_at":"2019-12-02T11:51:16Z","closed_at":"2019-12-02T11:42:18Z","author_association":"NONE","active_lock_reason":null,"body":"Apologies for opening bug here, cause we didn't get any feedback [here](https://discuss.elastic.co/t/filebeat-6-8-1-sporadically-stops-sending-logs/208064/2)\r\n\r\nWe have four separate kubernetes clusters, two on-prem and two in AWS setup with filebeat to ship logs. For several months we have been troubleshooting an issue where sporadically our filebeat daemonsets will altogether stop shipping logs. We were on filebeat 6.2.x when this started, and ended up upgrading to 6.8.1 hoping this would help.\r\n\r\nOften filebeat will continue shipping logs for an extended period of time, then suddenly will stop for several nodes. If we restart the filebeat daemonset it will immediately start shipping logs again, including the logs that were previously missed. At this point I am not sure what else to look at.\r\n\r\nMemory/Cpu consumption seems to be normal during these outages.\r\n\r\nSystem Version and component information:\r\n\r\nKubernetes 1.12.4\r\nFilebeat 6.8.1\r\nCNI: Kube-router\r\nDNS provider: CoreDNS\r\nRBAC: Enabled\r\n**Some errors we have seen:**\r\n```\r\nERROR   kubernetes/watcher.go:258       kubernetes: Watching API error read tcp x.x.x.x:59506->x.x.x.x:443: read: connection timed out\r\n\r\n\r\nERROR   kubernetes/watcher.go:248       kubernetes: Watching API error Get https://x.x.x.x:443/api/v1/pods?fieldSelector=spec.nodeName%3Dlg-l-p-obo00500&resourceVersion=&watch=true: dial tcp x.x.x.x:443: i/o timeout\r\n\r\nERROR   kubernetes/watcher.go:258       kubernetes: Watching API error EOF\r\n\r\nERROR   kubernetes/watcher.go:258       kubernetes: Watching API error read tcp x.x.x.x.184:39540->x.x.x.x:443: read: connection reset by peer  \r\n\r\nERROR   log/harvester.go:282    Read line error: invalid CRI log format; File: /var/lib/docker/containers/10e45645029f95adfdb1cee0c6341757e86d3c3115472d8076dc410fcb17eb30/10e45645029f95adfdb1cee0c6341757e86d3c3115472d8076dc410fcb17eb30-json.log\r\n\r\nERROR   log/harvester.go:282    Read line error: invalid CRI log format; File: /var/lib/docker/containers/10e45645029f95adfdb1cee0c6341757e86d3c3115472d8076dc410fcb17eb30/10e45645029f95adfdb1cee0c6341757e86d3c3115472d8076dc410fcb17eb30-json.log \r\n```\r\n**However the only error that reliably shows up on ALL nodes when the error occurs is this:**\r\n```\r\nERROR   kubernetes/watcher.go:258       kubernetes: Watching API error EOF\r\n```\r\n**Filebeat Configuration:**\r\n```\r\n---\r\napiVersion: rbac.authorization.k8s.io/v1beta1 1\r\nkind: ClusterRoleBinding\r\nmetadata:\r\nname: filebeat\r\nsubjects:\r\n- kind: ServiceAccount\r\nname: filebeat\r\nnamespace: monitor\r\nroleRef:\r\nkind: ClusterRole\r\nname: filebeat\r\napiGroup: rbac.authorization.k8s.io\r\n---\r\napiVersion: rbac.authorization.k8s.io/v1beta1 1\r\nkind: ClusterRole\r\nmetadata:\r\nname: filebeat\r\nlabels:\r\nk8s-app: filebeat\r\nrules:\r\n- apiGroups: [\"\"] # \"\" indicates the core API group\r\nresources:\r\n- namespaces\r\n- pods\r\nverbs:\r\n- get\r\n- watch\r\n- list\r\n---\r\napiVersion: v1\r\nkind: ServiceAccount\r\nmetadata:\r\nname: filebeat\r\nnamespace: monitor\r\nlabels:\r\nk8s-app: filebeat\r\n---\r\napiVersion: extensions/v1beta1\r\nkind: DaemonSet\r\nmetadata:\r\nname: filebeat\r\nnamespace: monitor\r\nlabels:\r\napp: filebeat\r\nspec:\r\ntemplate:\r\nmetadata:\r\nlabels:\r\napp: filebeat\r\nname: filebeat\r\nannotations:\r\nlgi.io/team: \"Platform\"\r\nspec:\r\nserviceAccountName: filebeat\r\ntolerations:\r\n- operator: Exists\r\ncontainers:\r\n- name: filebeat\r\nimage: registry.x.x/beats/filebeat:6.8.1\r\nimagePullPolicy: IfNotPresent\r\nargs: [\r\n\"-c\", \"/etc/filebeat/filebeat.yml\",\r\n\"-e\",\r\n]\r\nsecurityContext:\r\nrunAsUser: 0\r\nresources:\r\nlimits:\r\nmemory: 500Mi\r\nrequests:\r\ncpu: 100m\r\nmemory: 200Mi\r\nvolumeMounts:\r\n- name: config\r\nmountPath: /etc/filebeat\r\nreadOnly: true\r\n- name: data\r\nmountPath: /usr/share/filebeat/data\r\n- name: varlibdockercontainers\r\nmountPath: /var/lib/docker/containers\r\nreadOnly: true\r\n- name: varlog\r\nmountPath: /var/log\r\nterminationGracePeriodSeconds: 30\r\nvolumes:\r\n- name: config\r\nconfigMap:\r\ndefaultMode: 0600\r\nname: filebeat\r\n- name: data\r\n# avoid data resubmission and store registy on a hostPath\r\nhostPath:\r\npath: /var/run/filebeat\r\n- name: varlibdockercontainers\r\nhostPath:\r\npath: /app/containers\r\n- name: varlog\r\nhostPath:\r\npath: /var/log\r\n---\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\nname: filebeat\r\nnamespace: monitor\r\ndata:\r\nfilebeat.yml: |+\r\nfilebeat.inputs:\r\n- type: log\r\npaths:\r\n- /var/log/kube-.log\r\n- type: log\r\npaths:\r\n- /var/log/syslog\r\n- /var/log/messages\r\ninclude_lines: ['kubelet([[0-9]])?:', 'kernel:', 'etcd:']\r\n- type: docker\r\nscan_frequency: 5s\r\nclose_inactive: 5m\r\ncontainers.ids:\r\n- \"\"\r\nprocessors:\r\n- add_kubernetes_metadata:\r\nin_cluster: true\r\nlabels.dedot: true\r\nannotations.dedot: true\r\n- drop_event:\r\n# drop everything in kube-system, tools\r\n# except nginx-ingress-controller, prometheus2, kube-scheduler, kube-controller-manager, flannel\r\nwhen.and:\r\n- not.or:\r\n- equals:\r\n. . .\r\n- and:\r\n- equals:\r\nkubernetes.labels.app: x-service\r\n- equals:\r\nkubernetes.namespace: tools\r\n- or:\r\n- not.regexp:\r\nkubernetes.namespace: \".*\"\r\n- equals:\r\nkubernetes.namespace: kube-system\r\n- equals:\r\nkubernetes.namespace: monitor\r\n- equals:\r\nkubernetes.namespace: tools\r\n- decode_json_fields:\r\nwhen.regexp.message: '^{'\r\nfields: [\"message\"]\r\ntarget: \"\"\r\noverwrite_keys: true\r\n- drop_event:\r\nwhen.or:\r\n- equals: {level: DEBUG}\r\n- drop_fields:\r\nwhen.regexp.message: '^{'\r\nfields: [\"message\", \"timestamp\"]\r\n- dissect:\r\ntokenizer: \"%{namespace}\"\r\nfield: \"kubernetes.namespace\"\r\ntarget_prefix: \"\"\r\n- rename:\r\nfields:\r\n. . .\r\nmultiline:\r\n# Java exceptions and start scripts\r\npattern: '^[[:space:]]+(at|.{3})\\b|^Caused by:|^+'\r\nnegate: false\r\nmatch: after\r\nfields_under_root: true\r\noutput.kafka:\r\n          # initial brokers for reading cluster metadata\r\n          hosts: [\"x.x.x.x:9092\"]\r\n\r\n          # message topic selection + partitioning\r\n          topic: 'topic_name' # Changed in . . . .\r\n          partition.round_robin:\r\n            reachable_only: true\r\n\r\n          required_acks: 0\r\n          compression: gzip\r\n          max_message_bytes: 8388608\r\n          version: '0.10'\r\n          worker: 5\r\n          keep_alive: 30s\r\n          channel_buffer_size: 2048\r\n\r\n        logging.level: info\r\n```\r\n\r\n\r\n","closed_by":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"performed_via_github_app":null}