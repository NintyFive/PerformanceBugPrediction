[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/524092936","html_url":"https://github.com/elastic/elasticsearch/issues/45851#issuecomment-524092936","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45851","id":524092936,"node_id":"MDEyOklzc3VlQ29tbWVudDUyNDA5MjkzNg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-08-22T21:54:58Z","updated_at":"2019-08-22T21:54:58Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/524093183","html_url":"https://github.com/elastic/elasticsearch/issues/45851#issuecomment-524093183","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45851","id":524093183,"node_id":"MDEyOklzc3VlQ29tbWVudDUyNDA5MzE4Mw==","user":{"login":"jakelandis","id":976291,"node_id":"MDQ6VXNlcjk3NjI5MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/976291?v=4","gravatar_id":"","url":"https://api.github.com/users/jakelandis","html_url":"https://github.com/jakelandis","followers_url":"https://api.github.com/users/jakelandis/followers","following_url":"https://api.github.com/users/jakelandis/following{/other_user}","gists_url":"https://api.github.com/users/jakelandis/gists{/gist_id}","starred_url":"https://api.github.com/users/jakelandis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jakelandis/subscriptions","organizations_url":"https://api.github.com/users/jakelandis/orgs","repos_url":"https://api.github.com/users/jakelandis/repos","events_url":"https://api.github.com/users/jakelandis/events{/privacy}","received_events_url":"https://api.github.com/users/jakelandis/received_events","type":"User","site_admin":false},"created_at":"2019-08-22T21:55:48Z","updated_at":"2019-08-22T21:55:48Z","author_association":"CONTRIBUTOR","body":"I _think_ this is a duplicate of https://github.com/elastic/elasticsearch/issues/17213 \r\n\r\n@ragri8 - you may want to see if there are any suggestions on that thread that may help","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/524337429","html_url":"https://github.com/elastic/elasticsearch/issues/45851#issuecomment-524337429","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45851","id":524337429,"node_id":"MDEyOklzc3VlQ29tbWVudDUyNDMzNzQyOQ==","user":{"login":"ragri8","id":31997527,"node_id":"MDQ6VXNlcjMxOTk3NTI3","avatar_url":"https://avatars0.githubusercontent.com/u/31997527?v=4","gravatar_id":"","url":"https://api.github.com/users/ragri8","html_url":"https://github.com/ragri8","followers_url":"https://api.github.com/users/ragri8/followers","following_url":"https://api.github.com/users/ragri8/following{/other_user}","gists_url":"https://api.github.com/users/ragri8/gists{/gist_id}","starred_url":"https://api.github.com/users/ragri8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ragri8/subscriptions","organizations_url":"https://api.github.com/users/ragri8/orgs","repos_url":"https://api.github.com/users/ragri8/repos","events_url":"https://api.github.com/users/ragri8/events{/privacy}","received_events_url":"https://api.github.com/users/ragri8/received_events","type":"User","site_admin":false},"created_at":"2019-08-23T14:30:19Z","updated_at":"2019-08-23T14:30:19Z","author_association":"NONE","body":"As I said, this is a different issue, because when shard allocation was disabled on the unbalanced node because of the low watermark, primary shards of the same index still land on the same node, even when all available nodes are balanced.\r\n\r\nThe discussion emphasize on an upper bound of ressource use, while I'm suggesting a better heuristic to choose where to allocate each new shards. With the solution proposed, there would at most guarantee spread of new shards between every nodes, but won't help preventing a large index with multiple shard to land on a single node.\r\n\r\nLet me give a more detailed situation:\r\nI have 10 different index, rotating daily.\r\nIndex-1 biggest one has 10 shards (5 primaries and 5 replicas), index 3200docs/sec, and use around 150Gb of space on weekdays.\r\nIndex-2 one has 2 shards (1 primary and 1 replicas), index 900docs/sec and use around 60Gb. For optimal result, we will instead use 2 primaries and 2 replicas.\r\nIndex-3 to 10 each has 1 primary and 1 replica and together index less than 200docs/sec and use less than 10 Gb per day.\r\n\r\nSo there is 30 shards to spread between 5 nodes, but 14 shards hold 95% of the traffic and 75% come from a single index.\r\n\r\n**With any hard limit on ressources**,\r\n_at best_ this could limit to 6 the number of active shard per node. But the Index-1 might still place its 5 primary on one, giving it almost 40% of the traffic and maybe more if it gets a shard from Index-2. Also, if new shards are added while the older one are still indexing (it does happen for me), this would become even harder to set the ressource limit efficiently without getting the cluster to yellow or red status.\r\n\r\nIf instead, **we add a simple rule to always split shards from the same indice evenly accross nodes**,\r\neach node will get 2 shards from Index-1 and 1 shard at most from Index-2. Before adding the 16 small shards remaining, we have 4 nodes with 20% of traffic and one with 15%, and we didn't need even a single heuristic to get this almost perfect share of ressources.\r\nAnd if the low watermark is hit on the 5th node, remaining nodes will still get 29% and 21% of traffic each, which is still better than the hard limit rule and won't prevent allocation of new shards.\r\n\r\nOf course, not everyone has the same setup as me. Let's say that someone with as much node and new shards per day as me, has indexes with only 1 primary and 1 replica, my rule won't help them, but it won't hurt them either. More allocation rules could still be used around this one without conflict.\r\n\r\nI think this is the easiest approach to solve my issue and help many others and I still think this should be a default config.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/524746928","html_url":"https://github.com/elastic/elasticsearch/issues/45851#issuecomment-524746928","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45851","id":524746928,"node_id":"MDEyOklzc3VlQ29tbWVudDUyNDc0NjkyOA==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2019-08-26T07:17:06Z","updated_at":"2019-08-26T07:17:06Z","author_association":"CONTRIBUTOR","body":"This very much looks like a duplicate of #17213. You seem to be saying that your nodes cannot handle more than `N` shards of a particular index. If so, you can avoid allocating more than `N` shards of that index to any one node by setting `index.routing.allocation.total_shards_per_node: N` on that index. When the index no longer needs this constraint you can remove it (see #44070).\r\n\r\nThis setting applies per index, so you can set it only on the index you want to spread out more. It will not directly cause shards of older indices to move, although you may see a small amount of extra shard movement to fix any resulting imbalance.\r\n\r\nI think you might be confused about the difference between a primary and a replica in terms of their resource requirements. They both require the same resources. It does not matter whether a node has 5 primaries or 5 replicas of an index, it will see the same load. (For completeness, I should add that this is sometimes not the case, but it is true when indexing log data).\r\n\r\nYou also talk about \"a reallocation storm that I want to prevent at all cost\". If shard movements are damaging to your cluster stability then you may have misconfigured your cluster (e.g. set `indices.recovery.max_bytes_per_sec` or `cluster.routing.allocation.node_concurrent_recoveries` to too high a value).\r\n\r\nI think it would be best to continue this discussion on the [forums](https://discuss.elastic.co/c/elasticsearch), so I'm closing this issue. If the discussion in the forums identifies something that isn't already covered by #17213 then we can always reopen this issue.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/525002632","html_url":"https://github.com/elastic/elasticsearch/issues/45851#issuecomment-525002632","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45851","id":525002632,"node_id":"MDEyOklzc3VlQ29tbWVudDUyNTAwMjYzMg==","user":{"login":"ragri8","id":31997527,"node_id":"MDQ6VXNlcjMxOTk3NTI3","avatar_url":"https://avatars0.githubusercontent.com/u/31997527?v=4","gravatar_id":"","url":"https://api.github.com/users/ragri8","html_url":"https://github.com/ragri8","followers_url":"https://api.github.com/users/ragri8/followers","following_url":"https://api.github.com/users/ragri8/following{/other_user}","gists_url":"https://api.github.com/users/ragri8/gists{/gist_id}","starred_url":"https://api.github.com/users/ragri8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ragri8/subscriptions","organizations_url":"https://api.github.com/users/ragri8/orgs","repos_url":"https://api.github.com/users/ragri8/repos","events_url":"https://api.github.com/users/ragri8/events{/privacy}","received_events_url":"https://api.github.com/users/ragri8/received_events","type":"User","site_admin":false},"created_at":"2019-08-26T19:44:18Z","updated_at":"2019-08-26T19:44:18Z","author_association":"NONE","body":"> This very much looks like a duplicate of #17213.\r\n\r\nMy proposed solution is quite different.\r\nIn the issue #17213, the solution doesn't take into account separation of shards from the same indice and could only mitigate my issue by using multi-dimensionnal costs from ressources, which would be harder for a less experimented user like me to implement and misconfiguration could even lead to data loss.\r\nMy proposed solution would provide an out-of-the-box solution available for every type of user. Hard limits always have the potential to break something if users aren't aware of every change in the cluster and this does happen in large organizations.\r\n\r\n> If so, you can avoid allocating more than N shards of that index to any one node by setting index.routing.allocation.total_shards_per_node: N on that index.\r\n\r\nI could, but that would cause undesired tradeoff while I think it could be easily avoidable.\r\n\r\n>  I think you might be confused about the difference between a primary and a replica in terms of their resource requirements.\r\n\r\nI do understand ressource requirements of both primaries and replicas. While replicas are (most of the time?) evenly spread accross the cluster, primaries aren't, and 50% of the total traffic indexed using only 20% of the ressources create a hotspot.\r\n\r\n> It will not directly cause shards of older indices to move\r\n\r\n> If shard movements are damaging to your cluster stability then you may have misconfigured your cluster\r\n\r\nIn this case I admit I didn't understood properly how ES handle reallocations. The only reallocation problem we have is caused by the smaller node constantly moving away old shards when it hits the low watermark, but we solved this issue by blocking bigger indexes from writing to it.\r\n\r\n\r\nI think a soft rule spreading evenly shards from the same indice accross nodes should be the default configuration, as I don't see how could it be an unwanted behaviour, except for exceptionnal cases which already need custom allocation rules.\r\n\r\nI think this issue is closer to #43350 , but with a simpler proposal. It's also simpler than #17213.\r\n\r\nA new allocation algorithm could use the current one to allocate the first shard from an indice on a node, but then, after applying the configured hard limits, choose the node with the smallest number of shards from this indice and use the current algorithm to choose when multiple nodes has the lowest number of shards.\r\nAs an option or by default configuration, instead of a \"smallest first\" approach, nodes with shards from the same indice could have some \"weight\" to influence the choice and give other options to choose (like ressource costs, which would lead later to a merge with #17213).","performed_via_github_app":null}]