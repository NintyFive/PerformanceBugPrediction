[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/42314954","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-42314954","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":42314954,"node_id":"MDEyOklzc3VlQ29tbWVudDQyMzE0OTU0","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2014-05-06T15:10:50Z","updated_at":"2014-05-06T15:10:50Z","author_association":"CONTRIBUTOR","body":"It looks like Simon's prototype pauses the indexing thread if too many merges are in flight.  I'm not 100% clear on the code path that gets here.  Will that pause indexing or pause refreshing or both?  It'd be neat to slow down just the refreshing and let indexing be slowed down by the refresh backlog logic.  Or am I crazy?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/42347228","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-42347228","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":42347228,"node_id":"MDEyOklzc3VlQ29tbWVudDQyMzQ3MjI4","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2014-05-06T19:25:30Z","updated_at":"2014-05-06T19:25:30Z","author_association":"CONTRIBUTOR","body":"@nik9000 internally the IndexWriter has several threads states (8 by default) that we index into. If we limit to a single threads we only use on of the states and make sure we max out the RAM buffer and write the least amount of segments. This means we 1. reduce the number of segments to merge and 2. make sure flushes are only done if really needed. I think we can't slow down refreshes otherwise folks will see odd results since you don't get new documents. You also want to refresh to publish merged segments to further reduce the number of segments. We will do the right thing and provide backpressure on indexing not on refresh. Hope that makes sense?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/42349244","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-42349244","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":42349244,"node_id":"MDEyOklzc3VlQ29tbWVudDQyMzQ5MjQ0","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2014-05-06T19:42:31Z","updated_at":"2014-05-06T19:42:31Z","author_association":"CONTRIBUTOR","body":"> We will do the right thing and provide backpressure on indexing not on refresh. Hope that makes sense?\n\nI'd honestly forgotten about flushes.  Its what I get for only playing on the other side.  Anyway, I'm happy so long as back pressure is provided on indexing.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/42657513","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-42657513","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":42657513,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNjU3NTEz","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2014-05-09T11:43:56Z","updated_at":"2014-05-09T11:43:56Z","author_association":"CONTRIBUTOR","body":"I tested the current throttling branch with the refresh=-1 case, and we have problems because the \"abandoned\" thread states will never flush until a full flush ... workaround is you must use a refresh to get them flushed.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/42677542","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-42677542","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":42677542,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNjc3NTQy","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2014-05-09T15:17:14Z","updated_at":"2014-05-09T15:17:14Z","author_association":"CONTRIBUTOR","body":"I'm inclined to simply document that index throttling won't kick in if you use SerialMergeScheduler.\n\nSMS only allows one merge to run at a time, so apps that are doing heavy bulk indexing really should not be using it.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/42939629","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-42939629","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":42939629,"node_id":"MDEyOklzc3VlQ29tbWVudDQyOTM5NjI5","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2014-05-13T10:32:48Z","updated_at":"2014-05-13T10:32:48Z","author_association":"CONTRIBUTOR","body":"OK I reviewed these changes with Simon.  We decided we don't need to add a separate \"kill switch\" for this because you can just set max_merge_count higher to avoid throttling.  But we also decided not to document this new setting on the index-modules-merges docs: it's a very advanced setting, and playing with it could easily mess up merges.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/150040672","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-150040672","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":150040672,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MDA0MDY3Mg==","user":{"login":"l15k4","id":518855,"node_id":"MDQ6VXNlcjUxODg1NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/518855?v=4","gravatar_id":"","url":"https://api.github.com/users/l15k4","html_url":"https://github.com/l15k4","followers_url":"https://api.github.com/users/l15k4/followers","following_url":"https://api.github.com/users/l15k4/following{/other_user}","gists_url":"https://api.github.com/users/l15k4/gists{/gist_id}","starred_url":"https://api.github.com/users/l15k4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l15k4/subscriptions","organizations_url":"https://api.github.com/users/l15k4/orgs","repos_url":"https://api.github.com/users/l15k4/repos","events_url":"https://api.github.com/users/l15k4/events{/privacy}","received_events_url":"https://api.github.com/users/l15k4/received_events","type":"User","site_admin":false},"created_at":"2015-10-21T22:18:26Z","updated_at":"2015-10-21T22:18:55Z","author_association":"NONE","body":"Guys I don't think this works as expected, I'm getting : \n\n```\nnow throttling indexing: numMergesInFlight=4, maxNumMerges=3\nstop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n```\n\n**5 times a second** right at the beginning of bulk indexing. I'm disabling throttling and refresh interval, I start with `optimized` index, waiting until segment merging finishes, but segment merging is still falling behind... Why is the indexing throttling starting and stopping so frequently?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/150650731","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-150650731","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":150650731,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MDY1MDczMQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-10-23T18:08:22Z","updated_at":"2015-10-23T18:08:22Z","author_association":"CONTRIBUTOR","body":"@l15k4 your merges are not keeping up.  See https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-merge.html#scheduling\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/150711740","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-150711740","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":150711740,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MDcxMTc0MA==","user":{"login":"l15k4","id":518855,"node_id":"MDQ6VXNlcjUxODg1NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/518855?v=4","gravatar_id":"","url":"https://api.github.com/users/l15k4","html_url":"https://github.com/l15k4","followers_url":"https://api.github.com/users/l15k4/followers","following_url":"https://api.github.com/users/l15k4/following{/other_user}","gists_url":"https://api.github.com/users/l15k4/gists{/gist_id}","starred_url":"https://api.github.com/users/l15k4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l15k4/subscriptions","organizations_url":"https://api.github.com/users/l15k4/orgs","repos_url":"https://api.github.com/users/l15k4/repos","events_url":"https://api.github.com/users/l15k4/events{/privacy}","received_events_url":"https://api.github.com/users/l15k4/received_events","type":"User","site_admin":false},"created_at":"2015-10-23T23:00:10Z","updated_at":"2015-10-23T23:01:03Z","author_association":"NONE","body":"@clintongormley but they are not not keeping up right at the moment of starting indexing intto a a small (1M records) optimized index... increasing merge thread pool doesn't help...\n\nI have 4 `ec2.xlarge` instances clustered with 1B records in 30 indices (5 shards each). And if I create a new index and start bulk index into it then throttling happens right away. All fields are doc_values and  I think that it happened right I after I reindexed everything to doc_values around 1.6.0 .... I cannot shake it off since then... I tried everything...\n\nImho I need to scale it up just because of segment merging, but there will be plenty of unused resources ...\n\nI'm trying to solve this issue for months now...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/150783880","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-150783880","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":150783880,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MDc4Mzg4MA==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2015-10-24T10:48:39Z","updated_at":"2015-10-24T10:48:39Z","author_association":"CONTRIBUTOR","body":"@l15k4 did you disable store IO throttling (defaults to 20 MB/sec, which is too low for heavy indexing cases).\n\nWhere are you storing the shards (what IO devices), EBS or local instance storage?\n\nAlso try the ideas here: https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/150926771","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-150926771","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":150926771,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MDkyNjc3MQ==","user":{"login":"l15k4","id":518855,"node_id":"MDQ6VXNlcjUxODg1NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/518855?v=4","gravatar_id":"","url":"https://api.github.com/users/l15k4","html_url":"https://github.com/l15k4","followers_url":"https://api.github.com/users/l15k4/followers","following_url":"https://api.github.com/users/l15k4/following{/other_user}","gists_url":"https://api.github.com/users/l15k4/gists{/gist_id}","starred_url":"https://api.github.com/users/l15k4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l15k4/subscriptions","organizations_url":"https://api.github.com/users/l15k4/orgs","repos_url":"https://api.github.com/users/l15k4/repos","events_url":"https://api.github.com/users/l15k4/events{/privacy}","received_events_url":"https://api.github.com/users/l15k4/received_events","type":"User","site_admin":false},"created_at":"2015-10-25T14:12:10Z","updated_at":"2015-10-25T20:28:47Z","author_association":"NONE","body":"@mikemccand I set it up to 30, 40, 80, 100 MB/s ... it had no effect. I also tried to set `index.merge.scheduler.max_thread_count: 6` but it lead to throttling `now throttling indexing: numMergesInFlight=9` so it didn't help either... \n\nWe use EBS (General Purpose (SSD)) on `c4.xlarge` instances. 2 volumes, one for system and one dedicated for ES...\n\nIt seems that if you are doing bulk indexing and have all fields `doc_values` then you need either quad core machine or physically attached SSD or both, otherwise segment merging will always fall behind no matter what optimizations one does...\n\nIt always looks this way, it is throttling for some period of time like 15-20 minutes and then it stops  http://i.imgur.com/UyDTlHi.png \n\nI also tried to shrink `index` and `bulk` threadpools for segment merging to keep up with bulk indexing, but it didn't help either ... it doesn't keep up right when the first few bulk index requests come...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/150965755","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-150965755","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":150965755,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MDk2NTc1NQ==","user":{"login":"l15k4","id":518855,"node_id":"MDQ6VXNlcjUxODg1NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/518855?v=4","gravatar_id":"","url":"https://api.github.com/users/l15k4","html_url":"https://github.com/l15k4","followers_url":"https://api.github.com/users/l15k4/followers","following_url":"https://api.github.com/users/l15k4/following{/other_user}","gists_url":"https://api.github.com/users/l15k4/gists{/gist_id}","starred_url":"https://api.github.com/users/l15k4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l15k4/subscriptions","organizations_url":"https://api.github.com/users/l15k4/orgs","repos_url":"https://api.github.com/users/l15k4/repos","events_url":"https://api.github.com/users/l15k4/events{/privacy}","received_events_url":"https://api.github.com/users/l15k4/received_events","type":"User","site_admin":false},"created_at":"2015-10-25T20:23:41Z","updated_at":"2015-10-25T21:01:47Z","author_association":"NONE","body":"The best bulk indexing performance I can get on a machine with 4 hyper threads and EBS (750 Mbps) with all fields being doc_values is by increasing `index.merge.scheduler.max_thread_count` to 6 and decreasing `threadpool.bulk.size: 2`, this way it is throttling `now throttling indexing` like every 6th second but it is still throttling so the throughput is now http://i.imgur.com/wXCNZh7.png\n\nI think that after doc_values people don't have much of a choice, they'll need physically attached SSD...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/150995627","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-150995627","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":150995627,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MDk5NTYyNw==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2015-10-26T01:07:06Z","updated_at":"2015-10-26T01:07:06Z","author_association":"CONTRIBUTOR","body":"Hmm enabling doc values is typically a minor indexing performance hit in my experience, e.g. see the nightly benchmarks at https://benchmarks.elastic.co (annotation R on the first chart).\n\nDo you have provisioned IOPs for your EBS mounts?  Are you sure you're not running into that limit?\n\nCan you try the local instance SSD, just for comparison?  Your EBS is backed by SSD as well, so this would let us remove EBS from the equation.  (You'd need to switch to an i2.4xlarge instance for this test).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/151123698","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-151123698","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":151123698,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MTEyMzY5OA==","user":{"login":"l15k4","id":518855,"node_id":"MDQ6VXNlcjUxODg1NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/518855?v=4","gravatar_id":"","url":"https://api.github.com/users/l15k4","html_url":"https://github.com/l15k4","followers_url":"https://api.github.com/users/l15k4/followers","following_url":"https://api.github.com/users/l15k4/following{/other_user}","gists_url":"https://api.github.com/users/l15k4/gists{/gist_id}","starred_url":"https://api.github.com/users/l15k4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l15k4/subscriptions","organizations_url":"https://api.github.com/users/l15k4/orgs","repos_url":"https://api.github.com/users/l15k4/repos","events_url":"https://api.github.com/users/l15k4/events{/privacy}","received_events_url":"https://api.github.com/users/l15k4/received_events","type":"User","site_admin":false},"created_at":"2015-10-26T12:53:25Z","updated_at":"2015-10-26T12:53:25Z","author_association":"NONE","body":"General Purpose unfortunately,  the price of IO Provisioned SSDs surprised us. If you want to go beyond 160 MiB/s to 320 MiB/s it costs double than the volume itself.\n\nI guess it wouldn't throttle with IO Provisioned SSD with 9000 IOPS to reach those 320MiB/s ... but these machines cost fortune :-)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/151285903","html_url":"https://github.com/elastic/elasticsearch/issues/6066#issuecomment-151285903","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6066","id":151285903,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MTI4NTkwMw==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2015-10-26T21:10:50Z","updated_at":"2015-10-26T21:10:50Z","author_association":"CONTRIBUTOR","body":"> I guess it wouldn't throttle with IO Provisioned SSD with 9000 IOPS to reach those 320MiB/s ... but these machines cost fortune :-)\n\nOr just use the local instance attached SSDs on the i2.\\* instance types ...\n","performed_via_github_app":null}]