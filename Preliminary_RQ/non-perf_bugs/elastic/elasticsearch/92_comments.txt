[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172010","html_url":"https://github.com/elastic/elasticsearch/issues/92#issuecomment-172010","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/92","id":172010,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjAxMA==","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2010-03-29T10:01:42Z","updated_at":"2010-03-29T10:01:42Z","author_association":"MEMBER","body":"Implemented.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/178521","html_url":"https://github.com/elastic/elasticsearch/issues/92#issuecomment-178521","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/92","id":178521,"node_id":"MDEyOklzc3VlQ29tbWVudDE3ODUyMQ==","user":{"login":"lukas-vlcek","id":205174,"node_id":"MDQ6VXNlcjIwNTE3NA==","avatar_url":"https://avatars2.githubusercontent.com/u/205174?v=4","gravatar_id":"","url":"https://api.github.com/users/lukas-vlcek","html_url":"https://github.com/lukas-vlcek","followers_url":"https://api.github.com/users/lukas-vlcek/followers","following_url":"https://api.github.com/users/lukas-vlcek/following{/other_user}","gists_url":"https://api.github.com/users/lukas-vlcek/gists{/gist_id}","starred_url":"https://api.github.com/users/lukas-vlcek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lukas-vlcek/subscriptions","organizations_url":"https://api.github.com/users/lukas-vlcek/orgs","repos_url":"https://api.github.com/users/lukas-vlcek/repos","events_url":"https://api.github.com/users/lukas-vlcek/events{/privacy}","received_events_url":"https://api.github.com/users/lukas-vlcek/received_events","type":"User","site_admin":false},"created_at":"2010-04-05T21:55:58Z","updated_at":"2010-04-05T21:56:47Z","author_association":"CONTRIBUTOR","body":"Not an Tika expert but it seems that Tika somehow supports for documents having nested documents (as of writing this is used when extracting content from archive files: zip, tar, ... etc). This could be also customized and used in other use cases (like parsing large mbox files, see http://markmail.org/message/h47lnpxtmdskmest ). Does ES integration take account on this? Note that in case of extracting data from archives individual documents are separated by DIV tags having specific class only. Looking at current ES implementation it seems that all nested documents are simply merged into one output document (parsedContent = tika().parseToString(new FastByteArrayInputStream(content), metadata)). Is there any way how this can be customized?\nWhat I would love to see is an option to extract data from archive first, split into individual documents and then parse individual documents in parallel.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/178587","html_url":"https://github.com/elastic/elasticsearch/issues/92#issuecomment-178587","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/92","id":178587,"node_id":"MDEyOklzc3VlQ29tbWVudDE3ODU4Nw==","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2010-04-05T23:20:32Z","updated_at":"2010-04-05T23:20:32Z","author_association":"MEMBER","body":"Yea, archives are not really meant to be supported currently. This is for the simple reaons that archives are usually very large and it does not make sense to send them in a single HTTP request.\n\nOne option is to do the parsing on the client side, and feed elasticsearch with the documents. Another option is for the plugin to expose a streaming endpoint, that will parse and generate several documents out of the compound stream.\n","performed_via_github_app":null}]