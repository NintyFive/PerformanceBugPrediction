{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/19426","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19426/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19426/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19426/events","html_url":"https://github.com/elastic/elasticsearch/issues/19426","id":165451481,"node_id":"MDU6SXNzdWUxNjU0NTE0ODE=","number":19426,"title":"Each Concurrent shards batch takes more than an hour to get allocated after the node left","user":{"login":"shashank-moengage","id":12726639,"node_id":"MDQ6VXNlcjEyNzI2NjM5","avatar_url":"https://avatars3.githubusercontent.com/u/12726639?v=4","gravatar_id":"","url":"https://api.github.com/users/shashank-moengage","html_url":"https://github.com/shashank-moengage","followers_url":"https://api.github.com/users/shashank-moengage/followers","following_url":"https://api.github.com/users/shashank-moengage/following{/other_user}","gists_url":"https://api.github.com/users/shashank-moengage/gists{/gist_id}","starred_url":"https://api.github.com/users/shashank-moengage/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shashank-moengage/subscriptions","organizations_url":"https://api.github.com/users/shashank-moengage/orgs","repos_url":"https://api.github.com/users/shashank-moengage/repos","events_url":"https://api.github.com/users/shashank-moengage/events{/privacy}","received_events_url":"https://api.github.com/users/shashank-moengage/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2016-07-14T00:30:20Z","updated_at":"2016-07-14T01:45:04Z","closed_at":"2016-07-14T01:45:04Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version**: \"version\" : {\n    \"number\" : \"1.4.1\",\n    \"build_hash\" : \"89d3241d670db65f994242c8e8383b169779e2d4\",\n    \"build_timestamp\" : \"2014-11-26T15:49:29Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.2\"\n  },\n\n**JVM version**:java version \"1.8.0_40\"\nJava(TM) SE Runtime Environment (build 1.8.0_40-b25)\nJava HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)\n\n**OS version**: Linux Ubuntu\n\n**Description of the problem including expected versus actual behavior**:\nWhen the node leaves the cluster, a lot of shards are stuck at initializing state.\n\n**Steps to reproduce**:\n1. Disable allocation\n2. Stop elasticsearch\n3. enable allocation\n\n**Provide logs (if relevant)**:\nHere is the response of the _cat/pending_tasks  | head\n 01439777 56.5m URGENT shard-started ([error-newsflickss][0], node[xirsEXbZSpqldjylVrwzjw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[30GB_1TB_ComputeNodeNew13][4DtklsTUSd-eRhPG_c3uyw][ip-172-31-37-168][inet[/172.31.37.168:9300]]{master=false}]]  \n1439778 56.5m URGENT shard-started ([firstcrytest-notificationclickedmoe][0], node[xirsEXbZSpqldjylVrwzjw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[ES_r3_xlarge_1TB_Node_new15][sbwLxGR6RpmFHmHv3Vc6ag][ip-172-31-47-74][inet[/172.31.47.74:9300]]{master=false}]]  \n1439779 56.5m URGENT shard-started ([cleartrip-eamgc][0], node[Nm_bLbaGQWC0u0ofkIjWIw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[ES_r3_xlarge_1TB_Node_2][xirsEXbZSpqldjylVrwzjw][ip-172-31-41-73][inet[/172.31.41.73:9300]]{master=false}]]  \n1439783 56.5m URGENT shard-started ([emt-uat-fundtransfer][0], node[Nm_bLbaGQWC0u0ofkIjWIw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[30GB_1TB_ComputeNode11][jpNMJUlwQ7aQKk-hlcWLVQ][ip-172-31-40-207][inet[/172.31.40.207:9300]]{master=false}]]  \n1439822 56.4m URGENT shard-started ([firstcrytest-notificationclickedmoe][0], node[xirsEXbZSpqldjylVrwzjw], [R], s[INITIALIZING]), reason [master [ESMasterNode3][8E9mg0rZSHKITLFvvDDT2g][ip-172-31-46-130][inet[/172.31.46.130:9300]]{data=false, master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]  \n1439780 56.5m URGENT shard-started ([chillr-requestshowqr][0], node[xirsEXbZSpqldjylVrwzjw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[30GB_1TB_ComputeNode11][jpNMJUlwQ7aQKk-hlcWLVQ][ip-172-31-40-207][inet[/172.31.40.207:9300]]{master=false}]]  \n1439799 56.4m URGENT shard-started ([sdsellerzone-catalogpdpback][0], node[Nm_bLbaGQWC0u0ofkIjWIw], [R], s[INITIALIZING]), reason [master [ESMasterNode3][8E9mg0rZSHKITLFvvDDT2g][ip-172-31-46-130][inet[/172.31.46.130:9300]]{data=false, master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]  \n1439833 56.4m URGENT reroute_after_cluster_update_settings  \n1439824 56.4m URGENT shard-started ([cleartripprod-fbtan][0], node[xirsEXbZSpqldjylVrwzjw], [R], s[INITIALIZING]), reason [master [ESMasterNode3][8E9mg0rZSHKITLFvvDDT2g][ip-172-31-46-130][inet[/172.31.46.130:9300]]{data=false, master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]  \n1439288  5.9h HIGH   refresh-mapping [cleartripprod-apppl-2016-06-30][[datapoints]]\n\n_cat/health\nepoch      timestamp cluster           status node.total node.data shards   pri relo init unassign \n1468455843 00:24:03  DataPointsCluster yellow         22        19  26619 14017    0   20     1395 \n\n_cluster/settings\n{\"persistent\":{\"cluster\":{\"routing\":{\"allocation\":{\"cluster_concurrent_rebalance\":\"10\",\"node_concurrent_recoveries\":\"14\",\"node_initial_primaries_recoveries\":\"4\",\"enable\":\"all\"}}},\"threadpool\":{\"bulk\":{\"keep_alive\":\"2m\",\"size\":\"16\",\"queue_size\":\"2000\",\"type\":\"fixed\"}},\"indices\":{\"recovery\":{\"concurrent_streams\":\"6\",\"max_bytes_per_sec\":\"120mb\"}}},\"transient\":{\"cluster\":{\"routing\":{\"allocation\":{\"node_initial_primaries_recoveries\":\"10\",\"balance\":{\"index\":\"0.80f\"},\"enable\":\"all\",\"allow_rebalance\":\"indices_all_active\",\"cluster_concurrent_rebalance\":\"0\",\"node_concurrent_recoveries\":\"5\",\"exclude\":{\"_ip\":\"172.31.39.58\"}}}},\"indices\":{\"recovery\":{\"concurrent_streams\":\"10\"}}}}\n\nIs this because of too many shards?\n","closed_by":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"performed_via_github_app":null}