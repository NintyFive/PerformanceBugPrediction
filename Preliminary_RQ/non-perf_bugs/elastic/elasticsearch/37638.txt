{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/37638","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/37638/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/37638/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/37638/events","html_url":"https://github.com/elastic/elasticsearch/issues/37638","id":401096278,"node_id":"MDU6SXNzdWU0MDEwOTYyNzg=","number":37638,"title":"uneven shard allocation when adding new nodes","user":{"login":"jgq2008303393","id":657140,"node_id":"MDQ6VXNlcjY1NzE0MA==","avatar_url":"https://avatars0.githubusercontent.com/u/657140?v=4","gravatar_id":"","url":"https://api.github.com/users/jgq2008303393","html_url":"https://github.com/jgq2008303393","followers_url":"https://api.github.com/users/jgq2008303393/followers","following_url":"https://api.github.com/users/jgq2008303393/following{/other_user}","gists_url":"https://api.github.com/users/jgq2008303393/gists{/gist_id}","starred_url":"https://api.github.com/users/jgq2008303393/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jgq2008303393/subscriptions","organizations_url":"https://api.github.com/users/jgq2008303393/orgs","repos_url":"https://api.github.com/users/jgq2008303393/repos","events_url":"https://api.github.com/users/jgq2008303393/events{/privacy}","received_events_url":"https://api.github.com/users/jgq2008303393/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2019-01-20T13:08:14Z","updated_at":"2019-01-30T09:04:33Z","closed_at":"2019-01-30T09:04:33Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version** (`bin/elasticsearch --version`): ES 6.4.3\r\n\r\n**JVM version** (`java -version`): 1.8.0_181\r\n\r\n**OS version** (`uname -a` if on a Unix-like system): Ubuntu 16.04.5 LTS\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\nFor clusters with insufficient disk space, when doing scale-out operation, we often encounter the following problem: After we added 5 new nodes to the 20-node cluster,  almost all the new shards created the next day were allocated to the 5 new nodes. As the 5 new nodes were not able to cover the bulk pressure, the scale-out operation cause a lot of bulk rejections.\r\n\r\nWe have encountered this problem several times, and we used to set index.routing.allocation.total_shards_per_node to control this situation. Although it works, but is very troublesome to operate when there are many clusters. \r\n\r\nWe have traced the process of create-index, and found that the shard allocation is mainly decided by \r\nthe combination of node-level and index-level balanceï¼š\r\n```\r\n        private float weight(Balancer balancer, ModelNode node, String index, int numAdditionalShards) {\r\n            final float weightShard = node.numShards() + numAdditionalShards - balancer.avgShardsPerNode();\r\n            final float weightIndex = node.numShards(index) + numAdditionalShards - balancer.avgShardsPerNode(index);\r\n            return theta0 * weightShard + theta1 * weightIndex;\r\n        }\r\n```\r\nThis policy is good for rebalance operation, but bad for new shard allocation. For example, when old nodes has 100 shards and new nodes has 10 shards, the new shards are tend to place on the new nodes \r\n which have smaller weight.\r\n\r\n**Solution**:\r\nFor new shard allocation, we think it's better to just consider index-level balance, so that the new shards are evenly distributed across all nodes. And the rebalance operation will do node-level balance asynchronously.","closed_by":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"performed_via_github_app":null}