{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/44395","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/44395/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/44395/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/44395/events","html_url":"https://github.com/elastic/elasticsearch/issues/44395","id":468411612,"node_id":"MDU6SXNzdWU0Njg0MTE2MTI=","number":44395,"title":"springboot with es highlevel client do async bulk insert data error","user":{"login":"godlockin","id":31117750,"node_id":"MDQ6VXNlcjMxMTE3NzUw","avatar_url":"https://avatars1.githubusercontent.com/u/31117750?v=4","gravatar_id":"","url":"https://api.github.com/users/godlockin","html_url":"https://github.com/godlockin","followers_url":"https://api.github.com/users/godlockin/followers","following_url":"https://api.github.com/users/godlockin/following{/other_user}","gists_url":"https://api.github.com/users/godlockin/gists{/gist_id}","starred_url":"https://api.github.com/users/godlockin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/godlockin/subscriptions","organizations_url":"https://api.github.com/users/godlockin/orgs","repos_url":"https://api.github.com/users/godlockin/repos","events_url":"https://api.github.com/users/godlockin/events{/privacy}","received_events_url":"https://api.github.com/users/godlockin/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-07-16T02:12:37Z","updated_at":"2019-07-16T02:51:40Z","closed_at":"2019-07-16T02:51:40Z","author_association":"NONE","active_lock_reason":null,"body":"env:\r\nCentOS7, Java 8, SpringBoot, Maven, ES highlevel jar\r\nElasticsearch 6.7 which was running in docker on OSS service\r\n\r\nbackground:\r\nthis is a data handler node which need to do some scheduled data sync jobs, which i will load some data from mysql and do some data handlings then bulk insert into target index in ES\r\n\r\nissue:\r\nI was build a single singleton es client with the application starting\r\n`try {\r\n            String template = \"%s:%s\";\r\n            HttpHost[] httpHosts = Arrays.stream(ES_ADDRESSES.split(\",\")).parallel().map(x -> {\r\n                esHttpAddress.add(String.format(template, x, ES_HTTP_PORT));\r\n                return new HttpHost(x, ES_HTTP_PORT, \"http\");\r\n            }).toArray(HttpHost[]::new);\r\n\r\n            RestClientBuilder builder = RestClient.builder(httpHosts)\r\n                    .setRequestConfigCallback((RequestConfig.Builder requestConfigBuilder) ->\r\n                            requestConfigBuilder.setConnectTimeout(ES_CONNECT_TIMEOUT)\r\n                                    .setSocketTimeout(ES_SOCKET_TIMEOUT)\r\n                                    .setConnectionRequestTimeout(ES_CONNECTION_REQUEST_TIMEOUT))\r\n                    .setMaxRetryTimeoutMillis(ES_MAX_RETRY_TINEOUT_MILLIS);\r\n\r\n            restHighLevelClient = new RestHighLevelClient(builder);\r\n\r\n            bulkProcessor = BulkProcessor.builder((request, bulkListener) -> \r\n                            restHighLevelClient.bulkAsync(request, COMMON_OPTIONS, bulkListener),\r\n                        getBPListener())\r\n                    .setBulkActions(ES_BULK_FLUSH)\r\n                    .setBulkSize(new ByteSizeValue(ES_BULK_SIZE, ByteSizeUnit.MB))\r\n                    .setFlushInterval(TimeValue.timeValueSeconds(10L))\r\n                    .setConcurrentRequests(ES_BULK_CONCURRENT)\r\n                    .setBackoffPolicy(BackoffPolicy.constantBackoff(TimeValue.timeValueSeconds(1L), 3))\r\n                    .build();\r\n        } catch (Exception e) {\r\n            e.printStackTrace();\r\n            String errMsg = \"Error happened when we init ES transport client\" + e;\r\n            log.error(errMsg);\r\n            throw new MainServiceException(ResultEnum.ES_CLIENT_INIT);\r\n        }`\r\n\r\nand then, every data saving operations will be done by \r\n`bulkProcessor.add((DocWriteRequest) request);`\r\n\r\nbut when i start the data sync jobs, `java.lang.IllegalStateException` happens which announced that the exception happened for I/O STOPPED\r\n\r\ndoes it means that the bulkProcessor or restHighLevelClient has an up-to limit on requests, and it will close the connection when the requests' number is too much here? ","closed_by":{"login":"bizybot","id":902768,"node_id":"MDQ6VXNlcjkwMjc2OA==","avatar_url":"https://avatars2.githubusercontent.com/u/902768?v=4","gravatar_id":"","url":"https://api.github.com/users/bizybot","html_url":"https://github.com/bizybot","followers_url":"https://api.github.com/users/bizybot/followers","following_url":"https://api.github.com/users/bizybot/following{/other_user}","gists_url":"https://api.github.com/users/bizybot/gists{/gist_id}","starred_url":"https://api.github.com/users/bizybot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bizybot/subscriptions","organizations_url":"https://api.github.com/users/bizybot/orgs","repos_url":"https://api.github.com/users/bizybot/repos","events_url":"https://api.github.com/users/bizybot/events{/privacy}","received_events_url":"https://api.github.com/users/bizybot/received_events","type":"User","site_admin":false},"performed_via_github_app":null}