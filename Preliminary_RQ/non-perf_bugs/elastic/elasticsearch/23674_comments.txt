[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/289092956","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-289092956","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":289092956,"node_id":"MDEyOklzc3VlQ29tbWVudDI4OTA5Mjk1Ng==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2017-03-24T17:41:06Z","updated_at":"2017-03-24T17:41:06Z","author_association":"CONTRIBUTOR","body":"### POC results showing the need for sampling/duplicate text removal\r\n\r\nI've got a proof of concept running on the [Signal news media dataset](http://research.signalmedia.co/newsir16/signal-dataset.html) (Creative Commons 3.0 license, 1m news articles). \r\n\r\nIn the POC I can use significant terms without accessing fielddata - I re-analyze content of the top-matching docs to gather term stats. The results are a good example of the challenges of free-text analytics. Consider this search for news articles mentioning \"elasticsearch\":\r\n<img width=\"901\" alt=\"console_-_kibana\" src=\"https://cloud.githubusercontent.com/assets/170925/24305719/e441cf92-10b5-11e7-9066-a3b8967310f0.png\">\r\n\r\nI have to use a `sampler` agg to avoid re-tokenizing too many docs. The _diversified_ sampler is used to try and eliminate copies of the same press-release based on a hash of the doc title. However this exact-match de-duping is inadequate. Consider this \"significant\" term in the results: `currensee`. \r\nIf we drill-down with the highlighter we can see why \"currensee\" is statistically significant and it is down to near-duplicate docs:\r\n<img width=\"1273\" alt=\"console_-_kibana\" src=\"https://cloud.githubusercontent.com/assets/170925/24306002/d6e17914-10b6-11e7-9b8b-b3ae6cbbfdfa.png\">\r\n\r\nThis is a very typical scenario working with free-text where the same information is often copied and remixed and re-shared. Think press releases, news quotes from individuals, copyright notices/boilerplate, email reply chains, retweets etc. The DeDuplicatingTokenFilter first submitted [in this PR](https://github.com/elastic/elasticsearch/pull/6796/files#diff-62dba54dc0ed058b49f232634d94f9cb) is an effective means of trimming this sort of noise (at least on one shard). \r\n\r\nWe should consider whether near-duplicate removal is a necessary part of a first release of a `significant_text` agg. I have made the mistake previously of trying to lump too much functionality into a single PR.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/289502405","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-289502405","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":289502405,"node_id":"MDEyOklzc3VlQ29tbWVudDI4OTUwMjQwNQ==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2017-03-27T16:11:00Z","updated_at":"2017-03-27T16:11:00Z","author_association":"CONTRIBUTOR","body":"Some comments for future dev following an internal discussion:\r\n\r\n* Implement this as a new `significant_text` agg rather than adding to existing `significant_terms`\r\nWill allow for custom impl with custom settings.\r\n* The new agg impl should bypass the layers in the agg class hierarchy relating to aggs with a ValueSource. These classes were designed for structured data sources not processing TokenStreams of unstructured content. If necessary we can refactor later if we think other aggs would benefit from working on TokenStreams\r\n* First priority is an agg that works without requiring FieldData\r\n* Unravelling nested docs from stored JSON given a matching Lucene docID may prove challenging so we propose not to support nested docs initially.\r\n* Explain API may be required: advanced features like phrase-detection may be useful but rely on rationalization of suggestions e.g. given the candidate phrases \"platform for social trading\", \"social trading\" and \"social trading platform\" we might decide 80% of \"social trading\" uses were as part of the longer phrase \"social trading platform\" so we'd drop the \"social trading\" in favour of suggesting the longer \"social trading platform\". Conversely, the long phrase \"united states of america\" might be dropped because it is much rarer than the popular, terser form of \"united states\". These stats-driven choices may need to be explained in a special API\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/306922690","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-306922690","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":306922690,"node_id":"MDEyOklzc3VlQ29tbWVudDMwNjkyMjY5MA==","user":{"login":"pkphlam","id":573309,"node_id":"MDQ6VXNlcjU3MzMwOQ==","avatar_url":"https://avatars1.githubusercontent.com/u/573309?v=4","gravatar_id":"","url":"https://api.github.com/users/pkphlam","html_url":"https://github.com/pkphlam","followers_url":"https://api.github.com/users/pkphlam/followers","following_url":"https://api.github.com/users/pkphlam/following{/other_user}","gists_url":"https://api.github.com/users/pkphlam/gists{/gist_id}","starred_url":"https://api.github.com/users/pkphlam/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pkphlam/subscriptions","organizations_url":"https://api.github.com/users/pkphlam/orgs","repos_url":"https://api.github.com/users/pkphlam/repos","events_url":"https://api.github.com/users/pkphlam/events{/privacy}","received_events_url":"https://api.github.com/users/pkphlam/received_events","type":"User","site_admin":false},"created_at":"2017-06-07T20:57:42Z","updated_at":"2017-06-07T20:57:42Z","author_association":"NONE","body":"@markharwood Apologies for possibly hijacking a closed thread, but I just read the blog post on this feature and it is really interesting because it is quite similar to something that we have been trying to hack with Elasticsearch. The destemming for 2 is definitely a big issue for us. \r\n\r\nI wanted to flag two other potential issues/features that we have been thinking about that would possibly be good additions to the list you already have for future implementations.\r\n\r\n1. Using only terms within a certain window from query string matches. Essentially, if our query is a query string match, we've been thinking about ways to only consider terms that occur \"near\" a matched query term, where \"near\" can be user-defined. Think about really really long documents such as memos or patents, where there are many different sections, some of which are irrelevant. You get at this a little with the filtering of duplicate text, but there are times where the \"noise\" is not necessarily duplicate text. We would consider terms that are near one of our query terms to be more important contextually than terms that are very far away from our query terms, so some way to either de-emphasize or filter out terms that do not occur within a selected window of our query terms would be helpful. Think of it as filtering our document down to \"snippets\" and then drawing terms only from the snippets. Since you leverage the position information for each term, I would assume that this is possible.\r\n\r\n2. Exposing term frequencies as a possible parameter for scripted scoring in addition to the document frequencies.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/307026553","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-307026553","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":307026553,"node_id":"MDEyOklzc3VlQ29tbWVudDMwNzAyNjU1Mw==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2017-06-08T07:47:32Z","updated_at":"2017-06-08T07:47:32Z","author_association":"CONTRIBUTOR","body":"Hi and thanks for the input!\r\n\r\n> Using only terms within a certain window from query string matches.\r\n\r\nWe could reuse the highlighters to help isolate the interesting sections of docs.\r\n\r\n>Exposing term frequencies as a possible parameter\r\n\r\nI have an inherent distrust of individual documents - I find a more balanced view of a topic comes from looking at stats across many example docs. If a term is repeated legitimately (not a one-off webpage with spammy example of keyword stuffing) then this increased TF manifests itself from a higher percentage of docs likely to contain the term. \r\nThe same argument might apply to only looking at \"interesting\" sections of pages. If pages consist of `on-topic` and `neutral` sections then we do not need to trim the neutral sections using a highlighter to identify keywords - the significance algo has the ability to separate the signal from the noise given a healthy sample of docs.\r\n\r\nDo I detect from these suggestions that you have scenarios where the \"foreground sample\" being analyzed is perhaps a result set of only one document? When there are low-doc-count examples one approach I've used before is to internally consider a large doc as multiple fake docs segmented on sentence or n-words boundaries. You can then use the same significance heuristic algos which test percentage of foreground/background docs containing a term - you just happen to lie to it about what a \"doc\" is in the foreground set.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/307031611","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-307031611","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":307031611,"node_id":"MDEyOklzc3VlQ29tbWVudDMwNzAzMTYxMQ==","user":{"login":"pkphlam","id":573309,"node_id":"MDQ6VXNlcjU3MzMwOQ==","avatar_url":"https://avatars1.githubusercontent.com/u/573309?v=4","gravatar_id":"","url":"https://api.github.com/users/pkphlam","html_url":"https://github.com/pkphlam","followers_url":"https://api.github.com/users/pkphlam/followers","following_url":"https://api.github.com/users/pkphlam/following{/other_user}","gists_url":"https://api.github.com/users/pkphlam/gists{/gist_id}","starred_url":"https://api.github.com/users/pkphlam/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pkphlam/subscriptions","organizations_url":"https://api.github.com/users/pkphlam/orgs","repos_url":"https://api.github.com/users/pkphlam/repos","events_url":"https://api.github.com/users/pkphlam/events{/privacy}","received_events_url":"https://api.github.com/users/pkphlam/received_events","type":"User","site_admin":false},"created_at":"2017-06-08T08:10:47Z","updated_at":"2017-06-08T08:10:47Z","author_association":"NONE","body":"Thanks for the response! \r\n\r\nWe don't really use foreground sets of one document either. For the term frequency suggestion, it is less thought out, but we were thinking about ways to create new scoring metrics and one possible suggestion was comparing foreground and background TF-IDF type metrics in some way and it seemed interesting that TF was not exposed given that it is core to some of the other ES functions. But again, we were just starting to think about that.\r\n\r\nOn the suggestion for highlighters/windows, we actually currently do a version of your multiple fake docs example actually. We pre-segment documents before ingest and then we store each segment as a child in a parent-child setup where the main document metadata and text fields are indexed as the parent but the tokens are indexed as children, with each child document being one segment. We then run significant terms on the children doc segments that match the query. As you can imagine, while this sort of works, it is inflexible in that the segments are pre-set and thus not able to be tuned specifically to an even-spaced window around the matching terms. Also, matching the segments gets a little tricky because you often have multi-word queries where the full document itself matches the query but no specific child segment fully matches the query.\r\n\r\nThe reason why we try to do this segmenting on longer documents is because we try to differentiate between \"substantively meaningful\" significant terms and \"discriminant but not substantively meaningful\" significant terms. I do think that in many cases, you are correct in that the significance algorithm should be able to separate out interesting signal terms even with on-topic and neutral sections all combined. However, in practice we found that that is not always the case across all different document types. \r\n\r\nFor example, hypothetically, one might imagine a case where you have a large set of academic articles across different disciplines. You start with a query that is a small subfield or topic of one specific discipline. Now imagine that all papers have a \"methodology\" section and different disciplines talk about different methodologies in different ways. But the methodology terms itself are not necessarily that interesting. What is interesting is the substantive argument of the paper and the terms used in the arguments. What might end up happening is that some of the methodology terms end up being on your list (because they separate different disciplines well) and sometimes are even placed ahead of potentially interesting substantive terms depending on how the query was constructed.  However, if you are able to cull the sections so that only the on-topic non-methodology sections are considered (here it's assumed that the interesting substantive parts of a paper have little overlap in terminology with the methodology parts of a paper), then you might get a better result. But in general though, I think in many cases it is often true that more interesting words lie closer to the query matches and so having the ability to isolate sections can increase the precision and quality of terms.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/307034949","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-307034949","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":307034949,"node_id":"MDEyOklzc3VlQ29tbWVudDMwNzAzNDk0OQ==","user":{"login":"pkphlam","id":573309,"node_id":"MDQ6VXNlcjU3MzMwOQ==","avatar_url":"https://avatars1.githubusercontent.com/u/573309?v=4","gravatar_id":"","url":"https://api.github.com/users/pkphlam","html_url":"https://github.com/pkphlam","followers_url":"https://api.github.com/users/pkphlam/followers","following_url":"https://api.github.com/users/pkphlam/following{/other_user}","gists_url":"https://api.github.com/users/pkphlam/gists{/gist_id}","starred_url":"https://api.github.com/users/pkphlam/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pkphlam/subscriptions","organizations_url":"https://api.github.com/users/pkphlam/orgs","repos_url":"https://api.github.com/users/pkphlam/repos","events_url":"https://api.github.com/users/pkphlam/events{/privacy}","received_events_url":"https://api.github.com/users/pkphlam/received_events","type":"User","site_admin":false},"created_at":"2017-06-08T08:24:47Z","updated_at":"2017-06-08T08:25:00Z","author_association":"NONE","body":"Also, I know I mentioned this already, but I wanted to re-emphasize the destemming (2nd) suggestion that you had, both in terms of how much it is a pain point for us and how much our customers desire it. Our product uses significant terms heavily (along with some machine learning) as a keyword recommender and time and again, the number one most impressive feature that our clients have mentioned that we provide is the ability to present words grouped in their stem groups. It seems like such a simple thing to think about, but there is very little out there that provides a stem -> original word(s) presentation.\r\n\r\nIt's also a huge pain point for us because to support such a feature, what we currently have to do is on data ingest, we have to hit the mtermvector API to get the word -> stem mapping, store all the mappings with counts in a separate SQL database, and then call the database every time we suggest words. It adds an incredible amount of ingest time and makes our software stack all the bit more complicated. If there was a way to do on the fly destemming where ALL words in the document set that correspond to a specific stem can be reproduced on the fly, I think it would be really helpful and unique.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/307745978","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-307745978","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":307745978,"node_id":"MDEyOklzc3VlQ29tbWVudDMwNzc0NTk3OA==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2017-06-12T10:04:04Z","updated_at":"2017-06-12T10:04:04Z","author_association":"CONTRIBUTOR","body":"> there is very little out there that provides a stem -> original word(s) presentation.\r\n\r\nThat's another thing highlighter logic can help with. It already identifies the sections of docs that produced stemmed forms and de-stemming is finding the most-popular examples of text associated with a stemmed form.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/364922629","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-364922629","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":364922629,"node_id":"MDEyOklzc3VlQ29tbWVudDM2NDkyMjYyOQ==","user":{"login":"yoav2","id":16187579,"node_id":"MDQ6VXNlcjE2MTg3NTc5","avatar_url":"https://avatars1.githubusercontent.com/u/16187579?v=4","gravatar_id":"","url":"https://api.github.com/users/yoav2","html_url":"https://github.com/yoav2","followers_url":"https://api.github.com/users/yoav2/followers","following_url":"https://api.github.com/users/yoav2/following{/other_user}","gists_url":"https://api.github.com/users/yoav2/gists{/gist_id}","starred_url":"https://api.github.com/users/yoav2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yoav2/subscriptions","organizations_url":"https://api.github.com/users/yoav2/orgs","repos_url":"https://api.github.com/users/yoav2/repos","events_url":"https://api.github.com/users/yoav2/events{/privacy}","received_events_url":"https://api.github.com/users/yoav2/received_events","type":"User","site_admin":false},"created_at":"2018-02-12T13:30:25Z","updated_at":"2018-02-12T13:30:25Z","author_association":"NONE","body":"So currently there is no \"de-stemming\" in significant_text?\r\nAre there any plans to implement it? \r\nAre you suggesting to search for the returned results and take the marked highlights?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/364928191","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-364928191","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":364928191,"node_id":"MDEyOklzc3VlQ29tbWVudDM2NDkyODE5MQ==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2018-02-12T13:52:11Z","updated_at":"2018-02-12T13:52:11Z","author_association":"CONTRIBUTOR","body":">So currently there is no \"de-stemming\" in significant_text?\r\n>Are there any plans to implement it?\r\n\r\nCorrect - no immediate plans to look at this.\r\n\r\n>Are you suggesting to search for the returned results and take the marked highlights?\r\n\r\nCurrently we already retrieve and re-analyze the matching documents' source to find the significant words. Once determined, we could use something like a highlighter to find where the top-scoring terms lie in the source text and what the most popular form is for each stem.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/364931398","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-364931398","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":364931398,"node_id":"MDEyOklzc3VlQ29tbWVudDM2NDkzMTM5OA==","user":{"login":"yoav2","id":16187579,"node_id":"MDQ6VXNlcjE2MTg3NTc5","avatar_url":"https://avatars1.githubusercontent.com/u/16187579?v=4","gravatar_id":"","url":"https://api.github.com/users/yoav2","html_url":"https://github.com/yoav2","followers_url":"https://api.github.com/users/yoav2/followers","following_url":"https://api.github.com/users/yoav2/following{/other_user}","gists_url":"https://api.github.com/users/yoav2/gists{/gist_id}","starred_url":"https://api.github.com/users/yoav2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yoav2/subscriptions","organizations_url":"https://api.github.com/users/yoav2/orgs","repos_url":"https://api.github.com/users/yoav2/repos","events_url":"https://api.github.com/users/yoav2/events{/privacy}","received_events_url":"https://api.github.com/users/yoav2/received_events","type":"User","site_admin":false},"created_at":"2018-02-12T14:04:27Z","updated_at":"2018-02-12T14:04:27Z","author_association":"NONE","body":"Thank you.\r\nOther small qeustion - is there any performance differences between significant_text and significant_terms?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/364935457","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-364935457","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":364935457,"node_id":"MDEyOklzc3VlQ29tbWVudDM2NDkzNTQ1Nw==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2018-02-12T14:19:27Z","updated_at":"2018-02-12T14:23:45Z","author_association":"CONTRIBUTOR","body":"When it comes to working with text, the differences are more around memory cost and results quality rather than performance.\r\n`significant_terms` relies on you having to load _all_ your docs' text fields into RAM using fielddata - this is prohibitively expensive for systems with lots of docs. Any repetition of text (of which there's typically lots in real-world data) will skew the stats and you [start showing oddities](https://youtu.be/zH7bizwjj20?t=4m1s)\r\n\r\n`significant_text` does not rely on fielddata caching all of your docs' text and re-analyzes doc source on the fly meaning it can also remove duplicate sections of text, greatly improving the quality of suggestions.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/498032007","html_url":"https://github.com/elastic/elasticsearch/issues/23674#issuecomment-498032007","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23674","id":498032007,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5ODAzMjAwNw==","user":{"login":"eranhirs","id":3372820,"node_id":"MDQ6VXNlcjMzNzI4MjA=","avatar_url":"https://avatars1.githubusercontent.com/u/3372820?v=4","gravatar_id":"","url":"https://api.github.com/users/eranhirs","html_url":"https://github.com/eranhirs","followers_url":"https://api.github.com/users/eranhirs/followers","following_url":"https://api.github.com/users/eranhirs/following{/other_user}","gists_url":"https://api.github.com/users/eranhirs/gists{/gist_id}","starred_url":"https://api.github.com/users/eranhirs/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eranhirs/subscriptions","organizations_url":"https://api.github.com/users/eranhirs/orgs","repos_url":"https://api.github.com/users/eranhirs/repos","events_url":"https://api.github.com/users/eranhirs/events{/privacy}","received_events_url":"https://api.github.com/users/eranhirs/received_events","type":"User","site_admin":false},"created_at":"2019-06-02T13:35:31Z","updated_at":"2019-06-02T13:35:31Z","author_association":"NONE","body":"@markharwood would love to hear your thoughts regarding significant_text on issue #42780 ","performed_via_github_app":null}]