[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/438345932","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-438345932","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":438345932,"node_id":"MDEyOklzc3VlQ29tbWVudDQzODM0NTkzMg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-11-13T16:51:36Z","updated_at":"2018-11-13T16:51:36Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-core-infra","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/465345205","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-465345205","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":465345205,"node_id":"MDEyOklzc3VlQ29tbWVudDQ2NTM0NTIwNQ==","user":{"login":"gwbrown","id":1522844,"node_id":"MDQ6VXNlcjE1MjI4NDQ=","avatar_url":"https://avatars1.githubusercontent.com/u/1522844?v=4","gravatar_id":"","url":"https://api.github.com/users/gwbrown","html_url":"https://github.com/gwbrown","followers_url":"https://api.github.com/users/gwbrown/followers","following_url":"https://api.github.com/users/gwbrown/following{/other_user}","gists_url":"https://api.github.com/users/gwbrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gwbrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gwbrown/subscriptions","organizations_url":"https://api.github.com/users/gwbrown/orgs","repos_url":"https://api.github.com/users/gwbrown/repos","events_url":"https://api.github.com/users/gwbrown/events{/privacy}","received_events_url":"https://api.github.com/users/gwbrown/received_events","type":"User","site_admin":false},"created_at":"2019-02-19T22:55:12Z","updated_at":"2019-02-19T22:56:48Z","author_association":"CONTRIBUTOR","body":"After lots of trying, (10s of thousands of runs of this test) under various conditions, including on a heavily-loaded system, on 6.x and master, I have not been able to reproduce this failure once. This test isn't muted, and has not failed on CI since this issue was opened 3 months ago. While it has failed a few times on CI within the past year, it has only failed with this particular failure once, and the other times were before some significant Watcher refactorings/bug fixes, so I'm not sure if those failures are relevant.\r\n\r\nI'm very hesitant to make any changes to this test to try to address this failure given that I can't reproduce it, and thus have no way of verifying if any changes actually increased the stability of the test, so I'm wondering what we should do with this issue: Unless someone else has some insight into how we might be able to better reproduce this, I don't think we'll be able to do much for the moment.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/466328473","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-466328473","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":466328473,"node_id":"MDEyOklzc3VlQ29tbWVudDQ2NjMyODQ3Mw==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2019-02-22T09:14:23Z","updated_at":"2019-02-22T09:14:23Z","author_association":"CONTRIBUTOR","body":"[Build-stats](https://build-stats.elastic.co/app/kibana#/discover?_g=(refreshInterval:(pause:!t,value:0),time:(from:now-90d,mode:quick,to:now))&_a=(columns:!(_source),filters:!(),index:e58bf320-7efd-11e8-bf69-63c8ef516157,interval:auto,query:(language:lucene,query:'stacktrace:%22could%20not%20find%20watch%20%5B_name%5D%20to%20trigger%22'),sort:!(time,desc))) link","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/469883895","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-469883895","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":469883895,"node_id":"MDEyOklzc3VlQ29tbWVudDQ2OTg4Mzg5NQ==","user":{"login":"jtibshirani","id":7461306,"node_id":"MDQ6VXNlcjc0NjEzMDY=","avatar_url":"https://avatars3.githubusercontent.com/u/7461306?v=4","gravatar_id":"","url":"https://api.github.com/users/jtibshirani","html_url":"https://github.com/jtibshirani","followers_url":"https://api.github.com/users/jtibshirani/followers","following_url":"https://api.github.com/users/jtibshirani/following{/other_user}","gists_url":"https://api.github.com/users/jtibshirani/gists{/gist_id}","starred_url":"https://api.github.com/users/jtibshirani/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jtibshirani/subscriptions","organizations_url":"https://api.github.com/users/jtibshirani/orgs","repos_url":"https://api.github.com/users/jtibshirani/repos","events_url":"https://api.github.com/users/jtibshirani/events{/privacy}","received_events_url":"https://api.github.com/users/jtibshirani/received_events","type":"User","site_admin":false},"created_at":"2019-03-05T22:28:01Z","updated_at":"2019-03-05T22:28:01Z","author_association":"MEMBER","body":"I dug into these failures while investigating #37882, which seems to have the same root cause as both this issue and #39306.\r\n\r\nFrom looking at logs and stepping through code, I saw the same cause that @jaymode identified above. There is a change in shard routing that causes the watch service to be reloaded, which then clears the list of registered watches from the mock trigger engine. The assertion errors we're seeing can be consistently reproduced if we simulate a change in shard routing, and repeat a test case ~10 times:\r\n\r\n```\r\n--- a/x-pack/plugin/watcher/src/main/java/org/elasticsearch/xpack/watcher/WatcherLifeCycleService.java\r\n+++ b/x-pack/plugin/watcher/src/main/java/org/elasticsearch/xpack/watcher/WatcherLifeCycleService.java\r\n@@ -137,7 +137,7 @@ public class WatcherLifeCycleService implements ClusterStateListener {\r\n             .sorted(Comparator.comparing(ShardRouting::hashCode))\r\n             .collect(Collectors.toList());\r\n \r\n-        if (previousShardRoutings.get().equals(localAffectedShardRoutings) == false) {\r\n+        if (Math.random() < 0.1 || previousShardRoutings.get().equals(localAffectedShardRoutings) == false) {\r\n             if (watcherService.validate(event.state())) {\r\n                 previousShardRoutings.set(localAffectedShardRoutings);\r\n                 if (state.get() == WatcherState.STARTED) {\r\n```\r\n\r\nI don't think that waiting on a green cluster health will improve the situation, since we already wait for the watcher indices to be green in `AbstractWatcherIntegrationTestCase#_setup`. It looks like the reload is caused by a shard routing change that happens after the indices are created and watcher has already started:\r\n\r\n```\r\n  1> [2018-11-13T19:26:33,830][DEBUG][o.e.x.w.WatcherService   ] [node_s2] watch service has been reloaded, reason [starting]\r\n  ...\r\n  1> [2018-11-13T19:26:34,029][INFO ][o.e.x.w.WatcherService   ] [node_s1] reloading watcher, reason [new local watcher shard allocation ids], cancelled [0] queued tasks\r\n```\r\n\r\nFrom looking at the mock trigger engine, I actually don't think it's correct to be clearing the list of watches when watcher is reloaded. I opened a PR that proposes to remove this behavior, which could also help with these test failures: #39724. With that change, I can no longer reproduce the failures when simulating a change in shard routing.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/476827725","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-476827725","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":476827725,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3NjgyNzcyNQ==","user":{"login":"jtibshirani","id":7461306,"node_id":"MDQ6VXNlcjc0NjEzMDY=","avatar_url":"https://avatars3.githubusercontent.com/u/7461306?v=4","gravatar_id":"","url":"https://api.github.com/users/jtibshirani","html_url":"https://github.com/jtibshirani","followers_url":"https://api.github.com/users/jtibshirani/followers","following_url":"https://api.github.com/users/jtibshirani/following{/other_user}","gists_url":"https://api.github.com/users/jtibshirani/gists{/gist_id}","starred_url":"https://api.github.com/users/jtibshirani/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jtibshirani/subscriptions","organizations_url":"https://api.github.com/users/jtibshirani/orgs","repos_url":"https://api.github.com/users/jtibshirani/repos","events_url":"https://api.github.com/users/jtibshirani/events{/privacy}","received_events_url":"https://api.github.com/users/jtibshirani/received_events","type":"User","site_admin":false},"created_at":"2019-03-26T20:09:19Z","updated_at":"2019-03-26T20:09:19Z","author_association":"MEMBER","body":"I'm closing this now that #39724 has merged. Feel free to reopen the issue if similar failures pop up again.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/477150454","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-477150454","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":477150454,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3NzE1MDQ1NA==","user":{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false},"created_at":"2019-03-27T13:20:49Z","updated_at":"2019-03-27T13:25:01Z","author_association":"MEMBER","body":"Previous issues around these tests (but not convinced that they are the same cause or not). #35503 #39306 \r\n\r\nPrevious investigations have brought about changes in this PR #39724 \r\n\r\nPrevious guesses at the cause: reloading causes watches to be cleared (fixed via the PR), internalCluster not shutting down properly.\r\n\r\nFailing builds:\r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/951/consoleFull\r\nReproduction:\r\n```\r\n./gradlew :x-pack:plugin:watcher:unitTest -Dtests.seed=CF3EAFDCCBFC8BED -Dtests.class=org.elasticsearch.xpack.watcher.test.integration.BasicWatcherTests -Dtests.method=\"testConditionSearchWithIndexedTemplate\" -Dtests.security.manager=true -Dtests.locale=ca -Dtests.timezone=America/Atka -Dcompiler.java=12 -Druntime.java=8\r\n```\r\nRelevant trace:\r\n```\r\n1> [2019-03-26T22:30:43,355][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] before test\r\n  1> [2019-03-26T22:30:43,355][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] [BasicWatcherTests#testIndexWatch]: setting up test\r\n  1> [2019-03-26T22:30:43,360][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node_s2] adding template [random_index_template] for index patterns [*]\r\n  1> [2019-03-26T22:30:43,370][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] [BasicWatcherTests#testIndexWatch]: all set up test\r\n  1> [2019-03-26T22:30:43,371][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] [BasicWatcherTests#testIndexWatch]: freezing time on nodes\r\n  1> [2019-03-26T22:30:43,373][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] waiting to stop watcher, current states [Tuple [v1=node_s2, v2=STOPPED], Tuple [v1=node_s0, v2=STOPPED], Tuple [v1=node_s1, v2=STOPPED]]\r\n  1> [2019-03-26T22:30:43,389][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_s2] [.watchesyvkwv] creating index, cause [Index to test aliases with .watches index], templates [.watches, random_index_template], shards [1]/[0], mappings [_doc]\r\n  1> [2019-03-26T22:30:43,389][INFO ][o.e.c.r.a.AllocationService] [node_s2] updating number_of_replicas to [1] for indices [.watchesyvkwv]\r\n  1> [2019-03-26T22:30:43,465][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_s2] [vuebvrpaq] creating index, cause [api], templates [random_index_template], shards [1]/[1], mappings [_doc]\r\n  1> [2019-03-26T22:30:43,626][INFO ][o.e.c.r.a.AllocationService] [node_s2] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[vuebvrpaq][0]] ...]).\r\n  1> [2019-03-26T22:30:43,636][DEBUG][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] indices [vuebvrpaq] are green\r\n  1> [2019-03-26T22:30:43,638][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_s2] [.watchesyvkwv/YxVv6AmvQH6ojNeIB4BeCw] deleting index\r\n  1> [2019-03-26T22:30:43,676][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] set alias for .watches index to [vuebvrpaq]\r\n  1> [2019-03-26T22:30:43,680][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_s2] [.triggered_watches] creating index, cause [api], templates [.triggered_watches, random_index_template], shards [1]/[0], mappings [_doc]\r\n  1> [2019-03-26T22:30:43,681][INFO ][o.e.c.r.a.AllocationService] [node_s2] updating number_of_replicas to [1] for indices [.triggered_watches]\r\n  1> [2019-03-26T22:30:43,785][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_s2] [.watcher-history-9-2019.03.27] creating index, cause [api], templates [.watch-history-9, random_index_template], shards [1]/[0], mappings [_doc]\r\n  1> [2019-03-26T22:30:43,786][INFO ][o.e.c.r.a.AllocationService] [node_s2] updating number_of_replicas to [1] for indices [.watcher-history-9-2019.03.27]\r\n  1> [2019-03-26T22:30:43,874][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] creating watch history index [.watcher-history-9-2019.03.27]\r\n  1> [2019-03-26T22:30:43,967][INFO ][o.e.c.r.a.AllocationService] [node_s2] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.watcher-history-9-2019.03.27][0]] ...]).\r\n  1> [2019-03-26T22:30:43,978][DEBUG][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] indices [.watcher-history-9-2019.03.27, vuebvrpaq, .triggered_watches] are green\r\n  1> [2019-03-26T22:30:43,979][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] waiting to start watcher, current states [Tuple [v1=node_s2, v2=STOPPED], Tuple [v1=node_s0, v2=STOPPED], Tuple [v1=node_s1, v2=STOPPED]]\r\n  1> [2019-03-26T22:30:43,995][DEBUG][o.e.x.w.WatcherService   ] [node_s0] watch service has been reloaded, reason [starting]\r\n  1> [2019-03-26T22:30:44,005][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] waiting to start watcher, current states [Tuple [v1=node_s2, v2=STARTING], Tuple [v1=node_s0, v2=STARTED], Tuple [v1=node_s1, v2=STARTED]]\r\n  1> [2019-03-26T22:30:44,009][DEBUG][o.e.x.w.WatcherService   ] [node_s2] watch service has been reloaded, reason [starting]\r\n  1> [2019-03-26T22:30:44,009][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] waiting to start watcher, current states [Tuple [v1=node_s2, v2=STARTING], Tuple [v1=node_s0, v2=STARTED], Tuple [v1=node_s1, v2=STARTED]]\r\n  1> [2019-03-26T22:30:44,015][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] waiting to start watcher, current states [Tuple [v1=node_s2, v2=STARTED], Tuple [v1=node_s0, v2=STARTED], Tuple [v1=node_s1, v2=STARTED]]\r\n  1> [2019-03-26T22:30:44,018][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_s2] [idx] creating index, cause [api], templates [random_index_template], shards [2]/[0], mappings []\r\n  1> [2019-03-26T22:30:44,082][INFO ][o.e.c.r.a.AllocationService] [node_s2] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[idx][1]] ...]).\r\n  1> [2019-03-26T22:30:44,096][INFO ][o.e.c.m.MetaDataMappingService] [node_s2] [idx/HYDL5JI3RrWT65ANheMyHw] create_mapping [type]\r\n  1> [2019-03-26T22:30:44,135][DEBUG][o.e.x.w.WatcherIndexingListener] [node_s2] adding watch [_name] to trigger service\r\n  1> [2019-03-26T22:30:44,135][DEBUG][o.e.x.w.t.ScheduleTriggerEngineMock] [node_s2] adding watch [_name]\r\n  1> [2019-03-26T22:30:44,149][DEBUG][o.e.x.w.WatcherIndexingListener] [node_s0] watch [_name] should not be triggered\r\n  1> [2019-03-26T22:30:44,163][DEBUG][o.e.x.w.t.ScheduleTriggerEngineMock] [testIndexWatch] firing watch [_name] at [2019-03-27T07:30:43.372Z]\r\n  1> [2019-03-26T22:30:44,166][DEBUG][o.e.x.w.e.ExecutionService] [testIndexWatch] watcher execution service paused, not processing [1] events\r\n  1> [2019-03-26T22:30:54,339][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] Found [0] records for watch [_name]\r\n  1> [2019-03-26T22:30:54,339][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] [#testIndexWatch]: clearing watcher state\r\n  1> [2019-03-26T22:30:54,341][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] waiting to stop watcher, current states [Tuple [v1=node_s2, v2=STARTED], Tuple [v1=node_s0, v2=STARTED], Tuple [v1=node_s1, v2=STARTED]]\r\n  1> [2019-03-26T22:30:54,353][INFO ][o.e.x.w.WatcherService   ] [node_s0] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-26T22:30:54,353][INFO ][o.e.x.w.WatcherService   ] [node_s1] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-26T22:30:54,357][INFO ][o.e.x.w.WatcherService   ] [node_s2] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-26T22:30:54,362][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] waiting to stop watcher, current states [Tuple [v1=node_s2, v2=STOPPED], Tuple [v1=node_s0, v2=STOPPED], Tuple [v1=node_s1, v2=STOPPED]]\r\n  1> [2019-03-26T22:30:54,362][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] [BasicWatcherTests#testIndexWatch]: cleaning up after test\r\n  1> [2019-03-26T22:30:54,362][INFO ][o.e.t.InternalTestCluster] [testIndexWatch] Clearing active scheme time frozen, expected healing time 0s\r\n  1> [2019-03-26T22:30:54,403][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_s2] [idx/HYDL5JI3RrWT65ANheMyHw] deleting index\r\n  1> [2019-03-26T22:30:54,403][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_s2] [vuebvrpaq/sE_znfh3QHedE0HaGoXQXQ] deleting index\r\n  1> [2019-03-26T22:30:54,403][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_s2] [.watcher-history-9-2019.03.27/sIe7e7WWSaa4Xi6RwCtI7Q] deleting index\r\n  1> [2019-03-26T22:30:54,403][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_s2] [.triggered_watches/gu6jqmx3QguzdYDSU3dTgg] deleting index\r\n  1> [2019-03-26T22:30:54,462][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node_s2] removing template [random_index_template]\r\n  1> [2019-03-26T22:30:54,474][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] [BasicWatcherTests#testIndexWatch]: cleaned up after test\r\n  1> [2019-03-26T22:30:54,474][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testIndexWatch] after test\r\nFAILURE 11.2s J3 | BasicWatcherTests.testIndexWatch <<< FAILURES!\r\n   > Throwable #1: java.lang.AssertionError: could not find executed watch record for watch _name\r\n   > Expected: a value equal to or greater than <1L>\r\n   >      but: <0L> was less than <1L>\r\n``` \r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+intake/2761/console\r\n\r\nReproduction:\r\n```\r\n./gradlew :x-pack:plugin:watcher:unitTest -Dtests.seed=27A4E6E50AAC7BBF -Dtests.class=org.elasticsearch.xpack.watcher.test.integration.BasicWatcherTests -Dtests.method=\"testConditionSearchWithIndexedTemplate\" -Dtests.security.manager=true -Dtests.locale=is-IS -Dtests.timezone=America/Santarem -Dcompiler.java=12 -Druntime.java=8\r\n```\r\n\r\nTrace:\r\n```\r\n1> [2019-03-27T10:01:24,459][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] before test\r\n  1> [2019-03-27T10:01:24,459][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] [BasicWatcherTests#testConditionSearchWithIndexedTemplate]: setting up test\r\n  1> [2019-03-27T10:01:24,466][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node_sm2] adding template [random_index_template] for index patterns [*]\r\n  1> [2019-03-27T10:01:24,475][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] [BasicWatcherTests#testConditionSearchWithIndexedTemplate]: all set up test\r\n  1> [2019-03-27T10:01:24,477][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] [BasicWatcherTests#testConditionSearchWithIndexedTemplate]: freezing time on nodes\r\n  1> [2019-03-27T10:01:24,478][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] waiting to stop watcher, current states [Tuple [v1=node_sd3, v2=STOPPED], Tuple [v1=node_sm2, v2=STOPPED], Tuple [v1=node_sm1, v2=STOPPED], Tuple [v1=node_sm0, v2=STOPPED], Tuple [v1=node_sd5, v2=STOPPED], Tuple [v1=node_sd4, v2=STOPPED]]\r\n  1> [2019-03-27T10:01:24,482][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_sm2] [.watchesxyrni] creating index, cause [Index to test aliases with .watches index], templates [.watches, random_index_template], shards [1]/[0], mappings [_doc]\r\n  1> [2019-03-27T10:01:24,483][INFO ][o.e.c.r.a.AllocationService] [node_sm2] updating number_of_replicas to [1] for indices [.watchesxyrni]\r\n  1> [2019-03-27T10:01:24,536][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_sm2] [frabdmxca] creating index, cause [api], templates [random_index_template], shards [1]/[1], mappings [_doc]\r\n  1> [2019-03-27T10:01:24,608][INFO ][o.e.c.r.a.AllocationService] [node_sm2] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[frabdmxca][0]] ...]).\r\n  1> [2019-03-27T10:01:24,616][DEBUG][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] indices [frabdmxca] are green\r\n  1> [2019-03-27T10:01:24,619][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_sm2] [.watchesxyrni/VLE2R2FqSgy5i1duOszs7g] deleting index\r\n  1> [2019-03-27T10:01:24,652][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] set alias for .watches index to [frabdmxca]\r\n  1> [2019-03-27T10:01:24,655][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_sm2] [.triggered_watches-alias-index] creating index, cause [Index to test aliases with .triggered-watches index], templates [.triggered_watches, random_index_template], shards [1]/[1], mappings [_doc]\r\n  1> [2019-03-27T10:01:24,696][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_sm2] [murwfypk] creating index, cause [api], templates [random_index_template], shards [1]/[1], mappings [_doc]\r\n  1> [2019-03-27T10:01:24,760][INFO ][o.e.c.r.a.AllocationService] [node_sm2] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[murwfypk][0]] ...]).\r\n  1> [2019-03-27T10:01:24,770][DEBUG][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] indices [murwfypk] are green\r\n  1> [2019-03-27T10:01:24,773][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_sm2] [.triggered_watches-alias-index/8rnRreMKTyqgOEQnCiQaSA] deleting index\r\n  1> [2019-03-27T10:01:24,797][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] set alias for .triggered-watches index to [murwfypk]\r\n  1> [2019-03-27T10:01:24,802][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_sm2] [.watcher-history-9-2019.03.27] creating index, cause [api], templates [.watch-history-9, random_index_template], shards [1]/[0], mappings [_doc]\r\n  1> [2019-03-27T10:01:24,802][INFO ][o.e.c.r.a.AllocationService] [node_sm2] updating number_of_replicas to [1] for indices [.watcher-history-9-2019.03.27]\r\n  1> [2019-03-27T10:01:24,845][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] creating watch history index [.watcher-history-9-2019.03.27]\r\n  2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/workspace/elastic+elasticsearch+7.x+intake/x-pack/plugin/watcher/build/testrun/unitTest/J7/temp/org.elasticsearch.xpack.watcher.test.integration.BasicWatcherTests_27A4E6E50AAC7BBF-001\r\n  1> [2019-03-27T10:01:24,876][INFO ][o.e.c.r.a.AllocationService] [node_sm2] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.watcher-history-9-2019.03.27][0]] ...]).\r\n  1> [2019-03-27T10:01:24,884][DEBUG][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] indices [.watcher-history-9-2019.03.27, frabdmxca, murwfypk] are green\r\n  1> [2019-03-27T10:01:24,885][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] waiting to start watcher, current states [Tuple [v1=node_sd3, v2=STOPPED], Tuple [v1=node_sm2, v2=STOPPED], Tuple [v1=node_sm1, v2=STOPPED], Tuple [v1=node_sm0, v2=STOPPED], Tuple [v1=node_sd5, v2=STOPPED], Tuple [v1=node_sd4, v2=STOPPED]]\r\n  1> [2019-03-27T10:01:24,891][DEBUG][o.e.x.w.WatcherService   ] [node_sm0] watch service has been reloaded, reason [starting]\r\n  1> [2019-03-27T10:01:24,892][DEBUG][o.e.x.w.WatcherService   ] [node_sm1] watch service has been reloaded, reason [starting]\r\n  1> [2019-03-27T10:01:24,895][DEBUG][o.e.x.w.WatcherService   ] [node_sd5] watch service has been reloaded, reason [starting]\r\n  1> [2019-03-27T10:01:24,895][DEBUG][o.e.x.w.WatcherService   ] [node_sd4] watch service has been reloaded, reason [starting]\r\n  1> [2019-03-27T10:01:24,896][DEBUG][o.e.x.w.WatcherService   ] [node_sm2] watch service has been reloaded, reason [starting]\r\n  1> [2019-03-27T10:01:24,897][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] waiting to start watcher, current states [Tuple [v1=node_sd3, v2=STARTED], Tuple [v1=node_sm2, v2=STARTED], Tuple [v1=node_sm1, v2=STARTED], Tuple [v1=node_sm0, v2=STARTED], Tuple [v1=node_sd5, v2=STARTED], Tuple [v1=node_sd4, v2=STARTED]]\r\n  1> [2019-03-27T10:01:24,914][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] using custom data_path for index: [xZlFgQZDuX]\r\n  1> [2019-03-27T10:01:24,917][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_sm2] [events] creating index, cause [api], templates [random_index_template], shards [3]/[0], mappings [event]\r\n  1> [2019-03-27T10:01:24,952][INFO ][o.e.c.r.a.AllocationService] [node_sm2] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[events][1], [events][2]] ...]).\r\n  1> [2019-03-27T10:01:24,961][WARN ][o.e.d.x.w.s.s.WatcherSearchTemplateRequest] [testConditionSearchWithIndexedTemplate] [types removal] Specifying types in a watcher search request is deprecated.\r\n  1> [2019-03-27T10:01:24,963][DEBUG][o.e.x.w.WatcherIndexingListener] [node_sd4] watch [_name] should not be triggered\r\n  2> NOTE: All tests run in this JVM: [WatchAckTests, YearlyScheduleTests, JiraAccountTests, TextTemplateTests, BasicWatcherTests]\r\n  1> [2019-03-27T10:01:24,974][DEBUG][o.e.x.w.WatcherIndexingListener] [node_sd5] adding watch [_name] to trigger service\r\n  1> [2019-03-27T10:01:24,974][DEBUG][o.e.x.w.t.ScheduleTriggerEngineMock] [node_sd5] adding watch [_name]\r\n  1> [2019-03-27T10:01:24,984][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] created watch [_name] at [2019-03-27T13:01:24.984Z]\r\n  1> [2019-03-27T10:01:25,002][DEBUG][o.e.x.w.t.ScheduleTriggerEngineMock] [testConditionSearchWithIndexedTemplate] firing watch [_name] at [2019-03-27T13:01:25.914Z]\r\n  1> [2019-03-27T10:01:25,004][DEBUG][o.e.x.w.e.ExecutionService] [testConditionSearchWithIndexedTemplate] watcher execution service paused, not processing [1] events\r\n  1> [2019-03-27T10:01:35,109][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] Found [0] records for watch [_name]\r\n  1> [2019-03-27T10:01:35,109][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] [#testConditionSearchWithIndexedTemplate]: clearing watcher state\r\n  1> [2019-03-27T10:01:35,110][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] waiting to stop watcher, current states [Tuple [v1=node_sd3, v2=STARTED], Tuple [v1=node_sm2, v2=STARTED], Tuple [v1=node_sm1, v2=STARTED], Tuple [v1=node_sm0, v2=STARTED], Tuple [v1=node_sd5, v2=STARTED], Tuple [v1=node_sd4, v2=STARTED]]\r\n  1> [2019-03-27T10:01:35,116][INFO ][o.e.x.w.WatcherService   ] [node_sm1] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-27T10:01:35,117][INFO ][o.e.x.w.WatcherService   ] [node_sm0] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-27T10:01:35,119][INFO ][o.e.x.w.WatcherService   ] [node_sd4] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-27T10:01:35,119][INFO ][o.e.x.w.WatcherService   ] [node_sd3] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-27T10:01:35,119][INFO ][o.e.x.w.WatcherService   ] [node_sd5] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-27T10:01:35,121][INFO ][o.e.x.w.WatcherService   ] [node_sm2] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-27T10:01:35,124][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] waiting to stop watcher, current states [Tuple [v1=node_sd3, v2=STOPPED], Tuple [v1=node_sm2, v2=STOPPED], Tuple [v1=node_sm1, v2=STOPPED], Tuple [v1=node_sm0, v2=STOPPED], Tuple [v1=node_sd5, v2=STOPPED], Tuple [v1=node_sd4, v2=STOPPED]]\r\n  1> [2019-03-27T10:01:35,124][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] [BasicWatcherTests#testConditionSearchWithIndexedTemplate]: cleaning up after test\r\n  1> [2019-03-27T10:01:35,124][INFO ][o.e.t.InternalTestCluster] [testConditionSearchWithIndexedTemplate] Clearing active scheme time frozen, expected healing time 0s\r\n  1> [2019-03-27T10:01:35,148][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_sm2] [murwfypk/MDpXU-3TRmuH2S8doaI00g] deleting index\r\n  1> [2019-03-27T10:01:35,148][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_sm2] [frabdmxca/ks9SYzn5RGafbPcbzp2ccg] deleting index\r\n  1> [2019-03-27T10:01:35,148][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_sm2] [.watcher-history-9-2019.03.27/N21v22jHSy6h1-sJGbReyg] deleting index\r\n  1> [2019-03-27T10:01:35,148][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_sm2] [events/P5a_l-A2R1eNHAXV92bjjA] deleting index\r\n  1> [2019-03-27T10:01:35,181][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node_sm2] removing template [random_index_template]\r\n  1> [2019-03-27T10:01:35,192][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] [BasicWatcherTests#testConditionSearchWithIndexedTemplate]: cleaned up after test\r\n  1> [2019-03-27T10:01:35,192][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithIndexedTemplate] after test\r\nFAILURE 10.8s J7 | BasicWatcherTests.testConditionSearchWithIndexedTemplate <<< FAILURES!\r\n   > Throwable #1: java.lang.AssertionError: \r\n   > Expected: a value equal to or greater than <1L>\r\n```\r\n\r\nLooking at the two traces, it seems that the watcher reload occurs BEFORE the watch is created. So, I am not sure it is the same suspected cause as before (reloading causing the watches to be removed). \r\n\r\nSimilar failure:\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+7.x+intake/746/console\r\n\r\n```\r\n./gradlew :x-pack:plugin:watcher:unitTest -Dtests.seed=27A4E6E50AAC7BBF -Dtests.class=org.elasticsearch.xpack.watcher.test.integration.BasicWatcherTests -Dtests.method=\"testConditionSearchWithIndexedTemplate\" -Dtests.security.manager=true -Dtests.locale=is-IS -Dtests.timezone=America/Santarem -Dcompiler.java=12 -Druntime.java=8\r\n``` \r\n```\r\n  1> [2019-03-27T05:12:08,662][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] before test\r\n  1> [2019-03-27T05:12:08,662][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] [BasicWatcherTests#testConditionSearchWithSource]: setting up test\r\n  1> [2019-03-27T05:12:08,669][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node_s0] adding template [random_index_template] for index patterns [*]\r\n  1> [2019-03-27T05:12:08,678][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] [BasicWatcherTests#testConditionSearchWithSource]: all set up test\r\n  1> [2019-03-27T05:12:08,679][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] [BasicWatcherTests#testConditionSearchWithSource]: freezing time on nodes\r\n  1> [2019-03-27T05:12:08,680][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] waiting to stop watcher, current states [Tuple [v1=node_s1, v2=STOPPED], Tuple [v1=node_s2, v2=STOPPED], Tuple [v1=node_s0, v2=STOPPED]]\r\n  1> [2019-03-27T05:12:08,686][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_s0] [.watchesauonl] creating index, cause [Index to test aliases with .watches index], templates [.watches, random_index_template], shards [1]/[0], mappings [_doc]\r\n  1> [2019-03-27T05:12:08,687][INFO ][o.e.c.r.a.AllocationService] [node_s0] updating number_of_replicas to [1] for indices [.watchesauonl]\r\n  1> [2019-03-27T05:12:08,776][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_s0] [vrurzbbrej] creating index, cause [api], templates [random_index_template], shards [1]/[1], mappings [_doc]\r\n  1> [2019-03-27T05:12:08,874][INFO ][o.e.c.r.a.AllocationService] [node_s0] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[vrurzbbrej][0]] ...]).\r\n  1> [2019-03-27T05:12:08,888][DEBUG][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] indices [vrurzbbrej] are green\r\n  1> [2019-03-27T05:12:08,890][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_s0] [.watchesauonl/QcgNNCikRqeU9toD6E6MvQ] deleting index\r\n  1> [2019-03-27T05:12:08,922][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] set alias for .watches index to [vrurzbbrej]\r\n  1> [2019-03-27T05:12:08,927][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_s0] [.triggered_watches] creating index, cause [api], templates [.triggered_watches, random_index_template], shards [1]/[1], mappings [_doc]\r\n  1> [2019-03-27T05:12:09,006][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_s0] [.watcher-history-9-2019.03.27] creating index, cause [api], templates [.watch-history-9, random_index_template], shards [1]/[0], mappings [_doc]\r\n  1> [2019-03-27T05:12:09,007][INFO ][o.e.c.r.a.AllocationService] [node_s0] updating number_of_replicas to [1] for indices [.watcher-history-9-2019.03.27]\r\n  1> [2019-03-27T05:12:09,095][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] creating watch history index [.watcher-history-9-2019.03.27]\r\n  1> [2019-03-27T05:12:09,198][INFO ][o.e.c.r.a.AllocationService] [node_s0] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.watcher-history-9-2019.03.27][0]] ...]).\r\n  1> [2019-03-27T05:12:09,212][DEBUG][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] indices [.watcher-history-9-2019.03.27, vrurzbbrej, .triggered_watches] are green\r\n  1> [2019-03-27T05:12:09,213][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] waiting to start watcher, current states [Tuple [v1=node_s1, v2=STOPPED], Tuple [v1=node_s2, v2=STOPPED], Tuple [v1=node_s0, v2=STOPPED]]\r\n  1> [2019-03-27T05:12:09,228][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] waiting to start watcher, current states [Tuple [v1=node_s1, v2=STARTING], Tuple [v1=node_s2, v2=STARTING], Tuple [v1=node_s0, v2=STARTED]]\r\n  1> [2019-03-27T05:12:09,229][DEBUG][o.e.x.w.WatcherService   ] [node_s1] watch service has been reloaded, reason [starting]\r\n  1> [2019-03-27T05:12:09,230][DEBUG][o.e.x.w.WatcherService   ] [node_s2] watch service has been reloaded, reason [starting]\r\n  1> [2019-03-27T05:12:09,233][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] waiting to start watcher, current states [Tuple [v1=node_s1, v2=STARTED], Tuple [v1=node_s2, v2=STARTED], Tuple [v1=node_s0, v2=STARTED]]\r\n  1> [2019-03-27T05:12:09,237][INFO ][o.e.c.m.MetaDataCreateIndexService] [node_s0] [events] creating index, cause [api], templates [random_index_template], shards [7]/[1], mappings [event]\r\n  1> [2019-03-27T05:12:09,499][DEBUG][o.e.x.w.WatcherIndexingListener] [node_s2] watch [_name] should not be triggered\r\n  1> [2019-03-27T05:12:09,539][DEBUG][o.e.x.w.WatcherIndexingListener] [node_s1] adding watch [_name] to trigger service\r\n  1> [2019-03-27T05:12:09,539][DEBUG][o.e.x.w.t.ScheduleTriggerEngineMock] [node_s1] adding watch [_name]\r\n  1> [2019-03-27T05:12:09,592][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] created watch [_name] at [2019-03-27T09:12:09.592Z]\r\n  1> [2019-03-27T05:12:09,622][INFO ][o.e.c.r.a.AllocationService] [node_s0] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[events][1]] ...]).\r\n  1> [2019-03-27T05:12:09,651][DEBUG][o.e.x.w.t.ScheduleTriggerEngineMock] [testConditionSearchWithSource] firing watch [_name] at [2019-03-27T09:12:10.233Z]\r\n  1> [2019-03-27T05:12:09,654][DEBUG][o.e.x.w.e.ExecutionService] [testConditionSearchWithSource] watcher execution service paused, not processing [1] events\r\n  1> [2019-03-27T05:12:19,785][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] Found [0] records for watch [_name]\r\n  1> [2019-03-27T05:12:19,785][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] [#testConditionSearchWithSource]: clearing watcher state\r\n  1> [2019-03-27T05:12:19,786][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] waiting to stop watcher, current states [Tuple [v1=node_s1, v2=STARTED], Tuple [v1=node_s2, v2=STARTED], Tuple [v1=node_s0, v2=STARTED]]\r\n  1> [2019-03-27T05:12:19,791][INFO ][o.e.x.w.WatcherService   ] [node_s2] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-27T05:12:19,791][INFO ][o.e.x.w.WatcherService   ] [node_s1] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-27T05:12:19,793][INFO ][o.e.x.w.WatcherService   ] [node_s0] stopping watch service, reason [watcher manually marked to shutdown by cluster state update]\r\n  1> [2019-03-27T05:12:19,796][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] waiting to stop watcher, current states [Tuple [v1=node_s1, v2=STOPPED], Tuple [v1=node_s2, v2=STOPPED], Tuple [v1=node_s0, v2=STOPPED]]\r\n  1> [2019-03-27T05:12:19,796][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] [BasicWatcherTests#testConditionSearchWithSource]: cleaning up after test\r\n  1> [2019-03-27T05:12:19,796][INFO ][o.e.t.InternalTestCluster] [testConditionSearchWithSource] Clearing active scheme time frozen, expected healing time 0s\r\n  1> [2019-03-27T05:12:19,821][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_s0] [events/UURKiggsQcm349t1qHMpNQ] deleting index\r\n  1> [2019-03-27T05:12:19,821][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_s0] [.triggered_watches/8dqLO65eQASBHboBUPB3yQ] deleting index\r\n  1> [2019-03-27T05:12:19,821][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_s0] [.watcher-history-9-2019.03.27/YxxObJ3OThmY9cGQSSKF3w] deleting index\r\n  1> [2019-03-27T05:12:19,821][INFO ][o.e.c.m.MetaDataDeleteIndexService] [node_s0] [vrurzbbrej/7zLSzJHzQryFj0_XE14I5g] deleting index\r\n  1> [2019-03-27T05:12:19,885][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node_s0] removing template [random_index_template]\r\n  1> [2019-03-27T05:12:19,892][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] [BasicWatcherTests#testConditionSearchWithSource]: cleaned up after test\r\n  1> [2019-03-27T05:12:19,892][INFO ][o.e.x.w.t.i.BasicWatcherTests] [testConditionSearchWithSource] after test\r\nFAILURE 11.3s J7 | BasicWatcherTests.testConditionSearchWithSource <<< FAILURES!\r\n   > Throwable #1: java.lang.AssertionError: \r\n```\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/477155194","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-477155194","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":477155194,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3NzE1NTE5NA==","user":{"login":"benwtrent","id":4357155,"node_id":"MDQ6VXNlcjQzNTcxNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/4357155?v=4","gravatar_id":"","url":"https://api.github.com/users/benwtrent","html_url":"https://github.com/benwtrent","followers_url":"https://api.github.com/users/benwtrent/followers","following_url":"https://api.github.com/users/benwtrent/following{/other_user}","gists_url":"https://api.github.com/users/benwtrent/gists{/gist_id}","starred_url":"https://api.github.com/users/benwtrent/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/benwtrent/subscriptions","organizations_url":"https://api.github.com/users/benwtrent/orgs","repos_url":"https://api.github.com/users/benwtrent/repos","events_url":"https://api.github.com/users/benwtrent/events{/privacy}","received_events_url":"https://api.github.com/users/benwtrent/received_events","type":"User","site_admin":false},"created_at":"2019-03-27T13:31:32Z","updated_at":"2019-03-27T14:02:20Z","author_association":"MEMBER","body":"Another failure around the same thing:\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+intake/2765/console\r\n```\r\nFAILURE 13.1s J6 | WebhookIntegrationTests.testWebhookWithBasicAuth <<< FAILURES!\r\n   > Throwable #1: java.lang.AssertionError: could not find executed watch record for watch _id\r\n   > Expected: a value equal to or greater than <1L>\r\n   >      but: <0L> was less than <1L>\r\n   > \tat __randomizedtesting.SeedInfo.seed([552D2F2E1DEA46BD:BE10AAB71FC19159]:0)\r\n   > \tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n   > \tat \r\n```\r\n\r\nMuting these BasicWatcherTests and WebhookIntegrationTests as many of them appear to be failing across master and 7.x","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/478580978","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-478580978","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":478580978,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3ODU4MDk3OA==","user":{"login":"droberts195","id":7405510,"node_id":"MDQ6VXNlcjc0MDU1MTA=","avatar_url":"https://avatars0.githubusercontent.com/u/7405510?v=4","gravatar_id":"","url":"https://api.github.com/users/droberts195","html_url":"https://github.com/droberts195","followers_url":"https://api.github.com/users/droberts195/followers","following_url":"https://api.github.com/users/droberts195/following{/other_user}","gists_url":"https://api.github.com/users/droberts195/gists{/gist_id}","starred_url":"https://api.github.com/users/droberts195/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/droberts195/subscriptions","organizations_url":"https://api.github.com/users/droberts195/orgs","repos_url":"https://api.github.com/users/droberts195/repos","events_url":"https://api.github.com/users/droberts195/events{/privacy}","received_events_url":"https://api.github.com/users/droberts195/received_events","type":"User","site_admin":false},"created_at":"2019-04-01T13:33:55Z","updated_at":"2019-04-01T13:33:55Z","author_association":"CONTRIBUTOR","body":"Another test suffering from the same `could not find executed watch record for watch _id` error is `WebhookHttpsIntegrationTests.testHttps`.\r\n\r\nFor example:\r\n\r\n* https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+7.x+intake/829/console\r\n* https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+6.7+intake/476/console\r\n\r\nI muted this test too:\r\n\r\nmaster: dbbad817a9a800c0485183c3b8072c49301de1d9\r\n7.x: 54b0ff8e337bdf64f0ca1b348b15dfbf2f3847af\r\n7.0: 6b7a6078fb240fe790e13b19d74b74548135db5a\r\n6.7: 3e5fe29a729120600fd0165f608a046691cb17ed\r\n\r\nIt didn't reproduce locally for me with:\r\n\r\n```\r\n./gradlew :x-pack:plugin:watcher:unitTest \\\r\n  -Dtests.seed=3AFF7F9BF0A9BF85 \\\r\n  -Dtests.class=org.elasticsearch.xpack.watcher.actions.webhook.WebhookHttpsIntegrationTests \\\r\n  -Dtests.method=\"testHttps\" \\\r\n  -Dtests.security.manager=true \\\r\n  -Dtests.locale=lv-LV \\\r\n  -Dtests.timezone=Europe/Monaco \\\r\n  -Dcompiler.java=12 \\\r\n  -Druntime.java=8\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/481284773","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-481284773","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":481284773,"node_id":"MDEyOklzc3VlQ29tbWVudDQ4MTI4NDc3Mw==","user":{"login":"gwbrown","id":1522844,"node_id":"MDQ6VXNlcjE1MjI4NDQ=","avatar_url":"https://avatars1.githubusercontent.com/u/1522844?v=4","gravatar_id":"","url":"https://api.github.com/users/gwbrown","html_url":"https://github.com/gwbrown","followers_url":"https://api.github.com/users/gwbrown/followers","following_url":"https://api.github.com/users/gwbrown/following{/other_user}","gists_url":"https://api.github.com/users/gwbrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gwbrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gwbrown/subscriptions","organizations_url":"https://api.github.com/users/gwbrown/orgs","repos_url":"https://api.github.com/users/gwbrown/repos","events_url":"https://api.github.com/users/gwbrown/events{/privacy}","received_events_url":"https://api.github.com/users/gwbrown/received_events","type":"User","site_admin":false},"created_at":"2019-04-09T14:49:50Z","updated_at":"2019-04-09T14:49:50Z","author_association":"CONTRIBUTOR","body":"This should be fixed in master by https://github.com/elastic/elasticsearch/pull/40658, if anyone runs across this failure let me know. I'll wait for the backport before closing this.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/482746941","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-482746941","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":482746941,"node_id":"MDEyOklzc3VlQ29tbWVudDQ4Mjc0Njk0MQ==","user":{"login":"gwbrown","id":1522844,"node_id":"MDQ6VXNlcjE1MjI4NDQ=","avatar_url":"https://avatars1.githubusercontent.com/u/1522844?v=4","gravatar_id":"","url":"https://api.github.com/users/gwbrown","html_url":"https://github.com/gwbrown","followers_url":"https://api.github.com/users/gwbrown/followers","following_url":"https://api.github.com/users/gwbrown/following{/other_user}","gists_url":"https://api.github.com/users/gwbrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gwbrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gwbrown/subscriptions","organizations_url":"https://api.github.com/users/gwbrown/orgs","repos_url":"https://api.github.com/users/gwbrown/repos","events_url":"https://api.github.com/users/gwbrown/events{/privacy}","received_events_url":"https://api.github.com/users/gwbrown/received_events","type":"User","site_admin":false},"created_at":"2019-04-12T22:55:48Z","updated_at":"2019-04-12T22:55:48Z","author_association":"CONTRIBUTOR","body":"https://github.com/elastic/elasticsearch/pull/40658 has been merged to all currently maintained branches after a week of running in master with no failures in this test, so I'm closing this issue.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/489545651","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-489545651","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":489545651,"node_id":"MDEyOklzc3VlQ29tbWVudDQ4OTU0NTY1MQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2019-05-06T08:28:44Z","updated_at":"2019-05-06T08:28:44Z","author_association":"MEMBER","body":"`ActionErrorIntegrationTests.testErrorInAction` which has been tracked in https://github.com/elastic/elasticsearch/issues/39306 has failed again but https://github.com/elastic/elasticsearch/issues/39306#issuecomment-476828624 suggests to reopen this issue instead if additional test failures occur.\r\n\r\n* [Test failure link](https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+7.0+matrix-java-periodic/ES_BUILD_JAVA=openjdk12,ES_RUNTIME_JAVA=zulu12,nodes=immutable&&linux&&docker/153/consoleFull)\r\n* Test failure: `java.lang.AssertionError: watch was triggered on [0] schedulers, expected [1]` (same as was mentioned in https://github.com/elastic/elasticsearch/pull/40658#issuecomment-480267704).\r\n* [Full test log](https://github.com/elastic/elasticsearch/files/3147075/test.log)\r\n* Reproduction line (does not reproduce locally (I tried 100 iterations)):\r\n\r\n```\r\n./gradlew :x-pack:plugin:watcher:test --tests \"org.elasticsearch.xpack.watcher.actions.ActionErrorIntegrationTests.testErrorInAction\" \\\r\n  -Dtests.seed=930476443B322E24 \\\r\n  -Dtests.security.manager=true \\\r\n  -Dtests.locale=pt-MZ \\\r\n  -Dtests.timezone=America/Tegucigalpa \\\r\n  -Dcompiler.java=12 \\\r\n  -Druntime.java=12\r\n```\r\n\r\nNote: Later in the logs we see the output along the lines of `non-failed nodes do not form a quorum`. As far as I understand it this is related to nodes being shutdown as part of the suite's teardown process (i.e. *after* the test failure has occured) so I think that is unrelated.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/539576940","html_url":"https://github.com/elastic/elasticsearch/issues/35503#issuecomment-539576940","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/35503","id":539576940,"node_id":"MDEyOklzc3VlQ29tbWVudDUzOTU3Njk0MA==","user":{"login":"jakelandis","id":976291,"node_id":"MDQ6VXNlcjk3NjI5MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/976291?v=4","gravatar_id":"","url":"https://api.github.com/users/jakelandis","html_url":"https://github.com/jakelandis","followers_url":"https://api.github.com/users/jakelandis/followers","following_url":"https://api.github.com/users/jakelandis/following{/other_user}","gists_url":"https://api.github.com/users/jakelandis/gists{/gist_id}","starred_url":"https://api.github.com/users/jakelandis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jakelandis/subscriptions","organizations_url":"https://api.github.com/users/jakelandis/orgs","repos_url":"https://api.github.com/users/jakelandis/repos","events_url":"https://api.github.com/users/jakelandis/events{/privacy}","received_events_url":"https://api.github.com/users/jakelandis/received_events","type":"User","site_admin":false},"created_at":"2019-10-08T15:46:48Z","updated_at":"2019-10-08T15:46:48Z","author_association":"CONTRIBUTOR","body":"Closing this issue as I can not find any actionable items here.  None of the tests referenced here are currently disabled and (outside a very small handful of PRs) could not find any evidence of failures in the past 90 days. The small handful of PRs appear to have larger problems and include many other test failures. \r\n\r\nIf any of the referenced tests fail please log a new issue. ","performed_via_github_app":null}]