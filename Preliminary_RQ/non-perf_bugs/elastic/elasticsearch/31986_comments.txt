[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/404421336","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-404421336","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":404421336,"node_id":"MDEyOklzc3VlQ29tbWVudDQwNDQyMTMzNg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-07-12T07:47:39Z","updated_at":"2018-07-12T07:47:39Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-core-infra","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/404567815","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-404567815","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":404567815,"node_id":"MDEyOklzc3VlQ29tbWVudDQwNDU2NzgxNQ==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2018-07-12T16:16:15Z","updated_at":"2018-07-12T16:16:15Z","author_association":"MEMBER","body":"So here are the breakers we have:\r\n\r\n- *total*\r\n- *fielddata*\r\n- *request*\r\n- *accounting*\r\n- *inflight requests*\r\n\r\nSo out of those, the ones that spring to mind as \"retryable\" to me are `request`, and `inflight_requests`, since those are likely to be decremented by ES automatically rather than requiring admin intervention to lower them.\r\n\r\nFor the others, `fielddata` is not expired by default, so it'd require someone to clear the cache (or change mappings, etc) to fix. `accounting` is lucene memory, so it won't change unless the shards on the node are merged or relocated in some way. That's why I figured they are good candidates for staying in 503 response code territory.\r\n\r\nFor `total`, it's a little tricky, I could go either way, because it depends on what is actually causing it to trip, we could maybe default to it being retryable.\r\n\r\nWhat do you think?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/404790590","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-404790590","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":404790590,"node_id":"MDEyOklzc3VlQ29tbWVudDQwNDc5MDU5MA==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2018-07-13T10:13:36Z","updated_at":"2018-07-13T10:13:36Z","author_association":"MEMBER","body":"Your reasoning makes sense to me. Looking at the HTTP status codes I think we should probably always use 503 and only indicate retryable conditions with the `Retry-After` header instead of using a different status code.\r\n\r\n> For total, it's a little tricky, I could go either way, because it depends on what is actually causing it to trip, we could maybe default to it being retryable.\r\n\r\nIt is tricky indeed. For a moment I thought we could make this dependent on the child circuit breaker that causes the parent to break but I think we should not do this because this could be just pure coincidence and thus be misleading. Your suggestion of defaulting to retryable seems reasonable to me especially considering that the new default is to base this on real memory usage.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/404834184","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-404834184","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":404834184,"node_id":"MDEyOklzc3VlQ29tbWVudDQwNDgzNDE4NA==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2018-07-13T13:31:19Z","updated_at":"2018-07-13T13:31:19Z","author_association":"MEMBER","body":"We discussed this in Fix-it Friday. The main points are:\r\n\r\n* We need to be careful from which circuit-breakers we want to allow retries because if clients start to retry requests that are not retryable this puts more load on the cluster than necessary.\r\n* The 429 status code is probably preferable because (some of) the language clients handle this already (for example the Java client if you use the bulk processor)\r\n* We also need to coordinate with the clients team so they are aware of this change.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/405208202","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-405208202","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":405208202,"node_id":"MDEyOklzc3VlQ29tbWVudDQwNTIwODIwMg==","user":{"login":"javanna","id":832460,"node_id":"MDQ6VXNlcjgzMjQ2MA==","avatar_url":"https://avatars1.githubusercontent.com/u/832460?v=4","gravatar_id":"","url":"https://api.github.com/users/javanna","html_url":"https://github.com/javanna","followers_url":"https://api.github.com/users/javanna/followers","following_url":"https://api.github.com/users/javanna/following{/other_user}","gists_url":"https://api.github.com/users/javanna/gists{/gist_id}","starred_url":"https://api.github.com/users/javanna/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/javanna/subscriptions","organizations_url":"https://api.github.com/users/javanna/orgs","repos_url":"https://api.github.com/users/javanna/repos","events_url":"https://api.github.com/users/javanna/events{/privacy}","received_events_url":"https://api.github.com/users/javanna/received_events","type":"User","site_admin":false},"created_at":"2018-07-16T10:38:28Z","updated_at":"2018-07-16T10:38:28Z","author_association":"MEMBER","body":"> We need to be careful from which circuit-breakers we want to allow retries because if clients start to retry requests that are not retryable this puts more load on the cluster than necessary.\r\n\r\nI would say that this is what currently happens, as 503s are retried by our clients.\r\n\r\n> The 429 status code is probably preferable because (some of) the language clients handle this already (for example the Java client if you use the bulk processor)\r\n\r\nThat part of the discussion was around search, hence `BulkProcessor` does not come into the picture as it is used only when indexing. We said it sounds better not to retry on all nodes a request that will fail on any node. The hard part is to determine which circuit breaker exceptions shall be retried and which ones should not. The clients generally retry 5xx error codes, and whenever a node returns 5xx it is blacklisted for a while and retried a while later (applying some back-off).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/405222415","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-405222415","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":405222415,"node_id":"MDEyOklzc3VlQ29tbWVudDQwNTIyMjQxNQ==","user":{"login":"Mpdreamz","id":245275,"node_id":"MDQ6VXNlcjI0NTI3NQ==","avatar_url":"https://avatars3.githubusercontent.com/u/245275?v=4","gravatar_id":"","url":"https://api.github.com/users/Mpdreamz","html_url":"https://github.com/Mpdreamz","followers_url":"https://api.github.com/users/Mpdreamz/followers","following_url":"https://api.github.com/users/Mpdreamz/following{/other_user}","gists_url":"https://api.github.com/users/Mpdreamz/gists{/gist_id}","starred_url":"https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Mpdreamz/subscriptions","organizations_url":"https://api.github.com/users/Mpdreamz/orgs","repos_url":"https://api.github.com/users/Mpdreamz/repos","events_url":"https://api.github.com/users/Mpdreamz/events{/privacy}","received_events_url":"https://api.github.com/users/Mpdreamz/received_events","type":"User","site_admin":false},"created_at":"2018-07-16T11:50:30Z","updated_at":"2018-07-16T11:54:22Z","author_association":"MEMBER","body":"The python/.net clients also have bulk helpers that will retry on operations returning a `429`  with exponential backoffs.\r\n\r\nThe goal of the failover in the regular API mappings is to fail fast though. None of the clients provide a backoff mechanism in the 1-1 API call mappings. None of the clients, therefore retry `429` responses OOTB. Since the assumption is that the cluster will not be immediately free to handle the request again. (see also https://github.com/elastic/elasticsearch/issues/21141)\r\n\r\nReturning a `429` seems like the most appropriate response for requests that are likely to succeed in the very near future.\r\n\r\nFor circuit breaker conditions unlikely to change (in the near future) perhaps returning a `500` is more appropriate since the clients & logstash retry on `503`. \r\n\r\n`Retry-After` on `503` and `429` would be massively useful and might warrant its own ticket. E.g logstash and client helpers can use it to seed the initial retry value. Client API calls can use it to decide *NOT* to retry.\r\n\r\n\r\n\r\n\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/407436429","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-407436429","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":407436429,"node_id":"MDEyOklzc3VlQ29tbWVudDQwNzQzNjQyOQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2018-07-24T14:54:42Z","updated_at":"2018-07-24T15:00:33Z","author_association":"MEMBER","body":"I put some thought into how we can classify `CircuitBreakingException` as either temporary or persistent. At this point I am not talking about how this is mapped to HTTP status codes or headers as this is a separate problem.\r\n\r\n### Definitions\r\n\r\nThere are two classes regarding the persistence of a tripped circuit breaker:\r\n\r\n* `Ct`: the condition is temporary.\r\n* `Cp`: the condition is permanent, i.e. fixing it requires human intervention.\r\n\r\n`CircuitBreakingException`s of class `Ct` should be retried for a \"reasonable\" number of time with a backoff, `CircuitBreakingException`s of class `Cp` should lead to an immediate error.\r\n\r\n### Child circuit breakers\r\n\r\nWe can attribute the class `Ct` to the `request` and `inflight requests` circuit breakers and the class `Cp` to `fielddata` and `accounting`.\r\n\r\n### Parent circuit breaker\r\n\r\nFor the parent circuit breaker the assignment to one or another class is not so clear. The following is tracked by the (real memory) parent circuit breaker:\r\n\r\n1. Memory usage attributed to class `Ct` (by the child circuit breakers `request` and `inflight requests`)\r\n2. Memory usage attributed to class `Cp` (by the child circuit breakers `fielddata` and `accounting`)\r\n3. Memory usage not tracked by any of the child circuit breakers. We cannot reason about that type of memory usage so we cannot know whether it is of class `Ct` or class `Cp`.\r\n\r\nThis itemisation is just conceptual; in reality the real memory circuit breaker only measures the total of all three.\r\n\r\nThe crux of the matter is that we know nothing about untracked memory but our goal is still to categorize it. We also want to avoid false positives (actual class is `Cp` but we categorize as `Ct`) and false negatives (actual class is `Ct` but we categorize as `Cp`).\r\n\r\nThere are several strategies how we can attack this:\r\n\r\n1. A tripped parent circuit breaker is always of class `Ct`\r\n\r\nThis will lead to false positives and thus too many retries by clients.\r\n\r\n2. The parent's class depends on the current child circuit breaker\r\n\r\nThe parent breaker is always called in the context of one of the child circuit breakers. Therefore we could provide the information whether it is of class `Ct` or `Cp` to the parent circuit breaker and use that. However, this penalizes the current circuit breaker and does not take into consideration how much memory is taken up at this point.\r\n\r\n3. The parent's class depends on current usage of child circuit breakers\r\n\r\nIf the parent circuit breaker trips, we check the relative reserved memory of all child circuit breakers:\r\n\r\n* Let `R` be the amount of memory (including overhead) that is about to be reserved (in bytes)\r\n* Let `Uparent` be the total memory usage determined by the parent circuit breaker (in bytes)\r\n* Let `Ut` be the sum of the memory tracked by all breakers of class `Ct` (in bytes)\r\n* Let `Up` be the sum of the memory tracked by all breakers of class `Cp` (in bytes)\r\n\r\nThe `CircuitBreakingException`s thrown by the parent circuit breaker is of class `Ct` if and only if:\r\n\r\n```\r\nR <= Uparent * (Ut / Up)\r\n```\r\n\r\nThe rationale behind this formula is: If the currently tracked temporary memory usage (`Ut`) makes up for more than we are about to reserve than that reservation should succeed in the future once `Ut` is lower. In the other case (memory usage is dominated by `Up`) future attempts to reserve memory will also not succeed and thus the condition is of class `Cp`.\r\n\r\nConsequently we assume for the real memory parent circuit breaker that the untracked memory has the same ratio as tracked memory (of circuit breakage classes `Ct` and `Cp`). Note that this formula applies as well for the special case of the classic circuit breaker, just that we don't consider real memory usage in that case.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/407977782","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-407977782","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":407977782,"node_id":"MDEyOklzc3VlQ29tbWVudDQwNzk3Nzc4Mg==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2018-07-26T05:08:31Z","updated_at":"2018-07-26T05:08:31Z","author_association":"MEMBER","body":"In the previous discussion HTTP status codes 429 and 503 have been mentioned as potentially appropriate response status codes on `CircuitBreakingException `. I propose that we always use 503 for `CircuitBreakingException `. If and only if the condition is temporary (according to the definitions in my comment above), we should also set the `Retry-After` header.\r\n\r\n### Definitions\r\n\r\n[RFC 7231](https://tools.ietf.org/html/rfc7231) defines HTTP status code 503 (Service Unavailable) as:\r\n\r\n> The 503 (Service Unavailable) status code indicates that the server\r\n> is currently unable to handle the request due to a temporary overload\r\n> or scheduled maintenance, which will likely be alleviated after some\r\n> delay.\r\n\r\n[RFC 6585](https://tools.ietf.org/html/rfc7231) defines HTTP status code 429 (Too Many Requests):\r\n\r\n> The 429 status code indicates that the user has sent too many \r\n> requests in a given amount of time.\r\n\r\n### Analysis\r\n\r\nLet's do a thought experiment to decide whether 429 or 503 is more appropriate.\r\n\r\nConsider Elasticsearch as a black box. Any client request may succeed (status code 2xx) or fail (status code >= 400). Suppose a client gets status code 429 after sending their first request: `GET /`. This is clearly puzzling behavior as this status code indicates that the client has sent too many of them. From the client perspective nothing was wrong with that request. The behavior is non-deterministic and there is no explicit contract between client and server that would provide knowledge to the client how many requests it is allowed to send in a given time period.\r\n\r\nNow let's consider Elasticsearch as a white box and assume that multiple clients pushed the server close to overload. When the request from our previous example had arrived, it tripped the in flight requests circuit breaker because Elasticsearch was busy processing multiple large bulk requests at that time. Detecting this state clearly requires \"global\" knowledge that only the server but no individual client can have. In other words: This state is emerging behavior on the server-side and not caused by any individual client doing something \"wrong\" (e.g. sending too many or too large requests). This brings us into status code 5xx land. Out of the available status codes, 503 seems most appropriate: \"The 503 status code indicates that the server is currently unable to handle the request due to a **temporary overload**\". This is exactly what we want to convey to the client: Your request is probably fine but we just cannot handle it at the moment. Please come back later.\r\n\r\nTherefore, I argue we should stick to HTTP 503. For permanent conditions we will omit `Retry-After`, for temporary ones we will set it.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/409038339","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-409038339","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":409038339,"node_id":"MDEyOklzc3VlQ29tbWVudDQwOTAzODMzOQ==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2018-07-30T22:55:05Z","updated_at":"2018-07-30T22:55:05Z","author_association":"MEMBER","body":"I thought about this a bit over the weekend, I couldn't find a situation where the formula didn't work, so I think that it sounds like a reasonable way forward.\r\n\r\nI'm definitely in agreement with the RFC findings for keeping a 503 response with an optional `Retry-After` header, I think consistent response codes will help also (I imagine a number of people will never look at the `Retry-After` header and would be confused if we sometimes changed the status code)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/409138387","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-409138387","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":409138387,"node_id":"MDEyOklzc3VlQ29tbWVudDQwOTEzODM4Nw==","user":{"login":"Mpdreamz","id":245275,"node_id":"MDQ6VXNlcjI0NTI3NQ==","avatar_url":"https://avatars3.githubusercontent.com/u/245275?v=4","gravatar_id":"","url":"https://api.github.com/users/Mpdreamz","html_url":"https://github.com/Mpdreamz","followers_url":"https://api.github.com/users/Mpdreamz/followers","following_url":"https://api.github.com/users/Mpdreamz/following{/other_user}","gists_url":"https://api.github.com/users/Mpdreamz/gists{/gist_id}","starred_url":"https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Mpdreamz/subscriptions","organizations_url":"https://api.github.com/users/Mpdreamz/orgs","repos_url":"https://api.github.com/users/Mpdreamz/repos","events_url":"https://api.github.com/users/Mpdreamz/events{/privacy}","received_events_url":"https://api.github.com/users/Mpdreamz/received_events","type":"User","site_admin":false},"created_at":"2018-07-31T08:24:19Z","updated_at":"2018-07-31T09:18:37Z","author_association":"MEMBER","body":"I think your analysis and thought expirement are spot on @danielmitterdorfer but not the most pragmatic out in the wild.\r\n\r\nI would like to propose we use `429` as a general overload HTTP status code. There is precedence for this in other API's e.g:\r\n\r\nhttps://docs.newrelic.com/docs/apis/rest-api-v2/requirements/api-overload-protection-handling-429-errors\r\n\r\nThis also ties into our existing retry behaviour across the clients and the ingest products.\r\n\r\n### Retry behaviour \r\n☑️ = fast failover/retry no exponential backoff\r\n✅ = failover/retry with exponential backoff\r\n\r\nClient helpers are methods coordinating multiple request e.g bulk helpers, scroll helpers. \r\n\r\nStatus | Clients | Ingest | Client Helpers\r\n-------|---------|-------|-------------------------\r\n429 | | ✅ | ✅\r\n500 | |  |\r\n501 | ☑️  | ✅☑️  | ☑️\r\n502 | ☑️| ✅☑️ | ☑️\r\n503 | ☑️ | ✅☑️ |☑️\r\n504 | ☑️|  ✅☑️ |☑️\r\n\r\nNote:\r\n* ✅☑️ => if the ingest product uses an official client the call will failover and retry fast before returning and then the product may retry exponentially outside of the client\r\n* `503` are currently retried even for the `Cp` case. This needs fixing. \r\n* `503` are also quite often and liberally returned by proxies in the wild and quite often these proxies do header removal. Making taking decisions on the nature of the `503` hard to do.\r\n\r\nIssueing a `429` will tie in with our existing handling already.\r\n\r\nStatus | Behaviour\r\n-------|---------\r\n429 | Exponential backoff retries in ingest products and client helpers\r\n500 | return immediately\r\n501 | failover and retry\r\n502 | failover and retry\r\n503 | failover and retry\r\n504 | failover and retry\r\n\r\nTherefor if `CircuitBreakingException` returns a `429` with `Retry-After` for the `Ct` case and 500 for the `Cp` case it doesn't complicate our current handling. \r\n\r\n\r\n\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/409188714","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-409188714","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":409188714,"node_id":"MDEyOklzc3VlQ29tbWVudDQwOTE4ODcxNA==","user":{"login":"javanna","id":832460,"node_id":"MDQ6VXNlcjgzMjQ2MA==","avatar_url":"https://avatars1.githubusercontent.com/u/832460?v=4","gravatar_id":"","url":"https://api.github.com/users/javanna","html_url":"https://github.com/javanna","followers_url":"https://api.github.com/users/javanna/followers","following_url":"https://api.github.com/users/javanna/following{/other_user}","gists_url":"https://api.github.com/users/javanna/gists{/gist_id}","starred_url":"https://api.github.com/users/javanna/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/javanna/subscriptions","organizations_url":"https://api.github.com/users/javanna/orgs","repos_url":"https://api.github.com/users/javanna/repos","events_url":"https://api.github.com/users/javanna/events{/privacy}","received_events_url":"https://api.github.com/users/javanna/received_events","type":"User","site_admin":false},"created_at":"2018-07-31T11:32:36Z","updated_at":"2018-07-31T12:45:55Z","author_association":"MEMBER","body":"I personally don't mind which status code/header we use, but 429s are currently handled by bulk helpers, which are part of high-level REST clients. These helpers are API specific, while I think in this issue we are trying to come up with a generic mechanism that could be applied to low-level REST clients (like the retry with back-off that we already have). Or are we discussing some mechanism to be applied to specific API only (e.g. search)?\r\n\r\nWhen we generally talk about retries in the low-level clients we mean that the the node which returned a 503 error will be blacklisted and the request will be retried on another node, and so on. What is suggested in this issue for temporary failures is very different: retry that same request only on that same node, after a certain amount of time (possibly returned by the server). I am not convinced though that retrying on the same node only is a good strategy. Also should that node be blacklisted or should it keep on receiving other requests in the meantime?\r\n\r\nI would consider reducing the scope of this issue by addressing first the temporary vs permanent failure and doing the right thing out-of-the-box in the low-level clients, which is already not a trivial task. I would leave returning a proper retry interval to be applied in the clients for later, if we still want to do that. We should fix the current behaviour as our low-level clients currently end up retrying the same request on all nodes and marking nodes dead for both temporary and permanent failures, which is not a good behaviour either way.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/409191210","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-409191210","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":409191210,"node_id":"MDEyOklzc3VlQ29tbWVudDQwOTE5MTIxMA==","user":{"login":"codebrain","id":148974,"node_id":"MDQ6VXNlcjE0ODk3NA==","avatar_url":"https://avatars3.githubusercontent.com/u/148974?v=4","gravatar_id":"","url":"https://api.github.com/users/codebrain","html_url":"https://github.com/codebrain","followers_url":"https://api.github.com/users/codebrain/followers","following_url":"https://api.github.com/users/codebrain/following{/other_user}","gists_url":"https://api.github.com/users/codebrain/gists{/gist_id}","starred_url":"https://api.github.com/users/codebrain/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/codebrain/subscriptions","organizations_url":"https://api.github.com/users/codebrain/orgs","repos_url":"https://api.github.com/users/codebrain/repos","events_url":"https://api.github.com/users/codebrain/events{/privacy}","received_events_url":"https://api.github.com/users/codebrain/received_events","type":"User","site_admin":false},"created_at":"2018-07-31T11:43:43Z","updated_at":"2018-07-31T11:43:43Z","author_association":"CONTRIBUTOR","body":"+1 for `Retry-After`, and +1 for using `503`.\r\n\r\nThat said, I think there is a justification for using `429` on bulk operations. A bulk operation *feels* different than sending a flurry of individual requests - which may be originating from multiple clients that are unaware of each other.\r\n\r\nWould it also be possible to return a circuit breaker categorisation as well, `Retry-Category` or something similar? I'm not usually a fan of custom headers, but if we have additional information it could be useful to include this.\r\n\r\nAs I understand, not all operations are equal and some may be safe to send to the node (e.g. ingest) whilst others might not (e.g. search). If we have the categorisation we could potentially be smarter in the clients about how we route requests and keep the nodes 'alive' in the client for longer. Of course, we can ignore this header, but it would give us a choice in the future.\r\n\r\nHow is the back-pressure / circuit breaker information currently surfaced to the clients? Maybe this information is useful in a `Retry-Category` header.\r\n\r\nJust my 2 cents.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/409211264","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-409211264","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":409211264,"node_id":"MDEyOklzc3VlQ29tbWVudDQwOTIxMTI2NA==","user":{"login":"Mpdreamz","id":245275,"node_id":"MDQ6VXNlcjI0NTI3NQ==","avatar_url":"https://avatars3.githubusercontent.com/u/245275?v=4","gravatar_id":"","url":"https://api.github.com/users/Mpdreamz","html_url":"https://github.com/Mpdreamz","followers_url":"https://api.github.com/users/Mpdreamz/followers","following_url":"https://api.github.com/users/Mpdreamz/following{/other_user}","gists_url":"https://api.github.com/users/Mpdreamz/gists{/gist_id}","starred_url":"https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Mpdreamz/subscriptions","organizations_url":"https://api.github.com/users/Mpdreamz/orgs","repos_url":"https://api.github.com/users/Mpdreamz/repos","events_url":"https://api.github.com/users/Mpdreamz/events{/privacy}","received_events_url":"https://api.github.com/users/Mpdreamz/received_events","type":"User","site_admin":false},"created_at":"2018-07-31T12:57:32Z","updated_at":"2018-07-31T12:57:32Z","author_association":"MEMBER","body":"> I would consider reducing the scope of this issue by addressing first the temporary vs permanent failure and doing the right thing out-of-the-box in the clients, which is already not a trivial task.\r\n\r\nThis is exactly what I set out to achieve with my proposal. \r\n\r\n> I personally don't mind which status code/header we use, but an important point is that 429s are currently handled by bulk helpers, which are part of high-level REST clients. These helpers are API specific, while I think in this issue we are trying to come up with a generic mechanism that could be applied to low-level REST clients (like the retry with back-off that we already have). Or are we discussing some mechanism to be applied to specific API only (e.g. search)?.\r\n\r\nThe rules I am proposing would apply for all API's in the lifetime of a single request whether from a low or high-level client.\r\n\r\n- low level client, clients sending bytes/streams/dynamic objects, maps, hashes.\r\n- high level client, client sending and receiving typed responses.\r\n- helpers, various useful extensions that help coordinate over requests and do the right thing. \r\n\r\n> (like the retry with back-off that we already have)\r\n\r\nIn the scope of a single request, the clients do not have a backoff period. We retry a request by failing over to the next node immediately. When a node is marked dead it is not considered as a target for requests that follow for duration `X`. When that duration finishes the node is resurrected but if it still fails we exponentially increase `X`. This is not a `retry with back-off`.\r\n\r\n> I would leave returning a proper retry interval to be applied in the clients for later. \r\n\r\nClients should never do exponential retries. Helpers we ship with the clients or our ingest products should/could/will which is why I listed these as a seperate enitity\r\n\r\n> What we want to achieve here with temporary failures is rather different: retry that same request only on that same node, after a certain amount of time (possibly returned by the server). \r\n\r\nThis is a point that needs discussing. From the clients perspective I am off mind that the client should not do this OOTB. Again our helpers or ingest products should. @elastic/es-clients please weigh in.\r\n\r\nSince 503 is already a fast failover condition in the clients and classifying a 503 without a retry header as permanent will be hard using `429` will make the clients return fast while our helpers and ingest products can decide the appropiate course of action (likely retry). A `500` is also not retried and will return fast from the client and again we can yield the decision what to do to the helpers and ingest products (likely not retry).\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/429849777","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-429849777","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":429849777,"node_id":"MDEyOklzc3VlQ29tbWVudDQyOTg0OTc3Nw==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2018-10-15T13:20:42Z","updated_at":"2018-10-15T13:20:42Z","author_association":"MEMBER","body":"After discussions with the clients team we settled on using HTTP 429 for those exceptions in general. The durability (permanent vs. transient) will be returned in a dedicated new field.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/505887289","html_url":"https://github.com/elastic/elasticsearch/issues/31986#issuecomment-505887289","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31986","id":505887289,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTg4NzI4OQ==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2019-06-26T13:58:17Z","updated_at":"2019-06-26T13:58:17Z","author_association":"CONTRIBUTOR","body":"We spent some time on this issue differentiating between failures which should be retried and those that should not. This was all from the perspective of clients outside of elasticsearch but would it not also make sense to consider internal retry policies across replicas?\r\nIt looks like [we retry failed searches exhaustively](https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/action/search/InitialSearchPhase.java#L113) across all replicas and I can't see any distinction drawn between failures that might be safe to retry on another node and those that might be dangerous (e.g. memory hogs). \r\nThe advantage of automated replica-retries is they can deliver success if only a single node is experiencing memory pressures but the disadvantage is the impact of an abusive query is amplified on a cluster. Have we thought about differentiating the replica-retry policy for different types of failure?","performed_via_github_app":null}]