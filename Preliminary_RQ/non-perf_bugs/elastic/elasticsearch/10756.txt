{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/10756","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10756/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10756/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10756/events","html_url":"https://github.com/elastic/elasticsearch/issues/10756","id":70462699,"node_id":"MDU6SXNzdWU3MDQ2MjY5OQ==","number":10756,"title":"Vectorizers: export consistent datasets for statistical / ML tasks","user":{"login":"alexksikes","id":43475,"node_id":"MDQ6VXNlcjQzNDc1","avatar_url":"https://avatars2.githubusercontent.com/u/43475?v=4","gravatar_id":"","url":"https://api.github.com/users/alexksikes","html_url":"https://github.com/alexksikes","followers_url":"https://api.github.com/users/alexksikes/followers","following_url":"https://api.github.com/users/alexksikes/following{/other_user}","gists_url":"https://api.github.com/users/alexksikes/gists{/gist_id}","starred_url":"https://api.github.com/users/alexksikes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexksikes/subscriptions","organizations_url":"https://api.github.com/users/alexksikes/orgs","repos_url":"https://api.github.com/users/alexksikes/repos","events_url":"https://api.github.com/users/alexksikes/events{/privacy}","received_events_url":"https://api.github.com/users/alexksikes/received_events","type":"User","site_admin":false},"labels":[{"id":146832564,"node_id":"MDU6TGFiZWwxNDY4MzI1NjQ=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Search","name":":Search/Search","color":"0e8a16","default":false,"description":"Search-related issues that do not fall into other categories"},{"id":23172,"node_id":"MDU6TGFiZWwyMzE3Mg==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Efeature","name":">feature","color":"006b75","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2015-04-23T17:08:20Z","updated_at":"2018-02-14T13:44:20Z","closed_at":"2015-04-27T10:27:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Introduction\n\nToday if we want to use Elasticsearch as a data source for external statistical or machine learning packages, we can either export the source, use fields, fielddata_fields, scripted fields or export term vectors. However, machine learning algorithms expect the data to be returned in a document-term or term-document matrix, where the rows correspond to documents in a collection and columns correspond to terms, or more precisely to some numerical value associated with each term such as tf or tf-idf.\n\nThe idea behind vectorizers consists of specifying the format of this matrix in advance, so that each vector could be generated on the fly according to that specification. Enforcing how data should be turned into a vector in advance has a couple of benefits such as:\n- This approach generates consistent datasets. This is important because a model can be trained, fed more data to it and evaluated at any time. The datasets generated in between all have the same number of features and the feature values are at the same expected column index. This also means that data could be streamed to a machine learning model.\n- Most machine learned models work on a fix set of features, therefore adding new features means that the model should be retrained on these new features anyway. This approach makes this step explicit by clearly stating that the dataset is generated according to a given choice of vectorizer, and if new features are to be taken into account, then a new vectorizer should be defined.\n- Although there are potentially millions of features, only a few might be discriminating for the task. This approach leaves the feature selection step to user by asking for these features before hand. This is not necessarily a bad thing because feature selection could be a complex matter and the sole goal of vectorizers should be about facilitating interfacing between ES data and external statistical tools, not about feature selection itself. For string fields, depending on the task, a good set of features could already be obtained using significant terms.\n- The term-column dictionary required for text features would be stored in memory, with the user having direct control over its usage. Keeping track of more features mean more memory is required. This is to be contrasted with approaches that would consider every term to be a potential feature, bloating the index with noisy features, consuming memory and disk space in an uncontrollable manner.\n- This approach provides an iterative approach to machine learning, in which the user can try out different vectorizers on a small sample of the document collection. This would fit well with statistical environments such as R, where for example a data frame could be created according to the specification given by a vectorizer.\n## Defining a vectorizer\n\nWhat we need is a map from a document to a real valued vector. The map could be stored for a given index in a `.vectorizer` hidden type.\n\n```\nPUT /index/.vectorizer/my_vectorizer\n{\n    \"vectorizer\": [\n        {\n            \"field\": \"text\",\n            \"span\": [ ... list of terms ...],\n            \"value\": \"term_freq\"\n        },\n        {\n            \"field\": \"field_numeric_1\",\n            \"span\": 1\n        },\n        {\n            \"field\" : \"field_numeric_2\",\n            \"span\": 1,\n            \"script\": \"if _value > 0.5 then 1 else 0\"\n        },\n        {\n            \"field\": \"field_numeric_3\",\n            \"span\": 5\n        },\n        {\n            \"field\": \"label\",\n            \"span\": 1,\n            \"script\": \"if _value == \"yes\" then 1 else 0\"\n        }\n    ]\n}\n```\n\nA vectorizer specifies how the values of a given field in the index should be mapped to a real valued (fixed size) vector. For example, the first entry defines a first list of columns whose values are the term frequencies of the terms given in `span` in this order. The next column is simply defined as the value of the numerical field named `field_numeric_1`. For the next column, we binarize the numerical value at the field using a script. Next we assign 5 columns for the first 5 values in the multi-valued numerical field `field_numeric_3`. Last but not least, the last column is binarized as well and could be used as the target label for learning.\n\n`field`: Name of the field in the index to which a mapping from field values to real values should be applied.\n`span`: For numerical fields, specifies a number of columns to be assigned in the resulting vector. For string fields, specifies a list of terms, each term occupying a column in the vector if present in the document.\n`value`: For numerical fields, defaults to the value at the field. For string fields, defaults to term frequency. For the later case other values are possible such as document frequency, or payload.\n`script`: Allows for any field value transformation such as thresholding, categorizing, etc ...\n## Pulling vectorized data out of ES\n\nA vector or set of vectors could be pulled out of the index, given a choice of vectorizer. A vectorizer operates on top of the term vectors API in the following manner:\n\n```\nGET /index/type/id/_termvectors&vectorizer=my_vectorizer\n\nResponse:\n\n{\n    \"shape\": [1, 9],\n    \"vector\": [\n        {\"3\": 5, \"5\": 2, \"6\": 0.55, 7\": 1, \"8\": 0}\n    ]\n}\n```\n\nThe vector returned is in a sparse format, together with its dimension. This vector can then be loaded in memory in your favorite statistical environments, or fed to a machine learning package.\n\nWhen using the multi-term vectors API we obtain a n dimensional matrix:\n\n```\nGET /index/type/_mtermvectors\n{\n    \"docs\": [\n    {\n        \"_index\": \"index\",\n        \"_type\": \"type\",\n        \"_id\": \"id\",\n        \"vectorizer\": my_vectorizer\n    },\n    {\n        \"_index\": \"index\",\n        \"_type\": \"type\",\n        \"_id\": \"id2\",\n        \"vectorizer\": my_vectorizer\n    }\n   ]\n}\n\nResponse:\n\n{\n    \"shape\": [2, 9],\n    \"vector\": [\n        {\"3\": 5, \"5\": 2, \"6\": 0.55, 7\": 1, \"8\": 0},\n        {\"0\": 2, \"3\": 1, \"6\": 0.21, 7\": 1, \"8\": 1}\n    ]\n}\n```\n\nHere different vectorizers could have been chosen. In this case, the vectors are stacked up, columns over columns, with the largest vector defining the dimension of the matrix.\n\nFinally, we can obtain a dataset by using the scan and scroll together with a new search type called `matrix`:\n\n```\nGET /index/_search?search_type=matrix&scroll=1m\n{\n    \"query\": { \"match_all\": {}},\n    \"size\":  1000,\n\n    \"sample\": \"10%\",\n    \"step\": step,\n\n    \"vectorizer\": \"my_vectorizer\",\n    \"slice\": start:stop:step\n}\n\nAfter scroll request, we get the response:\n\n{\n    \"shape\": [1000, 9],\n    \"vector: [\n        {\"3\": 5, \"5\": 2, \"6\": 0.55, 7\": 1, \"8\": 0},\n        {\"0\": 2, \"3\": 1, \"6\": 0.21, 7\": 1, \"8\": 1},\n        {\"3\": 5, \"3\": 5, \"6\": 0.45, 7\": 0, \"8\": 0},\n        {\"3\": 5, \"5\": 3, \"6\": 0.56, 7\": 0, \"8\": 1},\n        ...\n    ]\n}\n```\n\nHere `slice` allows us to only select some columns in this matrix. For example, this could be useful in order to create a training set with labels, and a final test set without labels. The `step` or `sample` options could be added to scan and scroll in order to only retain documents at every x steps, or with x% chances. Again this could be useful in order to generate datasets.\n## Implementation Notes\n\nVectorizers would operate on top of term vectors, one field at a time. In fact, a vectorizer could be seen as specifying a different output format for term vectors. The Term Vectors API would have to be extended in order to return numerical values in the case a vectorizer is specified. By operating on top of term vectors, the implementation should be pretty straight forward, and we would also gain the other features of the API such as per field analyzers, on the fly term vectors creation, dfs or terms filtering. For scripting, a new term level scope would have to be introduced. The main difficulty seems to be in how to handle the in memory term-column dictionary.\n## Roadmap\n1. Make search also returns term vectors if specified.\n2. Make vectorizers as part of a term vectors request. This would be useful for debugging.\n3. Allow to store vectorizers that could then be referenced in term vectors request.\n4. Allow for term level scripting.\n5. Make vectorizers also operate on top of numerical fields.\n6. Add other options such as step, sample or slice.\n","closed_by":{"login":"alexksikes","id":43475,"node_id":"MDQ6VXNlcjQzNDc1","avatar_url":"https://avatars2.githubusercontent.com/u/43475?v=4","gravatar_id":"","url":"https://api.github.com/users/alexksikes","html_url":"https://github.com/alexksikes","followers_url":"https://api.github.com/users/alexksikes/followers","following_url":"https://api.github.com/users/alexksikes/following{/other_user}","gists_url":"https://api.github.com/users/alexksikes/gists{/gist_id}","starred_url":"https://api.github.com/users/alexksikes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexksikes/subscriptions","organizations_url":"https://api.github.com/users/alexksikes/orgs","repos_url":"https://api.github.com/users/alexksikes/repos","events_url":"https://api.github.com/users/alexksikes/events{/privacy}","received_events_url":"https://api.github.com/users/alexksikes/received_events","type":"User","site_admin":false},"performed_via_github_app":null}