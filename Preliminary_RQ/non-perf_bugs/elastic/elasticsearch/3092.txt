{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/3092","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3092/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3092/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3092/events","html_url":"https://github.com/elastic/elasticsearch/issues/3092","id":14755074,"node_id":"MDU6SXNzdWUxNDc1NTA3NA==","number":3092,"title":"Compression using a common LZW dictionary for the whole index or document types","user":{"login":"acmeguy","id":395419,"node_id":"MDQ6VXNlcjM5NTQxOQ==","avatar_url":"https://avatars0.githubusercontent.com/u/395419?v=4","gravatar_id":"","url":"https://api.github.com/users/acmeguy","html_url":"https://github.com/acmeguy","followers_url":"https://api.github.com/users/acmeguy/followers","following_url":"https://api.github.com/users/acmeguy/following{/other_user}","gists_url":"https://api.github.com/users/acmeguy/gists{/gist_id}","starred_url":"https://api.github.com/users/acmeguy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/acmeguy/subscriptions","organizations_url":"https://api.github.com/users/acmeguy/orgs","repos_url":"https://api.github.com/users/acmeguy/repos","events_url":"https://api.github.com/users/acmeguy/events{/privacy}","received_events_url":"https://api.github.com/users/acmeguy/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2013-05-25T08:37:55Z","updated_at":"2013-05-25T20:52:21Z","closed_at":"2013-05-25T10:29:14Z","author_association":"NONE","active_lock_reason":null,"body":"Hello,\n\nWe are storing massive amount of data which is quite redundant (documents do not vary greatly). For this reason I have been looking into the compression option of ES and see that individual documents or document in a bulk can be indexed together.\n\nThe drawback of the bulk method is that the whole bulk would needed to be uncompressed if a single document in the bulk is needed (as I understand it).\n\nAn interesting option for us would be the ability to use a single, growing, dictionary to compress all the _source documents. This would make result in a bigger LZW dictionary, but it would also mean a better compression ratio and the ability to uncompress single documents.\n\nIs this something which has been considered?\n\nVery best regards,\n  -Stefan Baxter\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}