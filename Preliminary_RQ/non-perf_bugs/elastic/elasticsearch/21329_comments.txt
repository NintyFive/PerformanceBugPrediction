[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/258425338","html_url":"https://github.com/elastic/elasticsearch/issues/21329#issuecomment-258425338","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21329","id":258425338,"node_id":"MDEyOklzc3VlQ29tbWVudDI1ODQyNTMzOA==","user":{"login":"barton-im","id":4304093,"node_id":"MDQ6VXNlcjQzMDQwOTM=","avatar_url":"https://avatars0.githubusercontent.com/u/4304093?v=4","gravatar_id":"","url":"https://api.github.com/users/barton-im","html_url":"https://github.com/barton-im","followers_url":"https://api.github.com/users/barton-im/followers","following_url":"https://api.github.com/users/barton-im/following{/other_user}","gists_url":"https://api.github.com/users/barton-im/gists{/gist_id}","starred_url":"https://api.github.com/users/barton-im/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/barton-im/subscriptions","organizations_url":"https://api.github.com/users/barton-im/orgs","repos_url":"https://api.github.com/users/barton-im/repos","events_url":"https://api.github.com/users/barton-im/events{/privacy}","received_events_url":"https://api.github.com/users/barton-im/received_events","type":"User","site_admin":false},"created_at":"2016-11-04T13:03:51Z","updated_at":"2016-11-04T13:06:05Z","author_association":"NONE","body":"So, I have managed to bring my data back. However a lot of hoops to jump through. I have investigated the log on the master and found some traces of the problematic shards, notably this line was repeating:\n\n`[....] not allocating, number_of_allocated_shards_found [0], required_number [1]\n`\nnote that this line appeared only on the master node, not on the node that actually carried the data. After few google searches I stumbled upon a setting for a cluster/index - index.recovery.initial_shards that allows to do recovery only in case a predefined number of replicas are already present. In my case, I needed that number to be 0 since none of the shard replicas was active (sometimes I had the index replicated 2x sometimes the replication was 0), in any case, I used the change setting API and set this to 0 on one testing index. It assigned the shard immediately, so I started changing this setting for other indices, one by one. However, on some of the indices the assign attempt was failing and the shards were going in loop from unassigned, to assign attempt on random nodes. I have tried to close and open the index bu that didnt help. The only thing that finally help to assign the most problematic shards was to locate the node where the data was physically stored and restart that node. \n\nSo my problem is solved, however, some lessons could be learned from this exercize: the ES sometimes gets into incosistent state during allocations and there is (at least not to my knowledge) much tools to debug that. So I hope this will be fixed in some future versions (I know that there has been a long way already gone since I didnt have to do anything with the physical files which was needed to troubleshoot some problems with teh pre-1 versions).\n\nThe error that reported by ES while assigning the most problematic shards was:\n\n```\nsending shard started for [XXXXX][2], node[zMucPFz0RJuhsSVqgE6jJg], [P], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAI\nLED], at[2016-11-04T12:48:58.087Z], details[shard failure [failed recovery][IndexShardGatewayRecoveryException[[XXXXX][2] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecov\neryException[[large-product-db-nm-20160518][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(\ndefault(mmapfs(XXXXX),niofs(XXXXX)), type=MERGE, rate=20.0), rate_limited(defau\nlt(mmapfs(XXXXX),niofs(XXXXX)), type=MERGE, rate=20.0)]): files: []]; ]]], inde\nxUUID [ogg1XgLjTzyWlgeX56uycw], reason [after recovery from gateway]\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/258436259","html_url":"https://github.com/elastic/elasticsearch/issues/21329#issuecomment-258436259","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21329","id":258436259,"node_id":"MDEyOklzc3VlQ29tbWVudDI1ODQzNjI1OQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-11-04T13:51:29Z","updated_at":"2016-11-04T13:51:29Z","author_association":"CONTRIBUTOR","body":"Sorry that you've had trouble with shard assignment\n\n> So I hope this will be fixed in some future versions \n\nYou should upgrade to 5.0 - so much has been fixed since 1.7\n","performed_via_github_app":null}]