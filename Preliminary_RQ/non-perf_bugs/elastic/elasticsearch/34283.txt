{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/34283","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34283/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34283/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34283/events","html_url":"https://github.com/elastic/elasticsearch/issues/34283","id":366510592,"node_id":"MDU6SXNzdWUzNjY1MTA1OTI=","number":34283,"title":"Nori (Korean) analyzer tokenization issues","user":{"login":"Trey314159","id":13836921,"node_id":"MDQ6VXNlcjEzODM2OTIx","avatar_url":"https://avatars0.githubusercontent.com/u/13836921?v=4","gravatar_id":"","url":"https://api.github.com/users/Trey314159","html_url":"https://github.com/Trey314159","followers_url":"https://api.github.com/users/Trey314159/followers","following_url":"https://api.github.com/users/Trey314159/following{/other_user}","gists_url":"https://api.github.com/users/Trey314159/gists{/gist_id}","starred_url":"https://api.github.com/users/Trey314159/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Trey314159/subscriptions","organizations_url":"https://api.github.com/users/Trey314159/orgs","repos_url":"https://api.github.com/users/Trey314159/repos","events_url":"https://api.github.com/users/Trey314159/events{/privacy}","received_events_url":"https://api.github.com/users/Trey314159/received_events","type":"User","site_admin":false},"labels":[{"id":142001965,"node_id":"MDU6TGFiZWwxNDIwMDE5NjU=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Analysis","name":":Search/Analysis","color":"0e8a16","default":false,"description":"How text is split into tokens"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2018-10-03T20:18:18Z","updated_at":"2018-12-03T15:14:16Z","closed_at":"2018-12-03T10:15:14Z","author_association":"NONE","active_lock_reason":null,"body":"It makes sense to me to report these all together, but I can split these into separate bugs if that's better.\r\n\r\n<!-- Bug report -->\r\n\r\n**Elasticsearch version** (`curl -XGET 'localhost:9200'`):\r\n```\r\n{\r\n  \"name\" : \"adOS8gy\",\r\n  \"cluster_name\" : \"elasticsearch\",\r\n  \"cluster_uuid\" : \"GVS7gpVBQDGwtHl3xnJbLw\",\r\n  \"version\" : {\r\n    \"number\" : \"6.4.0\",\r\n    \"build_flavor\" : \"default\",\r\n    \"build_type\" : \"deb\",\r\n    \"build_hash\" : \"595516e\",\r\n    \"build_date\" : \"2018-08-17T23:18:47.308994Z\",\r\n    \"build_snapshot\" : false,\r\n    \"lucene_version\" : \"7.4.0\",\r\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\r\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\r\n  },\r\n  \"tagline\" : \"You Know, for Search\"\r\n}\r\n```\r\n\r\n**Plugins installed**: [analysis-icu, analysis-nori]\r\n\r\n**JVM version** (`java -version`):\r\nopenjdk version \"1.8.0_181\"\r\nOpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-1~deb9u1-b13)\r\nOpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\r\n\r\n**OS version** (`uname -a` if on a Unix-like system):\r\nLinux vagrantes6 4.9.0-6-amd64 #1 SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\nI've uncovered a number of oddities in tokenization in the Nori analyzer. All examples are from [Korean Wikipedia](https://ko.wikipedia.org) or [Korean Wiktionary](https://ko.wiktionary.org) (including non-CJK examples). In rough order of importance:\r\n\r\nA. Tokens are split on different character POS types (which seem to not quite line up with Unicode character blocks), which leads to weird results for non-CJK tokens:\r\n* `εἰμί` is tokenized as three tokens: `ε/SL(Foreign language) + ἰ/SY(Other symbol) + μί/SL(Foreign language)`\r\n* `ka̠k̚t͡ɕ͈a̠k̚` is tokenized as `ka/SL(Foreign language) + ̠/SY(Other symbol) + k/SL(Foreign language) + ̚/SY(Other symbol) + t/SL(Foreign language) + ͡ɕ͈/SY(Other symbol) + a/SL(Foreign language) + ̠/SY(Other symbol) + k/SL(Foreign language) + ̚/SY(Other symbol)`\r\n* `Ба̀лтичко̄` is tokenized as `ба/SL(Foreign language) + ̀/SY(Other symbol) + лтичко/SL(Foreign language) + ̄/SY(Other symbol)`\r\n* `don't` is tokenized as `don + t`; same for `don’t` (with a curly apostrophe).\r\n* `אוֹג׳וּ` is tokenized as `אוֹג/SY(Other symbol) + וּ/SY(Other symbol)`\r\n* `Мoscow` (with a Cyrillic М and the rest in Latin) is tokenized as `м + oscow`\r\n\r\nWhile it is still possible to find these words using Nori, there are many more chances for false positives when the tokens are split up like this. In particular, individual numbers and combining diacritics are indexed separately (e.g., the `/` in the Cyrillic example above), which can lead to a performance hit on large corpora like Wiktionary or Wikipedia.\r\n\r\nWork around: use a character filter to get rid of combining diacritics before Nori processes the text. This doesn't solve the Greek, Hebrew, or English cases, though.\r\n\r\nSuggested fix: Characters in related Unicode blocks—like \"Greek\" and \"Greek Extended\", or \"Latin\" and \"IPA Extensions\"—should not trigger token splits. Combining diacritics should not trigger token splits. Non-CJK text should be tokenized on spaces and punctuation, not by character type shifts. Apostrophe-like characters should not trigger token splits (though I could see someone disagreeing on this one). \r\n\r\nB. The character \"arae-a\" (ㆍ, U+318D) is sometimes used instead of a middle dot (·, U+00B7) for [lists](https://en.wikipedia.org/wiki/Korean_punctuation#Differences_from_European_punctuation). When the arae-a is used, everything after the first one ends up in one giant token. `도로ㆍ지반ㆍ수자원ㆍ건설환경ㆍ건축ㆍ화재설비연구` is tokenized as `도로 + ㆍ지반ㆍ수자원ㆍ건설환경ㆍ건축ㆍ화재설비연구`.\r\n* Note that \"HANGUL LETTER ARAEA\" (ㆍ, U+318D) is used this way, while \"HANGUL JUNGSEONG ARAEA\" (ᆞ, U+119E) is used to create syllable blocks for which there is no precomposed Unicode character.\r\n\r\nWork around: use a character filter to convert arae-a (U+318D) to a space.\r\n\r\nSuggested fix: split tokens on all instances of arae-a (U+318D).\r\n\r\nC. Nori splits tokens on soft hyphens (U+00AD) and zero-width non-joiners (U+200C), splitting tokens that should not be split.\r\n* `hyphen­ation` is tokenized as `hyphen + ation`.\r\n* `بازی‌های ` is tokenized as `بازی + های`.\r\n\r\nWork around: use a character filter to strip soft hyphens and zero-width non-joiners before Nori.\r\n\r\nSuggested fix: Nori should strip soft hyphens and zero-width non-joiners.\r\n\r\nD. Analyzing 그레이맨 generates an extra empty token after it. There may be others, but this is the only one I've found. Work around: at a min length token filter with a minimum length of 1.\r\n\r\nE. Analyzing 튜토리얼 generates a token with an extra space at the end of it. There may be others, but this is the only one I've found. No work around needed, I guess, since this is only the internal representation of the token. I'm not sure if it has any negative effects.\r\n\r\n\r\n**Steps to reproduce**:\r\n\r\nPlease include a *minimal* but *complete* recreation of the problem, including\r\n(e.g.) index creation, mappings, settings, query etc.  The easier you make for\r\nus to reproduce it, the more likely that somebody will take the time to look at it.\r\n\r\n 1. Set up Nori analyzer\r\n```\r\ncurl -X PUT \"localhost:9200/nori?pretty\" -H 'Content-Type: application/json' -d'\r\n{\r\n  \"settings\" : {\r\n    \"index\": {\r\n      \"analysis\": {\r\n        \"analyzer\": {\r\n          \"text\": {\r\n            \"type\": \"nori\"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n'\r\n```\r\n 2. Analyze example tokens:\r\n\r\nA. POS Types cause token splits\r\n```\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"εἰμί\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"ε\",\r\n          \"start_offset\" : 0,\r\n          \"end_offset\" : 1,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"ἰ\",\r\n          \"start_offset\" : 1,\r\n          \"end_offset\" : 2,\r\n          \"type\" : \"word\",\r\n          \"position\" : 1,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        },\r\n        {\r\n          \"token\" : \"μί\",\r\n          \"start_offset\" : 2,\r\n          \"end_offset\" : 4,\r\n          \"type\" : \"word\",\r\n          \"position\" : 2,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n\r\n\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"ka̠k̚t͡ɕ͈a̠k̚\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"ka\",\r\n          \"start_offset\" : 0,\r\n          \"end_offset\" : 2,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"̠\",\r\n          \"start_offset\" : 2,\r\n          \"end_offset\" : 3,\r\n          \"type\" : \"word\",\r\n          \"position\" : 1,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        },\r\n        {\r\n          \"token\" : \"k\",\r\n          \"start_offset\" : 3,\r\n          \"end_offset\" : 4,\r\n          \"type\" : \"word\",\r\n          \"position\" : 2,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"̚\",\r\n          \"start_offset\" : 4,\r\n          \"end_offset\" : 5,\r\n          \"type\" : \"word\",\r\n          \"position\" : 3,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        },\r\n        {\r\n          \"token\" : \"t\",\r\n          \"start_offset\" : 5,\r\n          \"end_offset\" : 6,\r\n          \"type\" : \"word\",\r\n          \"position\" : 4,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"͡ɕ͈\",\r\n          \"start_offset\" : 6,\r\n          \"end_offset\" : 9,\r\n          \"type\" : \"word\",\r\n          \"position\" : 5,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        },\r\n        {\r\n          \"token\" : \"a\",\r\n          \"start_offset\" : 9,\r\n          \"end_offset\" : 10,\r\n          \"type\" : \"word\",\r\n          \"position\" : 6,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"̠\",\r\n          \"start_offset\" : 10,\r\n          \"end_offset\" : 11,\r\n          \"type\" : \"word\",\r\n          \"position\" : 7,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        },\r\n        {\r\n          \"token\" : \"k\",\r\n          \"start_offset\" : 11,\r\n          \"end_offset\" : 12,\r\n          \"type\" : \"word\",\r\n          \"position\" : 8,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"̚\",\r\n          \"start_offset\" : 12,\r\n          \"end_offset\" : 13,\r\n          \"type\" : \"word\",\r\n          \"position\" : 9,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"Ба̀лтичко̄\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"ба\",\r\n          \"start_offset\" : 0,\r\n          \"end_offset\" : 2,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"̀\",\r\n          \"start_offset\" : 2,\r\n          \"end_offset\" : 3,\r\n          \"type\" : \"word\",\r\n          \"position\" : 1,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        },\r\n        {\r\n          \"token\" : \"лтичко\",\r\n          \"start_offset\" : 3,\r\n          \"end_offset\" : 9,\r\n          \"type\" : \"word\",\r\n          \"position\" : 2,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"̄\",\r\n          \"start_offset\" : 9,\r\n          \"end_offset\" : 10,\r\n          \"type\" : \"word\",\r\n          \"position\" : 3,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"don'\"'\"'t\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"don\",\r\n          \"start_offset\" : 0,\r\n          \"end_offset\" : 3,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"t\",\r\n          \"start_offset\" : 4,\r\n          \"end_offset\" : 5,\r\n          \"type\" : \"word\",\r\n          \"position\" : 1,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"don’t\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"don\",\r\n          \"start_offset\" : 0,\r\n          \"end_offset\" : 3,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"t\",\r\n          \"start_offset\" : 4,\r\n          \"end_offset\" : 5,\r\n          \"type\" : \"word\",\r\n          \"position\" : 1,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"אוֹג׳וּ\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"אוֹג\",\r\n          \"start_offset\" : 0,\r\n          \"end_offset\" : 4,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        },\r\n        {\r\n          \"token\" : \"וּ\",\r\n          \"start_offset\" : 5,\r\n          \"end_offset\" : 7,\r\n          \"type\" : \"word\",\r\n          \"position\" : 1,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nB. arae-a as middle dot\r\n\r\n```\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"도로ㆍ지반ㆍ수자원ㆍ건설환경ㆍ건축ㆍ화재설비연구\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"도로\",\r\n          \"start_offset\" : 0,\r\n          \"end_offset\" : 2,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"NNG(General Noun)\"\r\n        },\r\n        {\r\n          \"token\" : \"ㆍ지반ㆍ수자원ㆍ건설환경ㆍ건축ㆍ화재설비연구\",\r\n          \"start_offset\" : 2,\r\n          \"end_offset\" : 24,\r\n          \"type\" : \"word\",\r\n          \"position\" : 1,\r\n          \"leftPOS\" : \"UNKNOWN(Unknown)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nC. soft hyphens and zero-width non-joiners\r\n\r\n```\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"hyphen­ation\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"hyphen\",\r\n          \"start_offset\" : 0,\r\n          \"end_offset\" : 6,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        },\r\n        {\r\n          \"token\" : \"ation\",\r\n          \"start_offset\" : 7,\r\n          \"end_offset\" : 12,\r\n          \"type\" : \"word\",\r\n          \"position\" : 1,\r\n          \"leftPOS\" : \"SL(Foreign language)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"بازی‌های\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"بازی\",\r\n          \"start_offset\" : 0,\r\n          \"end_offset\" : 4,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        },\r\n        {\r\n          \"token\" : \"های\",\r\n          \"start_offset\" : 5,\r\n          \"end_offset\" : 8,\r\n          \"type\" : \"word\",\r\n          \"position\" : 1,\r\n          \"leftPOS\" : \"SY(Other symbol)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\nD. 그레이맨 generates empty token\r\n\r\n```\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"그레이맨\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"그레이\",\r\n          \"start_offset\" : 1,\r\n          \"end_offset\" : 4,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"NNG(General Noun)\"\r\n        },\r\n        {\r\n          \"token\" : \"\",\r\n          \"start_offset\" : 4,\r\n          \"end_offset\" : 4,\r\n          \"type\" : \"word\",\r\n          \"position\" : 1,\r\n          \"leftPOS\" : \"NNG(General Noun)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nE. 튜토리얼 has a space added during tokenization\r\n\r\n```\r\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"튜토리얼\", \"attributes\" : [\"leftPOS\"], \"explain\": true }'\r\n\r\n{\r\n  \"detail\" : {\r\n    \"custom_analyzer\" : false,\r\n    \"analyzer\" : {\r\n      \"name\" : \"text\",\r\n      \"tokens\" : [\r\n        {\r\n          \"token\" : \"튜토리얼 \",\r\n          \"start_offset\" : 0,\r\n          \"end_offset\" : 4,\r\n          \"type\" : \"word\",\r\n          \"position\" : 0,\r\n          \"leftPOS\" : \"NNG(General Noun)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n\r\n","closed_by":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"performed_via_github_app":null}