[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/491579387","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-491579387","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":491579387,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5MTU3OTM4Nw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-05-12T09:23:30Z","updated_at":"2019-05-12T09:23:30Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/491580386","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-491580386","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":491580386,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5MTU4MDM4Ng==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2019-05-12T09:39:37Z","updated_at":"2019-05-12T09:39:37Z","author_association":"CONTRIBUTOR","body":"> too many shard-started events due to fluctuating nodes\r\n\r\nFluctuating nodes sounds like the more fundamental problem here. Why is this happening?\r\n\r\nIf your cluster is unstable then I would expect performance issues such as the one you describe, and although the performance characteristics would be _different_ with the change you propose I don't think they would necessarily be _better_. Starting shards in a timely fashion is quite important.\r\n\r\nThe consequence of delaying a mapping update should be a _bounded_ amount of buffering on the coordinator; if this bound is exceeded then the coordinator should start to reject requests. Is this what you're seeing?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/491581078","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-491581078","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":491581078,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5MTU4MTA3OA==","user":{"login":"Bukhtawar","id":12809319,"node_id":"MDQ6VXNlcjEyODA5MzE5","avatar_url":"https://avatars0.githubusercontent.com/u/12809319?v=4","gravatar_id":"","url":"https://api.github.com/users/Bukhtawar","html_url":"https://github.com/Bukhtawar","followers_url":"https://api.github.com/users/Bukhtawar/followers","following_url":"https://api.github.com/users/Bukhtawar/following{/other_user}","gists_url":"https://api.github.com/users/Bukhtawar/gists{/gist_id}","starred_url":"https://api.github.com/users/Bukhtawar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Bukhtawar/subscriptions","organizations_url":"https://api.github.com/users/Bukhtawar/orgs","repos_url":"https://api.github.com/users/Bukhtawar/repos","events_url":"https://api.github.com/users/Bukhtawar/events{/privacy}","received_events_url":"https://api.github.com/users/Bukhtawar/received_events","type":"User","site_admin":false},"created_at":"2019-05-12T09:50:07Z","updated_at":"2019-05-12T09:57:03Z","author_association":"CONTRIBUTOR","body":"By fluctuating nodes it could be a cluster scale up/down/zone changes that could also cause this essentially slowing down throughput. As far as the memory on the coordinator node is concerned here is another problem #35564. Changing the priority helps both the cases although they both would need their own specialized solution . Do you think there are major gains in keeping the priority `HIGH` other than lesser `URGENT` events. Also in general marking a shard started during relocation(cluster health GREEN all through) should it have greater precedence than a task that slows down ingestion","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/491583605","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-491583605","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":491583605,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5MTU4MzYwNQ==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2019-05-12T10:31:33Z","updated_at":"2019-05-12T10:31:33Z","author_association":"CONTRIBUTOR","body":"> By fluctuating nodes it could be a cluster scale up/down/zone changes that could also cause this \r\n\r\nI would not expect scaling the cluster up or down to swamp the master with `shard-started` tasks as you describe. I would expect mechanisms like `cluster.routing.allocation.node_concurrent_recoveries` to limit the number of such tasks happening at any one time, unless you have disabled this mechanism. Can you describe the scaling event in more detail to clarify the sequence of events that lead to this problem?\r\n\r\n> As far as the memory on the coordinator node is concerned here is another problem #35564. \r\n\r\nI think that issue is now mis-titled, because I expect the coordinating node not to run out of memory in such a situation in recent versions. That issue is making _failed_ put-mapping calls fail faster, which would cause bulk rejections sooner. That doesn't sound like what you want. I've adjusted the title of the linked issue.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/491588164","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-491588164","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":491588164,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5MTU4ODE2NA==","user":{"login":"Bukhtawar","id":12809319,"node_id":"MDQ6VXNlcjEyODA5MzE5","avatar_url":"https://avatars0.githubusercontent.com/u/12809319?v=4","gravatar_id":"","url":"https://api.github.com/users/Bukhtawar","html_url":"https://github.com/Bukhtawar","followers_url":"https://api.github.com/users/Bukhtawar/followers","following_url":"https://api.github.com/users/Bukhtawar/following{/other_user}","gists_url":"https://api.github.com/users/Bukhtawar/gists{/gist_id}","starred_url":"https://api.github.com/users/Bukhtawar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Bukhtawar/subscriptions","organizations_url":"https://api.github.com/users/Bukhtawar/orgs","repos_url":"https://api.github.com/users/Bukhtawar/repos","events_url":"https://api.github.com/users/Bukhtawar/events{/privacy}","received_events_url":"https://api.github.com/users/Bukhtawar/received_events","type":"User","site_admin":false},"created_at":"2019-05-12T11:41:40Z","updated_at":"2019-05-12T12:02:21Z","author_association":"CONTRIBUTOR","body":"There is no notion of `cluster_concurrent_recoveries` to limit the number of relocation across the cluster which is what impacts the master espl when the cluster has more number of nodes typically around 50-100 and upto 10-15k shards\r\nYou are right the issue doesn't exactly correlate to the proposed solution(point is not queue it up for master unless needed) but makes sense till that is sorted out and hence the reference.\r\nThere are two major concerns here with the priority of the task due to pending tasks at the master\r\n1. additional memory pressure on coordinating node \r\n2. ingestion slow down. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/498965929","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-498965929","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":498965929,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5ODk2NTkyOQ==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2019-06-05T07:10:41Z","updated_at":"2019-06-05T07:10:41Z","author_association":"CONTRIBUTOR","body":"I don't understand why `cluster.routing.allocation.node_concurrent_recoveries` doesn't help here. With 100 nodes there should be at most 200 concurrent recoveries by default, across the cluster, and the master should be able to handle this easily. We need more information about the specific situation you're talking about here, ideally with logs or other data showing the details of the problem.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/499043868","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-499043868","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":499043868,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5OTA0Mzg2OA==","user":{"login":"Bukhtawar","id":12809319,"node_id":"MDQ6VXNlcjEyODA5MzE5","avatar_url":"https://avatars0.githubusercontent.com/u/12809319?v=4","gravatar_id":"","url":"https://api.github.com/users/Bukhtawar","html_url":"https://github.com/Bukhtawar","followers_url":"https://api.github.com/users/Bukhtawar/followers","following_url":"https://api.github.com/users/Bukhtawar/following{/other_user}","gists_url":"https://api.github.com/users/Bukhtawar/gists{/gist_id}","starred_url":"https://api.github.com/users/Bukhtawar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Bukhtawar/subscriptions","organizations_url":"https://api.github.com/users/Bukhtawar/orgs","repos_url":"https://api.github.com/users/Bukhtawar/repos","events_url":"https://api.github.com/users/Bukhtawar/events{/privacy}","received_events_url":"https://api.github.com/users/Bukhtawar/received_events","type":"User","site_admin":false},"created_at":"2019-06-05T11:22:24Z","updated_at":"2019-06-05T11:22:24Z","author_association":"CONTRIBUTOR","body":"Typically not all the 200 shards start at the same time(due to varying sizes) and once one shard does start, master queues up a `shard-started` event. The task is processed by master in batches and there could be multiple such batches owing to the different times at which they get queued up(say some shards start after master starts processing the batch of shards already started).\r\nNow coming to the processing of a single batch. The complexity of a single allocation cycle(per batch) is roughly O(#shards) and here is a set of [benchmarks](https://github.com/elastic/elasticsearch/pull/27628#issuecomment-355707665) I ran whilst [working](https://github.com/elastic/elasticsearch/pull/33329) on improving the time with @ywelsch. The tp90 for single round of allocation for 10k shards came around few seconds and with 30k shards it is around 15-20s.\r\nNow with back to back batches of `URGENT` shard-started events the mapping update would need to be put on hold.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/499057203","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-499057203","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":499057203,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5OTA1NzIwMw==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2019-06-05T12:12:01Z","updated_at":"2019-06-05T12:12:01Z","author_association":"CONTRIBUTOR","body":"Ok I think I see. You seem to be saying that the master is receiving a stream of relatively few `shard-started` events over a longer period of time, which is starving the mapping updates of time on the master. Am I understanding this correctly?\r\n\r\nIf so, I don't think adjusting the priority of mapping updates will solve this problem. Even if mapping update tasks were top priority they'd still have to occasionally wait for a lower-priority task time since there is no pre-emption mechanism. If they have equal priority then you would still see starvation occurring because the tasks aren't executed fairly.\r\n\r\nI also think that increasing the priority of things is the wrong way to address this kind of performance issue. The logical conclusion of that line of thinking is that everything becomes top priority, but then nothing works. Instead, I think you should look at the following areas:\r\n\r\n- your cluster is generating too many `shard-started` tasks because of fluctuating nodes. Why are they fluctuating?\r\n\r\n- you are using dynamic mapping updates, which will always be a scaling bottleneck since they require the involvement of the master.\r\n\r\n- your ingest process cannot cope with back-pressure from Elasticsearch.\r\n\r\n- allocation is slower than you would like on clusters like yours with tens of thousands of shards. You suggested some ideas for improvements there but it seems that they haven't been enough. Would you continue working on this area?\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/499067432","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-499067432","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":499067432,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5OTA2NzQzMg==","user":{"login":"Bukhtawar","id":12809319,"node_id":"MDQ6VXNlcjEyODA5MzE5","avatar_url":"https://avatars0.githubusercontent.com/u/12809319?v=4","gravatar_id":"","url":"https://api.github.com/users/Bukhtawar","html_url":"https://github.com/Bukhtawar","followers_url":"https://api.github.com/users/Bukhtawar/followers","following_url":"https://api.github.com/users/Bukhtawar/following{/other_user}","gists_url":"https://api.github.com/users/Bukhtawar/gists{/gist_id}","starred_url":"https://api.github.com/users/Bukhtawar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Bukhtawar/subscriptions","organizations_url":"https://api.github.com/users/Bukhtawar/orgs","repos_url":"https://api.github.com/users/Bukhtawar/repos","events_url":"https://api.github.com/users/Bukhtawar/events{/privacy}","received_events_url":"https://api.github.com/users/Bukhtawar/received_events","type":"User","site_admin":false},"created_at":"2019-06-05T12:45:30Z","updated_at":"2019-06-05T12:45:30Z","author_association":"CONTRIBUTOR","body":">Ok I think I see. You seem to be saying that the master is receiving a stream of relatively few shard-started events over a longer period of time, which is starving the mapping updates of time on the master. Am I understanding this correctly?\r\n\r\nYes that seems correct\r\n\r\n>If so, I don't think adjusting the priority of mapping updates will solve this problem. Even if mapping update tasks were top priority they'd still have to occasionally wait for a lower-priority task time since there is no pre-emption mechanism. If they have equal priority then you would still see starvation occurring because the tasks aren't executed fairly.\r\n\r\nI understand there is no pre-emption, but the mapping update in the midst of streams of `shard-started` event will not need to be put on hold. It would be served once the ongoing cycle of shard allocation completes instead of waiting on the storm of events to finish. There is a significant time gap here.\r\n\r\n>I also think that increasing the priority of things is the wrong way to address this kind of performance issue. The logical conclusion of that line of thinking is that everything becomes top priority, but then nothing works. Instead, I think you should look at the following areas:\r\n\r\nIMHO shard relocation should have relatively low priority as compared to mapping updates espl given that these mapping updates are directly impacting client traffic and that these updates are relatively faster and cheaper than `shard-started`. Anything that slows down indexing should be top priority and bumping up the priority has value in cutting down the wait time in queue.\r\n\r\nHaving said that fluctuating node was just one scenario, AZ/hardware changes are the most common scenarios. Working on the remaining part of the optimization is on my radar will get back on that. The optimization wouldn't cover all cases and worst case would still be few seconds per iteration\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/501214486","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-501214486","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":501214486,"node_id":"MDEyOklzc3VlQ29tbWVudDUwMTIxNDQ4Ng==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2019-06-12T10:28:53Z","updated_at":"2019-06-12T10:28:53Z","author_association":"CONTRIBUTOR","body":"> It would be served once the ongoing cycle of shard allocation completes instead of waiting on the storm of events to finish\r\n\r\nThis is not true, because (as I said) the enqueued tasks aren't executed fairly.\r\n\r\n> Anything that slows down indexing should be top priority\r\n\r\nI do not share this opinion. Starting shards promptly is vital for resilience, and I think it is correct to prioritise resilience over indexing performance as we do today. I will re-iterate that the thing that's slowing your indexing down is your use of dynamic mapping updates, which means that your indexing process needs the master's involvement. Solve that, and the problem you describe here will go away.\r\n\r\nWe've discussed this internally and concluded that adjusting the priorities as suggested here is not the right thing to do, so I'm closing this.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/509349991","html_url":"https://github.com/elastic/elasticsearch/issues/42105#issuecomment-509349991","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/42105","id":509349991,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTM0OTk5MQ==","user":{"login":"Bukhtawar","id":12809319,"node_id":"MDQ6VXNlcjEyODA5MzE5","avatar_url":"https://avatars0.githubusercontent.com/u/12809319?v=4","gravatar_id":"","url":"https://api.github.com/users/Bukhtawar","html_url":"https://github.com/Bukhtawar","followers_url":"https://api.github.com/users/Bukhtawar/followers","following_url":"https://api.github.com/users/Bukhtawar/following{/other_user}","gists_url":"https://api.github.com/users/Bukhtawar/gists{/gist_id}","starred_url":"https://api.github.com/users/Bukhtawar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Bukhtawar/subscriptions","organizations_url":"https://api.github.com/users/Bukhtawar/orgs","repos_url":"https://api.github.com/users/Bukhtawar/repos","events_url":"https://api.github.com/users/Bukhtawar/events{/privacy}","received_events_url":"https://api.github.com/users/Bukhtawar/received_events","type":"User","site_admin":false},"created_at":"2019-07-08T18:58:44Z","updated_at":"2019-07-08T18:58:44Z","author_association":"CONTRIBUTOR","body":">Starting shards promptly is vital for resilience\r\n\r\nI think it's of importance for unassigned shards but does this hold good for shards already assigned. The point here is why should indexing performance suffer if say a `rebalance` was underway. \r\n","performed_via_github_app":null}]