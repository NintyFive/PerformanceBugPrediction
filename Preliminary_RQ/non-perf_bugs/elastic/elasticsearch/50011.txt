{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/50011","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50011/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50011/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50011/events","html_url":"https://github.com/elastic/elasticsearch/issues/50011","id":535578782,"node_id":"MDU6SXNzdWU1MzU1Nzg3ODI=","number":50011,"title":"It have a high Disk occupancy for updateByQuery frequently","user":{"login":"yida-lxw","id":12723117,"node_id":"MDQ6VXNlcjEyNzIzMTE3","avatar_url":"https://avatars3.githubusercontent.com/u/12723117?v=4","gravatar_id":"","url":"https://api.github.com/users/yida-lxw","html_url":"https://github.com/yida-lxw","followers_url":"https://api.github.com/users/yida-lxw/followers","following_url":"https://api.github.com/users/yida-lxw/following{/other_user}","gists_url":"https://api.github.com/users/yida-lxw/gists{/gist_id}","starred_url":"https://api.github.com/users/yida-lxw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yida-lxw/subscriptions","organizations_url":"https://api.github.com/users/yida-lxw/orgs","repos_url":"https://api.github.com/users/yida-lxw/repos","events_url":"https://api.github.com/users/yida-lxw/events{/privacy}","received_events_url":"https://api.github.com/users/yida-lxw/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-12-10T08:45:04Z","updated_at":"2019-12-10T13:10:17Z","closed_at":"2019-12-10T13:04:47Z","author_association":"NONE","active_lock_reason":null,"body":"when I add a new extension word into the IKAnalyzer, and then I save them into the MYSQL DB, I also send a message to the RabbitMQ at the same time. Then another service will consume the message and work with the updateByQuery method to update the Elasticsearch Index Data. But It will hit a high Disk occupancy  after adding a  extension word. because we need to update at lease 1000 document which related to the word.  the amount of instantaneous updates to the index will be very large If I add words at a slightly faster rate, and the number of segment files will continue to go up.but we can't merge the segments frequently. So I was caught in conflict. Is there any way to fit the scene of updating frequently in elasticsearch,and it is not affected to the Query latency at the same time?\r\nHere are some code fragments I used in my projectï¼š\r\n```BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\r\n        for (String keyword:keywordList) {\r\n            boolQueryBuilder.should(QueryBuilders.matchPhraseQuery(\"searchkeyword\", keyword)\r\n                    .slop(Integer.parseInt(CentralConfig.getProperty(\"index.refresh.index_keyword.slop\",\"0\"))));\r\n        }\r\n        boolQueryBuilder.minimumShouldMatch(1);\r\n\r\n        Map<String,List<String>> refreshMap = Maps.newHashMap();\r\n\r\n        SearchResponse response = client.prepareSearch(Const.INDEX_KEYWORD)\r\n                .setSize(1000).setQuery(boolQueryBuilder).setFetchSource(new String[]{\"refid\", \"type\"}, null).setScroll(\"10m\")\r\n                .execute().actionGet();\r\n\r\n        long totalHits = response.getHits().getHits().length;\r\n        String scrollId = response.getScrollId();\r\n        while (totalHits > 0) {\r\n            putRefreshId(response,refreshMap);\r\n            SearchResponse subResponse = client.prepareSearchScroll(scrollId).setScroll(\"10m\").execute().actionGet();\r\n            totalHits = subResponse.getHits().getHits().length;\r\n            scrollId = subResponse.getScrollId();\r\n            response=subResponse;\r\n        }\r\n       \r\n        if (MapUtils.isEmpty(refreshMap)){\r\n            return;\r\n        }\r\n       //refresh the index by UpdateByQuery\r\n        for (Map.Entry<String,List<String>> entry : refreshMap.entrySet()){\r\n            BulkByScrollResponse updateResponse = UpdateByQueryAction.INSTANCE.\r\n                    newRequestBuilder(client).source(entry.getKey())\r\n                    .abortOnVersionConflict(true)\r\n                    .execute()\r\n                    .actionGet();\r\n            LOGGER.info(\"update index : {} , refresh size : {}\",entry.getKey(),updateResponse.getUpdated());```","closed_by":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"performed_via_github_app":null}