[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/60910336","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-60910336","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":60910336,"node_id":"MDEyOklzc3VlQ29tbWVudDYwOTEwMzM2","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-10-29T11:51:19Z","updated_at":"2014-10-29T11:51:19Z","author_association":"MEMBER","body":"> Maybe a separate improvement we could make here is to not bother caching a filter that matched 0 docs? Such filters are usually (?) fast to re-execute...\n\nThat depends on how much effort it takes to come to that conclusion. Say a geo filter with an arc distance, or cached boolean clause etc.  Maybe we can make it filter dependent like a term filter with 0 hits is not cached but others like Range Terms might still be worth it? I'm not sure we need this complexity... \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/60912121","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-60912121","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":60912121,"node_id":"MDEyOklzc3VlQ29tbWVudDYwOTEyMTIx","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-10-29T12:07:08Z","updated_at":"2014-10-29T12:07:08Z","author_association":"CONTRIBUTOR","body":"What about just limiting the number of filters that can be added to the cache?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/60912632","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-60912632","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":60912632,"node_id":"MDEyOklzc3VlQ29tbWVudDYwOTEyNjMy","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2014-10-29T12:11:46Z","updated_at":"2014-10-29T12:11:46Z","author_association":"CONTRIBUTOR","body":"Agreed with Clinton. I think part of the bug is that a filter cache should never reach 5.5M entries: it only makes sense to cache filters if they are going to be reused, otherwise the caching logic is going to add overhead by consuming all documents (as opposed to using skipping in the case of a conjunction) and increasing memory pressure by promoting object to the old gen.\n\nSo maybe the fix is to allow to configure an absolute maximum size on the filter cache (with a reasonable value) and to cache filters less aggressively?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/60997835","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-60997835","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":60997835,"node_id":"MDEyOklzc3VlQ29tbWVudDYwOTk3ODM1","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2014-10-29T20:23:13Z","updated_at":"2014-10-29T20:23:13Z","author_association":"CONTRIBUTOR","body":"+1 for a simple size limit.  I think Solr (example solrconfig.xml) \"defaults\" to 512...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/61068882","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-61068882","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":61068882,"node_id":"MDEyOklzc3VlQ29tbWVudDYxMDY4ODgy","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-10-30T10:05:01Z","updated_at":"2014-10-30T10:05:01Z","author_association":"CONTRIBUTOR","body":"512 seems awful low to me.  Is this number per-segment? So if you have one filter and 20 segments, you'd have 20 entries?  Remember also that we have multiple shards per node.\n\nI think we need to choose a big number, which still allows for a lot of caching, without letting it grow to a ridiculous number like 5 million.  Perhaps 50,000?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/61073768","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-61073768","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":61073768,"node_id":"MDEyOklzc3VlQ29tbWVudDYxMDczNzY4","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2014-10-30T10:49:04Z","updated_at":"2014-10-30T10:49:04Z","author_association":"CONTRIBUTOR","body":"I think for Solr it's per-index, i.e. after 512 cached filters for the index it starts evicting by default.\n\nI agree this may be low, since we now compactly store the sparse cases ... but still caching is \"supposed\" to be for cases where you expect high re-use of the given filter and the cost to re-generate it is highish.\n\nIn general I think Elasticsearch should be less aggressive about filter caching; I suspect in many cases where we are caching, we are not saving that much time vs. letting the OS take that RAM instead and cache the IO pages Lucene will access to recreate that filter.  Running a TermFilter when the IO pages are hot should be quite fast.\n\nAnyway 50K seems OK...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/61076745","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-61076745","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":61076745,"node_id":"MDEyOklzc3VlQ29tbWVudDYxMDc2NzQ1","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-10-30T11:17:15Z","updated_at":"2014-10-30T11:17:15Z","author_association":"CONTRIBUTOR","body":"@mikemccand take this example:\n\nUsers are only interested in posts from people that they follow (or even 2 degrees - the people followed by the people that they follow).  eg there are 100,000 user IDs to filter on.\n\nThese can all be put into a `terms` query, potentially with a short custom cache key. The first execution takes a bit of time, but the filter remains valid for the rest of the user's session.  This represents a huge saving in execution time.  It is quite feasible that a big website could have 100k user sessions live at the same time.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/61082288","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-61082288","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":61082288,"node_id":"MDEyOklzc3VlQ29tbWVudDYxMDgyMjg4","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2014-10-30T12:14:26Z","updated_at":"2014-10-30T12:14:26Z","author_association":"CONTRIBUTOR","body":"I think 512 is actually overly large. _way over the top_\n\nIMO: we should only cache filters that:\n- are slow (stuff like wildcards and ranges, but not individual terms)\n- clear evidence of reuse (e.g. not the first time we have seen the filter)\n- being intersected with dense queries (where the risk-reward tradeoff is more clear)\n\nRetrieving every single matching document for a filter to put it into a bitset is extremely slow. Most times, its better to just intersect it on the fly. So this is a huge risk, and we should only do it when the reward is clear: and the reward is tiny unless the criteria above are being met.\n\nI'm not interested in seeing flawed benchmarks around this within current elasticsearch, because it bogusly does this slow way pretty much all the time, even when not caching. So of course mechanisms like caching and bulk bitset intersection will always look like a huge win with the current code. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106599726","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-106599726","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":106599726,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjU5OTcyNg==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-05-28T21:06:49Z","updated_at":"2015-05-28T21:06:49Z","author_association":"CONTRIBUTOR","body":"@mikemccand what's the status of this issue?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106603981","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-106603981","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":106603981,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjYwMzk4MQ==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-05-28T21:21:49Z","updated_at":"2015-05-28T21:21:49Z","author_association":"CONTRIBUTOR","body":"This is bounded to 100k currently in master. I don't much like this number, its ridiculously large, but other changes address the real problems (overcaching). For example only caching on large segments and only when reuse has been noted and so on. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106604388","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-106604388","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":106604388,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjYwNDM4OA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2015-05-28T21:23:52Z","updated_at":"2015-05-28T21:23:52Z","author_association":"CONTRIBUTOR","body":"I don't think this issue is relevant anymore now that we are on the lucene query cache:\n- keys are taken into account (the result of ramBytesUsed() if the query implements Accountable and a constant otherwise)\n- the cache has a limit on the number of filters that can be added to the cache to limit issues that would be caused by ram usage of keys being underestimated\n- also we require that segments have at least 10000 docs for caching so it should be less likely to have cache keys that are much larger than the values than before\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106605336","html_url":"https://github.com/elastic/elasticsearch/issues/8268#issuecomment-106605336","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8268","id":106605336,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjYwNTMzNg==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2015-05-28T21:29:00Z","updated_at":"2015-05-28T21:29:00Z","author_association":"CONTRIBUTOR","body":"Robert, you are right it's 100k currently, nor 10k. I agree we should reduce this number to a more reasonable value.\n","performed_via_github_app":null}]