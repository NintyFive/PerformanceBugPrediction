{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/315","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/315/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/315/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/315/events","html_url":"https://github.com/elastic/elasticsearch/issues/315","id":276444,"node_id":"MDU6SXNzdWUyNzY0NDQ=","number":315,"title":"Analysis: Add `char_filter` on top of `tokenizer`, `filter`, and `analyzer`. Add an `html_strip` char filter and `standard_html_strip` analyzer","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"labels":[{"id":23172,"node_id":"MDU6TGFiZWwyMzE3Mg==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Efeature","name":">feature","color":"006b75","default":false,"description":null},{"id":41149,"node_id":"MDU6TGFiZWw0MTE0OQ==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/v0.10.0","name":"v0.10.0","color":"DDDDDD","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2010-08-12T15:15:01Z","updated_at":"2010-08-19T12:09:54Z","closed_at":"2010-08-12T22:16:40Z","author_association":"MEMBER","active_lock_reason":null,"body":"The analysis process in Lucene allows also for `char_filter` to be used which are filters done on the actual character stream before the tokenization process. Allow to configure custom `char_filter` and provide an implementation for html stripping called `html_strip`. \n\nAlso, add a `standard_html_strip` analyzer that combines the standard analyzer with an html_strip char filter.\n\nHere are some examples how to configure it using both `yaml` and `json`:\n\nYAML:\n\n```\nindex :\n  analysis :\n    tokenizer :\n      standard :\n        type : standard\n    char_filter :\n      my_html :\n        type : html_strip\n        escaped_tags : [xxx, yyy]\n        read_ahead : 1024\n    filter :\n      stop :\n        type : stop\n        stopwords : [test-stop]\n      stop2 :\n        type : stop\n        stopwords : [stop2-1, stop2-2]\n    analyzer :\n      standard :\n        type : standard\n        stopwords : [test1, test2, test3]\n      custom1 :\n        tokenizer : standard\n        filter : [stop, stop2]\n      custom2 :\n        tokenizer : standard\n        char_filter : [html_strip, my_html]\n```\n\nJSON:\n\n```\n{\n    \"index\" : {\n        \"analysis\" : {\n            \"tokenizer\" : {\n                \"standard\" : {\n                    \"type\" : \"standard\"\n                }\n            },\n            \"char_filter\" : {\n                \"my_html\" : {\n                    \"type\" : \"html_strip\",\n                    \"escaped_tags\" : [\"xxx\", \"yyy\"],\n                    \"read_ahead\" : 1024\n                }\n            },\n            \"filter\" : {\n                \"stop\" : {\n                    \"type\" : \"stop\",\n                    \"stopwords\" : [\"test-stop\"]\n                },\n                \"stop2\" : {\n                    \"type\" : \"stop\",\n                    \"stopwords\" : [\"stop2-1\", \"stop2-2\"]\n                }\n            },\n            \"analyzer\" : {\n                \"standard\" : {\n                    \"type\" : \"standard\",\n                    \"stopwords\" : [\"test1\", \"test2\", \"test3\"]\n                },\n                \"custom1\" : {\n                    \"tokenizer\" : \"standard\",\n                    \"filter\" : [\"stop\", \"stop2\"]\n                },\n                \"custom2\" : {\n                    \"tokenizer\" : \"standard\",\n                    \"char_filter\" : [\"html_strip\", \"my_html\"]\n                }\n            }\n        }\n    }\n}\n```\n","closed_by":null,"performed_via_github_app":null}