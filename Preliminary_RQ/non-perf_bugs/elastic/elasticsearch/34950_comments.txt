[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/433749913","html_url":"https://github.com/elastic/elasticsearch/issues/34950#issuecomment-433749913","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34950","id":433749913,"node_id":"MDEyOklzc3VlQ29tbWVudDQzMzc0OTkxMw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-10-28T22:51:04Z","updated_at":"2018-10-28T22:51:04Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/435064923","html_url":"https://github.com/elastic/elasticsearch/issues/34950#issuecomment-435064923","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34950","id":435064923,"node_id":"MDEyOklzc3VlQ29tbWVudDQzNTA2NDkyMw==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-11-01T14:50:49Z","updated_at":"2018-11-01T14:50:49Z","author_association":"CONTRIBUTOR","body":"Investigating this. I will try and obtain the logs from the nodes in question.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/435109479","html_url":"https://github.com/elastic/elasticsearch/issues/34950#issuecomment-435109479","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34950","id":435109479,"node_id":"MDEyOklzc3VlQ29tbWVudDQzNTEwOTQ3OQ==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-11-01T17:00:01Z","updated_at":"2018-11-01T17:00:01Z","author_association":"CONTRIBUTOR","body":"I suspect #34950 has surfaced this issue. Before #34950 we had `minimum_master_nodes` set to 3 in the rolling-upgrade cluster which meant that there was no master while each node went away for an upgrade. Since #34950 we have `minimum_master_nodes` set to 2 so the master survives and possibly does things while the cluster is incomplete. The master node logs:\r\n\r\n```\r\n[2018-10-28T22:39:16,154][INFO ][o.e.c.s.MasterService    ] [node-2] zen-disco-node-failed({node-0}{dBUFa2wKQMiz0s9AKZXg8g}{u5OgRWsKQauhUhBP6S0bNA}{127.0.0.1}{127.0.0.1:46796}{testattr=test}), reason(transport disconnected)[{node-0}{dBUFa2wKQMiz0s9AKZXg8g}{u5OgRWsKQauhUhBP6S0bNA}{127.0.0.1}{127.0.0.1:46796}{testattr=test} transport disconnected], reason: removed {{node-0}{dBUFa2wKQMiz0s9AKZXg8g}{u5OgRWsKQauhUhBP6S0bNA}{127.0.0.1}{127.0.0.1:46796}{testattr=test},}\r\n[2018-10-28T22:39:16,273][INFO ][o.e.c.s.ClusterApplierService] [node-2] removed {{node-0}{dBUFa2wKQMiz0s9AKZXg8g}{u5OgRWsKQauhUhBP6S0bNA}{127.0.0.1}{127.0.0.1:46796}{testattr=test},}, reason: apply cluster state (from master [master {node-2}{-QA0MXO6RyW8LhiIJvxBcA}{I7ULDGeeTsujKeMZ9Fqkyg}{127.0.0.1}{127.0.0.1:46066}{testattr=test} committed version [71] source [zen-disco-node-failed({node-0}{dBUFa2wKQMiz0s9AKZXg8g}{u5OgRWsKQauhUhBP6S0bNA}{127.0.0.1}{127.0.0.1:46796}{testattr=test}), reason(transport disconnected)[{node-0}{dBUFa2wKQMiz0s9AKZXg8g}{u5OgRWsKQauhUhBP6S0bNA}{127.0.0.1}{127.0.0.1:46796}{testattr=test} transport disconnected]]])\r\n[2018-10-28T22:39:16,298][INFO ][o.e.i.s.IndexShard       ] [node-2] [reindexed_index_copy][3] primary-replica resync completed with 0 operations\r\n[2018-10-28T22:39:16,300][INFO ][o.e.i.s.IndexShard       ] [node-2] [test_search_template][3] primary-replica resync completed with 0 operations\r\n[2018-10-28T22:39:16,304][INFO ][o.e.i.s.IndexShard       ] [node-2] [queries][1] primary-replica resync completed with 0 operations\r\n[2018-10-28T22:39:16,308][INFO ][o.e.i.s.IndexShard       ] [node-2] [index_with_replicas][1] primary-replica resync completed with 0 operations\r\n[2018-10-28T22:39:16,314][INFO ][o.e.i.s.IndexShard       ] [node-2] [index_with_replicas][4] primary-replica resync completed with 0 operations\r\n[2018-10-28T22:39:16,318][INFO ][o.e.i.s.IndexShard       ] [node-2] [recover_synced_flush_index][0] primary-replica resync completed with 0 operations\r\n[2018-10-28T22:39:16,342][INFO ][o.e.c.r.DelayedAllocationService] [node-2] scheduling reroute for delayed shards in [0s] (23 delayed shards)\r\n[2018-10-28T22:39:16,399][INFO ][o.e.c.r.DelayedAllocationService] [node-2] scheduling reroute for delayed shards in [59.7s] (14 delayed shards)\r\n[2018-10-28T22:39:25,949][INFO ][o.e.c.s.MasterService    ] [node-2] zen-disco-node-join[{upgraded-node-0}{dBUFa2wKQMiz0s9AKZXg8g}{BkwfVWIZSMyNNZX28gROWA}{127.0.0.1}{127.0.0.1:33036}{testattr=test}], reason: added {{upgraded-node-0}{dBUFa2wKQMiz0s9AKZXg8g}{BkwfVWIZSMyNNZX28gROWA}{127.0.0.1}{127.0.0.1:33036}{testattr=test},}\r\n[2018-10-28T22:39:26,167][INFO ][o.e.c.s.ClusterApplierService] [node-2] added {{upgraded-node-0}{dBUFa2wKQMiz0s9AKZXg8g}{BkwfVWIZSMyNNZX28gROWA}{127.0.0.1}{127.0.0.1:33036}{testattr=test},}, reason: apply cluster state (from master [master {node-2}{-QA0MXO6RyW8LhiIJvxBcA}{I7ULDGeeTsujKeMZ9Fqkyg}{127.0.0.1}{127.0.0.1:46066}{testattr=test} committed version [80] source [zen-disco-node-join[{upgraded-node-0}{dBUFa2wKQMiz0s9AKZXg8g}{BkwfVWIZSMyNNZX28gROWA}{127.0.0.1}{127.0.0.1:33036}{testattr=test}]]])\r\n[2018-10-28T22:39:26,587][INFO ][o.e.c.r.a.AllocationService] [node-2] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[reindexed_index][2], [test_index][1]] ...]).\r\n[2018-10-28T22:39:30,015][WARN ][o.e.c.r.a.AllocationService] [node-2] [recovery_with_concurrent_indexing][0] marking unavailable shards as stale: [W45oWiTESx6kGVZT4J2XNQ]\r\n[2018-10-28T22:39:30,989][INFO ][o.e.c.m.MetaDataUpdateSettingsService] [node-2] updating number_of_replicas to [0] for indices [relocation_with_concurrent_indexing]\r\n[2018-10-28T22:39:31,122][WARN ][o.e.c.r.a.AllocationService] [node-2] [relocation_with_concurrent_indexing][0] marking unavailable shards as stale: [ujllO1ZiTFmw_KwpOQv1uQ, _5ZSl3E5QrKhuXJO_v3Lkw]\r\n```\r\n\r\nHowever I can't reproduce this because I can't even run the rolling upgrade tests locally at the moment, so I'm going to have to shave that yak first. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/435299314","html_url":"https://github.com/elastic/elasticsearch/issues/34950#issuecomment-435299314","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34950","id":435299314,"node_id":"MDEyOklzc3VlQ29tbWVudDQzNTI5OTMxNA==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-11-02T07:55:02Z","updated_at":"2018-11-02T07:55:02Z","author_association":"CONTRIBUTOR","body":"This test looks unsuitable for today's rolling-upgrade test environment in a couple of ways:\r\n\r\nhttps://github.com/elastic/elasticsearch/blob/19d6cf1b9ed1938f8488caaa9071a9db52c0bf1e/qa/rolling-upgrade/src/test/java/org/elasticsearch/upgrades/RecoveryIT.java#L202-L203\r\n\r\nSince #34950 this is no longer true.\r\n\r\nhttps://github.com/elastic/elasticsearch/blob/19d6cf1b9ed1938f8488caaa9071a9db52c0bf1e/qa/rolling-upgrade/src/test/java/org/elasticsearch/upgrades/RecoveryIT.java#L207-L208\r\n\r\nSince #30728 there are either two old nodes or two new nodes, so these are no longer as uniquely defined as they were.\r\n\r\nI also wonder about its usage of `asyncIndexDocs().get()` and whether this could just be `indexDocs()`.\r\n\r\nI suspect some combination of these things is resulting in this failure. However I have run many hundreds of iterations of this test overnight and none of them failed, which I find surprising given how often it was failing in CI. Also I can't quite construct a sequence of events that explain this failure: we set `index.routing.allocation.include._id: dBUFa2wKQMiz0s9AKZXg8g`, wait for the index to be green, and yet `GET /relocation_with_concurrent_indexing/_count?preference=_only_nodes:dBUFa2wKQMiz0s9AKZXg8g` says there's no shard on that node. @dnhatn do you have any insight?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/475981082","html_url":"https://github.com/elastic/elasticsearch/issues/34950#issuecomment-475981082","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34950","id":475981082,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3NTk4MTA4Mg==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2019-03-24T17:31:28Z","updated_at":"2019-03-24T17:31:28Z","author_association":"MEMBER","body":"> I can't quite construct a sequence of events that explain this failure: we set index.routing.allocation.include._id: dBUFa2wKQMiz0s9AKZXg8g, wait for the index to be green, and yet GET /relocation_with_concurrent_indexing/_count?preference=_only_nodes:dBUFa2wKQMiz0s9AKZXg8g says there's no shard on that node\r\n\r\nWe try to [move shards](https://github.com/elastic/elasticsearch/blob/abd861bb9191a3e3edd4f92f7d5c586dfa90281d/server/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java#L628) to the target node (i.e., dBUFa2wKQMiz0s9AKZXg8g) after we set `index.routing.allocation.include._id: dBUFa2wKQMiz0s9AKZXg8g`. However, that move is [throttled](https://github.com/elastic/elasticsearch/blob/abd861bb9191a3e3edd4f92f7d5c586dfa90281d/server/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java#L641) by the target node. Then the cluster health happily returns \"green\" even though the status of shards don't match the allocation filter since there's no relocating shard. This might be an issue if users want to execute some actions only after shards have been moved (e.g., moved from hot to cold nodes). We can add the pending move decision (only with canRemain=false) to ShardRouting, then extend `_cluster/health` API to wait for no pending shard move decision. @DaveCTurner @ywelsch WDYT?","performed_via_github_app":null}]