{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/9971","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9971/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9971/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9971/events","html_url":"https://github.com/elastic/elasticsearch/issues/9971","id":59701883,"node_id":"MDU6SXNzdWU1OTcwMTg4Mw==","number":9971,"title":"Add option for nGram tokenizer to always tokenize whole word","user":{"login":"BHSPitMonkey","id":33672,"node_id":"MDQ6VXNlcjMzNjcy","avatar_url":"https://avatars2.githubusercontent.com/u/33672?v=4","gravatar_id":"","url":"https://api.github.com/users/BHSPitMonkey","html_url":"https://github.com/BHSPitMonkey","followers_url":"https://api.github.com/users/BHSPitMonkey/followers","following_url":"https://api.github.com/users/BHSPitMonkey/following{/other_user}","gists_url":"https://api.github.com/users/BHSPitMonkey/gists{/gist_id}","starred_url":"https://api.github.com/users/BHSPitMonkey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/BHSPitMonkey/subscriptions","organizations_url":"https://api.github.com/users/BHSPitMonkey/orgs","repos_url":"https://api.github.com/users/BHSPitMonkey/repos","events_url":"https://api.github.com/users/BHSPitMonkey/events{/privacy}","received_events_url":"https://api.github.com/users/BHSPitMonkey/received_events","type":"User","site_admin":false},"labels":[{"id":142001965,"node_id":"MDU6TGFiZWwxNDIwMDE5NjU=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Analysis","name":":Search/Analysis","color":"0e8a16","default":false,"description":"How text is split into tokens"},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2015-03-03T20:38:54Z","updated_at":"2015-04-04T14:38:30Z","closed_at":"2015-04-04T14:38:30Z","author_association":"NONE","active_lock_reason":null,"body":"It would be great if the nGram tokenizer came with an option to force creation of a whole-word token for cases when the word length falls outside the min-gram/max-gram range. Currently, we have to resort to a dual-mapping approach to make sure these tokens get created, which limits how we can construct per-field queries.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}