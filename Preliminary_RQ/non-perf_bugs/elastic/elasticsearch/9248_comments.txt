[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/69818036","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-69818036","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":69818036,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODE4MDM2","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-01-13T20:55:18Z","updated_at":"2015-01-13T20:55:18Z","author_association":"CONTRIBUTOR","body":"Hi @imriz\n\nI tried this on 1.4.1, and it seems to be working correctly:\n\n```\nDELETE _all \n\nPUT t\n{\n  \"settings\": {\n    \"index.routing.allocation.total_shards_per_node\": 2,\n    \"index.number_of_shards\": 3,\n    \"index.number_of_replicas\": 1\n  }\n}\n\nGET _cat/shards\n```\n\nReturns:\n\n```\nt 0 r STARTED 0  79b 192.168.2.183 Paige Guthrie         \nt 0 p STARTED 0 115b 192.168.2.183 Sangre                \nt 1 p STARTED 0 115b 192.168.2.183 Paige Guthrie         \nt 1 r STARTED 0  79b 192.168.2.183 Herbert Edgar Wyndham \nt 2 r STARTED 0  79b 192.168.2.183 Sangre                \nt 2 p STARTED 0 115b 192.168.2.183 Herbert Edgar Wyndham \n```\n\nCould you provide a recreation of the problem?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/69819993","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-69819993","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":69819993,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODE5OTkz","user":{"login":"imriz","id":5706843,"node_id":"MDQ6VXNlcjU3MDY4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/5706843?v=4","gravatar_id":"","url":"https://api.github.com/users/imriz","html_url":"https://github.com/imriz","followers_url":"https://api.github.com/users/imriz/followers","following_url":"https://api.github.com/users/imriz/following{/other_user}","gists_url":"https://api.github.com/users/imriz/gists{/gist_id}","starred_url":"https://api.github.com/users/imriz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imriz/subscriptions","organizations_url":"https://api.github.com/users/imriz/orgs","repos_url":"https://api.github.com/users/imriz/repos","events_url":"https://api.github.com/users/imriz/events{/privacy}","received_events_url":"https://api.github.com/users/imriz/received_events","type":"User","site_admin":false},"created_at":"2015-01-13T21:06:58Z","updated_at":"2015-01-13T21:06:58Z","author_association":"NONE","body":"So far, I wasn't able to reproduce at will, but I can gather what ever\ndebugging information needed next time it occurs.\n\nOn Tue, Jan 13, 2015, 22:56 Clinton Gormley notifications@github.com\nwrote:\n\n> Hi @imriz https://github.com/imriz\n> \n> I tried this on 1.4.1, and it seems to be working correctly:\n> \n> DELETE _all\n> \n> PUT t\n> {\n>   \"settings\": {\n>     \"index.routing.allocation.total_shards_per_node\": 2,\n>     \"index.number_of_shards\": 3,\n>     \"index.number_of_replicas\": 1\n>   }\n> }\n> \n> GET _cat/shards\n> \n> Returns:\n> \n> t 0 r STARTED 0  79b 192.168.2.183 Paige Guthrie\n> t 0 p STARTED 0 115b 192.168.2.183 Sangre\n> t 1 p STARTED 0 115b 192.168.2.183 Paige Guthrie\n> t 1 r STARTED 0  79b 192.168.2.183 Herbert Edgar Wyndham\n> t 2 r STARTED 0  79b 192.168.2.183 Sangre\n> t 2 p STARTED 0 115b 192.168.2.183 Herbert Edgar Wyndham\n> \n> Could you provide a recreation of the problem?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/elasticsearch/elasticsearch/issues/9248#issuecomment-69818036\n> .\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70475143","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-70475143","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":70475143,"node_id":"MDEyOklzc3VlQ29tbWVudDcwNDc1MTQz","user":{"login":"imriz","id":5706843,"node_id":"MDQ6VXNlcjU3MDY4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/5706843?v=4","gravatar_id":"","url":"https://api.github.com/users/imriz","html_url":"https://github.com/imriz","followers_url":"https://api.github.com/users/imriz/followers","following_url":"https://api.github.com/users/imriz/following{/other_user}","gists_url":"https://api.github.com/users/imriz/gists{/gist_id}","starred_url":"https://api.github.com/users/imriz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imriz/subscriptions","organizations_url":"https://api.github.com/users/imriz/orgs","repos_url":"https://api.github.com/users/imriz/repos","events_url":"https://api.github.com/users/imriz/events{/privacy}","received_events_url":"https://api.github.com/users/imriz/received_events","type":"User","site_admin":false},"created_at":"2015-01-19T10:48:21Z","updated_at":"2015-01-19T14:07:13Z","author_association":"NONE","body":"Hi @clintongormley,\nThis happened again:\nlogstash-app-2015.01.19 0 p STARTED      498246   259mb 10.84.30.188 elasticsearch003\nlogstash-app-2015.01.19 0 r STARTED      498413 256.5mb 10.84.30.184 elasticsearch002\nlogstash-app-2015.01.19 1 r STARTED      499403 248.1mb 10.84.30.188 elasticsearch003\nlogstash-app-2015.01.19 1 p STARTED      499298 236.8mb 10.84.30.184 elasticsearch002\nlogstash-app-2015.01.19 2 p STARTED      499272 245.5mb 10.84.30.186 elasticsearch001\nlogstash-app-2015.01.19 2 r UNASSIGNED        \n\nThe only relevant logs are:\n[2015-01-19 04:18:39,824][INFO ][cluster.metadata         ] [elasticsearch001] [logstash-app-2015.01.19] creating index, cause [auto(bulk api)], shards [3]/[1], mappings [_default_]\n[2015-01-19 04:18:40,204][INFO ][cluster.metadata         ] [elasticsearch001] [logstash-app-2015.01.19] update_mapping [logs](dynamic)\n[2015-01-19 04:19:39,521][INFO ][cluster.metadata         ] [elasticsearch001] [logstash-app-2015.01.19] update_mapping [logs](dynamic)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70554048","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-70554048","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":70554048,"node_id":"MDEyOklzc3VlQ29tbWVudDcwNTU0MDQ4","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-01-19T20:18:29Z","updated_at":"2015-01-19T20:18:29Z","author_association":"CONTRIBUTOR","body":"Hi @imriz \n\nSo these are definitely per-index settings that you are setting? Nothing in the per-node config files or cluster settings? Any other allocation-related settings that we should know about?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70577519","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-70577519","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":70577519,"node_id":"MDEyOklzc3VlQ29tbWVudDcwNTc3NTE5","user":{"login":"imriz","id":5706843,"node_id":"MDQ6VXNlcjU3MDY4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/5706843?v=4","gravatar_id":"","url":"https://api.github.com/users/imriz","html_url":"https://github.com/imriz","followers_url":"https://api.github.com/users/imriz/followers","following_url":"https://api.github.com/users/imriz/following{/other_user}","gists_url":"https://api.github.com/users/imriz/gists{/gist_id}","starred_url":"https://api.github.com/users/imriz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imriz/subscriptions","organizations_url":"https://api.github.com/users/imriz/orgs","repos_url":"https://api.github.com/users/imriz/repos","events_url":"https://api.github.com/users/imriz/events{/privacy}","received_events_url":"https://api.github.com/users/imriz/received_events","type":"User","site_admin":false},"created_at":"2015-01-19T23:11:22Z","updated_at":"2015-01-19T23:11:22Z","author_association":"NONE","body":"Hi @clintongormley \n\nThe only thing remotely related is:\ncluster: \n  routing: \n    allocation: \n      disk: \n        threshold_enabled: true\n        watermark: \n          high: 0.99\n          low: 0.97\n\nBut we are not close to these thresholds, nor do I see how they can create the allocation topology I got here..\n\nI should emphasize that it looks completely random - an index creation few minutes later can result a perfectly sensible topology. \nMoreover, If I increase the total_number_of_shard to 3, and then reduce it, it will rebalance back to normality.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70709852","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-70709852","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":70709852,"node_id":"MDEyOklzc3VlQ29tbWVudDcwNzA5ODUy","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-01-20T18:53:24Z","updated_at":"2015-01-20T18:53:24Z","author_association":"CONTRIBUTOR","body":"Hi @imriz \n\nA colleague has encountered this before and explained to me what is happening:\n- On elasticsearch003, it has assigned p0 and r1\n- On elasticsearch002, it has assigned p1 and r0\n- On elasticsearch001, it has assigned p2, but it can't assign r2, because it never puts copies of the same shard on the same node.\n\nThe only way to solve this setup is to move one of the other shards to a different node, but it isn't going to do that until something else triggers the move, eg disk full, node failure, etc\n\nWhile it goes to some trouble to randomize things like this, so that this should happen seldom, it is possible (as you've seen) with hard limits like `total_shards_per_node` to reach situations where allocation can't proceed.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70781676","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-70781676","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":70781676,"node_id":"MDEyOklzc3VlQ29tbWVudDcwNzgxNjc2","user":{"login":"imriz","id":5706843,"node_id":"MDQ6VXNlcjU3MDY4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/5706843?v=4","gravatar_id":"","url":"https://api.github.com/users/imriz","html_url":"https://github.com/imriz","followers_url":"https://api.github.com/users/imriz/followers","following_url":"https://api.github.com/users/imriz/following{/other_user}","gists_url":"https://api.github.com/users/imriz/gists{/gist_id}","starred_url":"https://api.github.com/users/imriz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imriz/subscriptions","organizations_url":"https://api.github.com/users/imriz/orgs","repos_url":"https://api.github.com/users/imriz/repos","events_url":"https://api.github.com/users/imriz/events{/privacy}","received_events_url":"https://api.github.com/users/imriz/received_events","type":"User","site_admin":false},"created_at":"2015-01-21T04:09:03Z","updated_at":"2015-01-21T04:09:03Z","author_association":"NONE","body":"Hi @clintongormley \n\nYep, that is what I described in my initial post :)\n\nJust one small note, if total_shards_per_node is set to a higher value, the nodes will happily allocate the same shard to the same node.\n\ni think that the best approach is to have a configuration flag to ensure this will never happen (same shard on the same node), since the whole reason I've had to set total_shards_per_node to 2 is to prevent a node from recovering redundant shards when another node fails (which is very problematic when you don't have a lot of free disk space).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70836560","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-70836560","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":70836560,"node_id":"MDEyOklzc3VlQ29tbWVudDcwODM2NTYw","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-01-21T13:31:20Z","updated_at":"2015-01-21T13:31:20Z","author_association":"CONTRIBUTOR","body":"> Just one small note, if total_shards_per_node is set to a higher value, the nodes will happily allocate the same shard to the same node.\n\nThis is never be case.  I think what happens if you increase the total_shards_per_node is that you end up with 3 (different) shards on a single node, then the allocator rebalances by moving one of the shards to a different node.  But having two copies of the same shard on a single node will never be allowed.\n\n> the whole reason I've had to set total_shards_per_node to 2 is to prevent a node from recovering redundant shards when another node fails (which is very problematic when you don't have a lot of free disk space).\n\nas above, this never happens.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70911301","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-70911301","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":70911301,"node_id":"MDEyOklzc3VlQ29tbWVudDcwOTExMzAx","user":{"login":"imriz","id":5706843,"node_id":"MDQ6VXNlcjU3MDY4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/5706843?v=4","gravatar_id":"","url":"https://api.github.com/users/imriz","html_url":"https://github.com/imriz","followers_url":"https://api.github.com/users/imriz/followers","following_url":"https://api.github.com/users/imriz/following{/other_user}","gists_url":"https://api.github.com/users/imriz/gists{/gist_id}","starred_url":"https://api.github.com/users/imriz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imriz/subscriptions","organizations_url":"https://api.github.com/users/imriz/orgs","repos_url":"https://api.github.com/users/imriz/repos","events_url":"https://api.github.com/users/imriz/events{/privacy}","received_events_url":"https://api.github.com/users/imriz/received_events","type":"User","site_admin":false},"created_at":"2015-01-21T20:05:44Z","updated_at":"2015-01-21T20:05:44Z","author_association":"NONE","body":"> This is never be case. I think what happens if you increase the total_shards_per_node is that you end up with 3 (different) shards on a single node, then the allocator rebalances by moving one of the shards to a different node. But having two copies of the same shard on a single node will never be allowed.\n\nYou are, of course, correct.\nAnyway, that leaves us with the original issue where the allocator doesn't compute the optimal allocation scheme, which results unassigned shards.\n\n> as above, this never happens.\n\nWell, not the same shards, but indeed redundant shards.\nIn a cluster where storage is limited (for example, in ELK setups, where storage is computed to support a specific X days sliding window retention of logs), and when cluster is designed to only support one node failure (which I believe is a legitimate use case), one might be OK with not recovering replicas of shards after 1 node failure. If I would set total_shards_per_node to a higher value, I risk \"eating\" up all the free disk space at each node failure, leading to inability to index new logs.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70941870","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-70941870","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":70941870,"node_id":"MDEyOklzc3VlQ29tbWVudDcwOTQxODcw","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-01-21T23:15:47Z","updated_at":"2015-01-21T23:15:47Z","author_association":"MEMBER","body":"@imriz did you try looking at the the disk based allocation as a means to avoid assigning shards that will cause the node to run out of disk space? This will give the flexibility you need if possible while avoiding filling up the disk. More info is here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-allocation.html#disk . Note that you will have to be using 1.4.1 or higher for it to include the size of relocating shards (and prevent them): see https://github.com/elasticsearch/elasticsearch/pull/7785 which was back ported in https://github.com/elasticsearch/elasticsearch/commit/4e5264c8dcab392fd94554f5d036573b085b6450\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70971100","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-70971100","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":70971100,"node_id":"MDEyOklzc3VlQ29tbWVudDcwOTcxMTAw","user":{"login":"imriz","id":5706843,"node_id":"MDQ6VXNlcjU3MDY4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/5706843?v=4","gravatar_id":"","url":"https://api.github.com/users/imriz","html_url":"https://github.com/imriz","followers_url":"https://api.github.com/users/imriz/followers","following_url":"https://api.github.com/users/imriz/following{/other_user}","gists_url":"https://api.github.com/users/imriz/gists{/gist_id}","starred_url":"https://api.github.com/users/imriz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imriz/subscriptions","organizations_url":"https://api.github.com/users/imriz/orgs","repos_url":"https://api.github.com/users/imriz/repos","events_url":"https://api.github.com/users/imriz/events{/privacy}","received_events_url":"https://api.github.com/users/imriz/received_events","type":"User","site_admin":false},"created_at":"2015-01-22T04:49:01Z","updated_at":"2015-01-22T04:51:16Z","author_association":"NONE","body":"@bleskes I am aware of the disk based allocation thresholds, but that doesn't give me the functionality I need - I  want to utilize the disk space as much as I can  - I just don't want the nodes to recover any shards of another failing node (that is, I want to be able to sustain only 1 node failure).\nThe total_shards_per_node set to 2 with the combination of number_of_replicas set to 1 gives me exactly that, with the exception of these annoying unassigned shards every once and then.\nThis is definitely not a showstopper, but it would be nice to get the allocator improved so it will always compute the optimal allocation scheme :) \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/74237086","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-74237086","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":74237086,"node_id":"MDEyOklzc3VlQ29tbWVudDc0MjM3MDg2","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-13T10:58:22Z","updated_at":"2015-02-13T10:58:22Z","author_association":"MEMBER","body":"@imriz sorry for the late response. I hear you and your are right. The disk threshold allocator will give you some more flexibility here, if you have the space for it. Since we can't try all the shard allocations combinations (think thousands of shards, hundreds of nodes) we have to use an iterative algorithm and that might not find a global optimum. If you have any suggestion how to improve this I'll be very happy to discuss them.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/74242446","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-74242446","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":74242446,"node_id":"MDEyOklzc3VlQ29tbWVudDc0MjQyNDQ2","user":{"login":"imriz","id":5706843,"node_id":"MDQ6VXNlcjU3MDY4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/5706843?v=4","gravatar_id":"","url":"https://api.github.com/users/imriz","html_url":"https://github.com/imriz","followers_url":"https://api.github.com/users/imriz/followers","following_url":"https://api.github.com/users/imriz/following{/other_user}","gists_url":"https://api.github.com/users/imriz/gists{/gist_id}","starred_url":"https://api.github.com/users/imriz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imriz/subscriptions","organizations_url":"https://api.github.com/users/imriz/orgs","repos_url":"https://api.github.com/users/imriz/repos","events_url":"https://api.github.com/users/imriz/events{/privacy}","received_events_url":"https://api.github.com/users/imriz/received_events","type":"User","site_admin":false},"created_at":"2015-02-13T11:48:34Z","updated_at":"2015-02-13T11:48:34Z","author_association":"NONE","body":"@bleskes Maybe something like CRUSH maps could be of use?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/74251651","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-74251651","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":74251651,"node_id":"MDEyOklzc3VlQ29tbWVudDc0MjUxNjUx","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-13T13:16:13Z","updated_at":"2015-02-13T13:16:13Z","author_association":"MEMBER","body":"@imriz thx for the tip. Looks interesting, though it's not clear at first glance how removing the centralized nature (which is very good for other reasons) will allow to deal with local minima.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/74369299","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-74369299","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":74369299,"node_id":"MDEyOklzc3VlQ29tbWVudDc0MzY5Mjk5","user":{"login":"imriz","id":5706843,"node_id":"MDQ6VXNlcjU3MDY4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/5706843?v=4","gravatar_id":"","url":"https://api.github.com/users/imriz","html_url":"https://github.com/imriz","followers_url":"https://api.github.com/users/imriz/followers","following_url":"https://api.github.com/users/imriz/following{/other_user}","gists_url":"https://api.github.com/users/imriz/gists{/gist_id}","starred_url":"https://api.github.com/users/imriz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imriz/subscriptions","organizations_url":"https://api.github.com/users/imriz/orgs","repos_url":"https://api.github.com/users/imriz/repos","events_url":"https://api.github.com/users/imriz/events{/privacy}","received_events_url":"https://api.github.com/users/imriz/received_events","type":"User","site_admin":false},"created_at":"2015-02-14T10:06:29Z","updated_at":"2015-02-14T10:07:43Z","author_association":"NONE","body":"@bleskes CRUSH allows you to set allocation rules, which can prevent the issue\nat hand. Twitter's libcrunch is a good place to look at.\n\nOn Fri, Feb 13, 2015, 15:17 Boaz Leskes notifications@github.com wrote:\n\n> @imriz https://github.com/imriz thx for the tip. Looks interesting,\n> though it's not clear at first glance how removing the centralized nature\n> (which is very good for other reasons) will allow to deal with local minima.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/elasticsearch/elasticsearch/issues/9248#issuecomment-74251651\n> .\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/91337471","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-91337471","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":91337471,"node_id":"MDEyOklzc3VlQ29tbWVudDkxMzM3NDcx","user":{"login":"aochsner","id":222713,"node_id":"MDQ6VXNlcjIyMjcxMw==","avatar_url":"https://avatars2.githubusercontent.com/u/222713?v=4","gravatar_id":"","url":"https://api.github.com/users/aochsner","html_url":"https://github.com/aochsner","followers_url":"https://api.github.com/users/aochsner/followers","following_url":"https://api.github.com/users/aochsner/following{/other_user}","gists_url":"https://api.github.com/users/aochsner/gists{/gist_id}","starred_url":"https://api.github.com/users/aochsner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aochsner/subscriptions","organizations_url":"https://api.github.com/users/aochsner/orgs","repos_url":"https://api.github.com/users/aochsner/repos","events_url":"https://api.github.com/users/aochsner/events{/privacy}","received_events_url":"https://api.github.com/users/aochsner/received_events","type":"User","site_admin":false},"created_at":"2015-04-09T19:39:19Z","updated_at":"2015-04-09T19:39:48Z","author_association":"CONTRIBUTOR","body":"It's been a long time since I tried this approach (I was the author of the linked user group post).  But when talking to someone at ElasticON, they said it sounds like a bug and to create an issue.  Was about to try do that and came across this issue.  Glad I'm not the only one.  \n\nHaven't gone through to try to recreate this on 1.4.4.  We are currently on 1.4.2.  We are running a 3 node cluster, 3 primary, 1 replica, with daily rolling indexes.  There's probably about 12 indexes per day.  Of those, 2 are very large/hot.  We really care that those are distributed evenly (we are on spinning disk and pretty constrained w/ memory so anything we can do to balance it across the cluster, the better).  In fact, we'd just prefer everything get distributed evenly.  I've tried to play with `cluster.routing.allocation.balance.*` but that didn't seem to help.  Meant to go back and see if it was better but also saw this issue #9023 which sounds like I might want to upgrade to >= 1.4.4 first.  Anyways, that's all the background that lead us to trying this setting.  \n\nSeems the issue is clear now?  Or is there anything I can do to try to help recreate?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/122254353","html_url":"https://github.com/elastic/elasticsearch/issues/9248#issuecomment-122254353","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9248","id":122254353,"node_id":"MDEyOklzc3VlQ29tbWVudDEyMjI1NDM1Mw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-07-17T11:49:23Z","updated_at":"2015-07-17T11:49:23Z","author_association":"CONTRIBUTOR","body":"Closing this as a duplicate of #12273\n","performed_via_github_app":null}]