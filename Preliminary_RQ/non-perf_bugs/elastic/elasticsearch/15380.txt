{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/15380","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15380/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15380/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15380/events","html_url":"https://github.com/elastic/elasticsearch/issues/15380","id":121577785,"node_id":"MDU6SXNzdWUxMjE1Nzc3ODU=","number":15380,"title":"ElasticSearch 1.7.2 nodes periodically stop communicating among themselves","user":{"login":"tdoman","id":11098266,"node_id":"MDQ6VXNlcjExMDk4MjY2","avatar_url":"https://avatars1.githubusercontent.com/u/11098266?v=4","gravatar_id":"","url":"https://api.github.com/users/tdoman","html_url":"https://github.com/tdoman","followers_url":"https://api.github.com/users/tdoman/followers","following_url":"https://api.github.com/users/tdoman/following{/other_user}","gists_url":"https://api.github.com/users/tdoman/gists{/gist_id}","starred_url":"https://api.github.com/users/tdoman/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tdoman/subscriptions","organizations_url":"https://api.github.com/users/tdoman/orgs","repos_url":"https://api.github.com/users/tdoman/repos","events_url":"https://api.github.com/users/tdoman/events{/privacy}","received_events_url":"https://api.github.com/users/tdoman/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2015-12-10T21:08:35Z","updated_at":"2015-12-11T18:03:28Z","closed_at":"2015-12-11T09:22:45Z","author_association":"NONE","active_lock_reason":null,"body":"I've also seen this happen w/ an ES 1.7.3 system we have.  I have two different production clusters running two nodes each.  The nodes will both continue to respond to client requests but seem to lose communication w/ each other and never attempt to re-connect.  Restarting one of the nodes will reestablish communication but, so far, at some point I know they will lose communication again.  We are running our clusters in Azure on Windows Server 2012 VMs.  We have been running 1.7.2 successfully for several weeks now and just started experiencing this issue a couple of days ago.\n\nIn the logs, there really appears to be no activity at all on the system and then all of a sudden, the master node, ES1, we see:\n[2015-12-07 16:52:30,396][DEBUG][action.admin.cluster.node.stats] [ES1] failed to execute on node [GO31X74ET_aFfQUQHqakhw]\norg.elasticsearch.transport.NodeDisconnectedException: [ES2][inet[/10.0.0.5:9300]][cluster:monitor/nodes/stats[n]] disconnected\n[2015-12-07 16:52:30,398][DEBUG][action.search.type       ] [ES1] [ix1_v11][3], node[GO31X74ET_aFfQUQHqakhw], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6a57a770] lastShard [true]\norg.elasticsearch.transport.NodeDisconnectedException: [ES2][inet[/10.0.0.5:9300]][indices:data/read/search[phase/query]] disconnected\n[2015-12-07 16:52:30,542][DEBUG][action.admin.indices.stats] [ES1] [ix2_v2][10], node[GO31X74ET_aFfQUQHqakhw], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5a10f0df]\norg.elasticsearch.transport.NodeDisconnectedException: [ES2][inet[/10.0.0.5:9300]][indices:monitor/stats[s]] disconnected\n[2015-12-07 16:52:30,532][DEBUG][action.admin.indices.stats] [ES1] [ix3_v1][11], node[GO31X74ET_aFfQUQHqakhw], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5a10f0df]\norg.elasticsearch.transport.NodeDisconnectedException: [ES2][inet[/10.0.0.5:9300]][indices:monitor/stats[s]] disconnected\n\nOn the non-master node, ES2, we see:\n[2015-12-07 16:52:30,505][INFO ][discovery.zen            ] [ES2] master_left [[ES1][j5IETSp1S1OxOiqByeuaVA][es1][inet[/10.0.0.4:9300]]], reason [transport disconnected]\n[2015-12-07 16:52:30,536][WARN ][discovery.zen            ] [ES2] master left (reason = transport disconnected), current nodes: {[ES2][GO31X74ET_aFfQUQHqakhw][es2][inet[/10.0.0.5:9300]],}\n[2015-12-07 16:52:30,583][INFO ][cluster.service          ] [ES2] removed {[ES1][j5IETSp1S1OxOiqByeuaVA][es1][inet[/10.0.0.4:9300]],}, reason: zen-disco-master_failed ([ES1][j5IETSp1S1OxOiqByeuaVA][es1][inet[/10.0.0.4:9300]])\n[2015-12-07 16:52:30,614][DEBUG][action.bulk              ] [ES2] observer timed out. notifying listener. timeout setting [1s], time since start [8.3s]\n[2015-12-07 16:52:35,339][INFO ][cluster.service          ] [ES2] new_master [ES2][GO31X74ET_aFfQUQHqakhw][es2][inet[/10.0.0.5:9300]], reason: zen-disco-join (elected_as_master)\n[2015-12-07 16:52:35,355][INFO ][cluster.routing          ] [ES2] delaying allocation for [161] unassigned shards, next check in [59.8s]\n\nAfter this, as I said, the sync process will not recover unless we restart ES2.  Note that, so far, we haven't had to restart ES1, just restarting ES2 seems to do the trick.  I have suspected that we've reached some memory threshold or something but we haven't been adding large amounts of data to our system either.  Our VMs are running w/ 7GB machines of which 3.5GB we have configured to be available to the JVM (as recommended) to leave the other half for the Lucene file system cache.  The Task Manager will show the ES process as using anywhere from 1.2-1.4 GB (ie. not the 3.5GB we've configured to allow for it) and typically runs at ~98% memory usage.  I've always assumed the lion share of the rest was used by Lucene and despite running at that high percentage of memory usage, the system has performed just fine for many weeks now (probably ~8 weeks or so) so something has emerged to change that somehow.  Is this something that others have experienced?\n\nThanks,\nTom\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}