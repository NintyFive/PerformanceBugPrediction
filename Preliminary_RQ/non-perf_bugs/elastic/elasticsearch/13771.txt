{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/13771","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13771/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13771/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13771/events","html_url":"https://github.com/elastic/elasticsearch/issues/13771","id":108161770,"node_id":"MDU6SXNzdWUxMDgxNjE3NzA=","number":13771,"title":"Shard rebalancing too aggressive when adding new nodes","user":{"login":"avleen","id":539525,"node_id":"MDQ6VXNlcjUzOTUyNQ==","avatar_url":"https://avatars1.githubusercontent.com/u/539525?v=4","gravatar_id":"","url":"https://api.github.com/users/avleen","html_url":"https://github.com/avleen","followers_url":"https://api.github.com/users/avleen/followers","following_url":"https://api.github.com/users/avleen/following{/other_user}","gists_url":"https://api.github.com/users/avleen/gists{/gist_id}","starred_url":"https://api.github.com/users/avleen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/avleen/subscriptions","organizations_url":"https://api.github.com/users/avleen/orgs","repos_url":"https://api.github.com/users/avleen/repos","events_url":"https://api.github.com/users/avleen/events{/privacy}","received_events_url":"https://api.github.com/users/avleen/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2015-09-24T16:04:27Z","updated_at":"2018-02-14T14:04:48Z","closed_at":"2016-01-28T18:10:53Z","author_association":"NONE","active_lock_reason":null,"body":"Cluster:\n- 3 dedicated masters\n- 2 client nodes\n- 65 data nodes, each running 2 instances of ES\n- ES version 1.5.2, but tests on a smaller 1.7 cluster seem to behave the same way\n- Shard balancing based on disk usage\n- 64 indices, 1757 primary shards, 1757 replica shards (3514 shards total)\n\nUpon adding a new node to the cluster, the expectation is that this node will get populated quickly.\nIn reality, we end up with shards rebalancing across the entire cluster.\n\nExample:\nI added `logdb41` to the cluster today, and the result was primary shards getting relocated and replicas being rebuilt everywhere, and very few going to the new node at any one time:\n\n```\nindex               shard time   type       stage source_host          target_host\nlogstash-2015.08.29 26    268206 relocation index logdb74              logdb52\nlogstash-2015.08.23 36    268138 replica    index logdb70              logdb74\nlogstash-2015.08.23 15    268340 replica    index logdb26              logdb38\nlogstash-2015.08.25 15    267976 replica    index logdb39              logdb26\nlogstash-2015.08.25 28    268094 replica    index logdb32              logdb27\nlogstash-2015.09.08 6     245140 replica    index logdb48              logdb41\nlogstash-2015.08.25 12    268262 relocation index logdb40              logdb65\nlogstash-2015.09.19 25    264782 relocation index logdb65              logdb41\n```\n\nDoes the rebalancing algorithm try to get disk usage as even as possible across the entire cluster?\nOr does it allow some amount of variance between nodes?\nIt feels like the cluster is coming up with a new map of where all the shards should end up, but instead of initially putting as many shards as possible on the new node, it's prioiritising the rebalancing some other way.\nThis seems less efficient in terms of production operations. Do all these shards really need to move?\n\nThanks!\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}