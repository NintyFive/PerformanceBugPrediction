[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/35598819","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-35598819","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":35598819,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NTk4ODE5","user":{"login":"spinscale","id":667544,"node_id":"MDQ6VXNlcjY2NzU0NA==","avatar_url":"https://avatars2.githubusercontent.com/u/667544?v=4","gravatar_id":"","url":"https://api.github.com/users/spinscale","html_url":"https://github.com/spinscale","followers_url":"https://api.github.com/users/spinscale/followers","following_url":"https://api.github.com/users/spinscale/following{/other_user}","gists_url":"https://api.github.com/users/spinscale/gists{/gist_id}","starred_url":"https://api.github.com/users/spinscale/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spinscale/subscriptions","organizations_url":"https://api.github.com/users/spinscale/orgs","repos_url":"https://api.github.com/users/spinscale/repos","events_url":"https://api.github.com/users/spinscale/events{/privacy}","received_events_url":"https://api.github.com/users/spinscale/received_events","type":"User","site_admin":false},"created_at":"2014-02-20T08:36:24Z","updated_at":"2014-02-20T08:36:24Z","author_association":"MEMBER","body":"Can you run and attach the output of a hot_threads operation when the CPU is high? This will show you, which elasticsearch java classes use the most CPU\nDo you hit your system with scan search queries? That could be a reason for open file handles on deleted files.\n\nI dont know anything about the active mq river? Does this also happen when it is switched off? Or does this also happen on the node if the river is running on the other one?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/35600049","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-35600049","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":35600049,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NjAwMDQ5","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-02-20T08:56:48Z","updated_at":"2014-02-20T08:56:48Z","author_association":"CONTRIBUTOR","body":"Thanks for your answer Alexander.\nWe've been monitoring hot threads regularly, but nothing struck us so far. We will look again very closely within the next 2-3 days and get some samples regularly all over the critical time period.\n\nWe do no use scan search queries, only the default one (query_then_fetch). Although, granted, as most of our queries return no docs (i.e. with size=0) and are mostly run for facetting purposes, we could switch to search_type=count for those, that will probably improve things a bit.\n\nWe cannot switch off the river, unfortunately. Though, it seems to consistently happen on the node which is running the river (whether master or not).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/35600313","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-35600313","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":35600313,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NjAwMzEz","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2014-02-20T09:01:00Z","updated_at":"2014-02-20T09:01:00Z","author_association":"CONTRIBUTOR","body":"just a blind guess but this looks like a big merge - do you have stats of used disk space during that period? Do you have any non-standard configurations and are you doing something special during that time?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/35600735","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-35600735","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":35600735,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NjAwNzM1","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-02-20T09:06:58Z","updated_at":"2014-02-20T09:06:58Z","author_association":"CONTRIBUTOR","body":"We have about 250GB available disk space on those machines and ES is the only thing running on them. So substracting system files and our 25GB indexed data, that leaves about 200GB of free space.\n\nWe have a pretty standard configuration, we only tweaked the heap sizes for the filters and fielddata caches to better handle our situation. Aside from that nothing else is running.\n\nI'll watch again the disk space and other processes the next time it happens and report the findings here.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/35600801","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-35600801","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":35600801,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NjAwODAx","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-02-20T09:08:07Z","updated_at":"2014-02-20T09:08:07Z","author_association":"MEMBER","body":"As an experiment - is it possible for you add a third node (potentially on the same server as one of the current node), turn off it's master and data roles and force the river to be allocated on it. This will separate the river functionality from the other nodes and would allow us to see whether the problem is river specific.\n\nThe config on the river node (in the elasticsearch.yml) should be something like\n\n```\nnode.master: false\nnode.data: false\n\nnode.river: rabbitmq\n```\n\nOn the other two nodes, add the following to the elaticsearch.yml:\n\n```\nnode.river: _none_\n```\n\nYou will have to restart the nodes for this to have affect.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/35601825","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-35601825","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":35601825,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NjAxODI1","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-02-20T09:23:16Z","updated_at":"2014-02-20T09:23:16Z","author_association":"CONTRIBUTOR","body":"That's effectively one measure we were planning to enforce. We will try that as well.\nThanks guys!!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/35601910","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-35601910","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":35601910,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NjAxOTEw","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-02-20T09:24:30Z","updated_at":"2014-02-20T09:24:30Z","author_association":"MEMBER","body":"Just note that it is a temporary research solution as it will make the river node a single point of failure - the river will never be re-assigned upon a failure of that node.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/36145187","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-36145187","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":36145187,"node_id":"MDEyOklzc3VlQ29tbWVudDM2MTQ1MTg3","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-02-26T16:33:59Z","updated_at":"2014-02-26T16:33:59Z","author_association":"CONTRIBUTOR","body":"Quick note to point out that we've taken two measures so far:\n- upgrading to 0.90.11 (Lucene 4.6.1)\n- making sure that all queries which only return facets and no hits are using search_type=count\n\nBoth of these measures have had no effect so far, as the same pattern is repeating.\n\nWe've yet to experiment what @bleskes brought up above, but it's not trivial to do it in production with minimal interruption. Stay tuned...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37460998","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37460998","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37460998,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDYwOTk4","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2014-03-12T20:35:29Z","updated_at":"2014-03-12T20:35:29Z","author_association":"CONTRIBUTOR","body":"any updates on this one?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37517578","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37517578","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37517578,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NTE3NTc4","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-13T10:19:57Z","updated_at":"2014-03-13T10:43:44Z","author_association":"CONTRIBUTOR","body":"Nope, we haven't been able to run the experiment depicted by @bleskes yet, but we will soon.\n\nOne thing I'm eager to discover is whether\n1. the GC starts going south (and the CPU as well) because the file handles start leaking, or\n2. the file handles start leaking because the GC starts going south (and the CPU as well). \n\nJudging by the above charts, it looks like the GC starts behaving weird (i.e. when the heap can't be cleaned correctly anymore) before the file handles count starts increasing, but nothing clear cut.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37526844","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37526844","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37526844,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NTI2ODQ0","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-13T12:26:15Z","updated_at":"2014-03-13T12:26:15Z","author_association":"CONTRIBUTOR","body":"Here's another interesting capture from yesterday when we reindexed 10M+ documents. The red node is initially the master and runs the river. We've let it proceed until the heap reached 99%, at which point we restarted it (green vertical line) and the blue node became master.\n\nWhat we can see is that both nodes were actively indexing documents, yet the open file handle count only increases on the master.\n\nAlso when the blue node became master and picked up the river, its file handle count increased much more quickly, probably because its heap was already around 70% and kept increasing steadily up to 99%, where we restarted it and the red node became master again.\n\nThe dashed gray lines denote that whenever the file handle count crosses a certain threshold and keeps increasing, the CPU goes way up. Or maybe as commented earlier, it's vice versa, i.e. that whenever the CPU goes way up (because heap is stressed because GC can't do its job), the file handles are not recycled anymore and keep increasing.\n\nIt also looks like the heap on the master node never has as much leeway as the heap on the worker node, i.e. no \"sawtooth\" pattern.\n\n![screenshot](https://f.cloud.github.com/assets/1280019/2409157/192530da-aaa9-11e3-8141-c14831f5362a.png)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37529464","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37529464","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37529464,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NTI5NDY0","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2014-03-13T12:59:10Z","updated_at":"2014-03-13T12:59:10Z","author_association":"MEMBER","body":"I gave a Quick look at ActiveMQ river plugin and it looks like they create a new Bulk request for every new message: https://github.com/domdorn/elasticsearch-river-activemq/blob/master/src/main/java/org/elasticsearch/river/activemq/ActiveMQRiver.java#L263\n\nLooks bad to me.\n\nMay be I misread the code though.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37530623","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37530623","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37530623,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NTMwNjIz","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-13T13:12:07Z","updated_at":"2014-03-13T15:51:29Z","author_association":"CONTRIBUTOR","body":"Thank you David, I appreciate your input. That might be a good point. Though, the ActiveMQ river plugin was forked from the official RabbitMQ river plugin, which does pretty much the same: https://github.com/elasticsearch/elasticsearch-river-rabbitmq/blob/master/src/main/java/org/elasticsearch/river/rabbitmq/RabbitmqRiver.java#L323\n\nWe'll know soon if that's an issue, the third node has been prepared and the experiment is in progress. Stay tuned...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37807411","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37807411","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37807411,"node_id":"MDEyOklzc3VlQ29tbWVudDM3ODA3NDEx","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-17T11:53:54Z","updated_at":"2014-03-17T11:53:54Z","author_association":"CONTRIBUTOR","body":"We've ran the experiment that @bleskes described above. The problem doesn't seem to be tied to the river (Node 3 in green), as the count of (deleted) open file handles still increases on the master node (Node 2 in blue).\n\nI'm curious what makes the master node special in terms of why the file handles keep leaking on it, but not on the other worker node (Node 1 in red).\n\nAnother interesting piece of info is that the deleted docs ratio on both nodes is between 30% and 35%, which seems like a lot. The HQ plugin hints at slow IO when that ratio goes over 25%.\n\nWe're also making strong use of filter and fielddata caches (both set to only use 30% of the heap), but I'm unsure yet if and how this could be an issue.\n\nI appreciate any further insights. Thanks.\n\n![capture decran 2014-03-17 a 12 30 15](https://f.cloud.github.com/assets/1280019/2435601/aaf367a6-adca-11e3-9a7a-27a7fbc5948d.png)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37810275","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37810275","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37810275,"node_id":"MDEyOklzc3VlQ29tbWVudDM3ODEwMjc1","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-03-17T12:37:13Z","updated_at":"2014-03-17T12:37:13Z","author_association":"MEMBER","body":"@consulthys thx for the input. This indeed suggest the river is not the issue. I wonder if you have some background process that continuously checks for some master-level (i.e., cluster meta data) info like cluster health. Maybe that one doesn't properly close it's connections. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37831334","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37831334","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37831334,"node_id":"MDEyOklzc3VlQ29tbWVudDM3ODMxMzM0","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-17T15:46:25Z","updated_at":"2014-03-17T15:46:25Z","author_association":"CONTRIBUTOR","body":"We don't have anything else but ES running on those nodes. We do check the cluster health remotely via HTTP (mostly to get those graphs above), but the vast majority of (deleted) open file handles are not socket handles, but file handles to Lucene index files (lots of cfs, fdt, etc) and other ES internal files (doc, tim, pos, pay, etc).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37902383","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37902383","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37902383,"node_id":"MDEyOklzc3VlQ29tbWVudDM3OTAyMzgz","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-18T06:09:54Z","updated_at":"2014-03-18T06:09:54Z","author_association":"CONTRIBUTOR","body":"In order to shed some more light on those file handles I was mentioning earlier, here is a quick breakdown for the current set of the 12,194 open (deleted) file handles that we have right now:\n- cfs: 4046 (lucene compound file)\n- pos: 1358 (positions file)\n- pay: 1358 (payload file)\n- tim: 1358 (terms dictionary)\n- doc: 1358 (frequencies and skip data)\n- fdt: 1358 (field data)\n- nvd: 1358 (doc values data)\n\nIt somehow looks like an old Lucene bug (https://issues.apache.org/jira/browse/LUCENE-2762) but I'd assume it's fixed by now. I have yet to dive into the ES code that uses IndexReader/Writer/Searcher, but appreciate any kinds of insights regarding this.\n\nAlso, could it be that our filter cache is not sized adequately (currently 30% of 9GB heap)? When I look at the charts below representing the evolution of the filters and fielddata cache sizes during the experiment, it looks like the filters cache could use some more space. I'm not sure if this would have any impact at all, but I'm just pointing it out.\n\n![capture decran 2014-03-18 a 05 15 47](https://f.cloud.github.com/assets/1280019/2444423/2f35d70e-ae54-11e3-82dc-e147e938c3b8.png)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37911312","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37911312","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37911312,"node_id":"MDEyOklzc3VlQ29tbWVudDM3OTExMzEy","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-03-18T09:03:55Z","updated_at":"2014-03-18T09:03:55Z","author_association":"MEMBER","body":"@consulthys thx for the info. Although we typically only discuss issues here (and answer more general questions on the mailing list),I figured you earned the question :) - look at the amount of rejections from your filter cache and query performance (do you see spikes?) to decide wether you want to give more space to you filter cache.\n\nThat said - let's go back to the file handles. It's not uncommon to have so many file handles and that's OK. What I don't understand is why the master node seem to have it's file handles grow during indexing and just drop (same goes for high cpu usage). In the last image you gave both nodes are the same. I would be interested in the same kind of analysis of the master node when it's unusually high. Also can you give the output `GET _nodes/hot_threads` at the same time? This may give us insights into what it is actually doing.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37912527","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37912527","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37912527,"node_id":"MDEyOklzc3VlQ29tbWVudDM3OTEyNTI3","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-18T09:21:27Z","updated_at":"2014-03-18T09:21:27Z","author_association":"CONTRIBUTOR","body":"Thank you @bleskes.\n\nI'm ok with having a high file handle count on any node, too, as long as we know that it will decrease at some point. The thing is that it just never decreases. When you see a drop, it's because we had to restart the node. So far, we've basically **never** seen the file handle count decrease at all on the master, it's always steadily increasing and is only a matter of time until the resources (CPU, RAM, file handles, etc) get exhausted. Even though a couple K new documents are being indexed every couple seconds, I'd still expect the file handles count to grow/shrink as needed, but I'd expect to see the count shrink eventually, and most of all, I'd expect the (deleted) file handles to really be deleted at some point. This is telling me that something within ES (or Lucene) must still be referencing those files, and that's the big unknown for me so far.\n\nI'm going to dig up some more info (hot threads et al) and get back here as soon as I have something.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37927849","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37927849","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37927849,"node_id":"MDEyOklzc3VlQ29tbWVudDM3OTI3ODQ5","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-18T12:41:49Z","updated_at":"2014-03-18T12:41:49Z","author_association":"CONTRIBUTOR","body":"Moreover, if you think I should be moving this to the mailing list until we figure out if this is an issue or not, please let me know. My guts tell me that there's something here, and I can hardly believe I'm the only one witnessing this kind of behavior.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/37959018","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-37959018","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":37959018,"node_id":"MDEyOklzc3VlQ29tbWVudDM3OTU5MDE4","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-03-18T17:03:41Z","updated_at":"2014-03-18T17:03:41Z","author_association":"MEMBER","body":"We have everything here already, so I think we can do it here. Let me know when you have news.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38171081","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-38171081","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":38171081,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MTcxMDgx","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-20T14:13:30Z","updated_at":"2014-03-20T14:13:30Z","author_association":"CONTRIBUTOR","body":"As discussed, I'm providing some new insights on the hot threads running on the master node. What I did was to call `_nodes/hot_threads` on the master node every minute since the beginning of the last run until we had to restart the master a couple minutes ago (i.e. a time span of ~34h). That gives us ~5,3K samples to work on. From those, I'm filtering out all threads whose CPU usage is below 10% and the threads from the [management] pool as they are not that interesting. That leaves us 83 hot thread samples whose CPU usage ranges from 10% to 80%+, let's call them \"very hot threads\". Let me know if you need all 5K samples, but at quick glance, I think 95% of those are just noise.\n\nWhat we can see is that the very hot threads appearing most often come (unsurprisingly) from the following pools:\n- bulk\n- refresh \n- search\n- Lucene merge\n\nNote: For the latter group, you'll see I've renamed the index on which the merge operation is being carried out to \"biggest_index\", \"2nd_biggest_index\" and \"3rd_biggest_index\", because the real index names as such don't really mean anything without more business context.\n\nYou can find the gist here:\nhttps://gist.github.com/consulthys/49fd0fb6ad71222f4a75\n\nLet's see if we can find any meaningful nugget of information in there.\n\nAlso, for reference purposes, here is a discussion I could find on the mailing list which deals with a very similar topic, but no answers so far.\nhttps://groups.google.com/forum/?fromgroups#!searchin/elasticsearch/open$20file$20handle/elasticsearch/i-qpbSj1Qrc/R84KR50d7OwJ\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38425660","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-38425660","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":38425660,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDI1NjYw","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-24T09:41:20Z","updated_at":"2014-03-24T09:41:20Z","author_association":"CONTRIBUTOR","body":"In order to bring the hot threads output I shared earlier into perspective with the stats charts, I've also added on the chart the hot threads pools we're interested in (bulk, refresh, etc) and their usage percentage along the same timeline as the other charts. The result is shown below and what gets immediately apparent is that the refresh + Lucene merge threads are getting extra busy in the end (far right) which ultimately led to a restart of the master node (in red) since the heap was exhausted. \n![capture decran 2014-03-24 a 10 19 23](https://f.cloud.github.com/assets/1280019/2497503/ff1f6326-b337-11e3-8199-c1b06ddf90ab.png)\n\nI'm also attaching a zoom of the last 90 minutes of hot threads activity capture:\n![capture decran 2014-03-24 a 10 30 36](https://f.cloud.github.com/assets/1280019/2497483/bf376fe2-b337-11e3-930c-2726da404edf.png)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38428983","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-38428983","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":38428983,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDI4OTgz","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-03-24T10:24:41Z","updated_at":"2014-03-24T10:24:41Z","author_association":"MEMBER","body":"I've looked at the hot threads dumps and also at the latest charts (where filter evictions + cpu are interesting). It seems you are using a plugin to add an update by query functionality: https://github.com/yakaz/elasticsearch-action-updatebyquery . If I read the code correctly, that one makes all it's search on the primary shards and that puts those under heavy load. This makes we wonder if the issue is not so much which node is master but which holds the primary shards. As soon as the node is restarted, the shards on the other node are promoted to primary, which explains the pattern you see.\n\nAs an experiment - can you disable that plugin and see if this helps?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38431151","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-38431151","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":38431151,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDMxMTUx","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-24T10:53:24Z","updated_at":"2014-03-24T10:53:24Z","author_association":"CONTRIBUTOR","body":"Thanks for your input. We're indeed using the updatebyquery plugin (mainly because of https://github.com/elasticsearch/elasticsearch/issues/1607 and https://github.com/elasticsearch/elasticsearch/issues/2230). Unfortunately, we cannot disable that plugin in production.\n\nOn \"master vs primary\", I think that would make perfect sense. Most of the time, all primary shards are on the master since we're only bringing down one node at a time and waiting for full cluster recovery before restarting the second node. Doing so will always put primary shards on the master, indeed. But, I do remember having restarted both nodes at the same time and seeing the primary shards being distributed among both nodes. When that happened (see chart below), the \"open files count\" for the worker node wasn't flat as in the latter chart above, but instead the count was also increasing as on the master.\n\n![capture decran 2014-03-24 a 11 37 37](https://f.cloud.github.com/assets/1280019/2498009/606d947e-b340-11e3-9344-59e252d713ac.png)\n\nSo, we're zeroing in onto something here. It definitely looks like \"master node or not\" is not the real issue, but \"primary shard or not\" is, especially in the context of using the updatebyquery plugin which does all the work on the primary shards. Maybe @martijnvg and potentially @ofavre could shed some light on this.\n\nAs a side note, do you happen to have any insider info on the timeline for completing #2230?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38443084","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-38443084","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":38443084,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDQzMDg0","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-24T13:24:03Z","updated_at":"2014-03-24T13:24:03Z","author_association":"CONTRIBUTOR","body":"Looking at the source code of [TransportShardUpdateByQueryAction.java](https://github.com/yakaz/elasticsearch-action-updatebyquery/blob/master/src/main/java/org/elasticsearch/action/updatebyquery/TransportShardUpdateByQueryAction.java), I'm wondering if the index searcher/reader are being properly closed/released in all situations as pointed out in other discussions, such as:\n- http://elasticsearch-users.115913.n3.nabble.com/Open-deleted-file-handles-with-elasticsearch-td4019658.html\n- http://www.gossamer-threads.com/lists/lucene/java-user/52759\n- http://www.gossamer-threads.com/lists/lucene/general/167136\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38452956","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-38452956","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":38452956,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDUyOTU2","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2014-03-24T14:48:25Z","updated_at":"2014-03-24T14:48:25Z","author_association":"MEMBER","body":"@consulthys Yes, there might be a bug there. Not sure yet, it is a long time ago since I wrote that code and a lot has changed since then (the es code base). It should be released once the SearchContext is released. I'm rebasing the updatebyquery branch now and I'll double check is the searcher is released properly.\n\nThe query is executed only on the primary shards, so that the re-index operations can immediately happen in on the primary shards and then can be send to the replicas without executing the query there as well.\n\nThe reason that update by query is never pushed to ES, is that update by query operation can be a long running operation and there is no way of finding if an update by query is running and the cancel it.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38453530","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-38453530","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":38453530,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDUzNTMw","user":{"login":"ofavre","id":95129,"node_id":"MDQ6VXNlcjk1MTI5","avatar_url":"https://avatars3.githubusercontent.com/u/95129?v=4","gravatar_id":"","url":"https://api.github.com/users/ofavre","html_url":"https://github.com/ofavre","followers_url":"https://api.github.com/users/ofavre/followers","following_url":"https://api.github.com/users/ofavre/following{/other_user}","gists_url":"https://api.github.com/users/ofavre/gists{/gist_id}","starred_url":"https://api.github.com/users/ofavre/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ofavre/subscriptions","organizations_url":"https://api.github.com/users/ofavre/orgs","repos_url":"https://api.github.com/users/ofavre/repos","events_url":"https://api.github.com/users/ofavre/events{/privacy}","received_events_url":"https://api.github.com/users/ofavre/received_events","type":"User","site_admin":false},"created_at":"2014-03-24T14:52:56Z","updated_at":"2014-03-24T14:52:56Z","author_association":"CONTRIBUTOR","body":"@martijnvg You'd better work from the plugin as it has already been rebased a few times, and there are a few additions that have been made to your original code. Thanks!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38471589","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-38471589","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":38471589,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDcxNTg5","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2014-03-24T17:08:45Z","updated_at":"2014-03-24T17:08:45Z","author_association":"MEMBER","body":"The update by query code doesn't close the SearchContext when on a shard no documents match with the query (which is likely to happen) or no docs match at all with the specified query.\n\nIn the TransportShardUpdateByQueryAction#doExecuteInternal() method in the following if statement:\n\n``` java\nif (docsToUpdateCount == 0) {\n   ....\n}\n```\n\nThe following statement should be added in order to close the search context and release any resources associated with it:\n\n``` java\nsearchContext.release();\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38538263","html_url":"https://github.com/elastic/elasticsearch/issues/5189#issuecomment-38538263","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5189","id":38538263,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NTM4MjYz","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2014-03-25T07:53:48Z","updated_at":"2014-03-25T07:54:09Z","author_association":"CONTRIBUTOR","body":"@martijnvg and @ofavre I've pulled the 1.4 branch of the updatebyquery plugin, applied the proposed one-liner, repackaged the plugin and restarted both nodes. I'm monitoring right now and we'll see how it goes today, but one great news is that I can already see the count of deleted files decrease and get down to 0, which it never ever did. The open files count is currently the same on both nodes and steady around 2K after 3 hours of operation.\n\n@bleskes Thanks for showing the direction ;)\n","performed_via_github_app":null}]