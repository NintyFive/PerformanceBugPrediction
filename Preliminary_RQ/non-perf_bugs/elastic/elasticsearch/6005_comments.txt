[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/41914191","html_url":"https://github.com/elastic/elasticsearch/issues/6005#issuecomment-41914191","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6005","id":41914191,"node_id":"MDEyOklzc3VlQ29tbWVudDQxOTE0MTkx","user":{"login":"rtoma","id":2914051,"node_id":"MDQ6VXNlcjI5MTQwNTE=","avatar_url":"https://avatars2.githubusercontent.com/u/2914051?v=4","gravatar_id":"","url":"https://api.github.com/users/rtoma","html_url":"https://github.com/rtoma","followers_url":"https://api.github.com/users/rtoma/followers","following_url":"https://api.github.com/users/rtoma/following{/other_user}","gists_url":"https://api.github.com/users/rtoma/gists{/gist_id}","starred_url":"https://api.github.com/users/rtoma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rtoma/subscriptions","organizations_url":"https://api.github.com/users/rtoma/orgs","repos_url":"https://api.github.com/users/rtoma/repos","events_url":"https://api.github.com/users/rtoma/events{/privacy}","received_events_url":"https://api.github.com/users/rtoma/received_events","type":"User","site_admin":false},"created_at":"2014-05-01T14:25:32Z","updated_at":"2014-05-01T14:26:25Z","author_association":"NONE","body":"When setting number_of_replicas=2 (I have 3 nodes) the recovery failes on both replica hosts. So I guess the primary is broken?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/41915262","html_url":"https://github.com/elastic/elasticsearch/issues/6005#issuecomment-41915262","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6005","id":41915262,"node_id":"MDEyOklzc3VlQ29tbWVudDQxOTE1MjYy","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2014-05-01T14:37:05Z","updated_at":"2014-05-01T14:37:05Z","author_association":"MEMBER","body":"if you restart the node that will eventually hold the replicas, does it still happen? This might the caused because of this bug that is fixed in Lucene 4.8: https://issues.apache.org/jira/browse/LUCENE-5612\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/41917421","html_url":"https://github.com/elastic/elasticsearch/issues/6005#issuecomment-41917421","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6005","id":41917421,"node_id":"MDEyOklzc3VlQ29tbWVudDQxOTE3NDIx","user":{"login":"rtoma","id":2914051,"node_id":"MDQ6VXNlcjI5MTQwNTE=","avatar_url":"https://avatars2.githubusercontent.com/u/2914051?v=4","gravatar_id":"","url":"https://api.github.com/users/rtoma","html_url":"https://github.com/rtoma","followers_url":"https://api.github.com/users/rtoma/followers","following_url":"https://api.github.com/users/rtoma/following{/other_user}","gists_url":"https://api.github.com/users/rtoma/gists{/gist_id}","starred_url":"https://api.github.com/users/rtoma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rtoma/subscriptions","organizations_url":"https://api.github.com/users/rtoma/orgs","repos_url":"https://api.github.com/users/rtoma/repos","events_url":"https://api.github.com/users/rtoma/events{/privacy}","received_events_url":"https://api.github.com/users/rtoma/received_events","type":"User","site_admin":false},"created_at":"2014-05-01T14:58:36Z","updated_at":"2014-05-01T14:58:36Z","author_association":"NONE","body":"Hi Kimchy,\n\nWe will try to restart the node next week.\n\nWe were testdriving curator yesterday during the same timerange to optimize the index to 2 max_num_segments. Maybe this caused the index to get broken. Strange fact: searching it does not return errors (yet).\n\nI have looked for the 1st error regarding this index:\n\n```\n[2014-04-30 13:55:37,009][WARN ][index.merge.scheduler    ] [adm-logsearch-db-005.ams5] [logstash-adm-syslog-2014.04.07][0] failed to merge\njava.io.EOFException: read past EOF: MMapIndexInput(path=\"/srv/elastic5/logsearch-pro-cluster/nodes/0/indices/logstash-adm-syslog-2014.04.07/0/index/_n\n08.fdt\")\n        at org.apache.lucene.store.ByteBufferIndexInput.readBytes(ByteBufferIndexInput.java:101)\n        at org.apache.lucene.store.DataOutput.copyBytes(DataOutput.java:254)\n        at org.elasticsearch.index.store.Store$StoreIndexOutput.copyBytes(Store.java:619)\n        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader$ChunkIterator.copyCompressedData(CompressingStoredFieldsReader.java:499)\n        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.merge(CompressingStoredFieldsWriter.java:376)\n        at org.apache.lucene.index.SegmentMerger.mergeFields(SegmentMerger.java:316)\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:94)\n        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4071)\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3668)\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)\n        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/42421313","html_url":"https://github.com/elastic/elasticsearch/issues/6005#issuecomment-42421313","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6005","id":42421313,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNDIxMzEz","user":{"login":"rtoma","id":2914051,"node_id":"MDQ6VXNlcjI5MTQwNTE=","avatar_url":"https://avatars2.githubusercontent.com/u/2914051?v=4","gravatar_id":"","url":"https://api.github.com/users/rtoma","html_url":"https://github.com/rtoma","followers_url":"https://api.github.com/users/rtoma/followers","following_url":"https://api.github.com/users/rtoma/following{/other_user}","gists_url":"https://api.github.com/users/rtoma/gists{/gist_id}","starred_url":"https://api.github.com/users/rtoma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rtoma/subscriptions","organizations_url":"https://api.github.com/users/rtoma/orgs","repos_url":"https://api.github.com/users/rtoma/repos","events_url":"https://api.github.com/users/rtoma/events{/privacy}","received_events_url":"https://api.github.com/users/rtoma/received_events","type":"User","site_admin":false},"created_at":"2014-05-07T12:38:28Z","updated_at":"2014-05-07T12:38:28Z","author_association":"NONE","body":"@kimchy: I finished a rolling restart of my cluster holding that 1 bad index. After restart I increased replica count from 0 to 1 and now the replica is good.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/42657161","html_url":"https://github.com/elastic/elasticsearch/issues/6005#issuecomment-42657161","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6005","id":42657161,"node_id":"MDEyOklzc3VlQ29tbWVudDQyNjU3MTYx","user":{"login":"rtoma","id":2914051,"node_id":"MDQ6VXNlcjI5MTQwNTE=","avatar_url":"https://avatars2.githubusercontent.com/u/2914051?v=4","gravatar_id":"","url":"https://api.github.com/users/rtoma","html_url":"https://github.com/rtoma","followers_url":"https://api.github.com/users/rtoma/followers","following_url":"https://api.github.com/users/rtoma/following{/other_user}","gists_url":"https://api.github.com/users/rtoma/gists{/gist_id}","starred_url":"https://api.github.com/users/rtoma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rtoma/subscriptions","organizations_url":"https://api.github.com/users/rtoma/orgs","repos_url":"https://api.github.com/users/rtoma/repos","events_url":"https://api.github.com/users/rtoma/events{/privacy}","received_events_url":"https://api.github.com/users/rtoma/received_events","type":"User","site_admin":false},"created_at":"2014-05-09T11:38:33Z","updated_at":"2014-05-09T11:38:33Z","author_association":"NONE","body":"It gets stranger. Throwing away the bad index and recreating it, triggers the same issue. Somewhere state is maintained outside the index causing the corruption?\n\nDetails:\n\nAll of a sudden 1 shard of my 0-replica index got unassigned. Recovery is never ending.\n\n```\nindex                          shard prirep state          docs store ip           node                      \nlogstash-adm-syslog-2014.04.07 2     p      STARTED    11337750 4.2gb 10.98.252.85 adm-logsearch-db-005.ams5 \nlogstash-adm-syslog-2014.04.07 0     p      UNASSIGNED                                                       \nlogstash-adm-syslog-2014.04.07 3     p      STARTED    11324303 4.2gb 10.98.252.22 adm-logsearch-db-002.ams5 \nlogstash-adm-syslog-2014.04.07 1     p      STARTED    11348976 4.2gb 10.98.252.21 adm-logsearch-db-001.ams5 \nlogstash-adm-syslog-2014.04.07 4     p      STARTED    11326723 4.2gb 10.98.252.22 adm-logsearch-db-002.ams5 \n```\n\nSo we decided to deleted and recreate the whole index. The index was really gone (checked on disk), but recreation triggers a bad shard + replica again:\n\n```\nindex                          shard prirep state        docs store ip           node                      \nlogstash-adm-syslog-2014.04.07 0     p      INITIALIZING            10.98.252.85 adm-logsearch-db-005.ams5 \nlogstash-adm-syslog-2014.04.07 0     r      UNASSIGNED                                                     \nlogstash-adm-syslog-2014.04.07 1     r      STARTED         0   79b 10.98.252.21 adm-logsearch-db-001.ams5 \nlogstash-adm-syslog-2014.04.07 1     p      STARTED         0   99b 10.98.252.22 adm-logsearch-db-002.ams5 \nlogstash-adm-syslog-2014.04.07 2     r      STARTED         0   79b 10.98.252.21 adm-logsearch-db-001.ams5 \nlogstash-adm-syslog-2014.04.07 2     p      STARTED         0   99b 10.98.252.22 adm-logsearch-db-002.ams5 \n```\n\nLogging says:\n\n```\n[2014-05-09 13:32:34,746][WARN ][index.engine.internal    ] [adm-logsearch-db-005.ams5] [logstash-adm-syslog-2014.04.07][0] Could not lock IndexWriter isLocked [false]\norg.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/srv/elastic1/logsearch-pro-cluster/nodes/0/indices/logstash-adm-syslog-2014.04.07/0/index/write.lock\n        at org.apache.lucene.store.Lock.obtain(Lock.java:84)\n        at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:702)\n        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1399)\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:258)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.postRecovery(InternalIndexShard.java:684)\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n[2014-05-09 13:32:34,746][WARN ][indices.cluster          ] [adm-logsearch-db-005.ams5] [logstash-adm-syslog-2014.04.07][0] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-adm-syslog-2014.04.07][0] failed recovery\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: org.elasticsearch.index.engine.EngineCreationFailureException: [logstash-adm-syslog-2014.04.07][0] failed to create engine\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:260)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.postRecovery(InternalIndexShard.java:684)\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)\n        ... 3 more\nCaused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/srv/elastic1/logsearch-pro-cluster/nodes/0/indices/logstash-adm-syslog-2014.04.07/0/index/write.lock\n        at org.apache.lucene.store.Lock.obtain(Lock.java:84)\n        at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:702)\n        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1399)\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:258)\n        ... 6 more\n[2014-05-09 13:32:34,764][WARN ][cluster.action.shard     ] [adm-logsearch-db-005.ams5] [logstash-adm-syslog-2014.04.07][0] sending failed shard for [logstash-adm-syslog-2014.04.07][0], node[l_U2IDMwQwSNYzpsyGuWaQ], [P], s[INITIALIZING], indexUUID [p0dmJBQiROClRk3cIXZOkA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-adm-syslog-2014.04.07][0] failed recovery]; nested: EngineCreationFailureException[[logstash-adm-syslog-2014.04.07][0] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@/srv/elastic1/logsearch-pro-cluster/nodes/0/indices/logstash-adm-syslog-2014.04.07/0/index/write.lock]; ]]\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/49886222","html_url":"https://github.com/elastic/elasticsearch/issues/6005#issuecomment-49886222","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6005","id":49886222,"node_id":"MDEyOklzc3VlQ29tbWVudDQ5ODg2MjIy","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2014-07-23T15:02:57Z","updated_at":"2014-07-23T15:02:57Z","author_association":"CONTRIBUTOR","body":"this has been fixed with Lucene 4.8 upgrade\n","performed_via_github_app":null}]