{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/20768","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20768/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20768/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20768/events","html_url":"https://github.com/elastic/elasticsearch/issues/20768","id":181226666,"node_id":"MDU6SXNzdWUxODEyMjY2NjY=","number":20768,"title":"Scheduling fairness when batching cluster states","user":{"login":"ppf2","id":7216393,"node_id":"MDQ6VXNlcjcyMTYzOTM=","avatar_url":"https://avatars0.githubusercontent.com/u/7216393?v=4","gravatar_id":"","url":"https://api.github.com/users/ppf2","html_url":"https://github.com/ppf2","followers_url":"https://api.github.com/users/ppf2/followers","following_url":"https://api.github.com/users/ppf2/following{/other_user}","gists_url":"https://api.github.com/users/ppf2/gists{/gist_id}","starred_url":"https://api.github.com/users/ppf2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ppf2/subscriptions","organizations_url":"https://api.github.com/users/ppf2/orgs","repos_url":"https://api.github.com/users/ppf2/repos","events_url":"https://api.github.com/users/ppf2/events{/privacy}","received_events_url":"https://api.github.com/users/ppf2/received_events","type":"User","site_admin":false},"labels":[{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2016-10-05T18:10:59Z","updated_at":"2016-10-06T14:11:28Z","closed_at":"2016-10-06T14:11:28Z","author_association":"MEMBER","active_lock_reason":null,"body":"While performing RCA for a different tribe node behavior, Yannick identified an additional potential improvement around scheduling fairness when batching cluster states.  This can cause pending cluster state updates to accumulate.  \n\nSummary below:\n\n\"Pending tasks in most of the diagnostic snapshots show there are pending cluster state updates from all of the upstream clusters. Batching is done on a per upstream cluster level. This means that with 22 clusters, pending tasks should in the worst case take 22 clusters \\* 15 seconds per cluster state update = 5,5 minutes. This seems not to be the case, pending tasks show tasks waiting for 20 minutes (and more). The issue is I think how scheduling fairness comes into play when batching cluster states.\n\nAssume the following cluster state updates being submitted by the upstream tribe nodes to the \"compound\" tribe node:\n\n```\ncluster update from cluster A (A1)\ncluster update from cluster B (B1)\ncluster update from cluster A (A2)\ncluster update from cluster B (B2)\ncluster update from cluster A (A3)\ncluster update from cluster C (C1)\n```\n\nCluster state update tasks are processed using a priority queue as a work queue. If two tasks have the same priority, the first to arrive will be executed first (FIFO style). For the tribe node, all cluster state update tasks that are generated for incoming cluster state updates have the same priority, and are thus processed in FIFO order. Cluster service also has support for batching cluster state updates, explained later. The first cluster state update to be scheduled will be cluster update from cluster A. Due to batching it will process the payloads A1, A2, and A3 in one go. Even though the payloads A1, A2 and A3 are processed in one go, the tasks that had the payloads A2 and A3 are still in the queue:\n\n```\ncluster update from cluster B (B1)\ncluster update from cluster A (processed)\ncluster update from cluster B (B2)\ncluster update from cluster A (processed)\ncluster update from cluster C (C1)\n```\n\nThe next cluster state update to be processed will be the one from cluster B (payloads B1 and B2), yielding the queue:\n\n```\ncluster update from cluster A (processed)\ncluster update from cluster B (processed)\ncluster update from cluster A (processed)\ncluster update from cluster C (C1)\n```\n\nIf, in the meantime, a new update from cluster A has come in, the queue looks as follows:\n\n```\ncluster update from cluster A (processed)\ncluster update from cluster B (processed)\ncluster update from cluster A (processed)\ncluster update from cluster C (C1)\ncluster update from cluster A (A4)\n```\n\nThis means however that A4 gets to execute before C1 because it will be picked up by the first \"cluster update from cluster A\" task. If there are long sequences of these A and B before any C, it can take quite a while before C gets a chance to execute.\n\n@bleskes @ywelsch \n","closed_by":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"performed_via_github_app":null}