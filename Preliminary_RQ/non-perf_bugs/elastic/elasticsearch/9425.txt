{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/9425","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9425/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9425/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9425/events","html_url":"https://github.com/elastic/elasticsearch/issues/9425","id":55515589,"node_id":"MDU6SXNzdWU1NTUxNTU4OQ==","number":9425,"title":"Provide an option to only assign shards to nodes that already have them as part of allocation","user":{"login":"ppf2","id":7216393,"node_id":"MDQ6VXNlcjcyMTYzOTM=","avatar_url":"https://avatars0.githubusercontent.com/u/7216393?v=4","gravatar_id":"","url":"https://api.github.com/users/ppf2","html_url":"https://github.com/ppf2","followers_url":"https://api.github.com/users/ppf2/followers","following_url":"https://api.github.com/users/ppf2/following{/other_user}","gists_url":"https://api.github.com/users/ppf2/gists{/gist_id}","starred_url":"https://api.github.com/users/ppf2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ppf2/subscriptions","organizations_url":"https://api.github.com/users/ppf2/orgs","repos_url":"https://api.github.com/users/ppf2/repos","events_url":"https://api.github.com/users/ppf2/events{/privacy}","received_events_url":"https://api.github.com/users/ppf2/received_events","type":"User","site_admin":false},"labels":[{"id":836504707,"node_id":"MDU6TGFiZWw4MzY1MDQ3MDc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Distributed","name":":Distributed/Distributed","color":"0e8a16","default":false,"description":"A catch all label for anything in the Distributed Area. If you aren't sure, use this one."},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2015-01-26T18:02:14Z","updated_at":"2018-02-13T19:28:31Z","closed_at":"2015-08-05T10:53:19Z","author_association":"MEMBER","active_lock_reason":null,"body":"Consider the following repro:\n\n![image](https://cloud.githubusercontent.com/assets/7216393/5904896/e6ce34ea-a540-11e4-8140-d51257fd5acc.png)\n\n![image](https://cloud.githubusercontent.com/assets/7216393/5904901/edec01a8-a540-11e4-8b2a-418d0b87c6a9.png)\n\n![image](https://cloud.githubusercontent.com/assets/7216393/5904905/f5ee80c4-a540-11e4-8f5e-38480eac83c4.png)\n\n![image](https://cloud.githubusercontent.com/assets/7216393/5904908/fba44abc-a540-11e4-9f50-ac5c47d4a0b5.png)\n\n![image](https://cloud.githubusercontent.com/assets/7216393/5904912/07b3db74-a541-11e4-85b0-c38232bad05f.png)\n\n![image](https://cloud.githubusercontent.com/assets/7216393/5904916/0d9cef44-a541-11e4-89bc-99c9e8902701.png)\n- node 2 has [0], [1], [4] as its original allocation.\n- cluster.routing.allocation.node_concurrent_recoveries is set to 1 (on purpose for the reproduction).\n- allocation is disabled, and then node 2 is stopped.\n- node 2 is started back up and then allocation is enabled again.\n- [0] and [1] get allocated back to node 2 successfully when allocation is enabled (after its restart)\n- [4] ends up on node 1 (instead of node 2) because node 2 already has 1 target recovery outstanding (so it decides to find another node to allocate [4] to while node 2 is performing its recovery) while node 1 has 1 source recovery (but capable of being a target for the recovery of [4]).\n- At the end, rebalancing kicks in and moved [2] from node 1 to node 2, so node 2 now has [0], [1], and [2] instead of [0], [1], [4] .\n\nIt will be helpful to provide an additional option to `cluster.routing.allocation.enable` so that it will only assign shards to nodes that already have them to prevent it from performing unnecessary allocation of a shard to a different node as part of rolling restarts.  While increasing cluster.routing.allocation.node_concurrent_recoveries (from the default of 2) is a potential workaround for small deployments, it is not a viable solution for deployments with a large # of shards on each node due to its potential network and i/o implications.  For example, we can add an `existing` option to cluster.routing.allocation.enable that also works in conjunction with settings like new_primaries (eg. \"existing,new_primaries‚Äù).  \n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}