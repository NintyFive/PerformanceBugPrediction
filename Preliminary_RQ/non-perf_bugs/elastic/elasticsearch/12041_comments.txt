[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/118780100","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-118780100","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":118780100,"node_id":"MDEyOklzc3VlQ29tbWVudDExODc4MDEwMA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-07-06T09:06:11Z","updated_at":"2015-07-06T09:06:11Z","author_association":"MEMBER","body":"This message is worrisome - it seems you have 7GB of memory, which took 18.7 minutes to GC? What kind of hardware are you running on? did you disable swap?\n\n```\n[server05] [gc][old][266076][102] duration [18.7m], collections [69]/[19.9m], total [18.7m]/[25.8m], memory [7.9gb]->[7.8gb]/[7.9gb], all_pools {[young] [532.5mb]->[532.5mb]/[532.5mb]}{[survivor] [65.2mb]->[16.5mb]/[66.5mb]}{[old] [7.3gb]->[7.3gb]/[7.3gb]}\"\n```\n\nOnce you've reached an OOM you can better restart the node as many unexpected things can happen. It's especially bad of the oom happened in the network stack (like yours did). Can you specify some more details about the corruption reported? \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/118792737","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-118792737","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":118792737,"node_id":"MDEyOklzc3VlQ29tbWVudDExODc5MjczNw==","user":{"login":"kristoffer-dyrkorn","id":2276260,"node_id":"MDQ6VXNlcjIyNzYyNjA=","avatar_url":"https://avatars1.githubusercontent.com/u/2276260?v=4","gravatar_id":"","url":"https://api.github.com/users/kristoffer-dyrkorn","html_url":"https://github.com/kristoffer-dyrkorn","followers_url":"https://api.github.com/users/kristoffer-dyrkorn/followers","following_url":"https://api.github.com/users/kristoffer-dyrkorn/following{/other_user}","gists_url":"https://api.github.com/users/kristoffer-dyrkorn/gists{/gist_id}","starred_url":"https://api.github.com/users/kristoffer-dyrkorn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kristoffer-dyrkorn/subscriptions","organizations_url":"https://api.github.com/users/kristoffer-dyrkorn/orgs","repos_url":"https://api.github.com/users/kristoffer-dyrkorn/repos","events_url":"https://api.github.com/users/kristoffer-dyrkorn/events{/privacy}","received_events_url":"https://api.github.com/users/kristoffer-dyrkorn/received_events","type":"User","site_admin":false},"created_at":"2015-07-06T10:05:26Z","updated_at":"2015-07-06T10:59:15Z","author_association":"NONE","body":"Each data node (like the one above) in our cluster has 16 GB RAM and 8 GB heap.  The aggregations are very heavy, so long GCs are as such not the main problem here (we need to find ways to limit the queries on our part).\n\nMy main question here is: Should an OOM occur on one node, would shard corruption - and then data loss - then be one of the possible (expected) outcomes?\n\nThe net effect of the crash has been a data loss of 20% of the original data for a specific set of indices. By looking at shard information we see that some shards have less data (take less space on disk) than they should. We also see missing documents in the affected indices - very evenly spread out across our primary key (an event number id - our data are time series).\n\nWe suspect that due to OOM some shards have been corrupted and then discarded. We have 1 replica for each shard (that is, 1 primary and 1 replica) in the cluster but this has not prevented data loss. (Yes, we could have had more replicas but that is not the main issue here.)\n\nIt seems that long GC pauses have made data nodes unable to connect to the master (we see traces of that in the logs on the data nodes) so an additional factor here might be that the master has not had a chance to become aware of the shard corruption.\n\nMlockall is set to true by us (in elasticsearch.yml) but reading out the actual value from /_nodes/process shows the actual value is false. We are alredy aware of that - we believe the reason is that we have to use virtual machines on VMWare.\n\nWe have not disabled swap explicity, so we run on OS defaults. We are using RHEL (7.0, I think).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/118947389","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-118947389","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":118947389,"node_id":"MDEyOklzc3VlQ29tbWVudDExODk0NzM4OQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-07-06T18:26:09Z","updated_at":"2015-07-06T18:26:09Z","author_association":"MEMBER","body":"> The net effect of the crash has been a data loss of 20% of the original data for a specific set of indices. By looking at shard information we see that some shards have less data (take less space on disk) than they should.\n\nHow much less? Shards may very in size based on their merge schedules.\n\n> We also see missing documents in the affected indices - very evenly spread out across our primary key (an event number id - our data are time series).\n\nDo you delete documents ? if so, you should take them into account as the stats report the total amount of documents in the index. It has a separate entry for deleted docs (which will be purge away by the background merge process).\n\nAre there any other signs of corruption? A log entry would be great.\n\n> It seems that long GC pauses have made data nodes unable to connect to the master (we see traces of that in the logs on the data nodes) so an additional factor here might be that the master has not had a chance to become aware of the shard corruption.\n\nA non responsive will be removed from the cluster and it's shard will be assigned somewhere else. Once the node rejoins (after the GC finishes) it will be reassigned shards, which will be resynced to the primaries.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/119152280","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-119152280","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":119152280,"node_id":"MDEyOklzc3VlQ29tbWVudDExOTE1MjI4MA==","user":{"login":"kristoffer-dyrkorn","id":2276260,"node_id":"MDQ6VXNlcjIyNzYyNjA=","avatar_url":"https://avatars1.githubusercontent.com/u/2276260?v=4","gravatar_id":"","url":"https://api.github.com/users/kristoffer-dyrkorn","html_url":"https://github.com/kristoffer-dyrkorn","followers_url":"https://api.github.com/users/kristoffer-dyrkorn/followers","following_url":"https://api.github.com/users/kristoffer-dyrkorn/following{/other_user}","gists_url":"https://api.github.com/users/kristoffer-dyrkorn/gists{/gist_id}","starred_url":"https://api.github.com/users/kristoffer-dyrkorn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kristoffer-dyrkorn/subscriptions","organizations_url":"https://api.github.com/users/kristoffer-dyrkorn/orgs","repos_url":"https://api.github.com/users/kristoffer-dyrkorn/repos","events_url":"https://api.github.com/users/kristoffer-dyrkorn/events{/privacy}","received_events_url":"https://api.github.com/users/kristoffer-dyrkorn/received_events","type":"User","site_admin":false},"created_at":"2015-07-07T10:06:12Z","updated_at":"2015-07-07T12:13:35Z","author_association":"NONE","body":">  How much less? Shards may very in size based on their merge schedules.\n\nThe indices are time-partitioned (one index for each month of data). Each index has 6 shards. The distribution of data for the May index - after our problems - was as follows:\n\nShard 0, 2, 3, 4, 5: 4.1-4.2 Gb\nShard 1: 650-660 Mb (primary & replica)\n\nFor the February index, the distribution was:\n\nShard 1: 115 bytes / 79 bytes (primary & replica)\nShard 0, 2, 3, 4, 5: 0.95 - 1.1 Gb\n\nFor other indices (not affected by data loss) we see very even shard sizes, within 5% of each other. So we suspect merge schedules are not a factor here.\n\n> Do you delete documents ? \n\nNo. This is mostly an append-only system. At times we update existing documents (which we realise may be implemented internally in Lucene as delete + add), but also here we do not think the differences are due to deletes (i.e. updates), as the indices that not were affected by data loss look very similar to each other and they are all quite different to the ones affected.\n\n> Are there any other signs of corruption? A log entry would be great.\n\nThis is for us the the main sign of corruption:\n\n```\njava.nio.file.NoSuchFileException: /data/project/nodes/0/indices/metrics-2015-06/0/index/_2wk2.si\n```\n\nI will look for other log messages and update this issue. \n\nWe see problems on other data nodes as well (OOMs, missing cluster synchronisation) during the failure period. Here is an example of events from a different node.\n\nFirst occurence of an OOM:\n\n```\nTIMESTAMP=\"2015-06-29 12:08:42,117\" LEVEL=\"WARN\" THREAD=\"[elasticsearch[server06][refresh][T#3]]\" HOST=\"server06\" LOGGER=\"index.shard\" MESSAGE=\"[server06] [event_2015_06][4] Failed to perform scheduled engine refresh\"\norg.elasticsearch.index.engine.RefreshFailedEngineException: [event_2015_06][4] Refresh failed\n    at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:576) ~[elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:565) ~[elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1095) ~[elasticsearch-1.5.2.jar:na]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]\njava.lang.OutOfMemoryError: Java heap space\nTIMESTAMP=\"2015-06-29 12:08:42,117\" LEVEL=\"WARN\" THREAD=\"[elasticsearch[server06][flush][T#2]]\" HOST=\"server06\" LOGGER=\"index.engine\" MESSAGE=\"[server06] [event_2015_06][4] failed engine [lucene commit failed]\"\norg.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed\n    at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:700) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]\n    at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:714) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]\n    at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3100) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]\n    at org.elasticsearch.index.engine.InternalEngine.commitIndexWriter(InternalEngine.java:1226) [elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:637) [elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:593) [elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.index.shard.IndexShard.flush(IndexShard.java:675) [elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.index.translog.TranslogService$TranslogBasedFlush$1.run(TranslogService.java:203) [elasticsearch-1.5.2.jar:na]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]\nCaused by: java.lang.OutOfMemoryError: Java heap space\n```\n\nThe node is then unable to let the master know of the shard failure:\n\n```\nTIMESTAMP=\"2015-06-29 12:10:32,819\" LEVEL=\"WARN\" THREAD=\"[elasticsearch[server06][clusterService#updateTask][T#1]]\" HOST=\"server06\" LOGGER=\"indices.cluster\" MESSAGE=\"[server06] [[event_2015_06][4]] marking and sending shard failed due to [master [null] marked shard as started, but shard has not been created, mark shard as failed]\"\n```\n\nThe node is then unable to create shards:\n\n```\nTIMESTAMP=\"2015-06-29 12:15:44,793\" LEVEL=\"WARN\" THREAD=\"[elasticsearch[server06][clusterService#updateTask][T#1]]\" HOST=\"server06\" LOGGER=\"indices.cluster\" MESSAGE=\"[server06] [[event_2015_06][0]] marking and sending shard failed due to [failed to create shard]\"\norg.elasticsearch.index.shard.IndexShardCreationException: [event_2015_06][0] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357) ~[elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:699) [elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:600) [elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:183) [elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:467) [elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188) [elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158) [elasticsearch-1.5.2.jar:na]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [event_2015_06][0], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:415) ~[elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:343) ~[elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310) ~[elasticsearch-1.5.2.jar:na]\n    ... 9 common frames omitted\n```\n\n> Once the node rejoins (after the GC finishes) it will be reassigned shards, which will be resynced\n> to the primaries.\n\nWe experienced problems with reassignments so we had to allocate shards manually to get ES out of its \"red state\". It seems we hit the case described here:\n\nhttps://github.com/elastic/elasticsearch/issues/11309\n\nAt least, this comment fits very well with our hypothesis (and previous observations):\n\n> I suspect that the same behavior will occur if both primary and replica shards have index\n> corruptions.\n\nStill, my main question is: Should an OOM occur on one node, would shard corruption - and then data loss - then be one of the possible (expected) outcomes? \n\nIt would be very valuable for us to have your opinion on whether data loss (on a single node) could be expected should an OOM occur.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/119199964","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-119199964","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":119199964,"node_id":"MDEyOklzc3VlQ29tbWVudDExOTE5OTk2NA==","user":{"login":"kristoffer-dyrkorn","id":2276260,"node_id":"MDQ6VXNlcjIyNzYyNjA=","avatar_url":"https://avatars1.githubusercontent.com/u/2276260?v=4","gravatar_id":"","url":"https://api.github.com/users/kristoffer-dyrkorn","html_url":"https://github.com/kristoffer-dyrkorn","followers_url":"https://api.github.com/users/kristoffer-dyrkorn/followers","following_url":"https://api.github.com/users/kristoffer-dyrkorn/following{/other_user}","gists_url":"https://api.github.com/users/kristoffer-dyrkorn/gists{/gist_id}","starred_url":"https://api.github.com/users/kristoffer-dyrkorn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kristoffer-dyrkorn/subscriptions","organizations_url":"https://api.github.com/users/kristoffer-dyrkorn/orgs","repos_url":"https://api.github.com/users/kristoffer-dyrkorn/repos","events_url":"https://api.github.com/users/kristoffer-dyrkorn/events{/privacy}","received_events_url":"https://api.github.com/users/kristoffer-dyrkorn/received_events","type":"User","site_admin":false},"created_at":"2015-07-07T13:15:23Z","updated_at":"2015-07-07T13:26:35Z","author_association":"NONE","body":"Searching ES logs for information about the February index (where some shards were emptied) gave the following result:\n\n```\nTIMESTAMP=\"2015-06-29 13:18:56,511\" LEVEL=\"WARN\" THREAD=\"[elasticsearch[server03][generic][T#5]]\" HOST=\"server03\" LOGGER=\"indices.cluster\" MESSAGE=\"[server03] [[event_2015_02][1]] marking and sending shard failed due to [failed recovery]\"\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [event_2015_02][1] failed to fetch index version after copying it over\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:157) ~[elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112) ~[elasticsearch-1.5.2.jar:na]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]\nCaused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [event_2015_02][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:131) ~[elasticsearch-1.5.2.jar:na]\n    ... 4 common frames omitted\nCaused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(default(mmapfs(/data/project/nodes/0/indices/event_2015_02/1/index),niofs(/data/project/nodes/0/indices/event_2015_02/1/index)), type=MERGE, rate=20.0)]): files: []\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:881) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:769) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:458) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]\n    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:89) ~[elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:122) ~[elasticsearch-1.5.2.jar:na]\n    ... 4 common frames omitted\n```\n\nThis is the first occurrence of log entry from a data node that has handled those shards (within the period we had failures). So it seems at this point in time, the directory containing this shard (shard 1) has been unexpectedly emptied.\n\nA bit before this, we see that the master node cannot contact this data node. These two log entries (one above, one below) are the only ones that show up in our log analysis tool and that give hints on what happened to the shards.\n\n```\nTIMESTAMP=\"2015-06-29 12:28:09,173\" LEVEL=\"WARN\" THREAD=\"[elasticsearch[server01][clusterService#updateTask][T#1]]\" HOST=\"server01\" LOGGER=\"gateway.local\" MESSAGE=\"[server01] [event_2015_02][1]: failed to list shard stores on node [o7gM5FhPRSKUVqQpxPHq5A]\"\norg.elasticsearch.action.FailedNodeException: Failed node [o7gM5FhPRSKUVqQpxPHq5A]\n    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onFailure(TransportNodesOperationAction.java:206) ~[elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$1000(TransportNodesOperationAction.java:97) ~[elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$4.handleException(TransportNodesOperationAction.java:178) ~[elasticsearch-1.5.2.jar:na]\n    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529) ~[elasticsearch-1.5.2.jar:na]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]\nCaused by: org.elasticsearch.transport.ReceiveTimeoutTransportException: [server04][inet[/10.247.21.4:9300]][internal:cluster/nodes/indices/shard/store[n]] request_id [3857481] timed out after [30000ms]\n    ... 4 common frames omitted\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/124058524","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-124058524","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":124058524,"node_id":"MDEyOklzc3VlQ29tbWVudDEyNDA1ODUyNA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-07-23T11:02:54Z","updated_at":"2015-07-23T11:02:54Z","author_association":"MEMBER","body":"> Still, my main question is: Should an OOM occur on one node, would shard corruption - and then data loss - then be one of the possible (expected) outcomes?\n\nOOM should not cause data corruption and certainly not shards to be removed. \n\nIs the following something that you can reproduce/happens often? I would love to have some trace logs enabled and debug further... \n\n```\nCaused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [event_2015_02][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/128686286","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-128686286","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":128686286,"node_id":"MDEyOklzc3VlQ29tbWVudDEyODY4NjI4Ng==","user":{"login":"kristoffer-dyrkorn","id":2276260,"node_id":"MDQ6VXNlcjIyNzYyNjA=","avatar_url":"https://avatars1.githubusercontent.com/u/2276260?v=4","gravatar_id":"","url":"https://api.github.com/users/kristoffer-dyrkorn","html_url":"https://github.com/kristoffer-dyrkorn","followers_url":"https://api.github.com/users/kristoffer-dyrkorn/followers","following_url":"https://api.github.com/users/kristoffer-dyrkorn/following{/other_user}","gists_url":"https://api.github.com/users/kristoffer-dyrkorn/gists{/gist_id}","starred_url":"https://api.github.com/users/kristoffer-dyrkorn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kristoffer-dyrkorn/subscriptions","organizations_url":"https://api.github.com/users/kristoffer-dyrkorn/orgs","repos_url":"https://api.github.com/users/kristoffer-dyrkorn/repos","events_url":"https://api.github.com/users/kristoffer-dyrkorn/events{/privacy}","received_events_url":"https://api.github.com/users/kristoffer-dyrkorn/received_events","type":"User","site_admin":false},"created_at":"2015-08-07T12:19:44Z","updated_at":"2015-08-07T12:19:44Z","author_association":"NONE","body":"We are working on reproducing the issue now, will post further details here. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/128693515","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-128693515","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":128693515,"node_id":"MDEyOklzc3VlQ29tbWVudDEyODY5MzUxNQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-08-07T12:58:40Z","updated_at":"2015-08-07T12:58:40Z","author_association":"CONTRIBUTOR","body":"@kristoffer-dyrkorn this may be related to the issue fixed in https://github.com/elastic/elasticsearch/pull/12487 so I would advise upgrading.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/134586896","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-134586896","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":134586896,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNDU4Njg5Ng==","user":{"login":"kristoffer-dyrkorn","id":2276260,"node_id":"MDQ6VXNlcjIyNzYyNjA=","avatar_url":"https://avatars1.githubusercontent.com/u/2276260?v=4","gravatar_id":"","url":"https://api.github.com/users/kristoffer-dyrkorn","html_url":"https://github.com/kristoffer-dyrkorn","followers_url":"https://api.github.com/users/kristoffer-dyrkorn/followers","following_url":"https://api.github.com/users/kristoffer-dyrkorn/following{/other_user}","gists_url":"https://api.github.com/users/kristoffer-dyrkorn/gists{/gist_id}","starred_url":"https://api.github.com/users/kristoffer-dyrkorn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kristoffer-dyrkorn/subscriptions","organizations_url":"https://api.github.com/users/kristoffer-dyrkorn/orgs","repos_url":"https://api.github.com/users/kristoffer-dyrkorn/repos","events_url":"https://api.github.com/users/kristoffer-dyrkorn/events{/privacy}","received_events_url":"https://api.github.com/users/kristoffer-dyrkorn/received_events","type":"User","site_admin":false},"created_at":"2015-08-25T13:34:48Z","updated_at":"2015-08-25T13:34:48Z","author_association":"NONE","body":"We have not been able to recreate the issue and cannot spend further resources on trying. So I am afraid we cannot reach a final conclusion on what went wrong. \n\nWe will however upgrade now.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/134592456","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-134592456","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":134592456,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNDU5MjQ1Ng==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-08-25T13:50:59Z","updated_at":"2015-08-25T13:50:59Z","author_association":"CONTRIBUTOR","body":"thanks @kristoffer-dyrkorn - closing for now. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/137705956","html_url":"https://github.com/elastic/elasticsearch/issues/12041#issuecomment-137705956","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12041","id":137705956,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNzcwNTk1Ng==","user":{"login":"nilsga","id":42919,"node_id":"MDQ6VXNlcjQyOTE5","avatar_url":"https://avatars0.githubusercontent.com/u/42919?v=4","gravatar_id":"","url":"https://api.github.com/users/nilsga","html_url":"https://github.com/nilsga","followers_url":"https://api.github.com/users/nilsga/followers","following_url":"https://api.github.com/users/nilsga/following{/other_user}","gists_url":"https://api.github.com/users/nilsga/gists{/gist_id}","starred_url":"https://api.github.com/users/nilsga/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nilsga/subscriptions","organizations_url":"https://api.github.com/users/nilsga/orgs","repos_url":"https://api.github.com/users/nilsga/repos","events_url":"https://api.github.com/users/nilsga/events{/privacy}","received_events_url":"https://api.github.com/users/nilsga/received_events","type":"User","site_admin":false},"created_at":"2015-09-04T11:07:18Z","updated_at":"2015-09-04T11:07:18Z","author_association":"NONE","body":"This issue happende again today (still on 1.5.2), with the same sequence of errors as described earlier. It does indeed look very similar to https://github.com/elastic/elasticsearch/pull/12487. We lost four shards (2 x primary and replica) in this crash, and both primary and replica were located on the nodes that went OOM. After nodes had been restarted, the files and directories for shard 4 and 5 were no longer on disk on the affected data nodes. Both nodes had long GC pauses, so I guess there were good chances for a race condition to happen. Luckily, it only affected one of the indices, which are backed up nightly, so we were able to restore all the data.\n","performed_via_github_app":null}]