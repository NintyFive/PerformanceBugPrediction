{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/34285","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34285/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34285/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34285/events","html_url":"https://github.com/elastic/elasticsearch/issues/34285","id":366533967,"node_id":"MDU6SXNzdWUzNjY1MzM5Njc=","number":34285,"title":"CJK analyzer tokenization issues","user":{"login":"Trey314159","id":13836921,"node_id":"MDQ6VXNlcjEzODM2OTIx","avatar_url":"https://avatars0.githubusercontent.com/u/13836921?v=4","gravatar_id":"","url":"https://api.github.com/users/Trey314159","html_url":"https://github.com/Trey314159","followers_url":"https://api.github.com/users/Trey314159/followers","following_url":"https://api.github.com/users/Trey314159/following{/other_user}","gists_url":"https://api.github.com/users/Trey314159/gists{/gist_id}","starred_url":"https://api.github.com/users/Trey314159/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Trey314159/subscriptions","organizations_url":"https://api.github.com/users/Trey314159/orgs","repos_url":"https://api.github.com/users/Trey314159/repos","events_url":"https://api.github.com/users/Trey314159/events{/privacy}","received_events_url":"https://api.github.com/users/Trey314159/received_events","type":"User","site_admin":false},"labels":[{"id":142001965,"node_id":"MDU6TGFiZWwxNDIwMDE5NjU=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Analysis","name":":Search/Analysis","color":"0e8a16","default":false,"description":"How text is split into tokens"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2018-10-03T21:19:53Z","updated_at":"2019-04-03T08:34:16Z","closed_at":"2019-04-03T08:34:16Z","author_association":"NONE","active_lock_reason":null,"body":"It makes sense to me to report these all together, but I can split these into separate bugs if that's better.\r\n\r\n<!-- Bug report -->\r\n\r\n**Elasticsearch version** (`curl -XGET 'localhost:9200'`):\r\n```\r\n{\r\n  \"name\" : \"adOS8gy\",\r\n  \"cluster_name\" : \"elasticsearch\",\r\n  \"cluster_uuid\" : \"GVS7gpVBQDGwtHl3xnJbLw\",\r\n  \"version\" : {\r\n    \"number\" : \"6.4.0\",\r\n    \"build_flavor\" : \"default\",\r\n    \"build_type\" : \"deb\",\r\n    \"build_hash\" : \"595516e\",\r\n    \"build_date\" : \"2018-08-17T23:18:47.308994Z\",\r\n    \"build_snapshot\" : false,\r\n    \"lucene_version\" : \"7.4.0\",\r\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\r\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\r\n  },\r\n  \"tagline\" : \"You Know, for Search\"\r\n}\r\n```\r\n\r\n**Plugins installed**: [analysis-icu, analysis-nori]\r\n\r\n**JVM version** (`java -version`):\r\nopenjdk version \"1.8.0_181\"\r\nOpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-1~deb9u1-b13)\r\nOpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\r\n\r\n**OS version** (`uname -a` if on a Unix-like system):\r\nLinux vagrantes6 4.9.0-6-amd64 #1 SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\nI've uncovered a number of oddities in tokenization in the CJK analyzer. All examples are from Korean Wikipedia or Korean Wiktionary (including non-CJK examples). In rough order of importance:\r\n\r\nA. Mixed-script tokens (Korean and non-CJK—such as numbers, Latin characters) are treated as one long token, rather than being broken up into bigrams. For example, `안녕은하철도999극장판2.1981년8월8일.일본개봉작1999년재더빙video판` is tokenized as one token.\r\n\r\nB. Middle dots (·, U+00B7) can be used as list separators in Korean. When they are, the text is not broken up into bigrams. For example, `경승지·산악·협곡·해협·곶·심연·폭포·호수·급류` is tokenized as one token. I'm not sure whether this is a special case of (A) or not.\r\n\r\nWork around: use a character filter to convert middle dots to spaces before CJK.\r\n\r\nC. The CJK analyzer eats encircled numbers (①②③), \"dingbat\" circled numbers (➀➁➂), parenthesized numbers (⑴⑵⑶), fractions (¼ ⅓ ⅜ ½ ⅔ ¾), superscript numbers (¹²³), and subscript numbers (₁₂₃). They just disappear.\r\n\r\nWork around: use the icu_normalizer before CJK to convert these to ASCII numbers.\r\n\r\nD. Soft hyphens (U+00AD) and zero-width non-joiners (U+200C), and left-to-right and right-to-left markers (U+200E and U+200F) are left in tokens. They should be stripped out. Examples: hyphen­ation (soft hyphen) and بازی‌های (zero-width non-joiners), הארץ‎ (left-to-right mark).\r\n\r\nWork around: use a character filter to strip these characters before CJK.\r\n\r\n**Steps to reproduce**:\r\n\r\nPlease include a *minimal* but *complete* recreation of the problem, including\r\n(e.g.) index creation, mappings, settings, query etc.  The easier you make for\r\nus to reproduce it, the more likely that somebody will take the time to look at it.\r\n\r\n 1. Set up CJK analyzer:\r\n \r\n```\r\ncurl -X PUT \"localhost:9200/cjk?pretty\" -H 'Content-Type: application/json' -d'\r\n{\r\n  \"settings\" : {\r\n    \"index\": {\r\n      \"analysis\": {\r\n        \"analyzer\": {\r\n          \"text\": {\r\n            \"type\": \"cjk\"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n'\r\n```\r\n\r\n 2. Analyze example tokens:\r\n\r\nA. Mixed Korean–Non-CJK characters\r\n\r\n```\r\ncurl -sk localhost:9200/cjk/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"안녕은하철도999극장판2.1981년8월8일.일본개봉작1999년재더빙video판\"}'\r\n\r\n{\r\n  \"tokens\" : [\r\n    {\r\n      \"token\" : \"안녕은하철도999극장판2.1981년8월8일.일본개봉작1999년재더빙video판\",\r\n      \"start_offset\" : 0,\r\n      \"end_offset\" : 43,\r\n      \"type\" : \"<ALPHANUM>\",\r\n      \"position\" : 0\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n\r\nB. Middle dots as lists\r\n\r\n```\r\ncurl -sk localhost:9200/cjk/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"경승지·산악·협곡·해협·곶·심연·폭포·호수·급류\"}'\r\n\r\n{\r\n  \"tokens\" : [\r\n    {\r\n      \"token\" : \"경승지·산악·협곡·해협·곶·심연·폭포·호수·급류\",\r\n      \"start_offset\" : 0,\r\n      \"end_offset\" : 26,\r\n      \"type\" : \"<ALPHANUM>\",\r\n      \"position\" : 0\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n\r\nC. Unicode numerical characters disappear\r\n```\r\ncurl -sk localhost:9200/cjk/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"① ② ③ ➀ ➁ ➂ ⑴ ⑵ ⑶ ¼ ⅓ ⅜ ½ ⅔ ¾ ¹ ² ³ ₁ ₂ ₃\"}'\r\n\r\n{\r\n  \"tokens\" : [ ]\r\n}\r\n```\r\n\r\nD. soft hyphens, zero-width non-joiners, left-to-right and right-to-left markers (note that these are usually invisible)\r\n\r\n```\r\ncurl -sk localhost:9200/cjk/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"hyphen­ation\"}'\r\n\r\n{\r\n  \"tokens\" : [\r\n    {\r\n      \"token\" : \"hyphen­ation\",\r\n      \"start_offset\" : 0,\r\n      \"end_offset\" : 12,\r\n      \"type\" : \"<ALPHANUM>\",\r\n      \"position\" : 0\r\n    }\r\n  ]\r\n}\r\n\r\ncurl -sk localhost:9200/cjk/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"بازی‌های\"}'\r\n\r\n{\r\n  \"tokens\" : [\r\n    {\r\n      \"token\" : \"بازی‌های\",\r\n      \"start_offset\" : 0,\r\n      \"end_offset\" : 8,\r\n      \"type\" : \"<ALPHANUM>\",\r\n      \"position\" : 0\r\n    }\r\n  ]\r\n}\r\n\r\ncurl -sk localhost:9200/cjk/_analyze?pretty -H 'Content-Type: application/json' -d '{\"analyzer\": \"text\", \"text\" : \"הארץ‎\"}'\r\n\r\n{\r\n  \"tokens\" : [\r\n    {\r\n      \"token\" : \"הארץ‎\",\r\n      \"start_offset\" : 0,\r\n      \"end_offset\" : 5,\r\n      \"type\" : \"<ALPHANUM>\",\r\n      \"position\" : 0\r\n    }\r\n  ]\r\n}\r\n\r\n```\r\n","closed_by":{"login":"romseygeek","id":1347065,"node_id":"MDQ6VXNlcjEzNDcwNjU=","avatar_url":"https://avatars0.githubusercontent.com/u/1347065?v=4","gravatar_id":"","url":"https://api.github.com/users/romseygeek","html_url":"https://github.com/romseygeek","followers_url":"https://api.github.com/users/romseygeek/followers","following_url":"https://api.github.com/users/romseygeek/following{/other_user}","gists_url":"https://api.github.com/users/romseygeek/gists{/gist_id}","starred_url":"https://api.github.com/users/romseygeek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/romseygeek/subscriptions","organizations_url":"https://api.github.com/users/romseygeek/orgs","repos_url":"https://api.github.com/users/romseygeek/repos","events_url":"https://api.github.com/users/romseygeek/events{/privacy}","received_events_url":"https://api.github.com/users/romseygeek/received_events","type":"User","site_admin":false},"performed_via_github_app":null}