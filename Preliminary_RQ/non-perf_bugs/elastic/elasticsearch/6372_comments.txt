[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/44816799","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-44816799","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":44816799,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0ODE2Nzk5","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2014-06-02T09:18:57Z","updated_at":"2014-06-02T09:18:57Z","author_association":"CONTRIBUTOR","body":"hey,\n\ncan you give us some more info about what version of ES you are running?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/44817404","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-44817404","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":44817404,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0ODE3NDA0","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-06-02T09:26:02Z","updated_at":"2014-06-02T09:26:44Z","author_association":"NONE","body":"Hi, we use elasticsearch-1.0.2\n\nPlease keep in mind that his might also be related to\nhttps://github.com/elasticsearch/elasticsearch/issues/6295\nhttps://github.com/elasticsearch/elasticsearch/issues/5232\n\nWe set discovery.zen.publish_timeout:0 in order to start the cluster. Otherwise nodes wouldn't even join at first before the master node starts the recovery.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/44817581","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-44817581","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":44817581,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0ODE3NTgx","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2014-06-02T09:27:52Z","updated_at":"2014-06-02T09:27:52Z","author_association":"CONTRIBUTOR","body":"it seems like you are setting a lot of things on the cluster. can you provide the settings you are using on the cluster aside of the defaults.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/44818859","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-44818859","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":44818859,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0ODE4ODU5","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-06-02T09:44:07Z","updated_at":"2014-06-02T09:44:07Z","author_association":"NONE","body":"Here it is:\n\nOur river + master nodes have identical configuration, except that data node and master have datadir/master flags switched.\n\nWe only use one master in order to not have the split brain issue.\n\n```\n# Configuration Template for ES\n# used by es data nodes (river is disabled)\n\n\n# Set publish timeout to 0, otherwise cluster startup takes long time\n# See https://github.com/elasticsearch/elasticsearch/issues/5232\ndiscovery.zen.publish_timeout: 0\n\n\n#lock memory\nbootstrap.mlockall: true\n\n# Cluster Settings\ncluster:\n  name: xxx\n\nnode:\n  name: xxx\n  t_cluster: xxx\n  t_scluster: xxx\n  master: false\n  data: true\n  max_local_storage_nodes: 1\n  service: xxx\n\n  # disable river allocation to data node\n  river: _none_\n\n#disabled cluster awareness -> allocation settings are now being set on index level.\n#cluster.routing.allocation.awareness.attributes: rack_id\n\ncluster.routing.allocation.node_initial_primaries_recoveries: 30\ncluster.routing.allocation.node_concurrent_recoveries: 8\nindices.recovery.max_bytes_per_sec: 100mb\n\npath:\n  logs: /tmp/searchdata_log/\n  work: /elasticsearch_work/\n  data: /ed1/,/ed2/\n\nindex:\n  number_of_replicas: 1\n  number_of_shards: 1\n  query:\n    bool:\n      max_clause_count: 10240\n\n# When changing the analysis settings, don't forget to also update\n# com.xxx.modules.searchclient.analysis.SpecialNGramAnalyzer and\n# com.xxx.modules.searchclient.analysis.xxxDefaultAnalyzer\n  analysis:\n    analyzer:\n      xxx:\n        type: custom\n        tokenizer: uax_url_email\n        filter: [standard, lowercase, elision_split, asciifolding]\n      xxx-exact:\n        type: custom\n        tokenizer: whitespace\n        filter: [lowercase, exact-match]\n      xxx-exact-cs:\n        type: custom\n        tokenizer: whitespace\n        filter: [exact-match]\n      patternnewline:\n        type: pattern\n        pattern: \"[\\n\\r]+\"\n        lowercase: false\n\n# increase cache filter size to 30% (default is 20%)\nindices.cache.filter.size: 30%\n\n# Gateway Settings\ngateway:\n  recover_after_data_nodes: 425\n  recover_after_time: 2m\n  expected_data_nodes: 430\n#gateway.local.compress: false\n#gateway.local.pretty: true\n\n# Use fixed ports, prevents es from starting multiple instances\n# Should also already be prevented by max_local_storage_nodes set to 1\nhttp.port: 9200\ntransport.tcp.port: 9300\n\ndiscovery:\n  zen.ping_timeout: 5s\n  zen.minimum_master_nodes: 1\n  zen.ping:\n    multicast:\n      enabled: false\n    unicast:\n      hosts: masternode[9300]\n  zen.fd.ping_interval: 3s\n  zen.fd.ping_timeout: 60s\n\nscript.native.relevance.type: com.xxx.modules.search.es.script.RelevanceScore\nscript.native.orelevance.type: com.xxx.modules.search.es.script.OldRelevanceScore\n# script.native.sortvalue.type: com.xxx.modules.search.es.script.SortValue\nscript.native.dynamicdata.type: com.xxx.modules.search.es.script.DynamicDataScore\nscript.native.updateproject.type: com.xxx.modules.search.es.script.UpdateScript\nscript.native.projectcopies.type: com.xxx.modules.search.es.script.ProjectCopiesScript\nscript.native.propagatescript.type: com.xxx.modules.search.es.script.PropagateScript\n\n# disable deleting of indices in a single API call\naction.disable_delete_all_indices: true\n# disable automatic index creation during indexation\naction.auto_create_index: false\n\n# since _all is disabled for our documents, we have to define another default_field\n# edit: commented out, as the default_field has to be specified anyway during searching (content+title)\n#       so we can leave the _all field enabled for auxiliary indices\n# index.query.default_field: content\n\n# disable automatic index creation for indices\naction.auto_create_index: -dsearch_*,-nsearch_*,-tw_*,-twindex_*,-twproj_*,-twprojindex_*,-tw-*,+*\n\n################################## Slow Log ##################################\n# Shard level query and fetch threshold logging.\n\nindex.search.slowlog.level: TRACE\nindex.search.slowlog.threshold.query.warn: 30s\nindex.search.slowlog.threshold.query.info: 15s\nindex.search.slowlog.threshold.query.debug: 5s\nindex.search.slowlog.threshold.query.trace: 1s\n\nindex.search.slowlog.threshold.fetch.warn: 15s\nindex.search.slowlog.threshold.fetch.info: 10s\nindex.search.slowlog.threshold.fetch.debug: 5s\nindex.search.slowlog.threshold.fetch.trace: 1s\n\n################################## GC Logging ################################\n\nmonitor.jvm.gc.ParNew.warn: 1000ms\nmonitor.jvm.gc.ParNew.info: 700ms\nmonitor.jvm.gc.ParNew.debug: 400ms\n\nmonitor.jvm.gc.ConcurrentMarkSweep.warn: 10s\nmonitor.jvm.gc.ConcurrentMarkSweep.info: 5s\nmonitor.jvm.gc.ConcurrentMarkSweep.debug: 2s\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/45197714","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-45197714","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":45197714,"node_id":"MDEyOklzc3VlQ29tbWVudDQ1MTk3NzE0","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-06-05T09:15:39Z","updated_at":"2014-06-05T09:15:39Z","author_association":"NONE","body":"Please let me know if you need any further information. thanks\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/55563354","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-55563354","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":55563354,"node_id":"MDEyOklzc3VlQ29tbWVudDU1NTYzMzU0","user":{"login":"miccon","id":455015,"node_id":"MDQ6VXNlcjQ1NTAxNQ==","avatar_url":"https://avatars3.githubusercontent.com/u/455015?v=4","gravatar_id":"","url":"https://api.github.com/users/miccon","html_url":"https://github.com/miccon","followers_url":"https://api.github.com/users/miccon/followers","following_url":"https://api.github.com/users/miccon/following{/other_user}","gists_url":"https://api.github.com/users/miccon/gists{/gist_id}","starred_url":"https://api.github.com/users/miccon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/miccon/subscriptions","organizations_url":"https://api.github.com/users/miccon/orgs","repos_url":"https://api.github.com/users/miccon/repos","events_url":"https://api.github.com/users/miccon/events{/privacy}","received_events_url":"https://api.github.com/users/miccon/received_events","type":"User","site_admin":false},"created_at":"2014-09-15T08:23:34Z","updated_at":"2014-09-15T08:23:34Z","author_association":"NONE","body":"While restarting with 1000 indexes, 20k shards and 500 nodes, the master node took 15 minutes to get to the initial allocation.  We are still running elasticsearch 1.0.2.\nCommit 6af80d501797903e3a3b627cd7cc331e6806bc38 does some optimizations about the allocation, but it seems that the optimization is only after getting the shard stores, although our nodes are mostly waiting for the responses from the cluster. The load on the master node is low (network and cpu wise) during the whole time:\n\n```\n\"elasticsearch[I51N16][clusterService#updateTask][T#1]\" daemon prio=10 tid=0x00007fc420005000 nid=0x2234 waiting on condition [0x00007fc4d5bc9000]\n   java.lang.Thread.State: WAITING (parking)\n        at sun.misc.Unsafe.park(Native Method)\n        - parking to wait for  <0x00000006ca9c15e8> (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)\n        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)\n        at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)\n        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)\n        at org.elasticsearch.gateway.local.LocalGatewayAllocator.buildShardStores(LocalGatewayAllocator.java:441)\n        at org.elasticsearch.gateway.local.LocalGatewayAllocator.allocateUnassigned(LocalGatewayAllocator.java:279)\n        at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:74)\n        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:216)\n        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:159)\n        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:145)\n        at org.elasticsearch.cluster.routing.RoutingService$1.execute(RoutingService.java:144)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:308)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:724)\n```\n\nIt seems that the requests are being done sequentially, so maybe its possible to speed this up by running some requests in parallel / caching more information.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/57149991","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-57149991","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":57149991,"node_id":"MDEyOklzc3VlQ29tbWVudDU3MTQ5OTkx","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-09-29T11:54:33Z","updated_at":"2014-09-29T11:54:33Z","author_association":"MEMBER","body":"@miccon in those 15m, does the master node report the correct number of nodes? I wonder if it is the time it takes for the 500 nodes to join the master. We improved the latter considerably in 1.4 (yet to be released): https://github.com/elasticsearch/elasticsearch/pull/7493 (see batch joining bullet point)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/57305651","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-57305651","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":57305651,"node_id":"MDEyOklzc3VlQ29tbWVudDU3MzA1NjUx","user":{"login":"miccon","id":455015,"node_id":"MDQ6VXNlcjQ1NTAxNQ==","avatar_url":"https://avatars3.githubusercontent.com/u/455015?v=4","gravatar_id":"","url":"https://api.github.com/users/miccon","html_url":"https://github.com/miccon","followers_url":"https://api.github.com/users/miccon/followers","following_url":"https://api.github.com/users/miccon/following{/other_user}","gists_url":"https://api.github.com/users/miccon/gists{/gist_id}","starred_url":"https://api.github.com/users/miccon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/miccon/subscriptions","organizations_url":"https://api.github.com/users/miccon/orgs","repos_url":"https://api.github.com/users/miccon/repos","events_url":"https://api.github.com/users/miccon/events{/privacy}","received_events_url":"https://api.github.com/users/miccon/received_events","type":"User","site_admin":false},"created_at":"2014-09-30T12:28:26Z","updated_at":"2014-09-30T12:28:26Z","author_association":"NONE","body":"@bleskes I agree that the batch joining of the nodes will indeed help when the cluster starts, so it should help with issue #5232. In order to get the nodes to join quickly in 1.0 we have to set discovery.zen.publish_timeout to 0 as described in the other issue.\n\nHere, these 15m are after the nodes have joined, so yes the master reports the correct number of nodes. It might be related to how the master queries the nodes for which shards are available / when it calculates the allocation.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/57306320","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-57306320","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":57306320,"node_id":"MDEyOklzc3VlQ29tbWVudDU3MzA2MzIw","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-09-30T12:34:31Z","updated_at":"2014-09-30T12:34:31Z","author_association":"NONE","body":"@bleskes I added more logs here at the end regarding to the slow recoery issue (miccon and I work together):\nhttps://github.com/elasticsearch/elasticsearch/issues/6295\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/57365373","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-57365373","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":57365373,"node_id":"MDEyOklzc3VlQ29tbWVudDU3MzY1Mzcz","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-09-30T19:06:10Z","updated_at":"2014-09-30T19:06:10Z","author_association":"MEMBER","body":"> The master node seems to only request shard recovery every 11 minutes (but not for all shards), causing the long wait.\n\nBefore recovering indices from disk, the master asks all the nodes about what they have on disk. To do so the nodes need some information that's part of the cluster state and if the don't have it they respond with \"I don't know yet\". The problem is that you have set `discovery.zen.publish_timeout: 0`, which means the master doesn't wait on the nodes and continues processing join events. It does not speed the processing on the node side. I suspect that's way recovering shards from disk takes long.\n\nThat said, do you know what happens in the 11 minutes? how big are the shards? \n\nLast, with the cluster of your size, I would really recommend you upgrade as soon as you can . We have had so many optimizations that will help you (.batched joins , memory signature ... and many more)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/58473088","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-58473088","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":58473088,"node_id":"MDEyOklzc3VlQ29tbWVudDU4NDczMDg4","user":{"login":"shikhar","id":74267,"node_id":"MDQ6VXNlcjc0MjY3","avatar_url":"https://avatars1.githubusercontent.com/u/74267?v=4","gravatar_id":"","url":"https://api.github.com/users/shikhar","html_url":"https://github.com/shikhar","followers_url":"https://api.github.com/users/shikhar/followers","following_url":"https://api.github.com/users/shikhar/following{/other_user}","gists_url":"https://api.github.com/users/shikhar/gists{/gist_id}","starred_url":"https://api.github.com/users/shikhar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shikhar/subscriptions","organizations_url":"https://api.github.com/users/shikhar/orgs","repos_url":"https://api.github.com/users/shikhar/repos","events_url":"https://api.github.com/users/shikhar/events{/privacy}","received_events_url":"https://api.github.com/users/shikhar/received_events","type":"User","site_admin":false},"created_at":"2014-10-09T07:33:18Z","updated_at":"2015-05-14T18:06:22Z","author_association":"CONTRIBUTOR","body":"I have noticed cluster initialization 'hung' on the same cluster update task, like @miccon :\n\n```\n\"elasticsearch[blabla-es0][clusterService#updateTask][T#1]\" #79 daemon prio=5 os_prio=0 tid=0x00007fd16988d000 nid=0x6e01 waiting on condition [0x00007fd0bc279000]\n   java.lang.Thread.State: WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x0000000614a22508> (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)\n    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)\n    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)\n    at org.elasticsearch.gateway.local.LocalGatewayAllocator.buildShardStores(LocalGatewayAllocator.java:443)\n    at org.elasticsearch.gateway.local.LocalGatewayAllocator.allocateUnassigned(LocalGatewayAllocator.java:281)\n    at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:74)\n    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:217)\n    at org.elasticsearch.cluster.routing.allocation.AllocationService.applyStartedShards(AllocationService.java:86)\n    at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:278)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\n\nNotes:\n- This is on a cluster using [eskka discovery](https://github.com/shikhar/eskka) which might be reason to discount this report. However AFAICT I don't think it is doing anything wrong. eskka does publishing asynchronously, same as the effect of `discovery.zen.publish_timeout: 0` with Zen -- so that seems to be a common feature here\n- The tdump is from the master node, and it was aware of all cluster nodes at this point and had published a few cluster state versions prior to this happening \n- ES version 1.3.4, `Nodes: 20   Indices: 13   Shards: 1122   Data: 748.24 GB`\n- This has happened a few times during bounce and gets resolved by bouncing the cluster again. Seems like some sort of race condition. Normally going from start to green only takes a couple of minutes.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/58477444","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-58477444","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":58477444,"node_id":"MDEyOklzc3VlQ29tbWVudDU4NDc3NDQ0","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-10-09T08:23:18Z","updated_at":"2014-10-09T08:23:18Z","author_association":"MEMBER","body":"@shikhar when the master assigns replicas, it first asks all the nodes what files they have for that shard on disk. The idea is to assign it to the nodes that has most files already available. The stack trace you're seeing is master waiting on a node to answer this question. I can see optimizations we can do here, but but this requests should be relatively quick. \n- How long does the master wait? \n- do you see high cpu on it? \n- If you stack trace it a couple of times, does the lock pointer change? In your example it's - 0x0000000614a22508 . This will indicate the master is issuing multiple requests as opposed to hang on one.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/58483589","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-58483589","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":58483589,"node_id":"MDEyOklzc3VlQ29tbWVudDU4NDgzNTg5","user":{"login":"shikhar","id":74267,"node_id":"MDQ6VXNlcjc0MjY3","avatar_url":"https://avatars1.githubusercontent.com/u/74267?v=4","gravatar_id":"","url":"https://api.github.com/users/shikhar","html_url":"https://github.com/shikhar","followers_url":"https://api.github.com/users/shikhar/followers","following_url":"https://api.github.com/users/shikhar/following{/other_user}","gists_url":"https://api.github.com/users/shikhar/gists{/gist_id}","starred_url":"https://api.github.com/users/shikhar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shikhar/subscriptions","organizations_url":"https://api.github.com/users/shikhar/orgs","repos_url":"https://api.github.com/users/shikhar/repos","events_url":"https://api.github.com/users/shikhar/events{/privacy}","received_events_url":"https://api.github.com/users/shikhar/received_events","type":"User","site_admin":false},"created_at":"2014-10-09T09:22:32Z","updated_at":"2014-10-09T09:23:09Z","author_association":"CONTRIBUTOR","body":"For now I can only answer\n\n> - How long does the master wait?\n\nAs opposed to the normal cluster init time of a couple of minutes, it seems to be taking over 10-15 mins by which time alerts fire so we re-bounce it\n\nAs for\n\n> - do you see high cpu on it?\n> - If you stack trace it a couple of times, does the lock pointer change? In your example it's - 0x0000000614a22508 . This will indicate the master is issuing multiple requests as opposed to hang on one.\n\nI will be sure to check these out next time. Thanks @bleskes!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/60362740","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-60362740","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":60362740,"node_id":"MDEyOklzc3VlQ29tbWVudDYwMzYyNzQw","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-10-24T09:10:19Z","updated_at":"2014-10-24T09:10:19Z","author_association":"NONE","body":"Hi,\nJust a short update on our tests:\nWe created a 200 node empty cluster on 1.3.4, and the joining process took more than 45 minutes.\nIt worked fine (only 1-3 minutes) in 1.4.0 beta, so we will wait for the 1.4.0 version until we upgrade.\n\nAfter 1.4.0 is release, we will then also run some tests with production indexes, to see if the long shard initialisation phase has also improved.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/60364787","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-60364787","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":60364787,"node_id":"MDEyOklzc3VlQ29tbWVudDYwMzY0Nzg3","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-10-24T09:32:10Z","updated_at":"2014-10-24T09:32:10Z","author_association":"MEMBER","body":"@bluelu great news. Indeed 1.4 massively improved the time it takes to form large clusters by batching join requests. \n\nW.r.t shard initialization time, let's see how it goes. We still need to reach out to all the nodes and ask them for information about what shards they have on disk before primaries can be allocated and cluster becomes yellow.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/60368365","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-60368365","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":60368365,"node_id":"MDEyOklzc3VlQ29tbWVudDYwMzY4MzY1","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-10-24T10:08:54Z","updated_at":"2014-10-24T10:08:54Z","author_association":"NONE","body":"Just an idea there (Don't know how it's done at the moment, but I guess it's like that at the moment also in 1.4.0):\nI suppose that asking all nodes for their shards, also means that the shards need to do an integrity check (e.g. the new checksum checks, etc...) on their shards before they report back to the master node, which in our case could take some time on some servers (e.g. non ssd disks) with multiple shards, as it involves reading a lot of TBs of data per node.\nIf this process could be splitted in two, first list available shards (no integrety check, just check what is on the filesystem, should be very fast) and return, and start the current allocation phase. But during that phase, you could already allocate the primaries as soon as all nodes have finished the initialisation phases which contain a specific shard, as you know in advance already that no other nodes contain that shard.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65402668","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65402668","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65402668,"node_id":"MDEyOklzc3VlQ29tbWVudDY1NDAyNjY4","user":{"login":"shikhar","id":74267,"node_id":"MDQ6VXNlcjc0MjY3","avatar_url":"https://avatars1.githubusercontent.com/u/74267?v=4","gravatar_id":"","url":"https://api.github.com/users/shikhar","html_url":"https://github.com/shikhar","followers_url":"https://api.github.com/users/shikhar/followers","following_url":"https://api.github.com/users/shikhar/following{/other_user}","gists_url":"https://api.github.com/users/shikhar/gists{/gist_id}","starred_url":"https://api.github.com/users/shikhar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shikhar/subscriptions","organizations_url":"https://api.github.com/users/shikhar/orgs","repos_url":"https://api.github.com/users/shikhar/repos","events_url":"https://api.github.com/users/shikhar/events{/privacy}","received_events_url":"https://api.github.com/users/shikhar/received_events","type":"User","site_admin":false},"created_at":"2014-12-03T12:56:30Z","updated_at":"2015-05-14T18:06:53Z","author_association":"CONTRIBUTOR","body":"@bleskes \n\n> do you see high cpu on it?\n\nmaster cpu is pretty low\n\n> If you stack trace it a couple of times, does the lock pointer change? In your example it's - 0x0000000614a22508 . This will indicate the master is issuing multiple requests as opposed to hang on one.\n\nit does change\n\nI mainly wanted to update that in our case the problem might possibly be due to using JDK8u5. I was able to capture some more diagnostics when bouncing the nodes following one such event of super-slow cluster init. We have some automated thread-dumping if a node takes too long to go down nicely and kill-9 needs to be used. The thread dump on this (non-master) node revealed a bunch of threads executing code relevant to the RPC's issued by the master:\n\n```\n\"elasticsearch[blabla][generic][T#43]\" #160 daemon prio=5 os_prio=0 tid=0x00007f79180c5800 nid=0x3d1f in Object.wait() [0x00007f79d9289000]\n   java.lang.Thread.State: RUNNABLE\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:359)\n    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:457)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:912)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:758)\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:453)\n    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)\n    at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:126)\n    at org.elasticsearch.index.store.Store.access$300(Store.java:76)\n    at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:465)\n    at org.elasticsearch.index.store.Store$MetadataSnapshot.<init>(Store.java:456)\n    at org.elasticsearch.index.store.Store.readMetadataSnapshot(Store.java:281)\n    at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.listStoreMetaData(TransportNodesListShardStoreMetaData.java:186)\n    at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:140)\n    at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:61)\n    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:277)\n    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:268)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\n\nThe really weird thing is that these threads are reported to be RUNNABLE although they are supposedly in `Object.wait()`, and I can't even spot any relevant synchronization in `SegmentInfos.read()`.\n\nAnyway we have seen this weirdness a couple of times in some non-ES usage as well. We plan to upgrade our JDK8 version and hopefully this occasional issue will stop happening altogether.\n\n**UPDATE May 14, 15** https://issues.apache.org/jira/browse/LUCENE-6482 - unrelated to JDK version\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65892182","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65892182","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65892182,"node_id":"MDEyOklzc3VlQ29tbWVudDY1ODkyMTgy","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-12-06T10:21:03Z","updated_at":"2014-12-06T10:21:03Z","author_association":"NONE","body":"@bleskes \nWe upgraded now to 1.4.1:\n\nCluster join has increased much, that's great.\n\nStill the allocation will take a lot of time (it's now more than 30 minutes since cluster start and it hasn't much progressed, see below status it's still hanging at   \"initializing_shards\" : 8390). \nThe master node runs one Thread at 100% cpu. It's not stuck as the stack progresses.\n\nelasticsearch[I51N16][clusterService#updateTask][T#1]\" #52 daemon\nprio=5 os_prio=0 tid=0x00007f1ad25d9000 nid=0x127f runnable\n[0x00007f1a22171000]\n   java.lang.Thread.State: RUNNABLE\n        at org.elasticsearch.common.collect.UnmodifiableListIterator.<init>(UnmodifiableListIterator.java:34)\n        at org.elasticsearch.common.collect.AbstractIndexedListIterator.<init>(AbstractIndexedListIterator.java:68)\n        at org.elasticsearch.common.collect.Iterators$11.<init>(Iterators.java:1058)\n        at org.elasticsearch.common.collect.Iterators.forArray(Iterators.java:1058)\n        at org.elasticsearch.common.collect.RegularImmutableList.listIterator(RegularImmutableList.java:106)\n        at org.elasticsearch.common.collect.ImmutableList.listIterator(ImmutableList.java:344)\n        at org.elasticsearch.common.collect.ImmutableList.iterator(ImmutableList.java:340)\n        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:173)\n        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:46)\n        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.shardsWithState(IndexShardRoutingTable.java:552)\n        at org.elasticsearch.cluster.routing.IndexRoutingTable.shardsWithState(IndexRoutingTable.java:268)\n        at org.elasticsearch.cluster.routing.RoutingTable.shardsWithState(RoutingTable.java:114)\n        at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.sizeOfRelocatingShards(DiskThresholdDecider.java:225)\n        at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.canAllocate(DiskThresholdDecider.java:288)\n        at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:74)\n        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.tryRelocateShard(BalancedShardsAllocator.java:799)\n        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.balance(BalancedShardsAllocator.java:426)\n        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.rebalance(BalancedShardsAllocator.java:124)\n        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.allocateUnassigned(BalancedShardsAllocator.java:118)\n        at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:75)\n        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:217)\n        at org.elasticsearch.cluster.routing.allocation.AllocationService.applyStartedShards(AllocationService.java:86)\n        at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:281)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\nCluster health:\n\n{\n  \"cluster_name\" : \"cluster\",\n  \"status\" : \"red\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 651,\n  \"number_of_data_nodes\" : 650,\n  \"active_primary_shards\" : 0,\n  \"active_shards\" : 0,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 8390,\n  \"unassigned_shards\" : 16108\n}\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65892353","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65892353","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65892353,"node_id":"MDEyOklzc3VlQ29tbWVudDY1ODkyMzUz","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-12-06T10:28:28Z","updated_at":"2014-12-06T10:28:28Z","author_association":"MEMBER","body":"@thibaut Thx for the update. I'll read more carefully later but can you run http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html on the master a couple of times? I wonder what it does.\n\nOn Sat, Dec 6, 2014 at 11:21 AM, Thibaut notifications@github.com wrote:\n\n> @bleskes \n> We upgraded now to 1.4.1:\n> Cluster join has increased much, that's great.\n> Still the allocation will take a lot of time (it's now more than 30 minutes since cluster start and it hasn't much progressed, see below status it's still hanging at   \"initializing_shards\" : 8390). \n> The master node runs one Thread at 100% cpu. It's not stuck as the stack progresses.\n> elasticsearch[I51N16][clusterService#updateTask][T#1]\" #52 daemon\n> prio=5 os_prio=0 tid=0x00007f1ad25d9000 nid=0x127f runnable\n> [0x00007f1a22171000]\n>    java.lang.Thread.State: RUNNABLE\n>         at org.elasticsearch.common.collect.UnmodifiableListIterator.<init>(UnmodifiableListIterator.java:34)\n>         at org.elasticsearch.common.collect.AbstractIndexedListIterator.<init>(AbstractIndexedListIterator.java:68)\n>         at org.elasticsearch.common.collect.Iterators$11.<init>(Iterators.java:1058)\n>         at org.elasticsearch.common.collect.Iterators.forArray(Iterators.java:1058)\n>         at org.elasticsearch.common.collect.RegularImmutableList.listIterator(RegularImmutableList.java:106)\n>         at org.elasticsearch.common.collect.ImmutableList.listIterator(ImmutableList.java:344)\n>         at org.elasticsearch.common.collect.ImmutableList.iterator(ImmutableList.java:340)\n>         at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:173)\n>         at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:46)\n>         at org.elasticsearch.cluster.routing.IndexShardRoutingTable.shardsWithState(IndexShardRoutingTable.java:552)\n>         at org.elasticsearch.cluster.routing.IndexRoutingTable.shardsWithState(IndexRoutingTable.java:268)\n>         at org.elasticsearch.cluster.routing.RoutingTable.shardsWithState(RoutingTable.java:114)\n>         at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.sizeOfRelocatingShards(DiskThresholdDecider.java:225)\n>         at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.canAllocate(DiskThresholdDecider.java:288)\n>         at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:74)\n>         at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.tryRelocateShard(BalancedShardsAllocator.java:799)\n>         at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.balance(BalancedShardsAllocator.java:426)\n>         at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.rebalance(BalancedShardsAllocator.java:124)\n>         at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.allocateUnassigned(BalancedShardsAllocator.java:118)\n>         at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:75)\n>         at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:217)\n>         at org.elasticsearch.cluster.routing.allocation.AllocationService.applyStartedShards(AllocationService.java:86)\n>         at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:281)\n>         at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)\n>         at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n>         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n>         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n>         at java.lang.Thread.run(Thread.java:745)\n> Cluster health:\n> {\n>   \"cluster_name\" : \"cluster\",\n>   \"status\" : \"red\",\n>   \"timed_out\" : false,\n>   \"number_of_nodes\" : 651,\n>   \"number_of_data_nodes\" : 650,\n>   \"active_primary_shards\" : 0,\n>   \"active_shards\" : 0,\n>   \"relocating_shards\" : 0,\n>   \"initializing_shards\" : 8390,\n>   \"unassigned_shards\" : 16108\n> \n> ## }\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/elasticsearch/elasticsearch/issues/6372#issuecomment-65892182\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65892658","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65892658","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65892658,"node_id":"MDEyOklzc3VlQ29tbWVudDY1ODkyNjU4","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-12-06T10:42:35Z","updated_at":"2014-12-06T10:42:35Z","author_association":"NONE","body":"@bleskes \nI uploaded 4 hothreads outputs of the master node to pastebin:\nhttp://pastebin.com/H0mQcAFE\n\nIn the meantime, the allocation advanced a little bit:\n\n{\n  \"cluster_name\" : \"cluster\",\n  \"status\" : \"red\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 651,\n  \"number_of_data_nodes\" : 650,\n  \"active_primary_shards\" : 3468,\n  \"active_shards\" : 3468,\n  \"relocating_shards\" : 1,\n  \"initializing_shards\" : 7402,\n  \"unassigned_shards\" : 13628\n}\n\nWhat we normally do (or did before) was that we only added a few nodes first (main indexes, 200-300 nodes), waited for it to come up, and then added the other nodes 1 by 1 (since we could afford to have some indexes red in the beginning)  afterwards, while having the allocating/balancing disabled. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65893470","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65893470","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65893470,"node_id":"MDEyOklzc3VlQ29tbWVudDY1ODkzNDcw","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-12-06T11:17:23Z","updated_at":"2014-12-06T11:17:23Z","author_association":"MEMBER","body":"I see. The disk threshold allocator decider, in charge of making sure a node is not overloaded with shards is calculating the size of relocating shards walking the shards list again and again. We'd have to make it more efficient. A simple work around is to temporally set\n\n```\ncurl -XPUT localhost:9200/_cluster/settings -d '{\n    \"transient\" : {\n        \"cluster.routing.allocation.disk.include_relocations\" : false\n    }\n}'\n```\n\nand enable it while start up is done. Can you try?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65894654","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65894654","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65894654,"node_id":"MDEyOklzc3VlQ29tbWVudDY1ODk0NjU0","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-12-06T12:06:00Z","updated_at":"2014-12-06T12:06:21Z","author_association":"NONE","body":"@bleskes \nThanks for this suggestion :-)\n\nWe couldn't apply this change while it was running (timeouts setting the value when we tried multiple times), so we had to restart. I can confirm that the allocation works now at excellent speed (just a few minutes) :-). We will not reenable this feature for now and rather limit the allocation per node to 1 at most at the same time.\n\nOnly remaining issue is that it still takes 10 minutes until the allocation starts. But for us this is acceptable...\n\nAre you going to open up an issue for the include_relocations performance issue?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65903459","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65903459","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65903459,"node_id":"MDEyOklzc3VlQ29tbWVudDY1OTAzNDU5","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-12-06T16:09:11Z","updated_at":"2014-12-06T16:09:11Z","author_association":"NONE","body":"Just a further update on the blocking issue:\n\nWe enabled allocation of non primaries now, which caused the master node to block for another 20 minutes at the stacktrace below (local gateway recovery), not updating cluster health at all, nor sending any cluster updates. During that time no commands can be executed on the cluster (everything will timeout)\n\nThe more and more shards are allocated, the more and more the \"blocking\" time gets reduced and updates progress.\nprio=5 os_prio=0 tid=0x00007f1ad25d9000 nid=0x127f runnable\n[0x00007f1a22171000]\n   java.lang.Thread.State: RUNNABLE\n        at org.elasticsearch.common.collect.UnmodifiableListIterator.<init>(UnmodifiableListIterator.java:34)\n        at org.elasticsearch.common.collect.AbstractIndexedListIterator.<init>(AbstractIndexedListIterator.java:68)\n        at org.elasticsearch.common.collect.Iterators$11.<init>(Iterators.java:1058)\n        at org.elasticsearch.common.collect.Iterators.forArray(Iterators.java:1058)\n        at org.elasticsearch.common.collect.RegularImmutableList.listIterator(RegularImmutableList.java:106)\n        at org.elasticsearch.common.collect.ImmutableList.listIterator(ImmutableList.java:344)\n        at org.elasticsearch.common.collect.ImmutableList.iterator(ImmutableList.java:340)\n        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:173)\n        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:46)\n        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.shardsWithState(IndexShardRoutingTable.java:552)\n        at org.elasticsearch.cluster.routing.IndexRoutingTable.shardsWithState(IndexRoutingTable.java:268)\n        at org.elasticsearch.cluster.routing.RoutingTable.shardsWithState(RoutingTable.java:114)\n        at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.sizeOfRelocatingShards(DiskThresholdDecider.java:225)\n        at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.canAllocate(DiskThresholdDecider.java:288)\n        at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:74)\n        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.tryRelocateShard(BalancedShardsAllocator.java:799)\n        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.balance(BalancedShardsAllocator.java:426)\n        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.rebalance(BalancedShardsAllocator.java:124)\n        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.allocateUnassigned(BalancedShardsAllocator.java:118)\n        at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:75)\n        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:217)\n        at org.elasticsearch.cluster.routing.allocation.AllocationService.applyStartedShards(AllocationService.java:86)\n        at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:281)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65903946","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65903946","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65903946,"node_id":"MDEyOklzc3VlQ29tbWVudDY1OTAzOTQ2","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-12-06T16:22:22Z","updated_at":"2014-12-06T16:22:22Z","author_association":"MEMBER","body":"@bluelu it's the same stack trace as before. Are you sure you had that settings I mentioned disabled? It is valid for any allocation, both primary and replicas\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65904550","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65904550","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65904550,"node_id":"MDEyOklzc3VlQ29tbWVudDY1OTA0NTUw","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-12-06T16:38:43Z","updated_at":"2014-12-06T16:38:43Z","author_association":"MEMBER","body":"Btw, even if the settings update request times out , the master will get to it once it finishes the current task\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65905197","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65905197","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65905197,"node_id":"MDEyOklzc3VlQ29tbWVudDY1OTA1MTk3","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-12-06T16:58:03Z","updated_at":"2014-12-06T16:58:03Z","author_association":"NONE","body":"Yes, we have  \"cluster.routing.allocation.disk.include_relocations\" : false:\n\n{\n  \"persistent\" : {\n    \"action\" : {\n      \"destructive_requires_name\" : \"true\"\n    },\n    \"cluster\" : {\n      \"routing\" : {\n        \"allocation\" : {\n          \"node_concurrent_recoveries\" : \"1\",\n          \"disk\" : {\n            \"threshold_enabled\" : \"true\",\n            \"watermark\" : {\n              \"low\" : \"0.9\",\n              \"high\" : \"0.95\"\n            },\n            \"include_relocations\" : \"false\",\n            \"reroute_interval\" : \"60000s\"\n          },\n          \"enable\" : \"all\"\n        }\n      }\n    },\n    \"indices\" : {\n      \"recovery\" : {\n        \"concurrent_streams\" : \"3\"\n      }\n    },\n    \"threadpool\" : {\n      \"search\" : {\n        \"type\" : \"fixed\",\n        \"size\" : \"12\"\n      }\n    },\n    \"logger\" : {\n      \"cluster\" : {\n        \"service\" : \"DEBUG\"\n      }\n    }\n  },\n  \"transient\" : { }\n}\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65932317","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65932317","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65932317,"node_id":"MDEyOklzc3VlQ29tbWVudDY1OTMyMzE3","user":{"login":"bluelu","id":339893,"node_id":"MDQ6VXNlcjMzOTg5Mw==","avatar_url":"https://avatars1.githubusercontent.com/u/339893?v=4","gravatar_id":"","url":"https://api.github.com/users/bluelu","html_url":"https://github.com/bluelu","followers_url":"https://api.github.com/users/bluelu/followers","following_url":"https://api.github.com/users/bluelu/following{/other_user}","gists_url":"https://api.github.com/users/bluelu/gists{/gist_id}","starred_url":"https://api.github.com/users/bluelu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bluelu/subscriptions","organizations_url":"https://api.github.com/users/bluelu/orgs","repos_url":"https://api.github.com/users/bluelu/repos","events_url":"https://api.github.com/users/bluelu/events{/privacy}","received_events_url":"https://api.github.com/users/bluelu/received_events","type":"User","site_admin":false},"created_at":"2014-12-07T10:03:26Z","updated_at":"2014-12-07T10:03:26Z","author_association":"NONE","body":"@bleskes \nIt seems that both the reallocate and rebalance functions are just too slow if you have too many nodes and shards. We also use the Shard allocation filtering.\n\nWhat do you need in order to be able to debug this and reproduce the performance issue? Cluster state, settings and configuration? I can send you this in private?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65935081","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-65935081","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":65935081,"node_id":"MDEyOklzc3VlQ29tbWVudDY1OTM1MDgx","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-12-07T12:00:03Z","updated_at":"2014-12-07T12:15:16Z","author_association":"MEMBER","body":"@Thibaut I can't look at things in details right now. It is surprising that disabling include relocation didn't kick in. Being able to reroute quickly is important for the operations of the master. As a temporary work around (as I see from the tickets that it causes other problems as well) try disabling the disk threshold allocator all together. \n\nPs - Simon alread fixed that slowness we saw in your hot threads: #8803\n\nOn Sun, Dec 7, 2014 at 11:03 AM, Thibaut notifications@github.com wrote:\n\n> @bleskes \n> It seems that both the reallocate and rebalance functions are just too slow if you have too many nodes and shards. We also use the Shard allocation filtering.\n> \n> ## What do you need in order to be able to debug this and reproduce the performance issue? Cluster state, settings and configuration? I can send you this in private?\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/elasticsearch/elasticsearch/issues/6372#issuecomment-65932317\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/66036431","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-66036431","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":66036431,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MDM2NDMx","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-12-08T08:20:42Z","updated_at":"2014-12-08T08:20:42Z","author_association":"MEMBER","body":"@bluelu it seems that the include_relocations settings were backported to 1.4, but the dynamic update code didn't make it (although it is registered for dynamic updates, see https://github.com/elasticsearch/elasticsearch/commit/4e5264c8dcab392fd94554f5d036573b085b6450 ). Which misses https://github.com/elasticsearch/elasticsearch/commit/4185566e9344091a3ddad4090435fc3609fab208#diff-1b8dca987fbcfb8d8e452d7e29c4d058R92 ). I'm sorry for sending you down a wrong path. @dakrone  I think it makes sense to add the dynamic update logic for 1.4.2, right?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/66041815","html_url":"https://github.com/elastic/elasticsearch/issues/6372#issuecomment-66041815","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6372","id":66041815,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MDQxODE1","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2014-12-08T09:20:31Z","updated_at":"2014-12-08T09:20:31Z","author_association":"MEMBER","body":"@bleskes yes, most likely caused by a bad backport, I'll fix in the 1.4 branch.\n","performed_via_github_app":null}]