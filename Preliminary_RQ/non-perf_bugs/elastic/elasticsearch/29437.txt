{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/29437","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29437/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29437/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29437/events","html_url":"https://github.com/elastic/elasticsearch/issues/29437","id":312710317,"node_id":"MDU6SXNzdWUzMTI3MTAzMTc=","number":29437,"title":"All primary shards on the same node for every index in 6.x","user":{"login":"tdoman","id":11098266,"node_id":"MDQ6VXNlcjExMDk4MjY2","avatar_url":"https://avatars1.githubusercontent.com/u/11098266?v=4","gravatar_id":"","url":"https://api.github.com/users/tdoman","html_url":"https://github.com/tdoman","followers_url":"https://api.github.com/users/tdoman/followers","following_url":"https://api.github.com/users/tdoman/following{/other_user}","gists_url":"https://api.github.com/users/tdoman/gists{/gist_id}","starred_url":"https://api.github.com/users/tdoman/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tdoman/subscriptions","organizations_url":"https://api.github.com/users/tdoman/orgs","repos_url":"https://api.github.com/users/tdoman/repos","events_url":"https://api.github.com/users/tdoman/events{/privacy}","received_events_url":"https://api.github.com/users/tdoman/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2018-04-09T22:27:50Z","updated_at":"2018-04-10T07:33:47Z","closed_at":"2018-04-10T00:08:25Z","author_association":"NONE","active_lock_reason":null,"body":"ES 6.1.1\r\nNEST 6.0.1\r\n3 node cluster, 2 replicas for each index\r\n\r\nIn looking for the cause of a \"hot\" node, I read a lot about how \"updates\" can cause this since they need to be coordinated via the primary shard.  In my cluster, I have 34 indexes, most w/ 5 shards, and some \"user\" indexes w/ 20 shards.  Each and every primary shard for each and every index is on the same node which means that every update request that I make has to be handled by this node.\r\n\r\nAs I perused similar Q&A that I found via web searches and reviewing [Cluster Level Shard Allocation](https://www.elastic.co/guide/en/elasticsearch/reference/6.1/shards-allocation.html), I couldn't see that there was any way to redistribute the primary shards when we're \"fully replicated\" as we are.  So, I tried an experiment by setting one of my indexes to 1 replica.  Sure enough, now a couple of the primary shards are on another node.  So, then I set it back to 2 replicas.  Sure enough, the primaries stayed where they had been moved after changing to 1 replica.  So, that's a cheat.  Is there a way for me to more explicitly distribute the \"primary\" designation for shards across the nodes in my cluster?\r\n\r\nSome of the most highly updated indexes are ones where we use **_terms lookup_** where ES best practices dictate we have a replica so that the file system cache can be utilized so ES doesn't have to request the terms from another replica.  Ok, so we have 2 replicas but I also need to balance out the update traffic and I don't know if my little \"back and forth\" trick is even persistent (ie. our windows VMs auto update in a staggered way).  FWIW, I've been referred to [Elasticsearch Versioning Support](https://www.elastic.co/blog/elasticsearch-versioning-support) which we can consider and potentially implement (assuming the C# NEST library supports) but I'm definitely not sure we'll even want to do that.  Regardless, of course, we also have a running production cluster we need to keep performant in the meantime.","closed_by":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"performed_via_github_app":null}