[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164949417","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-164949417","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":164949417,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDk0OTQxNw==","user":{"login":"costin","id":76245,"node_id":"MDQ6VXNlcjc2MjQ1","avatar_url":"https://avatars3.githubusercontent.com/u/76245?v=4","gravatar_id":"","url":"https://api.github.com/users/costin","html_url":"https://github.com/costin","followers_url":"https://api.github.com/users/costin/followers","following_url":"https://api.github.com/users/costin/following{/other_user}","gists_url":"https://api.github.com/users/costin/gists{/gist_id}","starred_url":"https://api.github.com/users/costin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/costin/subscriptions","organizations_url":"https://api.github.com/users/costin/orgs","repos_url":"https://api.github.com/users/costin/repos","events_url":"https://api.github.com/users/costin/events{/privacy}","received_events_url":"https://api.github.com/users/costin/received_events","type":"User","site_admin":false},"created_at":"2015-12-16T00:58:32Z","updated_at":"2015-12-16T00:58:49Z","author_association":"MEMBER","body":"Pushed fixes [here](https://github.com/costin/elasticsearch/tree/hdfs-simplifications).\n1. the plugin has been updated to be hadoop2 only (dropped `-hadoop1` and `lite`)\n2. the plugin documentation does not mention `ES_CLASSPATH` or `lib/` - if this is mentioned anywhere, let me know\n3. I have removed `hadoop-yarn-common` to prevent jar hell from occuring in the IDE\n4. the MiniHDFS starts and stops a mini HDFS cluster. Its classpath is `testCompile` - I've tried creating a separate configuration however for some reason, it ends up with several dependencies with a different version that need to be specified in the resolution strategy and further more adds in jars that trigger jar hell (commons-beanutils vs commons-collection).`testCompile` has this already resolved so the configuration is smaller.\n5. The permissions have been double-checked. I have tested this locally on windows only hence the `jaas_nt` permission. `nix is next\n6. Updated the plugin code to use the `FileContext` class.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/165922059","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-165922059","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":165922059,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTkyMjA1OQ==","user":{"login":"costin","id":76245,"node_id":"MDQ6VXNlcjc2MjQ1","avatar_url":"https://avatars3.githubusercontent.com/u/76245?v=4","gravatar_id":"","url":"https://api.github.com/users/costin","html_url":"https://github.com/costin","followers_url":"https://api.github.com/users/costin/followers","following_url":"https://api.github.com/users/costin/following{/other_user}","gists_url":"https://api.github.com/users/costin/gists{/gist_id}","starred_url":"https://api.github.com/users/costin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/costin/subscriptions","organizations_url":"https://api.github.com/users/costin/orgs","repos_url":"https://api.github.com/users/costin/repos","events_url":"https://api.github.com/users/costin/events{/privacy}","received_events_url":"https://api.github.com/users/costin/received_events","type":"User","site_admin":false},"created_at":"2015-12-18T23:46:09Z","updated_at":"2015-12-18T23:46:09Z","author_association":"MEMBER","body":"I have updated the branch and rebased against master.\n1. Since the branch is Hadoop2 only, I have replaced the aging `FileSystem` interface with `FileContext`. The underlying code proved to be separate and in many cases, the new features in `FileContext` were ignored by `FileSystem`. The new class in use should be more reliable in terms of fsync semantics (through `hsync).\n2. the code explicitly writes blobs (creates new files) using `hsync`. Further more in case of a rename, it tries to `append` the file to trigger the `hsync` yet again.\n3. The number of Hadoop2 jars required has been removed - turns out some were not really used.\n4. Last but not least, the build system was improved a bit to start MiniHDFS before integration testing and to stop it afterwards. While it works when done manually, I couldn't get it so far to work properly in gradle:\n   a. Gradle starts the process in foreground which means the whole build is stopped. It needs to be wrapped to run in the background which is somewhat tricky because we don't want to compute the classpath ourselves but rather get it from gradle. ant.exec might help here instead of Gradle `JavaExec`\n   b. killing a process is tricky. I have the basic portable code in place (similar to @rjernst 's code) however I need to find a way to pass the pid info around and against, due to classpath issues (gradle vs the project) this might require some code duplication.\n\nBasically 4 is the last item completely addressing the issue. @rjernst  do you have some free time to help? If not, I'll push further over the weekend to get Gradle to play nice.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/165940364","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-165940364","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":165940364,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTk0MDM2NA==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-12-19T03:25:39Z","updated_at":"2015-12-19T03:25:39Z","author_association":"CONTRIBUTOR","body":"Thanks costin, ryan and I continued with your branch, he did some cleanup on dependencies, I am basically trying to help with the integration test and getting this working on unix systems.\n\nThe things resolved so far are:\n1. Hadoop jars were added one by one, only the most minimal set with no transitive dependencies. this way we can get this thing working without jar hell etc and minimize the surface area. It is now up to ConnectException so I think its fairly close. \n2. Hadoop will not work at all on non-windows systems without full execute permission across the entire filesystem (https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Shell.java#L381-L402). This is obviously unacceptable, I had to add a brutal hack to deliver a custom IOException from our security code (instead of SecurityException) when it tries to do this. This needs to ultimately be fixed in hadoop.\n3. Hadoop needs `jaas_unix` loadLibrary permission or it fails with security exception (because only `jaas_nt` is added right now.\n\nSee in-progress changes here: https://github.com/elastic/elasticsearch/compare/master...rjernst:hdfs_without_transitive\n\nOne thing we have right now is a bug in our handling of slf4j which affects hadoop. We don't actually ship it in any ES distributions, yet the build treats it as `provided`, so its also not in any plugin that wants it. I hacked around this temporarily, but that needs to be sorted out.\n\nI will try your minihdfs cluster now (manually) and try to get the integ  connecting / snapshotting / restoring. Ryan is working on hooking it into the integTest part of the build.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/165945836","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-165945836","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":165945836,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTk0NTgzNg==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-12-19T04:18:54Z","updated_at":"2015-12-19T04:18:54Z","author_association":"CONTRIBUTOR","body":"@costin i converted core's rest tests (take snapshots and restores) and they all pass against the minihdfs now. As there is no longer jar hell, and we can do a real integration test, I will see if i can do further simplifications.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/166035094","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-166035094","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":166035094,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NjAzNTA5NA==","user":{"login":"costin","id":76245,"node_id":"MDQ6VXNlcjc2MjQ1","avatar_url":"https://avatars3.githubusercontent.com/u/76245?v=4","gravatar_id":"","url":"https://api.github.com/users/costin","html_url":"https://github.com/costin","followers_url":"https://api.github.com/users/costin/followers","following_url":"https://api.github.com/users/costin/following{/other_user}","gists_url":"https://api.github.com/users/costin/gists{/gist_id}","starred_url":"https://api.github.com/users/costin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/costin/subscriptions","organizations_url":"https://api.github.com/users/costin/orgs","repos_url":"https://api.github.com/users/costin/repos","events_url":"https://api.github.com/users/costin/events{/privacy}","received_events_url":"https://api.github.com/users/costin/received_events","type":"User","site_admin":false},"created_at":"2015-12-20T00:12:20Z","updated_at":"2015-12-20T00:12:20Z","author_association":"MEMBER","body":"Wow, thanks for the improvements - this must have been quite an effort. Tested it locally and it worked great (note that under Windows one needs `winuntils.exe` and the proper permissions for it but since Windows is really a second class citizen under Hadoop, I haven't added them to the policy file since as you mentioned, it's simply too dangerous (and unacceptable).\nGlad to see the CL hack is no longer needed - makes things a lot easier.\n\nI have updated (simplified) the docs [here](https://github.com/costin/elasticsearch/tree/hdfs2-only).\nIs there anything else to push this into 2.2?\n\nThanks again for everything!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/166045076","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-166045076","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":166045076,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NjA0NTA3Ng==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-12-20T00:49:07Z","updated_at":"2015-12-20T00:49:07Z","author_association":"CONTRIBUTOR","body":"Well there are a few things we should do for merging back to master: \n1. fully automate the new rest tests, so they can run like normal tests in jenkins on all platforms, etc. this means launching the hdfs minicluster as a proper test fixture. Right now they have hardcoded port numbers from the last time i ran it on my machine :) There is some new stuff around this here: #15561. I will look into this.\n2. Ensure we only support `hdfs://`. We don't test anything else, and I know for a fact `file://` will not work, users should instead use the proper repo plugin which will be tested for that, and we should not expose hadoop VFS here in this plugin.  We should think about the configuration API here too, especially considering the warning in the docs about not putting paths in the URI. Instead of a URI at all, maybe we should just ask for the hdfs server. We can have an undocumented Setting that lets our hacky test filesystem still work for unit testing.\n3. open bug report at hadoop so that their classes can be initialized under unix without blanket process execution permissions across the entire machine. Our hack is ok only as a temporary one.\n4. fix the handling of `slf4j-api.jar` in our gradle build. the root cause is our gradle struggling on this `optional` dependency, really all are bogus and caused by the server having to perform double-duty as a client. currently the build is marking this as `provided` but it is in fact not provided, its excluded explicitly from all distributions. So it does not make its way into the HDFS plugin zip and then its missing. in this branch my hack just adds that jar to all distributions, I doubt that is what we really want to do.\n\nas far as the docs go, I strongly dislike having docs that recommend the user modify plugin jars. What is the protocol compatibility issue and how often does it break in hadoop?\n\nI have no clue about 2.2, I think we still have plenty of work to get this correct in master. Also 2.x would require additional maven/ant work to startup/shutdown hdfs during integration tests phase, etc.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/166155009","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-166155009","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":166155009,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NjE1NTAwOQ==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-12-20T21:06:01Z","updated_at":"2015-12-20T21:06:01Z","author_association":"CONTRIBUTOR","body":"The miniHDFS is working now for integration tests. I forked your branch here: https://github.com/elastic/elasticsearch/compare/master...rmuir:hdfs2-only?expand=1\n\nI will try to improve it now, but at least it is working and rest tests are passing with it.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/166158902","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-166158902","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":166158902,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NjE1ODkwMg==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-12-20T22:17:15Z","updated_at":"2015-12-20T22:17:15Z","author_association":"CONTRIBUTOR","body":"`gradle check` now works on both linux and os X and with reasonable speed. Windows is totally broken: unit tests do not work nor does the hdfs fixture work... I am working through those issues. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/166168260","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-166168260","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":166168260,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NjE2ODI2MA==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-12-21T00:36:17Z","updated_at":"2015-12-21T00:36:17Z","author_association":"CONTRIBUTOR","body":"On windows we can't start `hdfsFixture` correctly without having native libraries available. They aren't exactly packaged in any clean/efficient way and I don't think we want windows `dll` files flying around in our builds anyway.\n\nSo windows `integTest` skips the fixture and the snapshot/restore rest tests against it. Unit tests do work now though.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/166223846","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-166223846","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":166223846,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NjIyMzg0Ng==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-12-21T07:38:21Z","updated_at":"2015-12-21T07:38:21Z","author_association":"CONTRIBUTOR","body":"`gradle hdfs` (the simple javaexec test) works under `test/fixtures/hdfs-fixture` if you setup hadoop native libraries. You must set HADOOP_HOME and put HADOOP_HOME/bin in the PATH. I used https://github.com/karthikj1/Hadoop-2.7.1-Windows-64-binaries/releases as a quick test in a VM. \n\nSo this gives the possibility to test windows in jenkins, if some things are setup. It is optional and these things are still skipped if its not setup (only unit tests and simple \"plugin is installed\" rest test will run).\n\nBut the test fixture stuff is still totally broken on windows, it tries to do something crazy with a .bat file..., @rjernst needs to look.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/166398085","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-166398085","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":166398085,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NjM5ODA4NQ==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-12-21T19:32:17Z","updated_at":"2015-12-21T19:32:17Z","author_association":"CONTRIBUTOR","body":"I reviewed the hdfs code and ensured it matches the same semantics as our FS repository and removed any kind of leniency/exists/etc, with the exception of https://github.com/elastic/elasticsearch/issues/15579 (we fail with FileAlreadyExistsException on purpose)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/167007848","html_url":"https://github.com/elastic/elasticsearch/issues/15438#issuecomment-167007848","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15438","id":167007848,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NzAwNzg0OA==","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-12-23T23:21:11Z","updated_at":"2015-12-23T23:21:11Z","author_association":"CONTRIBUTOR","body":"I'm closing this. We are in fairly good shape in master... not perfect but the basics are there.\n","performed_via_github_app":null}]