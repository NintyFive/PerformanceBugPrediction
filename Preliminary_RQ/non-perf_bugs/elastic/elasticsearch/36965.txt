{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/36965","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36965/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36965/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36965/events","html_url":"https://github.com/elastic/elasticsearch/issues/36965","id":393740736,"node_id":"MDU6SXNzdWUzOTM3NDA3MzY=","number":36965,"title":"flood_stage setting marks indices read-only even if host has other free data disks","user":{"login":"phoerious","id":911270,"node_id":"MDQ6VXNlcjkxMTI3MA==","avatar_url":"https://avatars1.githubusercontent.com/u/911270?v=4","gravatar_id":"","url":"https://api.github.com/users/phoerious","html_url":"https://github.com/phoerious","followers_url":"https://api.github.com/users/phoerious/followers","following_url":"https://api.github.com/users/phoerious/following{/other_user}","gists_url":"https://api.github.com/users/phoerious/gists{/gist_id}","starred_url":"https://api.github.com/users/phoerious/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/phoerious/subscriptions","organizations_url":"https://api.github.com/users/phoerious/orgs","repos_url":"https://api.github.com/users/phoerious/repos","events_url":"https://api.github.com/users/phoerious/events{/privacy}","received_events_url":"https://api.github.com/users/phoerious/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2018-12-23T10:56:31Z","updated_at":"2018-12-24T20:34:03Z","closed_at":"2018-12-24T14:13:45Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\r\n\r\n** Please read the guidelines below. **\r\n\r\nIssues that do not follow these guidelines are likely to be closed.\r\n\r\n1.  GitHub is reserved for bug reports and feature requests. The best place to\r\n    ask a general question is at the Elastic [forums](https://discuss.elastic.co).\r\n    GitHub is not the place for general questions.\r\n\r\n2.  Is this bug report or feature request for a supported OS? If not, it\r\n    is likely to be closed.  See https://www.elastic.co/support/matrix#show_os\r\n\r\n3.  Please fill out EITHER the feature request block or the bug report block\r\n    below, and delete the other block.\r\n\r\n-->\r\n\r\n<!-- Bug report -->\r\n\r\n**Elasticsearch version** (`bin/elasticsearch --version`): 6.4.1\r\n\r\n**Plugins installed**: []\r\n\r\n**JVM version** (`java -version`): 1.8.0_162\r\n\r\n**OS version** (`uname -a` if on a Unix-like system): Elastic Docker Image on Ubuntu 14.04 host\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\nThe `cluster.routing.allocation.disk.watermark.flood_stage` setting seems to be based on individual disks instead of total storage capacity on a host, resulting in read-only indices in case a single disk runs out of space even if all nodes still have enough capacity to hold the shards.\r\n\r\nBackground: we have an Elasticsearch cluster of 130 nodes with 7 data disks per host. Although all nodes reported a remaining storage capacity between 1 and 10 TB, my indices were continuously being reset to read-only, even with `flood_stage` set to something as low as 10 GB. I tried lowering the `high` threshold so as to move shards away from \"fuller\" hosts, but it didn't solve the problem.\r\n\r\nIn the end, the reason turned out to be a single hosts with 1 full disk (only a few hundred MB left) and I had to manually remove the disk from the node's config file. The other 6 disks were more than capable of holding shards and ES had already moved all its shards from the full disk to somewhere else (meaning the data filling the disk belonged to other processes), but it kept marking my indices read-only anyway.\r\n\r\nIt would be great if the `flood_stage` setting were only applied if *all* disks on a host containing shards exceed the threshold.\r\n\r\n**Steps to reproduce**:\r\n 1. Deploy a cluster with multiple data disks per host\r\n 2. fill up one disk with stuff beyond the 'flood_stage' threshold\r\n 3. indices are marked read-only even though all hosts have enough space to hold shards\r\n","closed_by":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"performed_via_github_app":null}