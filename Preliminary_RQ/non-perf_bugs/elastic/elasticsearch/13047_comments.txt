[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/135723753","html_url":"https://github.com/elastic/elasticsearch/issues/13047#issuecomment-135723753","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13047","id":135723753,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNTcyMzc1Mw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-08-28T09:56:13Z","updated_at":"2015-08-28T09:58:54Z","author_association":"CONTRIBUTOR","body":"The problem here is that, while a replica may exist, we have no idea whether the replica is the same as the primary or not (unless the replica is sync flushed).  So we may end up having to copy over the whole primary (while still keeping the replica around just in case) which could push you over the limit.\n\nI think what should happen here is\n- existing primaries should be recovered automatically (needs checking)\n- sync-flushed replicas should be recovered automatically (probably needs adding)\n- once the index is green, unused shard copies should be deleted (which is what happens now)\n\nIt is still possible to get into a deadlock situation, eg:\n- two nodes, 1 replica, both nodes over the low watermark\n- the primary recovers automatically\n- the replica is not sync-flushed, but the node is over the low watermark and so the shard will not recover\n\nIn this case, the low watermark can be changed dynamically (and temporarily) to allow recovery.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/372989213","html_url":"https://github.com/elastic/elasticsearch/issues/13047#issuecomment-372989213","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13047","id":372989213,"node_id":"MDEyOklzc3VlQ29tbWVudDM3Mjk4OTIxMw==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-03-14T11:27:28Z","updated_at":"2018-03-14T11:27:28Z","author_association":"CONTRIBUTOR","body":"@clintongormley this issue is rather old but still sounds like it might be pertinent. However, running with disks fuller than the low disk watermark sounds like something to avoid, even if it's not immediately fatal. In the OP's case, with slow-growing data, perhaps the watermark can safely be set higher as a matter of course? And could the situation have been recovered by changing the watermark dynamically?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/382077120","html_url":"https://github.com/elastic/elasticsearch/issues/13047#issuecomment-382077120","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13047","id":382077120,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MjA3NzEyMA==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-04-17T17:31:42Z","updated_at":"2018-04-17T17:31:42Z","author_association":"CONTRIBUTOR","body":"@ywelsch and I discussed this and concluded that although we could see some ways to improve this situation in 6.x (e.g. nondestructively perform a seq#-based recovery of the existing shard copy rather than making a whole new one) on balance we prefer to close this to indicate that it's not something we foresee working on in the near future. Further feedback, including +1s, is welcome.","performed_via_github_app":null}]