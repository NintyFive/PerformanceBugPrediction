[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/47655046","html_url":"https://github.com/elastic/elasticsearch/issues/6618#issuecomment-47655046","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6618","id":47655046,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3NjU1MDQ2","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-07-01T13:24:58Z","updated_at":"2014-07-01T13:24:58Z","author_association":"CONTRIBUTOR","body":"@jpountz would this be possible with bucket reducers?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/47656265","html_url":"https://github.com/elastic/elasticsearch/issues/6618#issuecomment-47656265","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6618","id":47656265,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3NjU2MjY1","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2014-07-01T13:35:29Z","updated_at":"2014-07-01T13:35:29Z","author_association":"CONTRIBUTOR","body":"I think it could. Eg. you could always use second as an interval and then reduce as necessary to reach the desired number of buckets. But then if your actual data spans several years, you will be creating tens of millions of buckets to finally reduce into a couple of them. That would have important CPU/memory implications.\n\nI'm wondering that the best option might just be to run an additional request to figure out the optimal interval, eg. with min/max aggregations.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/47657989","html_url":"https://github.com/elastic/elasticsearch/issues/6618#issuecomment-47657989","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6618","id":47657989,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3NjU3OTg5","user":{"login":"uboness","id":211019,"node_id":"MDQ6VXNlcjIxMTAxOQ==","avatar_url":"https://avatars3.githubusercontent.com/u/211019?v=4","gravatar_id":"","url":"https://api.github.com/users/uboness","html_url":"https://github.com/uboness","followers_url":"https://api.github.com/users/uboness/followers","following_url":"https://api.github.com/users/uboness/following{/other_user}","gists_url":"https://api.github.com/users/uboness/gists{/gist_id}","starred_url":"https://api.github.com/users/uboness/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/uboness/subscriptions","organizations_url":"https://api.github.com/users/uboness/orgs","repos_url":"https://api.github.com/users/uboness/repos","events_url":"https://api.github.com/users/uboness/events{/privacy}","received_events_url":"https://api.github.com/users/uboness/received_events","type":"User","site_admin":false},"created_at":"2014-07-01T13:50:27Z","updated_at":"2014-07-01T13:50:27Z","author_association":"CONTRIBUTOR","body":"> I think it could. Eg. you could always use second as an interval and then reduce as necessary to reach the desired number of buckets. But then if your actual data spans several years, you will be creating tens of millions of buckets to finally reduce into a couple of them. That would have important CPU/memory implications.\n\n-1... that doesn't scale as a general solution for this functionality\n\n> I'm wondering that the best option might just be to run an additional request to figure out the optimal interval, eg. with min/max aggregations.\n\n+1.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/47679768","html_url":"https://github.com/elastic/elasticsearch/issues/6618#issuecomment-47679768","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6618","id":47679768,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3Njc5NzY4","user":{"login":"johnrodey","id":1458056,"node_id":"MDQ6VXNlcjE0NTgwNTY=","avatar_url":"https://avatars0.githubusercontent.com/u/1458056?v=4","gravatar_id":"","url":"https://api.github.com/users/johnrodey","html_url":"https://github.com/johnrodey","followers_url":"https://api.github.com/users/johnrodey/followers","following_url":"https://api.github.com/users/johnrodey/following{/other_user}","gists_url":"https://api.github.com/users/johnrodey/gists{/gist_id}","starred_url":"https://api.github.com/users/johnrodey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/johnrodey/subscriptions","organizations_url":"https://api.github.com/users/johnrodey/orgs","repos_url":"https://api.github.com/users/johnrodey/repos","events_url":"https://api.github.com/users/johnrodey/events{/privacy}","received_events_url":"https://api.github.com/users/johnrodey/received_events","type":"User","site_admin":false},"created_at":"2014-07-01T16:41:05Z","updated_at":"2014-07-01T16:41:05Z","author_association":"NONE","body":"I think executing a second request after I find the earliest and latest dates will work for me.  I do however think it would be desirable to possibly add this feature in the future, even if it is low priority.\n\nThanks for the insight!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/48021175","html_url":"https://github.com/elastic/elasticsearch/issues/6618#issuecomment-48021175","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6618","id":48021175,"node_id":"MDEyOklzc3VlQ29tbWVudDQ4MDIxMTc1","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2014-07-04T08:44:54Z","updated_at":"2014-07-04T08:44:54Z","author_association":"CONTRIBUTOR","body":"I have been thinking more about this one and I don't think we can do better than doing a first request to figure out the min/max values first and deciding on the interval based on those, which can be done from clients. So I will close this issue.\n","performed_via_github_app":null}]