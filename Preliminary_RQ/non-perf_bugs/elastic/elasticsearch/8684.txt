{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8684/events","html_url":"https://github.com/elastic/elasticsearch/issues/8684","id":50293007,"node_id":"MDU6SXNzdWU1MDI5MzAwNw==","number":8684,"title":"stuck initializing shards & NumberFormatException","user":{"login":"jillesvangurp","id":819187,"node_id":"MDQ6VXNlcjgxOTE4Nw==","avatar_url":"https://avatars2.githubusercontent.com/u/819187?v=4","gravatar_id":"","url":"https://api.github.com/users/jillesvangurp","html_url":"https://github.com/jillesvangurp","followers_url":"https://api.github.com/users/jillesvangurp/followers","following_url":"https://api.github.com/users/jillesvangurp/following{/other_user}","gists_url":"https://api.github.com/users/jillesvangurp/gists{/gist_id}","starred_url":"https://api.github.com/users/jillesvangurp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jillesvangurp/subscriptions","organizations_url":"https://api.github.com/users/jillesvangurp/orgs","repos_url":"https://api.github.com/users/jillesvangurp/repos","events_url":"https://api.github.com/users/jillesvangurp/events{/privacy}","received_events_url":"https://api.github.com/users/jillesvangurp/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2014-11-27T13:47:37Z","updated_at":"2016-05-18T16:31:32Z","closed_at":"2014-11-28T12:48:55Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I have a cluster that is currently stuck (yellow state) with a logstash index that was created midnight unable to assign a shard and two shards initializing. I examined the elasticsearch logs and found the following messages appaearing a lot:\n\n```\n[2014-11-27 10:15:12,585][WARN ][cluster.action.shard     ] [192.168.1.13] [logstash-2014.11.27][4] sending failed shard for [logstash-2014.11.27][4], node[o9vhU4BhSCuQ4BmLJjPtfA], [R], s[INITIALIZING], indexUUID [-mMLqYjAQuCUDcczYf5SHA], reason [Failed to start shard, message [RecoveryFailedException[[logstash-2014.11.27][4]: Recovery failed from [192.168.1.14][sE51TBxfQ2q6pD5k7G7piA][es2.inbot.io][inet[/192.168.1.14:9300]] into [192.168.1.13][o9vhU4BhSCuQ4BmLJjPtfA][es1.inbot.io][inet[/192.168.1.13:9300]]{master=true}]; nested: RemoteTransportException[[192.168.1.14][inet[/192.168.1.14:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[logstash-2014.11.27][4] Phase[2] Execution failed]; nested: RemoteTransportException[[192.168.1.13][inet[/192.168.1.13:9300]][internal:index/shard/recovery/translog_ops]]; nested: NumberFormatException[For input string: \"finished\"]; ]]\n```\n\nand\n\n```\n[2014-11-27 10:17:54,187][WARN ][cluster.action.shard     ] [192.168.1.14] [logstash-2014.11.27][4] sending failed shard for [logstash-2014.11.27][4], node[o9vhU4BhSCuQ4BmLJjPtfA], [R], s[INITIALIZING], indexUUID [-mMLqYjAQuCUDcczYf5SHA], reason [Failed to perform [indices:data/write/bulk[s]] on replica, message [RemoteTransportException[[192.168.1.13][inet[/192.168.1.13:9300]][indices:data/write/bulk[s][r]]]; nested: NumberFormatException[For input string: \"finished\"]; ]]\n```\n\nThe NumberFormatException looks like a possible cause. One possible explanation is that we have dynamically mapped logstash field that is sometimes a string and sometimes a number and since we roll over the index there's a chance that this field gets mapped incorrectly depending on what comes in first. However, I don't see how this should block shard initialization. Since midnight we've accumulated about 300M of errors like above. Normally our logs for each day are in the range of a few KB.\n\nThe index is actually available and accepting writes (i.e. kibana works as you would expect). But it likely is missing updates for some shards but if so, that is not apparent from the logs.\n\n```\n[linko@es3 elasticsearch]$ curl -XGET 'localhost:9200/_cluster/health/logstash-2014.11.27/?pretty'\n{\n  \"cluster_name\" : \"linko_elasticsearch\",\n  \"status\" : \"yellow\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 5,\n  \"number_of_data_nodes\" : 3,\n  \"active_primary_shards\" : 5,\n  \"active_shards\" : 12,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 2,\n  \"unassigned_shards\" : 1\n}\n```\n\nOur cluster has been running for a few weeks. We haven't really done any config changes lately. At least not on our logstash indices. This issue has happened before and I resolved it with a rolling restart at the time.\n\nI'm running 1.4.0 and have not upgraded to 1.4.1 yet. I'm planning to do so later today and I hope this problem will go away with a rolling restart. Meanwhile, I'm available for the next two hours or so to do more diagnostics on this cluster to get more info if needed/useful. If so, please let me know. \n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}