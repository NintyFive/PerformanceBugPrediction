[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261211523","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-261211523","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":261211523,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTIxMTUyMw==","user":{"login":"makeyang","id":13898618,"node_id":"MDQ6VXNlcjEzODk4NjE4","avatar_url":"https://avatars2.githubusercontent.com/u/13898618?v=4","gravatar_id":"","url":"https://api.github.com/users/makeyang","html_url":"https://github.com/makeyang","followers_url":"https://api.github.com/users/makeyang/followers","following_url":"https://api.github.com/users/makeyang/following{/other_user}","gists_url":"https://api.github.com/users/makeyang/gists{/gist_id}","starred_url":"https://api.github.com/users/makeyang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/makeyang/subscriptions","organizations_url":"https://api.github.com/users/makeyang/orgs","repos_url":"https://api.github.com/users/makeyang/repos","events_url":"https://api.github.com/users/makeyang/events{/privacy}","received_events_url":"https://api.github.com/users/makeyang/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T10:29:50Z","updated_at":"2016-11-17T10:29:50Z","author_association":"CONTRIBUTOR","body":"how about make hash method configable?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261329330","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-261329330","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":261329330,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTMyOTMzMA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T18:33:32Z","updated_at":"2016-11-17T18:33:32Z","author_association":"CONTRIBUTOR","body":"> how about make hash method configable?\n\nI like the current proposal better than making the hash method configurable, since there are fewer moving parts, so it is much easier to support in terms of backward compatibility for instance.\n\n@scottsom One thing I am wondering is whether in practice you would use the same number of shards for all users or not. I did not spend much time thinking about it, but I have the feeling that this feature only makes sense if all users have the same number of shards. Otherwise it means that the client has the logic to be able to know how to handle each user, meaning it could probably deal with multiple indices instead.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261346529","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-261346529","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":261346529,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTM0NjUyOQ==","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"created_at":"2016-11-17T19:36:50Z","updated_at":"2016-11-17T19:40:42Z","author_association":"CONTRIBUTOR","body":"Yes, the client would be unaware of the partitioning (aside from providing a routing value) and all users would use the same number of shards.\n\nYou would pick your partitions based on the size of the largest users. You would need to consider how many shards your biggest users would ideally span across and if you have a lot of them, how likely is it that they will fall into the same partition.\n\nIf you imagine an index with 64 shards then valid ways to partition this are [1, 2, 4, 8, 16, 32, 64] ways. If you partition it 64 ways then this is equivalent to how custom routing works today (each user lives in a single shard). If you partition it 1 way then this is equivalent to having no partitioning (each user lives on all shards). The values in-between is what this proposal aims to allow.\n\nYou can conclude that as the number of partitions decreases, the likelihood of imbalanced shards decreases at the cost of having to broadcast each search to more shards per request. When in doubt it would be best to start with a small number of partitions (or no partitioning) then ease into more partitions (through reindexing) to find the sweet spot based on your data distribution, how stable it is, and observed access patterns.\n\nTo draw an analogy, this is similar to what you would find in some database offerings. For example, Oracle's \"Composite Hash-Hash Partitioning\".\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261437676","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-261437676","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":261437676,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTQzNzY3Ng==","user":{"login":"makeyang","id":13898618,"node_id":"MDQ6VXNlcjEzODk4NjE4","avatar_url":"https://avatars2.githubusercontent.com/u/13898618?v=4","gravatar_id":"","url":"https://api.github.com/users/makeyang","html_url":"https://github.com/makeyang","followers_url":"https://api.github.com/users/makeyang/followers","following_url":"https://api.github.com/users/makeyang/following{/other_user}","gists_url":"https://api.github.com/users/makeyang/gists{/gist_id}","starred_url":"https://api.github.com/users/makeyang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/makeyang/subscriptions","organizations_url":"https://api.github.com/users/makeyang/orgs","repos_url":"https://api.github.com/users/makeyang/repos","events_url":"https://api.github.com/users/makeyang/events{/privacy}","received_events_url":"https://api.github.com/users/makeyang/received_events","type":"User","site_admin":false},"created_at":"2016-11-18T03:08:23Z","updated_at":"2016-11-18T03:08:37Z","author_association":"CONTRIBUTOR","body":"today someone ask for \"Composite Hash-Hash Partitioning\", tomorrow others will ask for other hash partitioning. so why not just provide configable and customerized hash function?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261501069","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-261501069","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":261501069,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTUwMTA2OQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-11-18T10:37:40Z","updated_at":"2016-11-18T10:37:40Z","author_association":"CONTRIBUTOR","body":"We just discussed this issue in FixitFriday. We do NOT want to make the hash function configurable in a way that would allow arbitary functions. However we are considering the idea of being able to specify a number of shards that documents may go to at index creation time, which could not be modified afterwards and would default to `1`. For instance one thing we would need to validate is whether we can make it play nicely with index shrinking. We would like to have a prototype implementing this idea in order to get a better idea of whether integrating this feature would be reasonable.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261540276","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-261540276","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":261540276,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTU0MDI3Ng==","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"created_at":"2016-11-18T14:11:38Z","updated_at":"2016-11-18T14:11:38Z","author_association":"CONTRIBUTOR","body":"I'm glad there is interest in adopting this feature - I will proceed with an implementation and send a pull request.\n\nFrom my brief foray into the code base, it did not appear that index shrinking should be impacted since routing is determined based on the original number of shards _then_ the shrinking factor is applied [0]. It is definitely a use case that will need to be verified though.\n\n[0] https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java#L223-L225\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261589199","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-261589199","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":261589199,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTU4OTE5OQ==","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"created_at":"2016-11-18T17:22:53Z","updated_at":"2016-11-18T17:22:53Z","author_association":"CONTRIBUTOR","body":"There are two different approaches that I have been considering. For both options, I would expect the data distribution to be roughly the same (will need to verify), so I think it is just a matter of preference.\n\n**1. Specify number of partitions**\n\nThis adds an index setting, `number_of_partitions`, which allows you to specify the number of ways to partition the index. It would be required that `number_of_shards % number_of_partitions == 0`, otherwise the partitions could not be of equal size.\n\nWith this, the calculation of which shards to search is fairly straight forward. For example, with a 4 shard index partitioned 2 ways then the calculation is just:\n\n`hash(routing_value) % number_of_partitions` with a value of 0 going to shards [0, 1] and a value of 1 going to shards [2, 3].\n\n**2. Specify a partition size**\n\nThis adds an index setting, `partition_size`, which allows you to specify the number of shards any given routing value can go to. The only requirement for this value is that it is `1 <= partition_size <= number_of_shards`.\n\nThe calculation for this one is a bit more complex. We could for example, [shuffle](https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) the shards and route the request to the first `partition_size` values. The seed value to our random number generator would be the hash of the routing value.\n\nFrom the user's perspective, option 2 is a bit more flexible because you could for example, allow any given routing value to go to 75% of the shards (e.g. an index of 4 shards with a partition size of 3). A side effect of this approach however, is that index shrinking would result in different routing values going to a different number of shards. For example, if we shrunk our 4 shard index to 2 shards then some routing values will go to 2 shards while others would go to 1.\n\nYour wording here:\n\n> we are considering the idea of being able to specify a number of shards that documents may go to at index creation time\n\nSeems to indicate that you have a preference to option 2?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261610266","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-261610266","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":261610266,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTYxMDI2Ng==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-11-18T18:50:14Z","updated_at":"2016-11-18T18:50:14Z","author_association":"CONTRIBUTOR","body":"> From my brief foray into the code base, it did not appear that index shrinking should be impacted since routing is determined based on the original number of shards then the shrinking factor is applied [0]. It is definitely a use case that will need to be verified though.\n\nSounds correct.\n\nI was just thinking about another potential issue: parent/child relations. This feature would mean that an index that has a partition size that is greater than 1 should not be able to create a parent/child mapping or the join would not work anymore?\n\n> The only requirement for this value is that it is 1 <= partition_size <= number_of_shards.\n\nMaybe we should even require that the partition size is <= number_of_shards/2. If routed documents can still go to a majority of shards, I'm not sure routing is still valuable?\n\n> Seems to indicate that you have a preference to option 2?\n\nYes indeed. For what it is worth, an implementation I was thinking about was to compute the target shard as something like `(hash(_routing) + hash(_id) % partition_size) % num_shards`.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261624903","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-261624903","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":261624903,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTYyNDkwMw==","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"created_at":"2016-11-18T19:48:40Z","updated_at":"2016-11-18T19:48:40Z","author_association":"CONTRIBUTOR","body":"> I was just thinking about another potential issue: parent/child relations. This feature would mean that an index that has a partition size that is greater than 1 should not be able to create a parent/child mapping or the join would not work anymore?\n\nRight, parent/child relationship could be sent to the same shard with some special handling but it would be too hackish (e.g. use the parent _id instead of the child _id for hashing, thus requiring the _parent and _routing values for child gets). For grandparent/grandchild data models, I don't see a way to get them all on the same shard.\n\nHopefully it would be an acceptable limitation for a partitioned index to just reject parent/child mappings?\n\n> Maybe we should even require that the partition size is <= number_of_shards/2. If routed documents can still go to a majority of shards, I'm not sure routing is still valuable?\n\nYes, routing to a majority is a bit weird but even if you partitioned it such that every request hit 9 out of 10 shards, I would expect that it would still be possible to measure a _slight_ performance gain, despite it being a bit nonsensical. Perhaps the option could be left open for the users to decide what works best for them?\n\n> Yes indeed. For what it is worth, an implementation I was thinking about was to compute the target shard as something like `(hash(_routing) + hash(_id) % partition_size) % num_shards`.\n\nThat sounds like a good way to choose the shards. This will result in each partition being a sequence of shards rather than a random permutation, which is easier to reason about.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/261823588","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-261823588","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":261823588,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTgyMzU4OA==","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"created_at":"2016-11-21T01:40:24Z","updated_at":"2016-11-21T01:42:13Z","author_association":"CONTRIBUTOR","body":"To back this up with some numbers, I've written an implementation and then loaded the [Apache Software Foundation Public Mail Archives](https://aws.amazon.com/datasets/apache-software-foundation-public-mail-archives/) into a cluster with a single node (i2.xlarge). The routing value is the mailing list.\r\n\r\nThere is about 1.7M emails in total and here is the distribution of the top 10 mailing lists.\r\n\r\nMailing List|% of documents\r\n-|-\r\ntomcat.apache.org-users|4.11%\r\nstruts.apache.org-user|3.00%\r\ntomcat.apache.org-dev|2.46%\r\nhttpd.apache.org-dev|2.27%\r\nwww .apache.org-apache-bugdb | 2.15%\r\ncommons.apache.org-dev|1.65%\r\nspamassassin.apache.org-users|1.61%\r\ncocoon.apache.org-dev|1.60%\r\ncocoon.apache.org-users|1.58%\r\naxis.apache.org-java-dev|1.57%\r\n\r\nI created 8 different indices, each with 8 shards and different partition sizes. The _partition_size_1_ index is how custom routing works today and the _no_partition_ is how the default routing works today.\r\n\r\nHere is the distribution of the emails between the shards:\r\n\r\nIndex|Shard 0|Shard 1|Shard 2|Shard 3|Shard 4|Shard 5|Shard 6|Shard 7|Quality†|Max - Min\r\n-|-|-|-|-|-|-|-|-|-|-|\r\npartition_size_1|167624|162620|148384|237575|194000|297919|306198|150497|1.08470836316453|157814\r\npartition_size_2|159448|164696|155727|192763|215957|246446|300951|228829|1.05140859885752|145224\r\npartition_size_3|208371|159902|159713|182503|193171|243665|265820|251672|1.0347405326755|106107\r\npartition_size_4|230471|196955|157411|179132|185405|218903|259334|237206|1.02348236763392|101923\r\npartition_size_5|222910|216243|187282|173571|181887|208067|236832|238025|1.01246333206541|64454\r\npartition_size_6|225629|212691|205436|195833|177401|201223|224552|222052|1.00559815982774|48228\r\npartition_size_7|214221|216155|204849|210426|195329|194444|215920|213473|1.00160620679272|21711\r\nno_partition|207835|208012|208214|207844|208655|208345|208583|207329|0.999999698460185|1326\r\n\r\n† The quality formula is for measuring the uniformity of a hash function and comes from [this page](https://www.strchr.com/hash_functions) which got it from the [Red Dragon Book](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools). A quality of 1 is a perfect score.\r\n\r\nAs expected, we see the distribution becoming more uniform as our partition sizes increase until we reach a near perfect distribution with no partitioning, which is simply hashing on the automatically generated ID.\r\n\r\nFinally, I ran some search load tests on each index using varying levels of concurrency. This test is running a match query on _all using a random dictionary word and filtering (and routing when applicable) on a random mailing list. Each worker thread would only complete after it had ran 1000 sequential queries. For tests with multiple workers, they all ran concurrently. The measured time is when all workers finished their 1000 queries.\r\n\r\nIndex|Time (s) – 1 Worker|Time (s) – 2 Workers|Time (s) – 4 Workers|Time (s) – 8 Workers|Time (s) – 16 Workers\r\n-|-|-|-|-|-|\r\npartition_size_1|2.00|2.52|3.78|7.17|13.99\r\npartition_size_2|2.09|3.04|4.85|9.41|17.99\r\npartition_size_3|2.34|3.64|5.96|11.19|21.94\r\npartition_size_4|2.86|4.31|7.15|13.70|26.31\r\npartition_size_5|3.32|4.85|8.34|15.78|30.43\r\npartition_size_6|3.56|5.31|9.26|17.46|33.38\r\npartition_size_7|3.72|5.76|10.24|19.31|37.30\r\nno_partition|4.02|6.25|11.18|21.40|40.93\r\n\r\nWe see that with respect to search performance, despite the shards being rather imbalanced, routing each request to a single shard is still the most efficient access pattern. This makes sense because the dataset we are working with is relatively small and each mailing list can sit in a single shard with plenty of room to spare.\r\n\r\nWe also see that routing to a majority of the shards can still yield a significant performance boost compared to no partitioning.\r\n\r\nThis is not an ideal dataset to demonstrate the usefulness of this optimization (need something much bigger that makes routing to a single shard impractical) but hopefully it gives you an idea of the potential.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262097921","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-262097921","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":262097921,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjA5NzkyMQ==","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"created_at":"2016-11-21T23:14:30Z","updated_at":"2016-11-21T23:14:30Z","author_association":"CONTRIBUTOR","body":"Another way this could be implemented is to leave the `_routing` field alone and introduce a new field, let's call it `_partitions`. This would allow us to support parent/child models, partitioning on N dimensions, and can be extended to support more than just hash partitioning (i.e. we could do range partitioning on dates, numbers, and/or coordinates).\r\n\r\nSuppose we could create an index like this:\r\n```javascript\r\n{\r\n  \"settings\": {\r\n    \"index\": {\r\n      \"number_of_shards\": 24,\r\n      \"partitions\": [\r\n        {\r\n          \"type\": \"hash\",\r\n          \"shards\": 6\r\n        },\r\n        {\r\n          \"type\": \"date\",\r\n          \"shards\": 2,\r\n          \"interval\": \"2h\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe shard count for each partition must be less than the total number of shards in the index _and_ less than the number of shards in the previous partition. In other words, each additional partition further refines the set of shards that need to be searched.\r\n\r\nAs an example, let's imagine we had a cluster that was consuming time series data from various systems. We observe that most searches are being done within the context of a `source_system` and usually on `event_time` over an hourly period. We want to optimize the cluster for this access pattern, so our first partition is done on `source_system`, which is hashed and picks 6 out of 24 shards. Next we partition on `event_time` such that the same 2 hour interval is distributed between 2 out of the 6 shards that were just picked. Finally, we hash on `_id` (or custom `_routing`) to pick the final shard out of the 2 remaining.\r\n\r\nNow we can support the following searches:\r\n* Neither `source_system` or `event_time` is provided - search all 24 shards\r\n* Only `source_system` is provided - search 6 shards\r\n* Both `source_system` and `event_time` is provided - search 2 shards (or more if the date range falls outside our interval)\r\n* Only `event_time` is provided - search all 24 shards\r\n\r\nIndexing our document would have a payload like this:\r\n\r\n```javascript\r\n{\r\n  \"_partitions\": [\"foo_system\", \"2016-11-21T12:34.56Z\"],\r\n  \"_source\": {\r\n    \"source_system\": \"foo_system\",\r\n    \"event_time\": \"2016-11-21T12:34.56Z\"\r\n  }\r\n}\r\n```\r\n\r\nFor what it's worth, I wouldn't aim to implement different types of partitioning (dates) in an initial implementation. Just wanted to illustrate the extensibility of this approach.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262865085","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-262865085","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":262865085,"node_id":"MDEyOklzc3VlQ29tbWVudDI2Mjg2NTA4NQ==","user":{"login":"compasses","id":10161171,"node_id":"MDQ6VXNlcjEwMTYxMTcx","avatar_url":"https://avatars1.githubusercontent.com/u/10161171?v=4","gravatar_id":"","url":"https://api.github.com/users/compasses","html_url":"https://github.com/compasses","followers_url":"https://api.github.com/users/compasses/followers","following_url":"https://api.github.com/users/compasses/following{/other_user}","gists_url":"https://api.github.com/users/compasses/gists{/gist_id}","starred_url":"https://api.github.com/users/compasses/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/compasses/subscriptions","organizations_url":"https://api.github.com/users/compasses/orgs","repos_url":"https://api.github.com/users/compasses/repos","events_url":"https://api.github.com/users/compasses/events{/privacy}","received_events_url":"https://api.github.com/users/compasses/received_events","type":"User","site_admin":false},"created_at":"2016-11-25T01:46:06Z","updated_at":"2016-11-25T01:46:06Z","author_association":"NONE","body":"@scottsom The routing cannot make sure each shard have  single one routing key's data. Do you consider this kind of case? Like i want each shard have only one routing key located in. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262978645","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-262978645","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":262978645,"node_id":"MDEyOklzc3VlQ29tbWVudDI2Mjk3ODY0NQ==","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"created_at":"2016-11-25T15:24:52Z","updated_at":"2016-11-25T15:24:52Z","author_association":"CONTRIBUTOR","body":"@compasses It is not the intention of this change to support a 1:1 mapping between shards and routing values. If that is your goal then you would be better off not using _routing and create an index per routing value, which only works as long as the cardinality of the routing value is low. Otherwise you will end up with an unmanageable amount of shards.\r\n\r\nFor reference, this is the commit I ran the benchmark above on and can be used as a prototype:\r\nhttps://github.com/scottsom/elasticsearch/commit/f48f28303dd02f0fc54376e5563693bb4354f016\r\n\r\nIt allows you to create an index like this:\r\n\r\n```bash\r\ncurl -XPUT 'localhost:9200/foo' -d '\r\n{               \r\n    \"settings\" : {\r\n        \"index\" : {\r\n            \"number_of_shards\" : 12,\r\n            \"number_of_replicas\" : 0,\r\n            \"partition_size\": 3\r\n        }\r\n    }\r\n}'\r\n```\r\n\r\nIt requires each mapping to have _routing required and does not allow _parent fields.\r\n\r\nIf this is a reasonable approach and the limitations are acceptable then I can add more tests and send a pull request.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/264076388","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-264076388","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":264076388,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NDA3NjM4OA==","user":{"login":"compasses","id":10161171,"node_id":"MDQ6VXNlcjEwMTYxMTcx","avatar_url":"https://avatars1.githubusercontent.com/u/10161171?v=4","gravatar_id":"","url":"https://api.github.com/users/compasses","html_url":"https://github.com/compasses","followers_url":"https://api.github.com/users/compasses/followers","following_url":"https://api.github.com/users/compasses/following{/other_user}","gists_url":"https://api.github.com/users/compasses/gists{/gist_id}","starred_url":"https://api.github.com/users/compasses/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/compasses/subscriptions","organizations_url":"https://api.github.com/users/compasses/orgs","repos_url":"https://api.github.com/users/compasses/repos","events_url":"https://api.github.com/users/compasses/events{/privacy}","received_events_url":"https://api.github.com/users/compasses/received_events","type":"User","site_admin":false},"created_at":"2016-12-01T04:31:37Z","updated_at":"2016-12-01T04:31:37Z","author_association":"NONE","body":"@scottsom get it. thanks","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/264440325","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-264440325","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":264440325,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NDQ0MDMyNQ==","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"created_at":"2016-12-02T12:06:56Z","updated_at":"2016-12-02T12:06:56Z","author_association":"CONTRIBUTOR","body":"@jpountz What are the next steps here?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/265543280","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-265543280","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":265543280,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NTU0MzI4MA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-12-07T19:14:42Z","updated_at":"2016-12-07T19:14:42Z","author_association":"CONTRIBUTOR","body":"@scottsom Last time we discussed about this feature, we said we could think about integrating it if it does not add too much complexity to how routing works in the code base. Would you like to give it a try?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/265605854","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-265605854","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":265605854,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NTYwNTg1NA==","user":{"login":"scottsom","id":23276852,"node_id":"MDQ6VXNlcjIzMjc2ODUy","avatar_url":"https://avatars1.githubusercontent.com/u/23276852?v=4","gravatar_id":"","url":"https://api.github.com/users/scottsom","html_url":"https://github.com/scottsom","followers_url":"https://api.github.com/users/scottsom/followers","following_url":"https://api.github.com/users/scottsom/following{/other_user}","gists_url":"https://api.github.com/users/scottsom/gists{/gist_id}","starred_url":"https://api.github.com/users/scottsom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/scottsom/subscriptions","organizations_url":"https://api.github.com/users/scottsom/orgs","repos_url":"https://api.github.com/users/scottsom/repos","events_url":"https://api.github.com/users/scottsom/events{/privacy}","received_events_url":"https://api.github.com/users/scottsom/received_events","type":"User","site_admin":false},"created_at":"2016-12-07T23:23:13Z","updated_at":"2016-12-07T23:23:13Z","author_association":"CONTRIBUTOR","body":"Here is a commit to my fork with the implementation that I used for the benchmarks above:\r\nhttps://github.com/scottsom/elasticsearch/commit/f48f28303dd02f0fc54376e5563693bb4354f016\r\n\r\nIf we want to proceed with this then I will look into adding more tests around it and send a pull request.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/267274152","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-267274152","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":267274152,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NzI3NDE1Mg==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-12-15T09:03:44Z","updated_at":"2016-12-15T09:03:44Z","author_association":"CONTRIBUTOR","body":"+1 to open a PR","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/269740356","html_url":"https://github.com/elastic/elasticsearch/issues/21585#issuecomment-269740356","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21585","id":269740356,"node_id":"MDEyOklzc3VlQ29tbWVudDI2OTc0MDM1Ng==","user":{"login":"makeyang","id":13898618,"node_id":"MDQ6VXNlcjEzODk4NjE4","avatar_url":"https://avatars2.githubusercontent.com/u/13898618?v=4","gravatar_id":"","url":"https://api.github.com/users/makeyang","html_url":"https://github.com/makeyang","followers_url":"https://api.github.com/users/makeyang/followers","following_url":"https://api.github.com/users/makeyang/following{/other_user}","gists_url":"https://api.github.com/users/makeyang/gists{/gist_id}","starred_url":"https://api.github.com/users/makeyang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/makeyang/subscriptions","organizations_url":"https://api.github.com/users/makeyang/orgs","repos_url":"https://api.github.com/users/makeyang/repos","events_url":"https://api.github.com/users/makeyang/events{/privacy}","received_events_url":"https://api.github.com/users/makeyang/received_events","type":"User","site_admin":false},"created_at":"2016-12-30T07:15:43Z","updated_at":"2016-12-30T07:15:43Z","author_association":"CONTRIBUTOR","body":"it's still a static way to solve hot share/hot data issue. \r\nbut the real case is that: before data is pushed into cluster, there is no way to know which part will be the hot part. so my question is: is there any plan to make ES can solve these kind of problem dynamic? \r\nnowsdays, with reindex feature and timestamp feature powered, maybe it's time to talk about dynamic sharding?","performed_via_github_app":null}]