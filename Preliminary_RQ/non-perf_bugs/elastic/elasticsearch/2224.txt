{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2224/events","html_url":"https://github.com/elastic/elasticsearch/issues/2224","id":6581562,"node_id":"MDU6SXNzdWU2NTgxNTYy","number":2224,"title":"Under moderate write only load a single shard goes crazy with high number of open file descriptors and multi hour flush time","user":{"login":"awick","id":427321,"node_id":"MDQ6VXNlcjQyNzMyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/427321?v=4","gravatar_id":"","url":"https://api.github.com/users/awick","html_url":"https://github.com/awick","followers_url":"https://api.github.com/users/awick/followers","following_url":"https://api.github.com/users/awick/following{/other_user}","gists_url":"https://api.github.com/users/awick/gists{/gist_id}","starred_url":"https://api.github.com/users/awick/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/awick/subscriptions","organizations_url":"https://api.github.com/users/awick/orgs","repos_url":"https://api.github.com/users/awick/repos","events_url":"https://api.github.com/users/awick/events{/privacy}","received_events_url":"https://api.github.com/users/awick/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2012-08-31T15:19:32Z","updated_at":"2013-03-11T19:46:31Z","closed_at":"2013-03-11T19:46:31Z","author_association":"NONE","active_lock_reason":null,"body":"I experience almost nightly a single shard that hangs and causes the rest of the shards in the index to hang and all bulk inserts to hang.  This hang lasts for multiple hours, and usually fixes it self when my daily script runs that optimizes the PREVIOUS daily index.\n\nMy application uses daily indexes, is http bulk inserting when this happens with NO reads, and the shard it happens to is usually not on a node that the application is connected to via http.\n\nLast night it happen again and I collected hopefully some useful data.\n- lsof shows 28k open file descriptors, all the other nodes have around 1.2k\n- Over 30k files in the sessions-120831/3/index directory.\n- Over 5600 segments in that directory\n- Total size of all files is 21 gigabytes\n- The hang usually happens around 4 hours AFTER the index is created.  I'm not sure if that is how long it takes before some autoflush happens or what\n- Only thing in that nodes log is when the index is created at 0:00 GMT\n\n[2012-08-30 20:00:00,221][DEBUG][index.gateway            ] [moloches-m10a] [sessions-120831][3] starting recovery from local ...\n[2012-08-30 20:00:00,227][DEBUG][index.gateway            ] [moloches-m10a] [sessions-120831][3] recovery completed from local, took [6ms]\n    index    : files           [0] with total_size [0b], took[0s]\n             : recovered_files [0] with total_size [0b]\n             : reusing_files   [0] with total_size [0b]\n    start    : took [6ms], check_index [0s]\n    translog : number_of_operations [0], took [0s]\n- Similar for master node just has \n  [2012-08-30 20:00:00,096][INFO ][cluster.metadata         ] [moloches-m02a] [sessions-120831] creating index, cause [auto(bulk api)], shards [10]/[0], mappings [session]\n- Hot threads for that bad node shows\n  30.0% (150ms out of 500ms) cpu usage by thread 'elasticsearch[moloches-m10a][bulk][T#986]'\n   10/10 snapshots sharing following 9 elements\n     org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:513)\n     org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)\n     org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)\n     org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)\n     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)\n     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)\n     java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)\n     java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n     java.lang.Thread.run(Unknown Source)\n  \n  30.0% (150ms out of 500ms) cpu usage by thread 'elasticsearch[moloches-m10a][bulk][T#796]'\n   10/10 snapshots sharing following 9 elements\n     org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:513)\n     org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)\n     org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)\n     org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)\n     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)\n     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)\n     java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)\n     java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n     java.lang.Thread.run(Unknown Source)\n  \n  10.0% (50ms out of 500ms) cpu usage by thread 'elasticsearch[moloches-m10a][bulk][T#857]'\n   10/10 snapshots sharing following 9 elements\n     org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:513)\n     org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)\n     org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)\n     org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)\n     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)\n     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)\n     java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)\n     java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n     java.lang.Thread.run(Unknown Source)\n- If I try and do a flush or any other operation it says that a flush is already in progress on that node\n- If I try and shutdown that node it just ignores it (would be useful to log if a node is ignoring a shutdown request and why)\n","closed_by":{"login":"awick","id":427321,"node_id":"MDQ6VXNlcjQyNzMyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/427321?v=4","gravatar_id":"","url":"https://api.github.com/users/awick","html_url":"https://github.com/awick","followers_url":"https://api.github.com/users/awick/followers","following_url":"https://api.github.com/users/awick/following{/other_user}","gists_url":"https://api.github.com/users/awick/gists{/gist_id}","starred_url":"https://api.github.com/users/awick/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/awick/subscriptions","organizations_url":"https://api.github.com/users/awick/orgs","repos_url":"https://api.github.com/users/awick/repos","events_url":"https://api.github.com/users/awick/events{/privacy}","received_events_url":"https://api.github.com/users/awick/received_events","type":"User","site_admin":false},"performed_via_github_app":null}