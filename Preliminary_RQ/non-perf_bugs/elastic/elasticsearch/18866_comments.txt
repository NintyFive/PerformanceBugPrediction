[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225982696","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-225982696","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":225982696,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTk4MjY5Ng==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T19:00:43Z","updated_at":"2016-06-14T19:00:43Z","author_association":"CONTRIBUTOR","body":"@imotov seems reasonable? Can't we just do this by default?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/225988867","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-225988867","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":225988867,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNTk4ODg2Nw==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T19:23:42Z","updated_at":"2016-06-14T19:23:42Z","author_association":"MEMBER","body":"@samcday, @clintongormley sorry, I must be missing something, but I don't really see how it would help. Replication in elasticsearch is document-based, so amount of work performed on primaries is exactly the same as amount of work performed on replicas (unless we run some very heavy update operations with some crazy scripts). We removed async operation option in 2.0, so an indexing operation is now going to always wait for the replica to finish the operation before returning response to the user. By moving \"the burden\" to a replica we are just going to move the place were we wait. Even with async replication, it doesn't make sense to me, because a slow replica would cause a pileup of in-flight indexing requests, which in turn can lead to running out of memory and cluster instability. Could you give a bit more details on how how this change would improve the overall indexing performance?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226011289","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-226011289","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":226011289,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjAxMTI4OQ==","user":{"login":"samcday","id":531550,"node_id":"MDQ6VXNlcjUzMTU1MA==","avatar_url":"https://avatars0.githubusercontent.com/u/531550?v=4","gravatar_id":"","url":"https://api.github.com/users/samcday","html_url":"https://github.com/samcday","followers_url":"https://api.github.com/users/samcday/followers","following_url":"https://api.github.com/users/samcday/following{/other_user}","gists_url":"https://api.github.com/users/samcday/gists{/gist_id}","starred_url":"https://api.github.com/users/samcday/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/samcday/subscriptions","organizations_url":"https://api.github.com/users/samcday/orgs","repos_url":"https://api.github.com/users/samcday/repos","events_url":"https://api.github.com/users/samcday/events{/privacy}","received_events_url":"https://api.github.com/users/samcday/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T20:47:55Z","updated_at":"2016-06-14T20:49:32Z","author_association":"NONE","body":"@imotov here's our problem in more detail:\n\nWe have a large cluster holding 9416 primary shards. Each of these shards has one replica in a different AZ (we're deploying in AWS). We've sunk a _lot_ of engineering effort into a pretty elaborate higher-level allocation strategy that ensures the indices that are actively being written to have shards evenly distributed across 4 \"indexing tiers\" made up of 24 nodes in total. The problem is, short of actually disabling Elasticsearch shard allocation and doing it 100% ourselves, we have no way to control where the primary shards are allocated. We're already using [index.routing.allocation.total_shards_per_node](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-total-shards.html) but this does not stop a large number of primary shards from clustering on a small set of indexing nodes. If the snapshot process randomly selected between the primary or one of the available replicas for the primary,  (the same way queries do today) then it would more evenly spread the snapshotting load across our active nodes.\n\nBefore you say it, yes, I know our use-case is crazy. I have virtually every PM and dev at Elastic telling me we're crazy (maybe we are. If we're cognizant of our possible craziness, wouldn't that mean we're definitely not crazy though? hmmm.). Anyway, you could simplify this way down:\n\nLet's say you have 2 indices each with 3 primary shards + 3 replica shards. Allocation looks like this:\n\n```\n      NODE A            NODE B\n+---------------+ +---------------+\n| index_1[0][p] | | index_1[0][r] |\n| index_1[1][p] | | index_1[1][r] |\n| index_1[2][r] | | index_1[2][p] |\n| index_2[0][p] | | index_2[0][r] |\n| index_2[1][p] | | index_2[1][r] |\n| index_2[2][r] | | index_2[2][p] |\n+---------------+ +---------------+\n```\n\nIn this case, Node A has 4 primary shards, Node B only has 2. So, during a snapshot, NodeA currently has to do 2x more work than Node B.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226015368","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-226015368","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":226015368,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjAxNTM2OA==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T21:02:50Z","updated_at":"2016-06-14T21:02:50Z","author_association":"MEMBER","body":"Now, it makes more sense. So, the problem is unbalanced allocation of primary shards, which leads to overloading nodes with disproportionate number of primary shards on operations that are executed only on primaries, which in this particular case the snapshot operation. \n\nI can clearly see how this can be an issue on two nodes cluster in your example, but I am having a bit of a hard time imagining this situation across 24 nodes unless you just did a rolling restart or using very long living indices. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226016309","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-226016309","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":226016309,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjAxNjMwOQ==","user":{"login":"samcday","id":531550,"node_id":"MDQ6VXNlcjUzMTU1MA==","avatar_url":"https://avatars0.githubusercontent.com/u/531550?v=4","gravatar_id":"","url":"https://api.github.com/users/samcday","html_url":"https://github.com/samcday","followers_url":"https://api.github.com/users/samcday/followers","following_url":"https://api.github.com/users/samcday/following{/other_user}","gists_url":"https://api.github.com/users/samcday/gists{/gist_id}","starred_url":"https://api.github.com/users/samcday/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/samcday/subscriptions","organizations_url":"https://api.github.com/users/samcday/orgs","repos_url":"https://api.github.com/users/samcday/repos","events_url":"https://api.github.com/users/samcday/events{/privacy}","received_events_url":"https://api.github.com/users/samcday/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T21:06:23Z","updated_at":"2016-06-14T21:06:23Z","author_association":"NONE","body":"You hit the nail on the head - we do a lot of rolling restarts to propagate config changes / ES minor version upgrades. For larger changes we do use shard allocation filtering to exclude nodes, wait for the node to be evacuated, and then kill it. In this case shards disperse more evenly, but we don't do this too often, seeing as each indexing node has 1tb+ of data to move around :(\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226029497","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-226029497","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":226029497,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjAyOTQ5Nw==","user":{"login":"samcday","id":531550,"node_id":"MDQ6VXNlcjUzMTU1MA==","avatar_url":"https://avatars0.githubusercontent.com/u/531550?v=4","gravatar_id":"","url":"https://api.github.com/users/samcday","html_url":"https://github.com/samcday","followers_url":"https://api.github.com/users/samcday/followers","following_url":"https://api.github.com/users/samcday/following{/other_user}","gists_url":"https://api.github.com/users/samcday/gists{/gist_id}","starred_url":"https://api.github.com/users/samcday/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/samcday/subscriptions","organizations_url":"https://api.github.com/users/samcday/orgs","repos_url":"https://api.github.com/users/samcday/repos","events_url":"https://api.github.com/users/samcday/events{/privacy}","received_events_url":"https://api.github.com/users/samcday/received_events","type":"User","site_admin":false},"created_at":"2016-06-14T22:01:08Z","updated_at":"2016-06-14T22:01:08Z","author_association":"NONE","body":"Whilst discussing this I've realised that maybe what I'm asking for is a band-aid. I've also raised #18874 as a feature request.\n\nI'd still like to keep this one open to see if others have a similar need.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226126604","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-226126604","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":226126604,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjEyNjYwNA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-06-15T08:49:48Z","updated_at":"2016-06-15T08:49:48Z","author_association":"CONTRIBUTOR","body":"> Whilst discussing this I've realised that maybe what I'm asking for is a band-aid. I've also raised #18874 as a feature request.\n\nActually I don't think this is a band-aid at all.  Primaries and replicas SHOULD be doing similar amounts of work.  It should not matter that all primaries are on one node and all replicas on another.\n\nWe should work at reducing reliance on the primary to avoid these hot spots.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226723804","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-226723804","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":226723804,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjcyMzgwNA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-06-17T09:28:30Z","updated_at":"2016-06-17T09:28:30Z","author_association":"MEMBER","body":"the main reason why we always snapshot from a primary is that we want to make sure we snapshot the same shard so we can do proper file based deltas and make snapshoting efficient (files across shards are different). At the time snapshot and restore was written, we only had one way to accomplish this - do it on the primary. These days we have allocation ids and we can explore making each snapshot sticky to the same allocation id (but not necessarily a primary) and distribute the load across copies. @samcday will that work for you?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226921383","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-226921383","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":226921383,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjkyMTM4Mw==","user":{"login":"samcday","id":531550,"node_id":"MDQ6VXNlcjUzMTU1MA==","avatar_url":"https://avatars0.githubusercontent.com/u/531550?v=4","gravatar_id":"","url":"https://api.github.com/users/samcday","html_url":"https://github.com/samcday","followers_url":"https://api.github.com/users/samcday/followers","following_url":"https://api.github.com/users/samcday/following{/other_user}","gists_url":"https://api.github.com/users/samcday/gists{/gist_id}","starred_url":"https://api.github.com/users/samcday/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/samcday/subscriptions","organizations_url":"https://api.github.com/users/samcday/orgs","repos_url":"https://api.github.com/users/samcday/repos","events_url":"https://api.github.com/users/samcday/events{/privacy}","received_events_url":"https://api.github.com/users/samcday/received_events","type":"User","site_admin":false},"created_at":"2016-06-18T04:34:26Z","updated_at":"2016-06-18T04:34:26Z","author_association":"NONE","body":"@bleskes yeah that sounds great. How would you decide which shard to use? Randomised selection?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/226924451","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-226924451","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":226924451,"node_id":"MDEyOklzc3VlQ29tbWVudDIyNjkyNDQ1MQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-06-18T06:06:12Z","updated_at":"2016-06-18T06:06:12Z","author_association":"MEMBER","body":"That would be my first attempt, yeah.\n\nOn 18 jun. 2016 6:34 AM +0200, Samnotifications@github.com, wrote:\n\n> @bleskes(https://github.com/bleskes)yeah that sounds great. How would you decide which shard to use? Randomised selection?\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly,view it on GitHub(https://github.com/elastic/elasticsearch/issues/18866#issuecomment-226921383), ormute the thread(https://github.com/notifications/unsubscribe/AA9bJxMe4-hj6CLzzP6p4FbMwq1SvPYAks5qM3VkgaJpZM4I1odk).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/229906584","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-229906584","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":229906584,"node_id":"MDEyOklzc3VlQ29tbWVudDIyOTkwNjU4NA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-07-01T09:51:39Z","updated_at":"2016-07-01T09:51:39Z","author_association":"CONTRIBUTOR","body":"I think we can remove the discuss label @bleskes please put it back if you feel like it needs more discussion\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/229917430","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-229917430","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":229917430,"node_id":"MDEyOklzc3VlQ29tbWVudDIyOTkxNzQzMA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-07-01T10:48:29Z","updated_at":"2016-07-01T10:48:35Z","author_association":"MEMBER","body":"@s1monw yeah, I should have removed the label. Thanks for cleaning up.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/278727486","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-278727486","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":278727486,"node_id":"MDEyOklzc3VlQ29tbWVudDI3ODcyNzQ4Ng==","user":{"login":"apatrida","id":182340,"node_id":"MDQ6VXNlcjE4MjM0MA==","avatar_url":"https://avatars3.githubusercontent.com/u/182340?v=4","gravatar_id":"","url":"https://api.github.com/users/apatrida","html_url":"https://github.com/apatrida","followers_url":"https://api.github.com/users/apatrida/followers","following_url":"https://api.github.com/users/apatrida/following{/other_user}","gists_url":"https://api.github.com/users/apatrida/gists{/gist_id}","starred_url":"https://api.github.com/users/apatrida/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apatrida/subscriptions","organizations_url":"https://api.github.com/users/apatrida/orgs","repos_url":"https://api.github.com/users/apatrida/repos","events_url":"https://api.github.com/users/apatrida/events{/privacy}","received_events_url":"https://api.github.com/users/apatrida/received_events","type":"User","site_admin":false},"created_at":"2017-02-09T18:20:41Z","updated_at":"2017-02-09T18:20:41Z","author_association":"CONTRIBUTOR","body":"A note for @bleskes  ... we use a model where we add and remove nodes all the time based on load (i.e. app servers come online and receive shards for indexes they use heavily so they can mostly query locally which still having live data, and then scale up or down).  Therefore allocations will change a lot as these extra expansions come and go.   Does this break the following?\r\n\r\n> These days we have allocation ids and we can explore making each snapshot sticky to the same allocation id (but not necessarily a primary) and distribute the load across copies. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/278883917","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-278883917","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":278883917,"node_id":"MDEyOklzc3VlQ29tbWVudDI3ODg4MzkxNw==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2017-02-10T08:17:36Z","updated_at":"2017-02-10T08:17:36Z","author_association":"MEMBER","body":"@apatrida if we end up doing this (there are more questions, like for example, are we willing to suffer the costs of a bigger snapshot when a shard relocates - at that point it is copied from the primary, not the original shard) it will be indeed problematic for you as it seems you would like to make sure that certain copies are never snapshotted. Note thought that even today you are not safe- if a primary fails, one of the copies on the app servers may be elected as primary. There is no way to control that and I don't see that happening soon.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/278900478","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-278900478","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":278900478,"node_id":"MDEyOklzc3VlQ29tbWVudDI3ODkwMDQ3OA==","user":{"login":"apatrida","id":182340,"node_id":"MDQ6VXNlcjE4MjM0MA==","avatar_url":"https://avatars3.githubusercontent.com/u/182340?v=4","gravatar_id":"","url":"https://api.github.com/users/apatrida","html_url":"https://github.com/apatrida","followers_url":"https://api.github.com/users/apatrida/followers","following_url":"https://api.github.com/users/apatrida/following{/other_user}","gists_url":"https://api.github.com/users/apatrida/gists{/gist_id}","starred_url":"https://api.github.com/users/apatrida/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apatrida/subscriptions","organizations_url":"https://api.github.com/users/apatrida/orgs","repos_url":"https://api.github.com/users/apatrida/repos","events_url":"https://api.github.com/users/apatrida/events{/privacy}","received_events_url":"https://api.github.com/users/apatrida/received_events","type":"User","site_admin":false},"created_at":"2017-02-10T09:44:37Z","updated_at":"2017-02-10T09:44:37Z","author_association":"CONTRIBUTOR","body":"So the consequence isn't really a breakage of snapshot/restore but more about how much data will need to be copied since maybe it will be less incremental for a given shard that happens to no longer have the same allocation IDs available between runs.  ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/278907064","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-278907064","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":278907064,"node_id":"MDEyOklzc3VlQ29tbWVudDI3ODkwNzA2NA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2017-02-10T10:16:18Z","updated_at":"2017-02-10T10:16:18Z","author_association":"MEMBER","body":"> So the consequence isn't really a breakage of snapshot/restore but more about how much data will need to be copied since maybe it will be less incremental \r\n\r\nCorrect.\r\n\r\n> that happens to no longer have the same allocation IDs available between runs.\r\n\r\nAlmost. When shard is moved around it gets a new allocation ID. You can think as an identifier of a disk folder - when the shard moves to a new folder on another node  it gets a new id. When it is moved back, the old data is \"deleted\" (not really, but conceptually yes, at least now - there are plans to change it) and the shard gets a new allocation id. The problem is more that the different shards have different merge cycles and thus different files. To make snapshot efficient (i.e., small deltas) we need to make it stick to a certain set of files. We currently do so by looking at the primary. If we move to aID we could stick to other shards, but when those shards are moved their file structure will change (it will be a copy of the primary)\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/278948611","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-278948611","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":278948611,"node_id":"MDEyOklzc3VlQ29tbWVudDI3ODk0ODYxMQ==","user":{"login":"apatrida","id":182340,"node_id":"MDQ6VXNlcjE4MjM0MA==","avatar_url":"https://avatars3.githubusercontent.com/u/182340?v=4","gravatar_id":"","url":"https://api.github.com/users/apatrida","html_url":"https://github.com/apatrida","followers_url":"https://api.github.com/users/apatrida/followers","following_url":"https://api.github.com/users/apatrida/following{/other_user}","gists_url":"https://api.github.com/users/apatrida/gists{/gist_id}","starred_url":"https://api.github.com/users/apatrida/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apatrida/subscriptions","organizations_url":"https://api.github.com/users/apatrida/orgs","repos_url":"https://api.github.com/users/apatrida/repos","events_url":"https://api.github.com/users/apatrida/events{/privacy}","received_events_url":"https://api.github.com/users/apatrida/received_events","type":"User","site_admin":false},"created_at":"2017-02-10T14:02:13Z","updated_at":"2017-02-10T14:02:13Z","author_association":"CONTRIBUTOR","body":"thanks.  that helps.  I think our shard manager will be ok in these models,\nwhen compared to the inner workings and one not breaking the other (much).\n\nOn Fri, Feb 10, 2017 at 7:16 AM, Boaz Leskes <notifications@github.com>\nwrote:\n\n> So the consequence isn't really a breakage of snapshot/restore but more\n> about how much data will need to be copied since maybe it will be less\n> incremental\n>\n> Correct.\n>\n> that happens to no longer have the same allocation IDs available between\n> runs.\n>\n> Almost. When shard is moved around it gets a new allocation ID. You can\n> think as an identifier of a disk folder - when the shard moves to a new\n> folder on another node it gets a new id. When it is moved back, the old\n> data is \"deleted\" (not really, but conceptually yes, at least now - there\n> are plans to change it) and the shard gets a new allocation id. The problem\n> is more that the different shards have different merge cycles and thus\n> different files. To make snapshot efficient (i.e., small deltas) we need to\n> make it stick to a certain set of files. We currently do so by looking at\n> the primary. If we move to aID we could stick to other shards, but when\n> those shards are moved their file structure will change (it will be a copy\n> of the primary)\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/elastic/elasticsearch/issues/18866#issuecomment-278907064>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALIROQIlqIEklMNVeyuc5HqkemsniFwks5rbDkWgaJpZM4I1odk>\n> .\n>\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/564584687","html_url":"https://github.com/elastic/elasticsearch/issues/18866#issuecomment-564584687","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18866","id":564584687,"node_id":"MDEyOklzc3VlQ29tbWVudDU2NDU4NDY4Nw==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-12-11T15:00:47Z","updated_at":"2019-12-11T15:00:47Z","author_association":"MEMBER","body":"We discussed this during out team sync today and decided to close this issue.\r\nGiven the low demand, limited usefulness and complexity of implementing this we don't see this as something to work on in the foreseeable future.","performed_via_github_app":null}]