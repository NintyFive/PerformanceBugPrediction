{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/28169","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28169/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28169/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28169/events","html_url":"https://github.com/elastic/elasticsearch/issues/28169","id":287441516,"node_id":"MDU6SXNzdWUyODc0NDE1MTY=","number":28169,"title":"[CI] StackOverflowError when executing SnapshotDisruptionIT.testDisruptionOnSnapshotInitialization","user":{"login":"tlrx","id":642733,"node_id":"MDQ6VXNlcjY0MjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/642733?v=4","gravatar_id":"","url":"https://api.github.com/users/tlrx","html_url":"https://github.com/tlrx","followers_url":"https://api.github.com/users/tlrx/followers","following_url":"https://api.github.com/users/tlrx/following{/other_user}","gists_url":"https://api.github.com/users/tlrx/gists{/gist_id}","starred_url":"https://api.github.com/users/tlrx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tlrx/subscriptions","organizations_url":"https://api.github.com/users/tlrx/orgs","repos_url":"https://api.github.com/users/tlrx/repos","events_url":"https://api.github.com/users/tlrx/events{/privacy}","received_events_url":"https://api.github.com/users/tlrx/received_events","type":"User","site_admin":false},"labels":[{"id":143077482,"node_id":"MDU6TGFiZWwxNDMwNzc0ODI=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Snapshot/Restore","name":":Distributed/Snapshot/Restore","color":"0e8a16","default":false,"description":"Anything directly related to the `_snapshot/*` APIs"},{"id":148612629,"node_id":"MDU6TGFiZWwxNDg2MTI2Mjk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Etest-failure","name":">test-failure","color":"207de5","default":false,"description":"Triaged test failures from CI"},{"id":1223177445,"node_id":"MDU6TGFiZWwxMjIzMTc3NDQ1","url":"https://api.github.com/repos/elastic/elasticsearch/labels/v7.0.0-beta1","name":"v7.0.0-beta1","color":"dddddd","default":false,"description":""}],"state":"closed","locked":false,"assignee":{"login":"tlrx","id":642733,"node_id":"MDQ6VXNlcjY0MjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/642733?v=4","gravatar_id":"","url":"https://api.github.com/users/tlrx","html_url":"https://github.com/tlrx","followers_url":"https://api.github.com/users/tlrx/followers","following_url":"https://api.github.com/users/tlrx/following{/other_user}","gists_url":"https://api.github.com/users/tlrx/gists{/gist_id}","starred_url":"https://api.github.com/users/tlrx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tlrx/subscriptions","organizations_url":"https://api.github.com/users/tlrx/orgs","repos_url":"https://api.github.com/users/tlrx/repos","events_url":"https://api.github.com/users/tlrx/events{/privacy}","received_events_url":"https://api.github.com/users/tlrx/received_events","type":"User","site_admin":false},"assignees":[{"login":"tlrx","id":642733,"node_id":"MDQ6VXNlcjY0MjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/642733?v=4","gravatar_id":"","url":"https://api.github.com/users/tlrx","html_url":"https://github.com/tlrx","followers_url":"https://api.github.com/users/tlrx/followers","following_url":"https://api.github.com/users/tlrx/following{/other_user}","gists_url":"https://api.github.com/users/tlrx/gists{/gist_id}","starred_url":"https://api.github.com/users/tlrx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tlrx/subscriptions","organizations_url":"https://api.github.com/users/tlrx/orgs","repos_url":"https://api.github.com/users/tlrx/repos","events_url":"https://api.github.com/users/tlrx/events{/privacy}","received_events_url":"https://api.github.com/users/tlrx/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2018-01-10T13:55:45Z","updated_at":"2019-02-07T10:37:39Z","closed_at":"2018-01-26T12:45:05Z","author_association":"MEMBER","active_lock_reason":null,"body":"The test `SnapshotDisruptionIT.testDisruptionOnSnapshotInitialization()` failed [on CI today](https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-windows-compatibility/1092) on Windows Server 2012 R2 6.3 amd64/Oracle Corporation 1.8.0_92 (64-bit).\r\n\r\nI first thought that it was a snapshot/restore regression due to my recent changes in #28078 or #27931 but after [looking at the test execution log](https://github.com/elastic/elasticsearch/files/1618707/consoleText.txt) I'm not so sure.\r\n\r\nI wonder if in this test the cluster ends up in a situation where a `listener.onFailure()` call caused a stack overflow error on a network thread (that was uncaught by the usual Elasticsearch's UncaughtExceptionHandler) that caused the NIO Selector to be closed too and not listening to incoming requests.\r\n\r\nThe test starts 3 master only nodes and 1 data only node. Once the cluster is stable, it sets up a snapshot repository and creates a first snapshot to check that everything is working correctly. Then it sets up a disruption scheme that is designed to isolate the master node as soon as a snapshot-in-progress entry in INIT state is found in the cluster state. Next step in the test is to create a second snapshot that triggers the disruption scheme and waits for the cluster to elect a new master that terminates this second snapshot.\r\n\r\nIn this execution [logs](https://github.com/elastic/elasticsearch/files/1618707/consoleText.txt) `node_tm0`, `node_tm1` and `node_tm2` are started as master only nodes. `node_tm1` is elected as master and adds the data only node `node_td3` to the cluster. \r\n\r\nThe test runs correctly to the point where the second snapshot is created:\r\n\r\n> [2018-01-10T05:23:10,388][INFO ][o.e.d.SnapshotDisruptionIT] --> starting disruption\r\n> ...\r\n> [2018-01-10T05:23:10,395][INFO ][o.e.s.SnapshotsService   ] [node_tm1] snapshot [test-repo:test-snap-2/vyIDc0GFSIKVtGd--HP_hQ] started\r\n\r\nOnce the disruption is started the master node `node_tm1` is isolated. Other nodes think it left:\r\n\r\n> [2018-01-10T05:23:12,061][INFO ][o.e.t.d.TestZenDiscovery ] [node_tm0] master_left [{node_tm1}{05tuxEPVT5ypY7Mz6o_VRA}{p_hm4MVNRoOvl-n281Co2A}{127.0.0.1}{127.0.0.1:30101}], reason [failed to ping, tried [1] times, each with  maximum [1s] timeout]\r\n> ...\r\n> [2018-01-10T05:23:12,061][INFO ][o.e.t.d.TestZenDiscovery ] [node_tm2] master_left [{node_tm1}{05tuxEPVT5ypY7Mz6o_VRA}{p_hm4MVNRoOvl-n281Co2A}{127.0.0.1}{127.0.0.1:30101}], reason [failed to ping, tried [1] times, each with  maximum [1s] timeout]\r\n> ...\r\n> [2018-01-10T05:23:12,125][INFO ][o.e.t.d.TestZenDiscovery ] [node_td3] master_left [{node_tm1}{05tuxEPVT5ypY7Mz6o_VRA}{p_hm4MVNRoOvl-n281Co2A}{127.0.0.1}{127.0.0.1:30101}], reason [failed to ping, tried [1] times, each with  maximum [1s] timeout]\r\n\r\nSo the remaining nodes elect `node_tm0` as the new master node. It uses the last commited cluster state in version 25:\r\n\r\n> [2018-01-10T05:23:16,069][INFO ][o.e.c.s.ClusterApplierService] [node_tm0] new_master {node_tm0}{JvhDBfzERq6UD13DUGBUJg}{Grp3AjfZR4m4Wb4Re-p5MA}{127.0.0.1}{127.0.0.1:30100}, reason: apply cluster state (from master [master {node_tm0}{JvhDBfzERq6UD13DUGBUJg}{Grp3AjfZR4m4Wb4Re-p5MA}{127.0.0.1}{127.0.0.1:30100} committed version [25] source [zen-disco-elected-as-master ([1] nodes joined)[{node_tm2}{nPk7Z6bvRI-MufskXAhGTw}{-NZTnhd1SrSolB7FjkDwcg}{127.0.0.1}{127.0.0.1:30102}]]])\r\n\r\nThe new master node updates the cluster state to notify that it is the master now. But the publishing of the cluster state version 26 is not processed by the old master node which is still isolated:\r\n\r\n> [2018-01-10T05:23:17,075][WARN ][o.e.d.z.PublishClusterStateAction] [node_tm0] timed out waiting for all nodes to process published state [26]  (timeout [1s], pending nodes: [{node_tm1} ....)\r\n\r\nSo the master node gives up, removes the old master node from the cluster state and publish a new version 27 of the cluster state where `node_tm1` is removed:\r\n\r\n> [2018-01-10T05:23:17,089][INFO ][o.e.c.s.ClusterApplierService] [node_td3] removed {{node_tm1}[redacted], reason: apply cluster state (from master [master {node_tm0}[redacted] committed version [27]])\r\n> \r\n> [2018-01-10T05:23:17,089][INFO ][o.e.c.s.ClusterApplierService] [node_tm2] removed {{node_tm1}[redacted], reason: apply cluster state (from master [master {node_tm0}[redacted] committed version [27]])\r\n> \r\n> [2018-01-10T05:23:17,092][INFO ][o.e.c.s.ClusterApplierService] [node_tm0] removed {{node_tm1}[redacted], reason: apply cluster state (from master [master {node_tm0}[redacted] committed version [27] source [zen-disco-node-failed({node_tm1}\r\n\r\nAnd the new master node cleans up the second snapshot as expected:\r\n\r\n> [2018-01-10T05:23:17,378][INFO ][o.e.s.SnapshotsService   ] [node_tm0] snapshot [test-repo:test-snap-2/vyIDc0GFSIKVtGd--HP_hQ] completed with state [SUCCESS]\r\n> [2018-01-10T05:23:17,523][INFO ][o.e.s.SnapshotsService   ] [node_tm0] snapshot [test-repo:test-snap-2/vyIDc0GFSIKVtGd--HP_hQ] deleted\r\n\r\nSo the test is somewhat good as its first purpose is to test that the snapshot is correctly terminated by the new master. Before the test ends, it stops the disruption and waits for the cluster to be stable again:\r\n\r\n> [2018-01-10T05:23:18,605][INFO ][o.e.d.SnapshotDisruptionIT] --> stopping disrupting\r\n\r\nThe old master node detects the timeout when it tried to publish the initial cluster state 25 (when the second snapshot is STARTED in cluster state):\r\n\r\n> [2018-01-10T05:23:40,406][WARN ][o.e.t.d.TestZenDiscovery ] [node_tm1] zen-disco-failed-to-publish, \r\n> \r\n [2018-01-10T05:23:40,407][WARN ][o.e.c.s.MasterService    ] [node_tm1] failing [update_snapshot [test-repo:test-snap-2/vyIDc0GFSIKVtGd--HP_hQ]]: failed to commit cluster state version [25]\r\n\r\nAnd the old master fails the snapshot locally, which is also expected. \r\n\r\n> [2018-01-10T05:23:40,408][WARN ][o.e.s.SnapshotsService   ] [node_tm1] [test-snap-2/vyIDc0GFSIKVtGd--HP_hQ] failed to create snapshot\r\n\r\nAnd the old master joins back the cluster...\r\n\r\n> [2018-01-10T05:23:45,588][INFO ][o.e.c.s.MasterService    ] [node_tm0] zen-disco-node-join[{node_tm1}\r\n\r\n... and this is where things get blurred for me. \r\n\r\nIt seems that `node_tm1` cannot ping the other nodes, and the ESSelector loop is closed:\r\n\r\n> [2018-01-10T05:23:45,792][WARN ][o.e.t.n.MockNioTransport ] [node_tm1] send message failed [channel: NioSocketChannel{localAddress=/127.0.0.1:30101, remoteAddress=/127.0.0.1:49349}]\r\n  1> java.nio.channels.ClosedChannelException: null\r\n  1> \tat org.elasticsearch.nio.NioSocketChannel.closeFromSelector(NioSocketChannel.java:53) ~[elasticsearch-nio-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n  1> \tat org.elasticsearch.nio.EventHandler.handleClose(EventHandler.java:72) ~[elasticsearch-nio-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n  1> \tat org.elasticsearch.nio.ESSelector.closePendingChannels(ESSelector.java:239) ~[elasticsearch-nio-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n  1> \tat org.elasticsearch.nio.ESSelector.cleanupAndCloseChannels(ESSelector.java:135) ~[elasticsearch-nio-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n  1> \tat org.elasticsearch.nio.ESSelector.runLoop(ESSelector.java:81) ~[elasticsearch-nio-7.0.0-alpha1-SNAPSHOT.jar:7.0.0-alpha1-SNAPSHOT]\r\n  1> \tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_92]\r\n\r\nThen all connection attempts from node_tm1 failed, so the cluster cannot recover to 4 nodes and the test suite times out. I think that all errors after that are caused by the test framework trying to stop the nodes.\r\n\r\nBut I'm worried about is the StackOverflowError in logs:\r\n\r\n> jan 10, 2018 5:23:45 AM com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException\r\n> WARNING: Uncaught exception in thread: Thread[elasticsearch[node_tm1][es_nio_transport_worker][T#2],5,TGRP-SnapshotDisruptionIT]\r\n> java.lang.StackOverflowError\r\n> \tat __randomizedtesting.SeedInfo.seed([F1A2A7B82C4D4B21]:0)\r\n> \tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$1.onResponse(TransportMasterNodeAction.java:167)\r\n> \tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$1.onResponse(TransportMasterNodeAction.java:164)\r\n> \tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$1.onResponse(TransportMasterNodeAction.java:167)\r\n> \tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$1.onResponse(TransportMasterNodeAction.java:164)\r\n> \tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$1.onResponse(TransportMasterNodeAction.java:167)\r\n> \tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$1.onResponse(TransportMasterNodeAction.java:164)\r\n> \tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$1.onResponse(TransportMasterNodeAction.java:167)\r\n \r\nAs well as the NPEs at the beginning of the tests:\r\n\r\n>  [2018-01-10T05:23:03,033][WARN ][o.e.d.z.UnicastZenPing   ] [node_tm0] unexpected error while pinging\r\n> java.lang.NullPointerException: null\r\n> \tat java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320) ~[?:1.8.0_92]\r\n> \tat java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ~[?:1.8.0_92]\r\n> \tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) ~[?:1.8.0_92]\r\n> \tat java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742) ~[?:1.8.0_92]\r\n> \tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_92]\r\n> \tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_92]\r\n> \tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[?:1.8.0_92]\r\n> \tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_92]\r\n> \tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_92]\r\n> \tat org.elasticsearch.discovery.zen.UnicastZenPing.sendPings(UnicastZenPing.java:475) ~[main/:?]\r\n> \tat org.elasticsearch.discovery.zen.UnicastZenPing$1.doRun(UnicastZenPing.java:332) [main/:?]\r\n> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:635) [main/:?]\r\n> \tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [main/:?]\r\n> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_92]\r\n> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_92]\r\n> \tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_92]\r\n\r\n\r\n@tbrooks8 @bleskes I have some trouble digging more this failure, so I'd be happy to have your opinion / gut feeling on this. Just an overlook would be helpfull at this stage.","closed_by":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"performed_via_github_app":null}