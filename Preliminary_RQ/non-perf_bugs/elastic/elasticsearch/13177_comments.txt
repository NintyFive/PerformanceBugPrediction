[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/135745656","html_url":"https://github.com/elastic/elasticsearch/issues/13177#issuecomment-135745656","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13177","id":135745656,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNTc0NTY1Ng==","user":{"login":"gingerwizard","id":12695796,"node_id":"MDQ6VXNlcjEyNjk1Nzk2","avatar_url":"https://avatars0.githubusercontent.com/u/12695796?v=4","gravatar_id":"","url":"https://api.github.com/users/gingerwizard","html_url":"https://github.com/gingerwizard","followers_url":"https://api.github.com/users/gingerwizard/followers","following_url":"https://api.github.com/users/gingerwizard/following{/other_user}","gists_url":"https://api.github.com/users/gingerwizard/gists{/gist_id}","starred_url":"https://api.github.com/users/gingerwizard/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gingerwizard/subscriptions","organizations_url":"https://api.github.com/users/gingerwizard/orgs","repos_url":"https://api.github.com/users/gingerwizard/repos","events_url":"https://api.github.com/users/gingerwizard/events{/privacy}","received_events_url":"https://api.github.com/users/gingerwizard/received_events","type":"User","site_admin":false},"created_at":"2015-08-28T11:35:12Z","updated_at":"2015-08-28T11:35:12Z","author_association":"NONE","body":"@martijnvg suggested this was worth consideration\n@polyfractal may have some interesting thoughts re this especially regards the relationship between latency and throughput.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/135795198","html_url":"https://github.com/elastic/elasticsearch/issues/13177#issuecomment-135795198","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13177","id":135795198,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNTc5NTE5OA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-08-28T14:48:23Z","updated_at":"2015-08-28T14:48:23Z","author_association":"CONTRIBUTOR","body":"Not sure this would be effective in production scenarios.  Typically, you'll be doing multiple mpercolate requests.  Each mpercolate request would end up on a single shard, but because you're running multiple requests, you already have parallelization (without the added cost of fanning out). If all you're doing is one mpercolate request, then probably latency isn't as important.\n\nIt's the same as running a search on a single shard: it has to hit every segment in the shard, which it does serially.  You could search the segments in parallel which could speed up the results if you only ever run one query at a time, but in practice you run lots of queries simultaneously.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/135797901","html_url":"https://github.com/elastic/elasticsearch/issues/13177#issuecomment-135797901","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13177","id":135797901,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNTc5NzkwMQ==","user":{"login":"gingerwizard","id":12695796,"node_id":"MDQ6VXNlcjEyNjk1Nzk2","avatar_url":"https://avatars0.githubusercontent.com/u/12695796?v=4","gravatar_id":"","url":"https://api.github.com/users/gingerwizard","html_url":"https://github.com/gingerwizard","followers_url":"https://api.github.com/users/gingerwizard/followers","following_url":"https://api.github.com/users/gingerwizard/following{/other_user}","gists_url":"https://api.github.com/users/gingerwizard/gists{/gist_id}","starred_url":"https://api.github.com/users/gingerwizard/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gingerwizard/subscriptions","organizations_url":"https://api.github.com/users/gingerwizard/orgs","repos_url":"https://api.github.com/users/gingerwizard/repos","events_url":"https://api.github.com/users/gingerwizard/events{/privacy}","received_events_url":"https://api.github.com/users/gingerwizard/received_events","type":"User","site_admin":false},"created_at":"2015-08-28T14:59:39Z","updated_at":"2015-08-28T14:59:39Z","author_association":"NONE","body":"MPercolate currently just acts a network optimisation.  The most common application seems to be to buffer N documents and send them off for percolation in one go.  This is particularly applicable when users are percolating prior to indexing.  Its unlikely they are running multiple percolate requests in parallel therefore, and hence not utilising the full capacity of their cluster with respect to CPU. \n\nIf the advice is simply to send N smaller mpercolate requests in parallel i'm fine with that if we think it will achieve the same effect. I imagine you would need to set N to the number of replicas you have.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/135877159","html_url":"https://github.com/elastic/elasticsearch/issues/13177#issuecomment-135877159","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13177","id":135877159,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNTg3NzE1OQ==","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2015-08-28T20:12:38Z","updated_at":"2015-08-28T20:12:38Z","author_association":"MEMBER","body":"I think adding this optimisation at some point does make sense (not high priority). Instead of parallelising   the mpercolate request up to `primary_shard` times, a mpercolate request can be parallelised `primary_shard * num_replica` times. Multiple mpercolate requests can be send at the same time, but this is something that we can solve nicely on the ES side and then clients don't have to worry about dividing their mpercolate requests evenly.  \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/229910273","html_url":"https://github.com/elastic/elasticsearch/issues/13177#issuecomment-229910273","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13177","id":229910273,"node_id":"MDEyOklzc3VlQ29tbWVudDIyOTkxMDI3Mw==","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2016-07-01T10:08:45Z","updated_at":"2016-07-01T10:08:45Z","author_association":"MEMBER","body":"Closing this issue as the mpercolate api in 5.0 is deprecated and redirects to msearch api. The msearch api does utilise shard copies better than mpercolate api did.\n","performed_via_github_app":null}]