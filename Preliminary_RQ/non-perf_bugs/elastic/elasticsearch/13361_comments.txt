[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/137971896","html_url":"https://github.com/elastic/elasticsearch/issues/13361#issuecomment-137971896","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13361","id":137971896,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNzk3MTg5Ng==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-09-05T16:11:36Z","updated_at":"2015-09-05T16:11:36Z","author_association":"MEMBER","body":"> I had a look at the pending tasks and I can see that it is mainly this tasks that get buffered:\n\nThis is the task that gets executed when deleting shard copies that are no longer assigned to the node. Can you check what the currently executing task is? it's marked with `executing: true`. \n\nAlso - what is the max time in queue? are thy all low-ish (10m)?\n\n> In the meantime I get the error about a rejected execution due to the queue capacity:\n\nThe task queue is unbound. What operations due you get this error from? by the queue capacity , I would guess that these are indexing requests.\n\nAnything else of interest in the node logs?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/137971904","html_url":"https://github.com/elastic/elasticsearch/issues/13361#issuecomment-137971904","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13361","id":137971904,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNzk3MTkwNA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-09-05T16:11:51Z","updated_at":"2015-09-05T16:11:51Z","author_association":"MEMBER","body":"and of course, what version of ES are you using?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/137972990","html_url":"https://github.com/elastic/elasticsearch/issues/13361#issuecomment-137972990","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13361","id":137972990,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNzk3Mjk5MA==","user":{"login":"Unitech","id":757747,"node_id":"MDQ6VXNlcjc1Nzc0Nw==","avatar_url":"https://avatars0.githubusercontent.com/u/757747?v=4","gravatar_id":"","url":"https://api.github.com/users/Unitech","html_url":"https://github.com/Unitech","followers_url":"https://api.github.com/users/Unitech/followers","following_url":"https://api.github.com/users/Unitech/following{/other_user}","gists_url":"https://api.github.com/users/Unitech/gists{/gist_id}","starred_url":"https://api.github.com/users/Unitech/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Unitech/subscriptions","organizations_url":"https://api.github.com/users/Unitech/orgs","repos_url":"https://api.github.com/users/Unitech/repos","events_url":"https://api.github.com/users/Unitech/events{/privacy}","received_events_url":"https://api.github.com/users/Unitech/received_events","type":"User","site_admin":false},"created_at":"2015-09-05T16:35:23Z","updated_at":"2015-09-05T16:35:23Z","author_association":"NONE","body":"After that all shards got assigned, 10minutes after the `number_of_pending_tasks` went from 2 millions to around 50k automatically :)\n\nSometimes using ES without deep knowledge about it can be quite stressful in case of outage, but it so performant!\n\nI'm using elasticsearch 1.6,\n\nI think I can close the issue, thanks for your help\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/137980762","html_url":"https://github.com/elastic/elasticsearch/issues/13361#issuecomment-137980762","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13361","id":137980762,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNzk4MDc2Mg==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-09-05T17:41:30Z","updated_at":"2015-09-05T17:41:30Z","author_association":"MEMBER","body":"thanks for coming back with the info. If you don't mind I would like to know more- what you describe shouldn't happen under normal circumstances. How many shards & nodes do you have in your cluster? Do you run on local disks or shared file system? I also understand you don't have dedicated master nodes at the moment. Is that correct?\n\nOn Sat, Sep 5, 2015 at 6:35 PM, Alexandre Strzelewicz\nnotifications@github.com wrote:\n\n> After that all shards got assigned, 10minutes after the `number_of_pending_tasks` went from 2 millions to around 50k automatically :)\n> Sometimes using ES without deep knowledge about it can be quite stressful in case of outage, but it so performant!\n> I'm using elasticsearch 1.6,\n> \n> ## I think I can close the issue, thanks for your help\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/elastic/elasticsearch/issues/13361#issuecomment-137972990\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/138258890","html_url":"https://github.com/elastic/elasticsearch/issues/13361#issuecomment-138258890","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13361","id":138258890,"node_id":"MDEyOklzc3VlQ29tbWVudDEzODI1ODg5MA==","user":{"login":"iravid","id":283332,"node_id":"MDQ6VXNlcjI4MzMzMg==","avatar_url":"https://avatars2.githubusercontent.com/u/283332?v=4","gravatar_id":"","url":"https://api.github.com/users/iravid","html_url":"https://github.com/iravid","followers_url":"https://api.github.com/users/iravid/followers","following_url":"https://api.github.com/users/iravid/following{/other_user}","gists_url":"https://api.github.com/users/iravid/gists{/gist_id}","starred_url":"https://api.github.com/users/iravid/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iravid/subscriptions","organizations_url":"https://api.github.com/users/iravid/orgs","repos_url":"https://api.github.com/users/iravid/repos","events_url":"https://api.github.com/users/iravid/events{/privacy}","received_events_url":"https://api.github.com/users/iravid/received_events","type":"User","site_admin":false},"created_at":"2015-09-07T10:09:03Z","updated_at":"2015-09-07T10:10:12Z","author_association":"NONE","body":"Hi @bleskes, I'd like to chime in since I'm seeing the same issue. I'm running a cluster of 5 nodes on AWS, with around 28k shards. We're running on EBS volumes. ES version is 1.5.2.\n\nThe issue started after adding the 5th node (due to the shard rebalancing, I assume). During debugging, I noticed that pending tasks rises indefinitely; counting the tasks by type using _cat/pending_tasks resulted in a large number of indices_store tasks. When setting cluster.routing.allocation.cluster_concurrent_rebalance to 0, the tasks quickly disappear (as @Unitech mentioned).\n\nWhile the rebalance is enabled, the master node's heap usage rises to around 90-95% (causing big GC cycles). When the rebalance is disabled as mentioned above, heap usage drops sharply.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/138326015","html_url":"https://github.com/elastic/elasticsearch/issues/13361#issuecomment-138326015","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13361","id":138326015,"node_id":"MDEyOklzc3VlQ29tbWVudDEzODMyNjAxNQ==","user":{"login":"Unitech","id":757747,"node_id":"MDQ6VXNlcjc1Nzc0Nw==","avatar_url":"https://avatars0.githubusercontent.com/u/757747?v=4","gravatar_id":"","url":"https://api.github.com/users/Unitech","html_url":"https://github.com/Unitech","followers_url":"https://api.github.com/users/Unitech/followers","following_url":"https://api.github.com/users/Unitech/following{/other_user}","gists_url":"https://api.github.com/users/Unitech/gists{/gist_id}","starred_url":"https://api.github.com/users/Unitech/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Unitech/subscriptions","organizations_url":"https://api.github.com/users/Unitech/orgs","repos_url":"https://api.github.com/users/Unitech/repos","events_url":"https://api.github.com/users/Unitech/events{/privacy}","received_events_url":"https://api.github.com/users/Unitech/received_events","type":"User","site_admin":false},"created_at":"2015-09-07T15:32:29Z","updated_at":"2015-09-07T15:38:23Z","author_association":"NONE","body":"> what you describe shouldn't happen under normal circumstances. \n\nMy setup is one master (with data) and 2 slaves. I've approximately 24k shards.\n\nThe ES master was not responding to some queries (/_cat/indices for example). So I stopped the 2 slaves and then I restarted the master. I put back online the 2 slaves and I waited about 2hours but the shard assignement was taking too much time, so I tuned some recovery configuration options and it was faster.\n\nI know I made some mistakes. I then followed this tutorial [Full Cluster restart upgrade](https://www.elastic.co/guide/en/elasticsearch/reference/master/restart-upgrade.html#restart-upgrade)\n\n> How many shards & nodes do you have in your cluster? \n\n25k\n\n> Do you run on local disks or shared file system?\n\nLocal disk on bare metal servers (SSD)\n\n> I also understand you don't have dedicated master nodes at the moment. Is that correct?\n\nYes I have one dedicated master\n\nHere is my configuration file:\nhttps://gist.github.com/Unitech/9faa8d7e943e02b76895\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/138461102","html_url":"https://github.com/elastic/elasticsearch/issues/13361#issuecomment-138461102","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/13361","id":138461102,"node_id":"MDEyOklzc3VlQ29tbWVudDEzODQ2MTEwMg==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-09-08T07:31:51Z","updated_at":"2015-09-08T07:31:51Z","author_association":"MEMBER","body":"@iravid those task are expected. They are the last protection in a set of checks we do before deleting data from disk. More specifically, when a shard is moved from node A to B, node A will not delete it's copy until it get's a confirmation from B and all other nodes that have a copy of the shard (typically, 1 node if you have 1 replica) that they shard is active is OK. I can see how in the case of 25K shards ( a lot for such a small cluster, you should look at reducing this number for other reasons too- it's typically inefficient) these can accumulate when a lot of other things happen (they don't have high priority) but they are really light and the node should process them super quickly.\n\n@Unitech I see thx. there are a couple of issues in your config file (though I don't see yet how it explains what you were seeing).:\n\n1) setting node.master: true , doesn't make a node a dedicated master node. To do so you also need to make the node not hold data by setting node.data: false\n2) recovery settings (gateway.recover_after_nodes, gateway.expected_nodes) suggest you expect 1 node, but I think it should be two (or three, depending whether you want data on your master).\n3) discovery.zen.ping.timeout: 30s  - why is this so high?\n4) discovery.zen.minimum_master_nodes: 1 - this is the default and can be removed (see point about running with one master later on)\n5) Unless you had a very reason (which I would be curious to hear), I would remove all the thread pool settings. The default should be good. Also index.store.type: mmapfs can be removed (we have a smart default of using this where it helps).\n\nLast - are you sure you want to run with a single dedicated master? if so, when the master dies, the cluster will not except writes until a new master is brought back to life.\n","performed_via_github_app":null}]