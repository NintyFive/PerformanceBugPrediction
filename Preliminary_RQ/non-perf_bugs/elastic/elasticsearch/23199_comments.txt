[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/280254284","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-280254284","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":280254284,"node_id":"MDEyOklzc3VlQ29tbWVudDI4MDI1NDI4NA==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2017-02-16T07:32:58Z","updated_at":"2017-02-16T07:32:58Z","author_association":"CONTRIBUTOR","body":"I was able to get my cluster working again by manually rerouting that shard:\r\n\r\n```\r\ncurl -XPOST 'localhost:9200/_cluster/reroute?pretty' -d '{\r\n    \"commands\" : [ {\r\n        \"allocate_stale_primary\" :\r\n            {\r\n              \"index\" : \"da-prod8-other\", \"shard\" : 3,\r\n              \"node\" : \"node-2-data-pod\",\r\n              \"accept_data_loss\" : true\r\n            }\r\n        }\r\n    ]\r\n}'\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/280272888","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-280272888","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":280272888,"node_id":"MDEyOklzc3VlQ29tbWVudDI4MDI3Mjg4OA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-02-16T09:01:27Z","updated_at":"2017-02-16T13:09:42Z","author_association":"CONTRIBUTOR","body":"It looks like there was a primary with no replicas allocated to the node when it got disconnected from the master. When rejoining the cluster, the locally allocated shard copy on the node was not able to free previously used resources in the time where the master had already made 5 attempts to unsuccessfully allocate the shard to the node again.\r\n\r\nAfter 5 unsuccessful allocation attempts, the master gives up and needs manual triggering to give it another allocation attempt (see https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html#_retry_failed_shards). By the time you ran the command above, the shard was finished freeing resources. It would have been sufficient though to just run\r\n\r\n```\r\ncurl -XPOST 'localhost:9200/_cluster/reroute?retry_failed\r\n```\r\n\r\nwhich is a much safer command. Data loss did not occur in this specific case where you used the `allocate_stale_primary` command, but using it in an incorrect situation can easily do so.\r\n\r\nNote that it's always good to first run the allocation explain API (`_cluster/allocation/explain`) in case where the cluster is red - it would also have provided the `retry_failed` suggestion.\r\n\r\nNote: I made a short edit to the explanation in the first paragraph (I initially misread the log output above thinking that the primary was relocating).\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/280329767","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-280329767","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":280329767,"node_id":"MDEyOklzc3VlQ29tbWVudDI4MDMyOTc2Nw==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-02-16T13:26:54Z","updated_at":"2017-02-16T13:26:54Z","author_association":"CONTRIBUTOR","body":"Also, do you by chance know whether there were long running search/scroll requests or snapshots running at the time of the failure? This could explain why the shard was not able to free its existing resources within a minute.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/280386391","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-280386391","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":280386391,"node_id":"MDEyOklzc3VlQ29tbWVudDI4MDM4NjM5MQ==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2017-02-16T16:44:35Z","updated_at":"2017-02-16T16:44:35Z","author_association":"CONTRIBUTOR","body":"@ywelsch Thank you for the analysis and the workaround if it occurs again. Any suggestion on how to prevent this from occurring in the first place (e.g., increasing timeouts)? \r\n\r\nI don't think there was a long running search at the time. The index that went down is not small (~130GB, 6 shards, 429M docs) but the queries to it are generally straightforward. I have a few other indexes on the same cluster that take much more complex queries (e.g., multiple nested aggregations). Most are relatively fast (5-20S), but one in particular can take over a minute to run... that said, I don't think it was run at the time.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/280389383","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-280389383","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":280389383,"node_id":"MDEyOklzc3VlQ29tbWVudDI4MDM4OTM4Mw==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2017-02-16T16:54:02Z","updated_at":"2017-03-15T15:30:12Z","author_association":"CONTRIBUTOR","body":"I'm going to try the following, which will hopefully reduce the likelihood of this error:\r\n```\r\ncurl -XPUT localhost:9200/da-prod8-*/_settings?pretty -d '{\r\n  \"index.allocation.max_retries\" : 10\r\n}'\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/280390691","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-280390691","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":280390691,"node_id":"MDEyOklzc3VlQ29tbWVudDI4MDM5MDY5MQ==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2017-02-16T16:58:12Z","updated_at":"2017-02-16T16:58:12Z","author_association":"CONTRIBUTOR","body":"Also, a small suggestion based on your comments: Perhaps the documentation for [retrying failed shards should](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html#_retry_failed_shards) be moved above the manual override section. Or at least, there should be a warning suggesting that the user tries the `retry_failed` option before manual override. \r\n\r\nHad I known about the `retry_failed` option, it would have been far less panic-inducing. I was reading that documentation page, but for some reason I didn't see it, and focused in on the manual shard allocation section instead.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/284666226","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-284666226","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":284666226,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NDY2NjIyNg==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2017-03-07T09:21:33Z","updated_at":"2017-03-07T09:21:33Z","author_association":"CONTRIBUTOR","body":"I just hit this error again, below is the output of `_cluster/allocation/explain?pretty`. It looks like I'm getting a `ShardLockObtainFailedException` error.\r\n\r\n\r\n```\r\n{\r\n  \"index\" : \"da-prod8-tmark\",\r\n  \"shard\" : 5,\r\n  \"primary\" : true,\r\n  \"current_state\" : \"unassigned\",\r\n  \"unassigned_info\" : {\r\n    \"reason\" : \"ALLOCATION_FAILED\",\r\n    \"at\" : \"2017-03-04T04:59:38.099Z\",\r\n    \"failed_allocation_attempts\" : 5,\r\n    \"details\" : \"failed to create shard, failure IOException[failed to obtain in-memory shard lock]; nested: ShardLockObtainFailedException[[da-prod8-tmark][5]: obtaining shard lock timed out after 5000ms]; \",\r\n    \"last_allocation_status\" : \"no\"\r\n  },\r\n  \"can_allocate\" : \"no\",\r\n  \"allocate_explanation\" : \"cannot allocate because allocation is not permitted to any of the nodes that hold an in-sync shard copy\",\r\n  \"node_allocation_decisions\" : [\r\n    {\r\n      \"node_id\" : \"uviqqBXkR9a63SRtoW28Wg\",\r\n      \"node_name\" : \"node-1-data-pod\",\r\n      \"transport_address\" : \"10.0.25.4:9300\",\r\n      \"node_decision\" : \"no\",\r\n      \"store\" : {\r\n        \"in_sync\" : true,\r\n        \"allocation_id\" : \"OjjjGv5ZRIKPA1r3TRn3Cg\",\r\n        \"store_exception\" : {\r\n          \"type\" : \"shard_lock_obtain_failed_exception\",\r\n          \"reason\" : \"[da-prod8-tmark][5]: obtaining shard lock timed out after 5000ms\",\r\n          \"index_uuid\" : \"vS61_OAERrq0qdD7LqEuLA\",\r\n          \"shard\" : \"5\",\r\n          \"index\" : \"da-prod8-tmark\"\r\n        }\r\n      },\r\n      \"deciders\" : [\r\n        {\r\n          \"decider\" : \"max_retry\",\r\n          \"decision\" : \"NO\",\r\n          \"explanation\" : \"shard has exceeded the maximum number of retries [5] on failed allocation attempts - manually call [/_cluster/reroute?retry_failed=true] to retry, [unassigned_info[[reason=ALLOCATION_FAILED], at[2017-03-04T04:59:38.099Z], failed_attempts[5], delayed=false, details[failed to create shard, failure IOException[failed to obtain in-memory shard lock]; nested: ShardLockObtainFailedException[[da-prod8-tmark][5]: obtaining shard lock timed out after 5000ms]; ], allocation_status[deciders_no]]]\"\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n}\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/284791735","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-284791735","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":284791735,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NDc5MTczNQ==","user":{"login":"abeyad","id":1631297,"node_id":"MDQ6VXNlcjE2MzEyOTc=","avatar_url":"https://avatars2.githubusercontent.com/u/1631297?v=4","gravatar_id":"","url":"https://api.github.com/users/abeyad","html_url":"https://github.com/abeyad","followers_url":"https://api.github.com/users/abeyad/followers","following_url":"https://api.github.com/users/abeyad/following{/other_user}","gists_url":"https://api.github.com/users/abeyad/gists{/gist_id}","starred_url":"https://api.github.com/users/abeyad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abeyad/subscriptions","organizations_url":"https://api.github.com/users/abeyad/orgs","repos_url":"https://api.github.com/users/abeyad/repos","events_url":"https://api.github.com/users/abeyad/events{/privacy}","received_events_url":"https://api.github.com/users/abeyad/received_events","type":"User","site_admin":false},"created_at":"2017-03-07T17:16:39Z","updated_at":"2017-03-07T17:16:39Z","author_association":"CONTRIBUTOR","body":"@speedplane it looks like the same issue as before, where resources on the shard were not cleared in time for the allocation to take place on the node after it rejoined the cluster.  It can be remedied with the same command which @ywelsch gave earlier:\r\n```\r\ncurl -XPOST 'localhost:9200/_cluster/reroute?retry_failed\r\n```\r\n\r\nBut the larger issue to address is why you repeatedly run into the `ShardLockObtainFailedException`.  It likely signals some longer running job on the shard that failed to abort when the node was temporarily removed from the cluster.  Are you experiencing long GC cycles on the node?  Or is it a network issue?  \r\n\r\nIt would be helpful to know what was happening on that node prior to this issue manifesting.  Are there logs of that node that you can share with us?  If so, you can email it to my_first_name @ elastic dot co\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/285339225","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-285339225","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":285339225,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NTMzOTIyNQ==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2017-03-09T12:32:08Z","updated_at":"2017-03-09T12:32:08Z","author_association":"CONTRIBUTOR","body":"Just happened again, this time within just a few days of the last failure. Tests caught it in production during business hours, so I had to just fix it. Right now (due to completely separate issues) I can't SSH into the servers (ridiculous I know), so I can't grab the logs. I suspect this will happen again, and I'll post whatever logs I can get, but if there are any work-arounds that you can think of, I'm all ears.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/286542048","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-286542048","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":286542048,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NjU0MjA0OA==","user":{"login":"tmegow","id":10726968,"node_id":"MDQ6VXNlcjEwNzI2OTY4","avatar_url":"https://avatars0.githubusercontent.com/u/10726968?v=4","gravatar_id":"","url":"https://api.github.com/users/tmegow","html_url":"https://github.com/tmegow","followers_url":"https://api.github.com/users/tmegow/followers","following_url":"https://api.github.com/users/tmegow/following{/other_user}","gists_url":"https://api.github.com/users/tmegow/gists{/gist_id}","starred_url":"https://api.github.com/users/tmegow/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tmegow/subscriptions","organizations_url":"https://api.github.com/users/tmegow/orgs","repos_url":"https://api.github.com/users/tmegow/repos","events_url":"https://api.github.com/users/tmegow/events{/privacy}","received_events_url":"https://api.github.com/users/tmegow/received_events","type":"User","site_admin":false},"created_at":"2017-03-14T20:01:01Z","updated_at":"2017-03-14T20:01:01Z","author_association":"NONE","body":"@abeyad I've also been struggling against ShardLockObtainFailedException occurring during times of heavy bulk indexing activity. The symptoms are: a data node (2.3) becoming unresponsive, GC times spike, and the affected data node is removed from the cluster. Immediately after rejoining the cluster (this is a kubernetes environment) the node receives ShardLockObtainFailedException repeatedly, and ES performance suffers. So far, I've been forced to reduce/re-raise the number of replicas and let the shards sync from primary again. \r\nDo you have any insight into how I can avoid this symptom?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/286777036","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-286777036","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":286777036,"node_id":"MDEyOklzc3VlQ29tbWVudDI4Njc3NzAzNg==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2017-03-15T15:24:51Z","updated_at":"2017-03-15T15:24:51Z","author_association":"CONTRIBUTOR","body":"Just happened again, this time I was able to collect some logs. It looks similar to the situation described by @tmegow, there's an issue during bulk indexing, and then it drops out.\r\n\r\n```\r\n[2017-03-15T07:24:54,984][INFO ][o.e.c.s.ClusterService   ] [node-4-data-pod] removed {{node-3-data-pod}{kzr2o00tSzyuY-ekWuiNng}{x3RMwZicQ46ljZS-muWy-g}{10.0.32.6}{10.0.32.6:9300},}, reason: zen-disco-receive(from master [master {es-master-714112077-ae5jq}{KfcqAA57R02arAOj1kshuw}{8YGdiFciTWeiidJXI4uh3A}{10.0.3.58}{10.0.3.58:9300} committed version [2098]])\r\n[2017-03-15T07:24:55,032][WARN ][o.e.a.b.TransportShardBulkAction] [node-4-data-pod] [[da-prod8-other][1]] failed to perform indices:data/write/bulk[s] on replica [da-prod8-other][1], node[kzr2o00tSzyuY-ekWuiNng], [R], s[STARTED], a[id=wXUXFzduRYifGmjfzR3dEw]\r\norg.elasticsearch.transport.NodeDisconnectedException: [node-3-data-pod][10.0.32.6:9300][indices:data/write/bulk[s][r]] disconnected\r\n[2017-03-15T07:24:55,032][WARN ][o.e.a.b.TransportShardBulkAction] [node-4-data-pod] [[da-prod8-other][1]] failed to perform indices:data/write/bulk[s] on replica [da-prod8-other][1], node[kzr2o00tSzyuY-ekWuiNng], [R], s[STARTED], a[id=wXUXFzduRYifGmjfzR3dEw]\r\norg.elasticsearch.transport.NodeDisconnectedException: [node-3-data-pod][10.0.32.6:9300][indices:data/write/bulk[s][r]] disconnected\r\n[2017-03-15T07:25:07,330][INFO ][o.e.c.s.ClusterService   ] [node-4-data-pod] added {{node-3-data-pod}{kzr2o00tSzyuY-ekWuiNng}{x3RMwZicQ46ljZS-muWy-g}{10.0.32.6}{10.0.32.6:9300},}, reason: zen-disco-receive(from master [master {es-master-714112077-ae5jq}{KfcqAA57R02arAOj1kshuw}{8YGdiFciTWeiidJXI4uh3A}{10.0.3.58}{10.0.3.58:9300} committed version [2102]])\r\n[2017-03-15T13:41:28,863][INFO ][o.e.c.s.ClusterService   ] [node-4-data-pod] removed {{node-5-data-pod}{0aaunpa-Qkab66Ti5mFoTw}{BC27Z_hiRcGgaDQcmHgEaA}{10.0.37.3}{10.0.37.3:9300},}, reason: zen-disco-receive(from master [master {es-master-714112077-ae5jq}{KfcqAA57R02arAOj1kshuw}{8YGdiFciTWeiidJXI4uh3A}{10.0.3.58}{10.0.3.58:9300} committed version [2115]])\r\n[2017-03-15T13:41:55,099][INFO ][o.e.c.s.ClusterService   ] [node-4-data-pod] added {{node-5-data-pod}{0aaunpa-Qkab66Ti5mFoTw}{BC27Z_hiRcGgaDQcmHgEaA}{10.0.37.3}{10.0.37.3:9300},}, reason: zen-disco-receive(from master [master {es-master-714112077-ae5jq}{KfcqAA57R02arAOj1kshuw}{8YGdiFciTWeiidJXI4uh3A}{10.0.3.58}{10.0.3.58:9300} committed version [2118]])\r\n[2017-03-15T13:57:03,432][INFO ][o.e.c.s.ClusterService   ] [node-4-data-pod] removed {{node-0-data-pod}{uUZCt9RrS2aY_gqkSmNV5A}{SUYJlFEGQzyrF1tyocLj3w}{10.0.28.4}{10.0.28.4:9300},}, reason: zen-disco-receive(from master [master {es-master-714112077-ae5jq}{KfcqAA57R02arAOj1kshuw}{8YGdiFciTWeiidJXI4uh3A}{10.0.3.58}{10.0.3.58:9300} committed version [2140]])\r\n[2017-03-15T13:57:03,465][WARN ][o.e.a.b.TransportShardBulkAction] [node-4-data-pod] [[da-prod8-ptab][1]] failed to perform indices:data/write/bulk[s] on replica [da-prod8-ptab][1], node[uUZCt9RrS2aY_gqkSmNV5A], [R], s[STARTED], a[id=3QiLMQsdS1GnzumWOm2SFw]\r\norg.elasticsearch.transport.NodeDisconnectedException: [node-0-data-pod][10.0.28.4:9300][indices:data/write/bulk[s][r]] disconnected\r\n[2017-03-15T13:57:11,946][INFO ][o.e.c.s.ClusterService   ] [node-4-data-pod] added {{node-0-data-pod}{uUZCt9RrS2aY_gqkSmNV5A}{SUYJlFEGQzyrF1tyocLj3w}{10.0.28.4}{10.0.28.4:9300},}, reason: zen-disco-receive(from master [master {es-master-714112077-ae5jq}{KfcqAA57R02arAOj1kshuw}{8YGdiFciTWeiidJXI4uh3A}{10.0.3.58}{10.0.3.58:9300} committed version [2144]])\r\n[2017-03-15T13:58:39,531][INFO ][o.e.m.j.JvmGcMonitorService] [node-4-data-pod] [gc][2603293] overhead, spent [438ms] collecting in the last [1.3s]\r\n[2017-03-15T13:58:41,773][INFO ][o.e.m.j.JvmGcMonitorService] [node-4-data-pod] [gc][2603295] overhead, spent [608ms] collecting in the last [1.2s]\r\n[2017-03-15T13:58:47,774][INFO ][o.e.m.j.JvmGcMonitorService] [node-4-data-pod] [gc][2603301] overhead, spent [336ms] collecting in the last [1s]\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/287161303","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-287161303","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":287161303,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NzE2MTMwMw==","user":{"login":"abeyad","id":1631297,"node_id":"MDQ6VXNlcjE2MzEyOTc=","avatar_url":"https://avatars2.githubusercontent.com/u/1631297?v=4","gravatar_id":"","url":"https://api.github.com/users/abeyad","html_url":"https://github.com/abeyad","followers_url":"https://api.github.com/users/abeyad/followers","following_url":"https://api.github.com/users/abeyad/following{/other_user}","gists_url":"https://api.github.com/users/abeyad/gists{/gist_id}","starred_url":"https://api.github.com/users/abeyad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abeyad/subscriptions","organizations_url":"https://api.github.com/users/abeyad/orgs","repos_url":"https://api.github.com/users/abeyad/repos","events_url":"https://api.github.com/users/abeyad/events{/privacy}","received_events_url":"https://api.github.com/users/abeyad/received_events","type":"User","site_admin":false},"created_at":"2017-03-16T19:10:02Z","updated_at":"2017-03-16T19:10:02Z","author_association":"CONTRIBUTOR","body":"@speedplane @tmegow thank you for the extra insights; just to keep you posted, we are actively looking into this issue (its a tricky one) and have some plans around providing more insight into which types of threads are holding onto shard locks.  I'll keep this issue updated as we find out more.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/287163969","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-287163969","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":287163969,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NzE2Mzk2OQ==","user":{"login":"tmegow","id":10726968,"node_id":"MDQ6VXNlcjEwNzI2OTY4","avatar_url":"https://avatars0.githubusercontent.com/u/10726968?v=4","gravatar_id":"","url":"https://api.github.com/users/tmegow","html_url":"https://github.com/tmegow","followers_url":"https://api.github.com/users/tmegow/followers","following_url":"https://api.github.com/users/tmegow/following{/other_user}","gists_url":"https://api.github.com/users/tmegow/gists{/gist_id}","starred_url":"https://api.github.com/users/tmegow/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tmegow/subscriptions","organizations_url":"https://api.github.com/users/tmegow/orgs","repos_url":"https://api.github.com/users/tmegow/repos","events_url":"https://api.github.com/users/tmegow/events{/privacy}","received_events_url":"https://api.github.com/users/tmegow/received_events","type":"User","site_admin":false},"created_at":"2017-03-16T19:20:30Z","updated_at":"2017-03-16T19:20:30Z","author_association":"NONE","body":"Thank you for the update, @abeyad !\r\nA bit more info, I enabled the slowlog and it seems that indexing the newly added items is taking longer than I anticipated. My current workaround is to have my syncing app communicate with the node stats endpoint and wait to send more items until `.nodes.*.indices.merges` & `.nodes.*.jvm.gc` return to a baseline threshold.\r\n@speedplane What's your merge time and garbage collection rate look like right before you lose the data node? Mine was spiking to 10s of hours immediately prior to losing the data node.\r\n\r\nWould giving my data nodes more resources help here? Each of my data nodes currently has 2 vCPUs and 16GB of mem.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/287204221","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-287204221","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":287204221,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NzIwNDIyMQ==","user":{"login":"abeyad","id":1631297,"node_id":"MDQ6VXNlcjE2MzEyOTc=","avatar_url":"https://avatars2.githubusercontent.com/u/1631297?v=4","gravatar_id":"","url":"https://api.github.com/users/abeyad","html_url":"https://github.com/abeyad","followers_url":"https://api.github.com/users/abeyad/followers","following_url":"https://api.github.com/users/abeyad/following{/other_user}","gists_url":"https://api.github.com/users/abeyad/gists{/gist_id}","starred_url":"https://api.github.com/users/abeyad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abeyad/subscriptions","organizations_url":"https://api.github.com/users/abeyad/orgs","repos_url":"https://api.github.com/users/abeyad/repos","events_url":"https://api.github.com/users/abeyad/events{/privacy}","received_events_url":"https://api.github.com/users/abeyad/received_events","type":"User","site_admin":false},"created_at":"2017-03-16T22:00:44Z","updated_at":"2017-03-16T22:00:44Z","author_association":"CONTRIBUTOR","body":"> Would giving my data nodes more resources help here? Each of my data nodes currently has 2 vCPUs and 16GB of mem.\r\n\r\nGiving data nodes more resources can always help, preventing GC's and thereby potentially improving cluster stability.  The `ShardLockObtainFailedException` manifests as a result of cluster instabilities having nodes come and go quickly.  More resources on data nodes can, but is not guaranteed to, remedy such situations.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/287461669","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-287461669","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":287461669,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NzQ2MTY2OQ==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2017-03-17T20:23:46Z","updated_at":"2017-03-17T20:23:46Z","author_association":"CONTRIBUTOR","body":"@abeyad @tmegow My nodes are not under much stress. It's possible some bursty activity happens which causes this, but I feel like my servers are already underutilized, and would prefer to not increase them further.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/292080085","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-292080085","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":292080085,"node_id":"MDEyOklzc3VlQ29tbWVudDI5MjA4MDA4NQ==","user":{"login":"sikelong123","id":19286508,"node_id":"MDQ6VXNlcjE5Mjg2NTA4","avatar_url":"https://avatars2.githubusercontent.com/u/19286508?v=4","gravatar_id":"","url":"https://api.github.com/users/sikelong123","html_url":"https://github.com/sikelong123","followers_url":"https://api.github.com/users/sikelong123/followers","following_url":"https://api.github.com/users/sikelong123/following{/other_user}","gists_url":"https://api.github.com/users/sikelong123/gists{/gist_id}","starred_url":"https://api.github.com/users/sikelong123/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sikelong123/subscriptions","organizations_url":"https://api.github.com/users/sikelong123/orgs","repos_url":"https://api.github.com/users/sikelong123/repos","events_url":"https://api.github.com/users/sikelong123/events{/privacy}","received_events_url":"https://api.github.com/users/sikelong123/received_events","type":"User","site_admin":false},"created_at":"2017-04-06T06:22:11Z","updated_at":"2017-04-06T06:22:11Z","author_association":"NONE","body":"I also encountered the same problem, but my log is like this：\r\n\r\n> [2017-04-03T10:04:48,503][WARN ][o.e.i.IndexService       ] [mogu015052] [es_xp_item_mgj] [2] failed to close store on shard removal (reason: [initialization failed])\r\njava.lang.NullPointerException\r\n        at org.elasticsearch.index.IndexService.closeShard(IndexService.java:409) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.index.IndexService.createShard(IndexService.java:361) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:449) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:137) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.createShard(IndicesClusterStateService.java:534) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.createOrUpdateShards(IndicesClusterStateService.java:511) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:200) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:708) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:894) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:444) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_72]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_72]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_72]\r\n[2017-04-03T10:04:48,504][WARN ][o.e.i.c.IndicesClusterStateService] [mogu015052] [[es_xp_item_mgj][2]] marking and sending shard failed due to [failed to create shard]\r\njava.io.IOException: failed to obtain in-memory shard lock\r\n        at org.elasticsearch.index.IndexService.createShard(IndexService.java:355) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:449) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.indices.IndicesService.createShard(IndicesService.java:137) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.createShard(IndicesClusterStateService.java:534) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.createOrUpdateShards(IndicesClusterStateService.java:511) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:200) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:708) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:894) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:444) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200) [elasticsearch-5.0.0.jar:5.0.0]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_72]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_72]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_72]\r\nCaused by: org.elasticsearch.env.ShardLockObtainFailedException: [es_xp_item_mgj][2]: obtaining shard lock timed out after 5000ms\r\n        at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:711) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:630) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n        at org.elasticsearch.index.IndexService.createShard(IndexService.java:285) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n        ... 13 more\r\n\r\nThere is a null pointer exception，So far, I haven't find out why.This problem is also from disconnected to re-election, rejoin the cluster was wrong. @speedplane @abeyad \r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/292086927","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-292086927","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":292086927,"node_id":"MDEyOklzc3VlQ29tbWVudDI5MjA4NjkyNw==","user":{"login":"sikelong123","id":19286508,"node_id":"MDQ6VXNlcjE5Mjg2NTA4","avatar_url":"https://avatars2.githubusercontent.com/u/19286508?v=4","gravatar_id":"","url":"https://api.github.com/users/sikelong123","html_url":"https://github.com/sikelong123","followers_url":"https://api.github.com/users/sikelong123/followers","following_url":"https://api.github.com/users/sikelong123/following{/other_user}","gists_url":"https://api.github.com/users/sikelong123/gists{/gist_id}","starred_url":"https://api.github.com/users/sikelong123/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sikelong123/subscriptions","organizations_url":"https://api.github.com/users/sikelong123/orgs","repos_url":"https://api.github.com/users/sikelong123/repos","events_url":"https://api.github.com/users/sikelong123/events{/privacy}","received_events_url":"https://api.github.com/users/sikelong123/received_events","type":"User","site_admin":false},"created_at":"2017-04-06T07:02:03Z","updated_at":"2017-04-06T07:02:03Z","author_association":"NONE","body":"Did you solve the problem? @speedplane ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/293562780","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-293562780","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":293562780,"node_id":"MDEyOklzc3VlQ29tbWVudDI5MzU2Mjc4MA==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2017-04-12T12:37:18Z","updated_at":"2017-04-12T12:40:36Z","author_association":"CONTRIBUTOR","body":"@sikelong123 No I didn't, I just saw it pop up yesterday again. As a work-around, I added a cron job that calls `/_cluster/reroute?retry_failed` every few hours.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/294224791","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-294224791","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":294224791,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NDIyNDc5MQ==","user":{"login":"HenleyChiu","id":788695,"node_id":"MDQ6VXNlcjc4ODY5NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/788695?v=4","gravatar_id":"","url":"https://api.github.com/users/HenleyChiu","html_url":"https://github.com/HenleyChiu","followers_url":"https://api.github.com/users/HenleyChiu/followers","following_url":"https://api.github.com/users/HenleyChiu/following{/other_user}","gists_url":"https://api.github.com/users/HenleyChiu/gists{/gist_id}","starred_url":"https://api.github.com/users/HenleyChiu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HenleyChiu/subscriptions","organizations_url":"https://api.github.com/users/HenleyChiu/orgs","repos_url":"https://api.github.com/users/HenleyChiu/repos","events_url":"https://api.github.com/users/HenleyChiu/events{/privacy}","received_events_url":"https://api.github.com/users/HenleyChiu/received_events","type":"User","site_admin":false},"created_at":"2017-04-14T19:58:22Z","updated_at":"2017-04-14T19:58:22Z","author_association":"NONE","body":"@speedplane Are you running Kibana or X-Pack too by any chance? ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/294225858","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-294225858","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":294225858,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NDIyNTg1OA==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2017-04-14T20:04:37Z","updated_at":"2017-04-14T20:04:49Z","author_association":"CONTRIBUTOR","body":"@HenleyChiu Yup. Kibana 5.2 with monitoring enabled. Not the full X-Pack. Here's the relevant section from my `kibana.yaml` if it helps:\r\n\r\n```\r\nserver:\r\n  basePath: /kibana\r\n\r\nlogging:\r\n    dest: /data/log/kibana.log\r\n    silent: false\r\n    quiet: false\r\n    verbose: false\r\n\r\nxpack:\r\n  monitoring:\r\n    enabled: true\r\n  security:\r\n    enabled: false\r\n  graph:\r\n    enabled: false\r\n  reporting:\r\n    enabled: false\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/294226787","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-294226787","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":294226787,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NDIyNjc4Nw==","user":{"login":"HenleyChiu","id":788695,"node_id":"MDQ6VXNlcjc4ODY5NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/788695?v=4","gravatar_id":"","url":"https://api.github.com/users/HenleyChiu","html_url":"https://github.com/HenleyChiu","followers_url":"https://api.github.com/users/HenleyChiu/followers","following_url":"https://api.github.com/users/HenleyChiu/following{/other_user}","gists_url":"https://api.github.com/users/HenleyChiu/gists{/gist_id}","starred_url":"https://api.github.com/users/HenleyChiu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HenleyChiu/subscriptions","organizations_url":"https://api.github.com/users/HenleyChiu/orgs","repos_url":"https://api.github.com/users/HenleyChiu/repos","events_url":"https://api.github.com/users/HenleyChiu/events{/privacy}","received_events_url":"https://api.github.com/users/HenleyChiu/received_events","type":"User","site_admin":false},"created_at":"2017-04-14T20:09:08Z","updated_at":"2017-04-14T20:09:08Z","author_association":"NONE","body":"I saw your comment on issue 22551: https://github.com/elastic/elasticsearch/issues/22551\r\n\r\nThey seemed to have ignored the 2nd comment by the OP about the ShardLockException. They mentioned disabling monitoring fixes the IllegalStateException, but I wonder if disabling it also fixes the ShardLockException as well?\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/294262717","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-294262717","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":294262717,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NDI2MjcxNw==","user":{"login":"abeyad","id":1631297,"node_id":"MDQ6VXNlcjE2MzEyOTc=","avatar_url":"https://avatars2.githubusercontent.com/u/1631297?v=4","gravatar_id":"","url":"https://api.github.com/users/abeyad","html_url":"https://github.com/abeyad","followers_url":"https://api.github.com/users/abeyad/followers","following_url":"https://api.github.com/users/abeyad/following{/other_user}","gists_url":"https://api.github.com/users/abeyad/gists{/gist_id}","starred_url":"https://api.github.com/users/abeyad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abeyad/subscriptions","organizations_url":"https://api.github.com/users/abeyad/orgs","repos_url":"https://api.github.com/users/abeyad/repos","events_url":"https://api.github.com/users/abeyad/events{/privacy}","received_events_url":"https://api.github.com/users/abeyad/received_events","type":"User","site_admin":false},"created_at":"2017-04-15T01:05:21Z","updated_at":"2017-04-15T01:05:21Z","author_association":"CONTRIBUTOR","body":"@HenleyChiu If you are encountering the `ShardLockObtainException` frequently, it could be a cascading effect as a result of that bug.  The `ShardLockObtainException` itself can happen as a result of other causes too though, so its no guarantee, but given that #22551 causes frequent disconnects of the node, which can lead to `ShardLockObtainException` if the cluster is unstable, then its probably a good idea to disable stats collection.  Note that the fix (#22317) went into 5.2.0, and the version number on this issue from the OP is 5.2.0.  If you are running 5.1.1 or before, then yes, you should disable stats collection.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/304345903","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-304345903","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":304345903,"node_id":"MDEyOklzc3VlQ29tbWVudDMwNDM0NTkwMw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2017-05-26T17:43:45Z","updated_at":"2017-05-26T17:43:45Z","author_association":"CONTRIBUTOR","body":"/cc myself","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/336100278","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-336100278","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":336100278,"node_id":"MDEyOklzc3VlQ29tbWVudDMzNjEwMDI3OA==","user":{"login":"tomsommer","id":149171,"node_id":"MDQ6VXNlcjE0OTE3MQ==","avatar_url":"https://avatars2.githubusercontent.com/u/149171?v=4","gravatar_id":"","url":"https://api.github.com/users/tomsommer","html_url":"https://github.com/tomsommer","followers_url":"https://api.github.com/users/tomsommer/followers","following_url":"https://api.github.com/users/tomsommer/following{/other_user}","gists_url":"https://api.github.com/users/tomsommer/gists{/gist_id}","starred_url":"https://api.github.com/users/tomsommer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tomsommer/subscriptions","organizations_url":"https://api.github.com/users/tomsommer/orgs","repos_url":"https://api.github.com/users/tomsommer/repos","events_url":"https://api.github.com/users/tomsommer/events{/privacy}","received_events_url":"https://api.github.com/users/tomsommer/received_events","type":"User","site_admin":false},"created_at":"2017-10-12T11:30:59Z","updated_at":"2017-10-12T11:30:59Z","author_association":"NONE","body":"I had a similar problem, with 4 shards jumping around with the error:\r\n\r\n`from primary shard with sync id but number of docs differ: 3232205 (XXX, primary) vs 3232204(YYYY)\r\n`\r\nIt didn't increment the error counter on the index and it always tried to allocate to the same node that failed. \r\n\r\nSètting `cluster.routing.allocation.enable` to `none` and then to `all` fixed the problem, maybe it resets some internal logic?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/337798364","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-337798364","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":337798364,"node_id":"MDEyOklzc3VlQ29tbWVudDMzNzc5ODM2NA==","user":{"login":"adichad","id":383215,"node_id":"MDQ6VXNlcjM4MzIxNQ==","avatar_url":"https://avatars3.githubusercontent.com/u/383215?v=4","gravatar_id":"","url":"https://api.github.com/users/adichad","html_url":"https://github.com/adichad","followers_url":"https://api.github.com/users/adichad/followers","following_url":"https://api.github.com/users/adichad/following{/other_user}","gists_url":"https://api.github.com/users/adichad/gists{/gist_id}","starred_url":"https://api.github.com/users/adichad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/adichad/subscriptions","organizations_url":"https://api.github.com/users/adichad/orgs","repos_url":"https://api.github.com/users/adichad/repos","events_url":"https://api.github.com/users/adichad/events{/privacy}","received_events_url":"https://api.github.com/users/adichad/received_events","type":"User","site_admin":false},"created_at":"2017-10-19T04:47:22Z","updated_at":"2017-10-19T04:47:49Z","author_association":"NONE","body":"Also seen multiple times on multiple clusters running ES 5.5.1","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/351084112","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-351084112","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":351084112,"node_id":"MDEyOklzc3VlQ29tbWVudDM1MTA4NDExMg==","user":{"login":"mvar","id":1286752,"node_id":"MDQ6VXNlcjEyODY3NTI=","avatar_url":"https://avatars0.githubusercontent.com/u/1286752?v=4","gravatar_id":"","url":"https://api.github.com/users/mvar","html_url":"https://github.com/mvar","followers_url":"https://api.github.com/users/mvar/followers","following_url":"https://api.github.com/users/mvar/following{/other_user}","gists_url":"https://api.github.com/users/mvar/gists{/gist_id}","starred_url":"https://api.github.com/users/mvar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mvar/subscriptions","organizations_url":"https://api.github.com/users/mvar/orgs","repos_url":"https://api.github.com/users/mvar/repos","events_url":"https://api.github.com/users/mvar/events{/privacy}","received_events_url":"https://api.github.com/users/mvar/received_events","type":"User","site_admin":false},"created_at":"2017-12-12T15:24:40Z","updated_at":"2017-12-12T15:24:40Z","author_association":"NONE","body":"Experiencing similar issues on cluster running ES 5.6.3. Status stays yellow with single shard unassigned.\r\n\r\nHappens on cluster upscaling. Every second time or so.\r\n\r\nFew different errors today:\r\n\r\n> failed to create shard, failure IOException[failed to obtain in-memory shard lock]; nested: ShardLockObtainFailedException[[XXX][13]: obtaining shard lock timed out after 5000ms]; \r\n\r\n> failed recovery, failure RecoveryFailedException[[XXX][38]: Recovery failed from ... into ...]; nested: RemoteTransportException[ZZZ[internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [XXX][38] from primary shard with sync id but number of docs differ: 69887 (node1, primary) vs 69868(node2)]; \r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/361056964","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-361056964","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":361056964,"node_id":"MDEyOklzc3VlQ29tbWVudDM2MTA1Njk2NA==","user":{"login":"Klezer","id":4738611,"node_id":"MDQ6VXNlcjQ3Mzg2MTE=","avatar_url":"https://avatars3.githubusercontent.com/u/4738611?v=4","gravatar_id":"","url":"https://api.github.com/users/Klezer","html_url":"https://github.com/Klezer","followers_url":"https://api.github.com/users/Klezer/followers","following_url":"https://api.github.com/users/Klezer/following{/other_user}","gists_url":"https://api.github.com/users/Klezer/gists{/gist_id}","starred_url":"https://api.github.com/users/Klezer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Klezer/subscriptions","organizations_url":"https://api.github.com/users/Klezer/orgs","repos_url":"https://api.github.com/users/Klezer/repos","events_url":"https://api.github.com/users/Klezer/events{/privacy}","received_events_url":"https://api.github.com/users/Klezer/received_events","type":"User","site_admin":false},"created_at":"2018-01-28T11:49:40Z","updated_at":"2018-01-28T11:51:06Z","author_association":"NONE","body":"Also experiencing the same behavior (ShardLockObtainException when the cluster is undergoing heavy load - heavy GC -> shard allocation blocked). We are running ES on version 2.3.1. The only possible solution I read here is to disable the stats collection, right? any other ideas for a solution. Note that we could also upgrade to version 2.4, but could that make a difference in solving this issue?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374564144","html_url":"https://github.com/elastic/elasticsearch/issues/23199#issuecomment-374564144","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23199","id":374564144,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDU2NDE0NA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T11:28:46Z","updated_at":"2018-03-20T11:28:46Z","author_association":"MEMBER","body":"I have opened https://github.com/elastic/elasticsearch/pull/29160 containing the [suggested](https://github.com/elastic/elasticsearch/issues/23199#issuecomment-280390691) doc change. I'm going to close the issue as it have become a catch for people to note that run into the shard locking exception. This can be caused by many reasons, each of them, once sorted out, should be tracked in a dedicated issue.","performed_via_github_app":null}]