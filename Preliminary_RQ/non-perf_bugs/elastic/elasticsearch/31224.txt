{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/31224","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31224/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31224/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31224/events","html_url":"https://github.com/elastic/elasticsearch/issues/31224","id":330872176,"node_id":"MDU6SXNzdWUzMzA4NzIxNzY=","number":31224,"title":"Synonym Token Filter wrongly assume that term is completely eliminated by analyzer","user":{"login":"damienalexandre","id":225704,"node_id":"MDQ6VXNlcjIyNTcwNA==","avatar_url":"https://avatars2.githubusercontent.com/u/225704?v=4","gravatar_id":"","url":"https://api.github.com/users/damienalexandre","html_url":"https://github.com/damienalexandre","followers_url":"https://api.github.com/users/damienalexandre/followers","following_url":"https://api.github.com/users/damienalexandre/following{/other_user}","gists_url":"https://api.github.com/users/damienalexandre/gists{/gist_id}","starred_url":"https://api.github.com/users/damienalexandre/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/damienalexandre/subscriptions","organizations_url":"https://api.github.com/users/damienalexandre/orgs","repos_url":"https://api.github.com/users/damienalexandre/repos","events_url":"https://api.github.com/users/damienalexandre/events{/privacy}","received_events_url":"https://api.github.com/users/damienalexandre/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2018-06-09T09:54:50Z","updated_at":"2018-06-09T10:26:42Z","closed_at":"2018-06-09T10:26:41Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Elasticsearch version** (`bin/elasticsearch --version`): 6.2.4\r\n\r\n**Plugins installed**: [`analysis-icu`]\r\n\r\n**JVM version** (`java -version`): 9.0.4\r\n\r\n**OS version** (`uname -a` if on a Unix-like system): Ubuntu 16.04\r\n\r\n## Description of the problem:\r\n\r\nIn Elasticsearch 6.0, the synonym token filter was change, it's now running the analyzer on each term to validate the Synonym list (see related issues https://github.com/elastic/elasticsearch/issues/27481, https://github.com/elastic/elasticsearch/issues/30968).\r\n\r\nIn my case I made **sure** my text was properly tokenized and I don't use any token filter except the Synonyms, but I still get the following error:\r\n\r\n```json\r\n{\r\n  \"error\": {\r\n    \"root_cause\": [\r\n      {\r\n        \"type\": \"illegal_argument_exception\",\r\n        \"reason\": \"failed to build synonyms\"\r\n      }\r\n    ],\r\n    \"type\": \"illegal_argument_exception\",\r\n    \"reason\": \"failed to build synonyms\",\r\n    \"caused_by\": {\r\n      \"type\": \"parse_exception\",\r\n      \"reason\": \"Invalid synonym rule at line 1\",\r\n      \"caused_by\": {\r\n        \"type\": \"illegal_argument_exception\",\r\n        \"reason\": \"term: üçï was completely eliminated by analyzer\"\r\n      }\r\n    }\r\n  },\r\n  \"status\": 400\r\n}\r\n```\r\nThis makes no sense because when I use the `_analyze` API, there is a `üçï` token.\r\n\r\nSadly I can't use the `_analyze` API with the synonym token filter (related issue: https://github.com/elastic/elasticsearch/issues/23943) so I can only assume my tokenizer is valid.\r\n\r\n## Steps to reproduce\r\n\r\n 1. Install Analysis-ICU plugin\r\n 2. Install this custom RBBI file in your config directory (`Emoji.rbbi`):\r\n```\r\n#\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n# This file is from ICU (with some small modifications, to avoid CJK dictionary break,\r\n# and status code change related to that)\r\n#\r\n# Copyright (C) 2016 and later: Unicode, Inc. and others.\r\n# License & terms of use: http://www.unicode.org/copyright.html\r\n# Copyright (C) 2002-2016, International Business Machines Corporation\r\n# and others. All Rights Reserved.\r\n#\r\n# file:  word.txt\r\n#\r\n# ICU Word Break Rules\r\n#      See Unicode Standard Annex #29.\r\n#      These rules are based on UAX #29 Revision 29 for Unicode Version 9.0\r\n#      with additions for Emoji Sequences from https://goo.gl/cluFCn\r\n#      Plus additional characters introduces with Emoji 5, http://www.unicode.org/reports/tr51/proposed.html\r\n#\r\n# Note:  Updates to word.txt will usually need to be merged into\r\n#        word_POSIX.txt also.\r\n\r\n##############################################################################\r\n#\r\n#  Character class definitions from TR 29\r\n#\r\n##############################################################################\r\n\r\n!!chain;\r\n\r\n# Emoji Search edits\r\n# File from https://github.com/apache/lucene-solr/blob/4522e45bdadd4268a9270135130fc28a7f46c627/lucene/analysis/icu/src/data/uax29/Default.rbbi\r\n# Had to comment this line:\r\n#!!quoted_literals_only;\r\n\r\n\r\n#\r\n#  Character Class Definitions.\r\n#\r\n\r\n$CR                 = [\\p{Word_Break = CR}];\r\n$LF                 = [\\p{Word_Break = LF}];\r\n$Newline            = [\\p{Word_Break = Newline} ];\r\n$Extend             = [\\p{Word_Break = Extend}];\r\n$ZWJ                = [\\p{Word_Break = ZWJ}];\r\n$Regional_Indicator = [\\p{Word_Break = Regional_Indicator}];\r\n$Format             = [\\p{Word_Break = Format}];\r\n$Katakana           = [\\p{Word_Break = Katakana}];\r\n$Hebrew_Letter      = [\\p{Word_Break = Hebrew_Letter}];\r\n$ALetter            = [\\p{Word_Break = ALetter}];\r\n$Single_Quote       = [\\p{Word_Break = Single_Quote}];\r\n$Double_Quote       = [\\p{Word_Break = Double_Quote}];\r\n$MidNumLet          = [\\p{Word_Break = MidNumLet}];\r\n$MidLetter          = [\\p{Word_Break = MidLetter}];\r\n$MidNum             = [\\p{Word_Break = MidNum}];\r\n$Numeric            = [\\p{Word_Break = Numeric}[[:Decomposition_Type=Wide:]&[:General_Category=Decimal_Number:]]];\r\n$ExtendNumLet       = [\\p{Word_Break = ExtendNumLet}];\r\n$E_Base             = [\\p{Word_Break = EB}];\r\n$E_Modifier         = [\\p{Word_Break = EM}];\r\n\r\n# Data for Extended Pictographic scraped from CLDR common/properties/ExtendedPictographic.txt, r13267\r\n$Extended_Pict = [\\U0001F774-\\U0001F77F\\U00002700-\\U00002701\\U00002703-\\U00002704\\U0000270E\\U00002710-\\U00002711\\U00002765-\\U00002767\\U0001F030-\\U0001F093\\U0001F094-\\U0001F09F\\U0001F10D-\\U0001F10F\\U0001F12F\\U0001F16C-\\U0001F16F\\U0001F1AD-\\U0001F1E5\\U0001F260-\\U0001F265\\U0001F203-\\U0001F20F\\U0001F23C-\\U0001F23F\\U0001F249-\\U0001F24F\\U0001F252-\\U0001F25F\\U0001F266-\\U0001F2FF\\U0001F7D5-\\U0001F7FF\\U0001F000-\\U0001F003\\U0001F005-\\U0001F02B\\U0001F02C-\\U0001F02F\\U0001F322-\\U0001F323\\U0001F394-\\U0001F395\\U0001F398\\U0001F39C-\\U0001F39D\\U0001F3F1-\\U0001F3F2\\U0001F3F6\\U0001F4FE\\U0001F53E-\\U0001F548\\U0001F54F\\U0001F568-\\U0001F56E\\U0001F571-\\U0001F572\\U0001F57B-\\U0001F586\\U0001F588-\\U0001F589\\U0001F58E-\\U0001F58F\\U0001F591-\\U0001F594\\U0001F597-\\U0001F5A3\\U0001F5A6-\\U0001F5A7\\U0001F5A9-\\U0001F5B0\\U0001F5B3-\\U0001F5BB\\U0001F5BD-\\U0001F5C1\\U0001F5C5-\\U0001F5D0\\U0001F5D4-\\U0001F5DB\\U0001F5DF-\\U0001F5E0\\U0001F5E2\\U0001F5E4-\\U0001F5E7\\U0001F5E9-\\U0001F5EE\\U0001F5F0-\\U0001F5F2\\U0001F5F4-\\U0001F5F9\\U00002605\\U00002607-\\U0000260D\\U0000260F-\\U00002610\\U00002612\\U00002616-\\U00002617\\U00002619-\\U0000261C\\U0000261E-\\U0000261F\\U00002621\\U00002624-\\U00002625\\U00002627-\\U00002629\\U0000262B-\\U0000262D\\U00002630-\\U00002637\\U0000263B-\\U00002647\\U00002654-\\U0000265F\\U00002661-\\U00002662\\U00002664\\U00002667\\U00002669-\\U0000267A\\U0000267C-\\U0000267E\\U00002680-\\U00002691\\U00002695\\U00002698\\U0000269A\\U0000269D-\\U0000269F\\U000026A2-\\U000026A9\\U000026AC-\\U000026AF\\U000026B2-\\U000026BC\\U000026BF-\\U000026C3\\U000026C6-\\U000026C7\\U000026C9-\\U000026CD\\U000026D0\\U000026D2\\U000026D5-\\U000026E8\\U000026EB-\\U000026EF\\U000026F6\\U000026FB-\\U000026FC\\U000026FE-\\U000026FF\\U00002388\\U0001FA00-\\U0001FFFD\\U0001F0A0-\\U0001F0AE\\U0001F0B1-\\U0001F0BF\\U0001F0C1-\\U0001F0CF\\U0001F0D1-\\U0001F0F5\\U0001F0AF-\\U0001F0B0\\U0001F0C0\\U0001F0D0\\U0001F0F6-\\U0001F0FF\\U0001F80C-\\U0001F80F\\U0001F848-\\U0001F84F\\U0001F85A-\\U0001F85F\\U0001F888-\\U0001F88F\\U0001F8AE-\\U0001F8FF\\U0001F900-\\U0001F90B\\U0001F91F\\U0001F928-\\U0001F92F\\U0001F931-\\U0001F932\\U0001F94C\\U0001F95F-\\U0001F96B\\U0001F992-\\U0001F997\\U0001F9D0-\\U0001F9E6\\U0001F90C-\\U0001F90F\\U0001F93F\\U0001F94D-\\U0001F94F\\U0001F96C-\\U0001F97F\\U0001F998-\\U0001F9BF\\U0001F9C1-\\U0001F9CF\\U0001F9E7-\\U0001F9FF\\U0001F6C6-\\U0001F6CA\\U0001F6D3-\\U0001F6D4\\U0001F6E6-\\U0001F6E8\\U0001F6EA\\U0001F6F1-\\U0001F6F2\\U0001F6F7-\\U0001F6F8\\U0001F6D5-\\U0001F6DF\\U0001F6ED-\\U0001F6EF\\U0001F6F9-\\U0001F6FF];\r\n$EBG                = [\\p{Word_Break = EBG}];\r\n$EmojiNRK           = [[\\p{Emoji}] - [\\p{Word_Break = Regional_Indicator}\\u002a\\u00230-9¬©¬Æ‚Ñ¢„Ä∞„ÄΩ]];\r\n\r\n$Han                = [:Han:];\r\n$Hiragana           = [:Hiragana:];\r\n\r\n\r\n#   Dictionary character set, for triggering language-based break engines. Currently\r\n#   limited to LineBreak=Complex_Context. Note that this set only works in Unicode\r\n#   5.0 or later as the definition of Complex_Context was corrected to include all\r\n#   characters requiring dictionary break.\r\n\r\n$Control        = [\\p{Grapheme_Cluster_Break = Control}]; \r\n$HangulSyllable = [\\uac00-\\ud7a3];\r\n$ComplexContext = [:LineBreak = Complex_Context:];\r\n$KanaKanji      = [$Han $Hiragana $Katakana];\r\n$dictionaryCJK  = [$Han $Hiragana $HangulSyllable];\r\n$dictionary     = [$ComplexContext];\r\n\r\n# leave CJK scripts out of ALetterPlus\r\n$ALetterPlus  = [$ALetter-$dictionaryCJK [$ComplexContext-$Extend-$Control]];\r\n\r\n\r\n#\r\n#  Rules 4    Ignore Format and Extend characters, \r\n#             except when they appear at the beginning of a region of text.\r\n#\r\n# TODO: check if handling of katakana in dictionary makes rules incorrect/void\r\n$KatakanaEx           = $Katakana           ($Extend |  $Format | $ZWJ)*;\r\n$Hebrew_LetterEx      = $Hebrew_Letter      ($Extend |  $Format | $ZWJ)*;\r\n$ALetterEx            = $ALetterPlus        ($Extend |  $Format | $ZWJ)*;\r\n$Single_QuoteEx       = $Single_Quote       ($Extend |  $Format | $ZWJ)*;\r\n$Double_QuoteEx       = $Double_Quote       ($Extend |  $Format | $ZWJ)*;\r\n$MidNumLetEx          = $MidNumLet          ($Extend |  $Format | $ZWJ)*;\r\n$MidLetterEx          = $MidLetter          ($Extend |  $Format | $ZWJ)*;\r\n$MidNumEx             = $MidNum             ($Extend |  $Format | $ZWJ)*;\r\n$NumericEx            = $Numeric            ($Extend |  $Format | $ZWJ)*;\r\n$ExtendNumLetEx       = $ExtendNumLet       ($Extend |  $Format | $ZWJ)*;\r\n$Regional_IndicatorEx = $Regional_Indicator ($Extend |  $Format | $ZWJ)*;\r\n\r\n$Ideographic    = [\\p{Ideographic}];\r\n$HiraganaEx     = $Hiragana     ($Extend |  $Format | $ZWJ)*;\r\n$IdeographicEx  = $Ideographic  ($Extend |  $Format | $ZWJ)*;\r\n\r\n## -------------------------------------------------\r\n\r\n!!forward;\r\n\r\n\r\n\r\n# Rule 3 - CR x LF\r\n#\r\n$CR $LF;\r\n\r\n$EmojiNRK {200};\r\n\r\n\r\n\r\n# Rule 3c   ZWJ x (Extended_Pict | EmojiNRK).  Precedes WB4, so no intervening Extend chars allowed.\r\n#\r\n$ZWJ ($Extended_Pict | $EmojiNRK) {200};\r\n\r\n\r\n# Rule 4 - ignore Format and Extend characters, except when they appear at the beginning\r\n#          of a region of Text.   The rule here comes into play when the start of text\r\n#          begins with a group of Format chars, or with a \"word\" consisting of a single\r\n#          char that is not in any of the listed word break categories followed by\r\n#          format char(s), or is not a CJK dictionary character.\r\n[^$CR $LF $Newline]? ($Extend |  $Format | $ZWJ)+;\r\n\r\n$NumericEx {100};\r\n$ALetterEx {200};\r\n$HangulSyllable {200};\r\n$Hebrew_LetterEx{200};\r\n$KatakanaEx {300};       # note:  these status values override those from rule 5\r\n$HiraganaEx {300};       #        by virtue of being numerically larger.\r\n$IdeographicEx {400};    #\r\n\r\n$E_Base ($Extend | $Format | $ZWJ)* {200};\r\n$E_Modifier ($Extend | $Format | $ZWJ)* {200};\r\n$Extended_Pict ($Extend | $Format | $ZWJ)* {200};\r\n\r\n#\r\n# rule 5\r\n#    Do not break between most letters.\r\n#\r\n($ALetterEx | $Hebrew_LetterEx)  ($ALetterEx | $Hebrew_LetterEx) {200};\r\n\r\n# rule 6 and 7\r\n($ALetterEx | $Hebrew_LetterEx) ($MidLetterEx | $MidNumLetEx | $Single_QuoteEx) ($ALetterEx | $Hebrew_LetterEx) {200};\r\n\r\n# rule 7a\r\n$Hebrew_LetterEx $Single_QuoteEx {200};\r\n\r\n# rule 7b and 7c\r\n$Hebrew_LetterEx $Double_QuoteEx $Hebrew_LetterEx {200};\r\n\r\n# rule 8\r\n\r\n$NumericEx $NumericEx {100};\r\n\r\n# rule 9\r\n\r\n($ALetterEx | $Hebrew_LetterEx) $NumericEx {200};\r\n\r\n# rule 10\r\n\r\n$NumericEx ($ALetterEx | $Hebrew_LetterEx) {200};\r\n\r\n# rule 11 and 12 \r\n\r\n$NumericEx ($MidNumEx | $MidNumLetEx | $Single_QuoteEx) $NumericEx {100};\r\n\r\n# rule 13\r\n$KatakanaEx  $KatakanaEx {300};\r\n\r\n# rule 13a/b\r\n\r\n$ALetterEx       $ExtendNumLetEx {200};    #  (13a)\r\n$Hebrew_LetterEx $ExtendNumLetEx {200};    #  (13a)\r\n$NumericEx       $ExtendNumLetEx {100};    #  (13a)\r\n$KatakanaEx      $ExtendNumLetEx {300};    #  (13a)\r\n$ExtendNumLetEx  $ExtendNumLetEx {200};    #  (13a)\r\n\r\n$ExtendNumLetEx  $ALetterEx      {200};    #  (13b)\r\n$ExtendNumLetEx  $Hebrew_Letter  {200};    #  (13b)\r\n$ExtendNumLetEx  $NumericEx      {100};    #  (13b)\r\n$ExtendNumLetEx  $KatakanaEx     {300};    #  (13b)\r\n\r\n# rule 14\r\n#    Do not break within emoji modifier sequences\r\n\r\n($E_Base | $EBG) ($Format | $Extend | $ZWJ)* $E_Modifier {200};\r\n\r\n# rules 15 - 17\r\n#    Pairs of Regional Indicators stay together.\r\n#    With rule chaining disabled by ^, this rule will match exactly two of them.\r\n#    No other rule begins with a Regional_Indicator, so chaining cannot extend the match.\r\n#\r\n^$Regional_IndicatorEx $Regional_IndicatorEx;\r\n\r\n# special handling for CJK characters: chain for later dictionary segmentation\r\n$HangulSyllable $HangulSyllable {200};\r\n\r\n# Rule 999\r\n#     Match a single code point if no other rule applies.\r\n.;\r\n\r\n\r\n## -------------------------------------------------\r\n\r\n!!safe_reverse;\r\n\r\n# rule 3\r\n($Extend | $Format | $ZWJ)+ .?;\r\n\r\n# rule 6\r\n($MidLetter | $MidNumLet | $Single_Quote) ($Format | $Extend | $ZWJ)* ($Hebrew_Letter | $ALetterPlus);\r\n\r\n# rule 7b\r\n$Double_Quote ($Format | $Extend | $ZWJ)* $Hebrew_Letter;\r\n\r\n\r\n# rule 11\r\n($MidNum | $MidNumLet | $Single_Quote) ($Format | $Extend | $ZWJ)* $Numeric;\r\n\r\n# rule 13c\r\n$Regional_Indicator ($Format | $Extend | $ZWJ)* $Regional_Indicator;\r\n```\r\n\r\n### :heavy_check_mark: Testing the tokenizer\r\n```\r\nGET _analyze\r\n{\r\n  \"tokenizer\": {\r\n    \"type\": \"icu_tokenizer\",\r\n    \"rule_files\": \"Latn:Emoji.rbbi\"\r\n  },\r\n  \"text\": [\r\n    \"This üçï is pizza\"\r\n  ]\r\n}\r\n```\r\n\r\nYou will get 3 tokens, including `üçï`.\r\n\r\n### :x: Trying to create an analyzer with ICU Tokenizer\r\n\r\n```\r\nPUT demo\r\n{\r\n  \"settings\": {\r\n    \"analysis\": {\r\n      \"analyzer\": {\r\n        \"icu_with_emoji\": {\r\n          \"tokenizer\": \"icu_emoji_tokenizer\",\r\n          \"filter\": [\r\n            \"english_emoji\"\r\n          ]\r\n        }\r\n      },\r\n      \"tokenizer\": {\r\n        \"icu_emoji_tokenizer\": {\r\n          \"type\": \"icu_tokenizer\",\r\n          \"rule_files\": \"Latn:Emoji.rbbi\"\r\n        }\r\n      },\r\n      \"filter\": {\r\n        \"english_emoji\": {\r\n          \"type\": \"synonym\",\r\n          \"synonyms\": [\r\n            \"üçï, pizza\"\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe valid tokenizer tested via `_analyze` API, and a new synonym token filter. This is gonna throw the following exception:\r\n\r\n```\r\n{\r\n  \"error\": {\r\n    \"root_cause\": [\r\n      {\r\n        \"type\": \"illegal_argument_exception\",\r\n        \"reason\": \"failed to build synonyms\"\r\n      }\r\n    ],\r\n    \"type\": \"illegal_argument_exception\",\r\n    \"reason\": \"failed to build synonyms\",\r\n    \"caused_by\": {\r\n      \"type\": \"parse_exception\",\r\n      \"reason\": \"Invalid synonym rule at line 1\",\r\n      \"caused_by\": {\r\n        \"type\": \"illegal_argument_exception\",\r\n        \"reason\": \"term: üçï was completely eliminated by analyzer\"\r\n      }\r\n    }\r\n  },\r\n  \"status\": 400\r\n}\r\n```\r\n\r\n### :heavy_check_mark: Synonyms and a non ICU Tokenizer\r\n\r\nTrying with the whitespace tokenizer, our synonyms are accepted:\r\n\r\n```\r\nPUT demo\r\n{\r\n  \"settings\": {\r\n    \"analysis\": {\r\n      \"analyzer\": {\r\n        \"icu_with_emoji\": {\r\n          \"tokenizer\": \"whitespace\",\r\n          \"filter\": [\r\n            \"english_emoji\"\r\n          ]\r\n        }\r\n      },\r\n      \"filter\": {\r\n        \"english_emoji\": {\r\n          \"type\": \"synonym\",\r\n          \"synonyms\": [\r\n            \"üçï, pizza\"\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThis is working.\r\n\r\n## Hypothesis\r\n\r\nIt looks like when using a custom ICU Tokenizer, the synonym validation phase does not take it into account. So I think there is a bug in the SynonymMap analysis process but I can't see this.\r\n\r\nhttps://github.com/apache/lucene-solr/blob/c524dc2606719876004f7a9fade6fa0cc4741db9/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java#L312-L343","closed_by":{"login":"damienalexandre","id":225704,"node_id":"MDQ6VXNlcjIyNTcwNA==","avatar_url":"https://avatars2.githubusercontent.com/u/225704?v=4","gravatar_id":"","url":"https://api.github.com/users/damienalexandre","html_url":"https://github.com/damienalexandre","followers_url":"https://api.github.com/users/damienalexandre/followers","following_url":"https://api.github.com/users/damienalexandre/following{/other_user}","gists_url":"https://api.github.com/users/damienalexandre/gists{/gist_id}","starred_url":"https://api.github.com/users/damienalexandre/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/damienalexandre/subscriptions","organizations_url":"https://api.github.com/users/damienalexandre/orgs","repos_url":"https://api.github.com/users/damienalexandre/repos","events_url":"https://api.github.com/users/damienalexandre/events{/privacy}","received_events_url":"https://api.github.com/users/damienalexandre/received_events","type":"User","site_admin":false},"performed_via_github_app":null}