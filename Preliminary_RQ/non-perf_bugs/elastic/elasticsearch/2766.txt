{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/2766","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2766/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2766/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/2766/events","html_url":"https://github.com/elastic/elasticsearch/issues/2766","id":11944654,"node_id":"MDU6SXNzdWUxMTk0NDY1NA==","number":2766,"title":"Repeated ConnectExceptions in logs until node is restarted","user":{"login":"alambert","id":69652,"node_id":"MDQ6VXNlcjY5NjUy","avatar_url":"https://avatars2.githubusercontent.com/u/69652?v=4","gravatar_id":"","url":"https://api.github.com/users/alambert","html_url":"https://github.com/alambert","followers_url":"https://api.github.com/users/alambert/followers","following_url":"https://api.github.com/users/alambert/following{/other_user}","gists_url":"https://api.github.com/users/alambert/gists{/gist_id}","starred_url":"https://api.github.com/users/alambert/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alambert/subscriptions","organizations_url":"https://api.github.com/users/alambert/orgs","repos_url":"https://api.github.com/users/alambert/repos","events_url":"https://api.github.com/users/alambert/events{/privacy}","received_events_url":"https://api.github.com/users/alambert/received_events","type":"User","site_admin":false},"labels":[{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null},{"id":28458722,"node_id":"MDU6TGFiZWwyODQ1ODcyMg==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/v0.20.6","name":"v0.20.6","color":"DDDDDD","default":false,"description":null},{"id":29805870,"node_id":"MDU6TGFiZWwyOTgwNTg3MA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/v0.90.0.RC1","name":"v0.90.0.RC1","color":"DDDDDD","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2013-03-12T20:24:20Z","updated_at":"2013-03-12T21:49:13Z","closed_at":"2013-03-12T21:49:13Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"We were reviewing our logs today and found this particular log message repeated for the same channel for multiple days:\n\n```\n[2013-03-08 17:56:32,922][TRACE][transport.netty          ] [xxx] (ignoring) exception caught on transport layer [[id: 0xe611b9cc]]\njava.net.ConnectException: connection timed out\n        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:136)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:82)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n[2013-03-08 17:56:32,923][TRACE][transport.netty          ] [xxx] (ignoring) exception caught on transport layer [[id: 0xe611b9cc]]\njava.net.ConnectException: connection timed out\n        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:136)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:82)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n[2013-03-08 17:56:33,425][TRACE][transport.netty          ] [xxx] (ignoring) exception caught on transport layer [[id: 0xe611b9cc]]\njava.net.ConnectException: connection timed out\n        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:136)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:82)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n```\n\nThe message went away when the node was restarted. I don't understand the Netty code well enough to attempt a fix on my own, but here's what I've found: given that the channel ID was constant for the 48+ hours the message was appearing, it looks like ES was continuing to use a dead channel instead of initiating a fresh connection to the remote node. I think the fix is to explicitly close the channel in all cases in NettyTransport#exceptionCaught (as the isCloseConnectionException and fall-through cases do) but given that the log message specifically says the exception is ignored, I'm not sure if this is current behavior is intentional or not.\n\nThanks,\nAlex\n","closed_by":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"performed_via_github_app":null}