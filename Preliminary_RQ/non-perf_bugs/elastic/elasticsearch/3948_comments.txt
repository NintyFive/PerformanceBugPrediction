[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/26827696","html_url":"https://github.com/elastic/elasticsearch/issues/3948#issuecomment-26827696","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3948","id":26827696,"node_id":"MDEyOklzc3VlQ29tbWVudDI2ODI3Njk2","user":{"login":"spinscale","id":667544,"node_id":"MDQ6VXNlcjY2NzU0NA==","avatar_url":"https://avatars2.githubusercontent.com/u/667544?v=4","gravatar_id":"","url":"https://api.github.com/users/spinscale","html_url":"https://github.com/spinscale","followers_url":"https://api.github.com/users/spinscale/followers","following_url":"https://api.github.com/users/spinscale/following{/other_user}","gists_url":"https://api.github.com/users/spinscale/gists{/gist_id}","starred_url":"https://api.github.com/users/spinscale/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spinscale/subscriptions","organizations_url":"https://api.github.com/users/spinscale/orgs","repos_url":"https://api.github.com/users/spinscale/repos","events_url":"https://api.github.com/users/spinscale/events{/privacy}","received_events_url":"https://api.github.com/users/spinscale/received_events","type":"User","site_admin":false},"created_at":"2013-10-22T18:13:29Z","updated_at":"2013-10-22T18:13:29Z","author_association":"MEMBER","body":"Hey Nik,\n\nmaybe it is just late and I misunderstood your requirement (if so, just ignore me), but what part of that you cannot solve using http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-allocation.html#shard-allocation-filtering\n\nYou could use includes/excludes based on SSD/RAM/Cores (configured per node) and apply those per index depending on size/queries/updates.\n\nOf course, you could put all this into a shard allocation decider similar to the `DiskThresholdDecider`... but I think maintaining all of the logic is at the end very similar to maintain a couple of tags which describe the hardware the elasticsearch instance runs on plus the requirement per index.\n\nDo you see any means of automation here which I dont?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/26829437","html_url":"https://github.com/elastic/elasticsearch/issues/3948#issuecomment-26829437","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3948","id":26829437,"node_id":"MDEyOklzc3VlQ29tbWVudDI2ODI5NDM3","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2013-10-22T18:34:50Z","updated_at":"2013-10-22T18:34:50Z","author_association":"CONTRIBUTOR","body":"I _think_ includes/excludes works to keep the hot shards on hot hardware but it doesn't help prevent hot shards from being allocated together and it doesn't give me a good way of saying \"hosting a shard in this index is ten time more work then hosting one in this index\".  ES could use that information keep the high traffic shards apart and keep the medium traffic shards away from the high traffic shards.\n\nMy situation is a bit unique because I have physical hardware that is a bit of a jumble and some of my indexes get .1 searches per second and some get a hundred or so.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/51637396","html_url":"https://github.com/elastic/elasticsearch/issues/3948#issuecomment-51637396","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3948","id":51637396,"node_id":"MDEyOklzc3VlQ29tbWVudDUxNjM3Mzk2","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-08-08T18:07:44Z","updated_at":"2014-08-08T18:07:44Z","author_association":"CONTRIBUTOR","body":"Closed in favour of #4435 \n","performed_via_github_app":null}]