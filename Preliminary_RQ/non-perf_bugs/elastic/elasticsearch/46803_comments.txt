[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/532586443","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-532586443","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":532586443,"node_id":"MDEyOklzc3VlQ29tbWVudDUzMjU4NjQ0Mw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-09-18T08:46:45Z","updated_at":"2019-09-18T08:46:45Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/532586599","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-532586599","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":532586599,"node_id":"MDEyOklzc3VlQ29tbWVudDUzMjU4NjU5OQ==","user":{"login":"tlrx","id":642733,"node_id":"MDQ6VXNlcjY0MjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/642733?v=4","gravatar_id":"","url":"https://api.github.com/users/tlrx","html_url":"https://github.com/tlrx","followers_url":"https://api.github.com/users/tlrx/followers","following_url":"https://api.github.com/users/tlrx/following{/other_user}","gists_url":"https://api.github.com/users/tlrx/gists{/gist_id}","starred_url":"https://api.github.com/users/tlrx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tlrx/subscriptions","organizations_url":"https://api.github.com/users/tlrx/orgs","repos_url":"https://api.github.com/users/tlrx/repos","events_url":"https://api.github.com/users/tlrx/events{/privacy}","received_events_url":"https://api.github.com/users/tlrx/received_events","type":"User","site_admin":false},"created_at":"2019-09-18T08:47:08Z","updated_at":"2019-09-18T08:47:08Z","author_association":"MEMBER","body":"@original-brownbear as discussed via another channel, I'm assigning this issue to you.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/532603020","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-532603020","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":532603020,"node_id":"MDEyOklzc3VlQ29tbWVudDUzMjYwMzAyMA==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-09-18T09:28:39Z","updated_at":"2019-09-18T09:28:39Z","author_association":"MEMBER","body":"The problem is this:\r\n\r\nWe create a 1Mb direct buffer as a `ThreadLocal` for each Netty IO thread that we use (and that could be a few depending on the set of tests we run). Reclaiming the memory from these might take a while.\r\n\r\nSince this is a pretty unlikely thing to happen (first time I'm seeing it) since we introduced that code, I'm wondering if we should just up the `-Xmx` (or just the allowed direct memory in case we're low on memory) for the test JVMs that use Netty to 2x what it is right now and be happy?\r\n\r\n@atorok wdyt, do we have some room for more direct memory here or should we try to fix this by limiting the Netty thread count? (currently we use the default of `2 * processors` in tests)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/532618338","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-532618338","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":532618338,"node_id":"MDEyOklzc3VlQ29tbWVudDUzMjYxODMzOA==","user":{"login":"alpar-t","id":2565652,"node_id":"MDQ6VXNlcjI1NjU2NTI=","avatar_url":"https://avatars1.githubusercontent.com/u/2565652?v=4","gravatar_id":"","url":"https://api.github.com/users/alpar-t","html_url":"https://github.com/alpar-t","followers_url":"https://api.github.com/users/alpar-t/followers","following_url":"https://api.github.com/users/alpar-t/following{/other_user}","gists_url":"https://api.github.com/users/alpar-t/gists{/gist_id}","starred_url":"https://api.github.com/users/alpar-t/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alpar-t/subscriptions","organizations_url":"https://api.github.com/users/alpar-t/orgs","repos_url":"https://api.github.com/users/alpar-t/repos","events_url":"https://api.github.com/users/alpar-t/events{/privacy}","received_events_url":"https://api.github.com/users/alpar-t/received_events","type":"User","site_admin":false},"created_at":"2019-09-18T10:10:28Z","updated_at":"2019-09-18T10:10:28Z","author_association":"CONTRIBUTOR","body":"@original-brownbear We could definitely allow internal cluster tests to use more memory.. There's no telling how many nodes are actually running as part of those in general and we don't have that many of them. \r\nI would be careful to allow for more for rest clusters since we have more instances of those. \r\n\r\nI don't think we need a bigger worker at this time, we could just go ahead and increase it. I think we are doing fine even including the ramdisk.  Also we allow over-commit in CI so OOM will be late to the game, and if we do get it we'll just up the RAM on the workers.  ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/532646872","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-532646872","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":532646872,"node_id":"MDEyOklzc3VlQ29tbWVudDUzMjY0Njg3Mg==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-09-18T11:42:23Z","updated_at":"2019-09-18T11:42:23Z","author_association":"MEMBER","body":"@atorok yea, we're just talking about a small subset of tests (no REST tests) here. I'll open a PR with a suggested fix shortly and will ping you there.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/532663053","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-532663053","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":532663053,"node_id":"MDEyOklzc3VlQ29tbWVudDUzMjY2MzA1Mw==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-09-18T12:32:44Z","updated_at":"2019-09-18T12:32:44Z","author_association":"MEMBER","body":"I looked through the build stats again and this really only happens for the tests the original issue mentions.\r\nI opened https://github.com/elastic/elasticsearch/pull/46816 to lower the worker count just for these now via the settings. \r\n\r\nSorry for the noise @atorok, I think one test suit doesn't justify messing with the memory settings here in hindsight. My bad","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/543578997","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-543578997","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":543578997,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzU3ODk5Nw==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-10-18T07:53:10Z","updated_at":"2019-10-18T07:53:10Z","author_association":"MEMBER","body":"This unfortunately happened again: https://gradle-enterprise.elastic.co/s/rv4veifcuahus\r\n\r\nThe fix in #46816 doesn't improve the situation enough it seems. In hindsight just limiting the Netty worker (and hence number of concurrently allocated buffers) might not be enough of a solution here. These tests create clusters of multiple nodes in rapid succession so even if we only allow 3 workers per node, that's still 15 MB for each cluster (and that is not counting all the other buffers needed by encryption and whatnot). I'll try to come up with a better way to limit direct memory use here.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/543633288","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-543633288","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":543633288,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzYzMzI4OA==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-10-18T09:36:34Z","updated_at":"2019-10-18T09:36:34Z","author_association":"MEMBER","body":"Update: the deeper issue here seems to be that we're tracking direct byte buffers via an actual thread local while in the NIO networking we track them via a field that is effectively a thread local but does not use a weak reference. This seems to create situations where many nodes + low-heap-pressure means that the weak references for the thread local buffers aren't cleaned up quickly.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/545534404","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-545534404","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":545534404,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTUzNDQwNA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2019-10-23T16:46:14Z","updated_at":"2019-10-23T16:46:14Z","author_association":"CONTRIBUTOR","body":"This means that transport-netty4 without direct buffers is unusable with our ESIntegTestCase infrastructure as it's not properly cleaning resources up after a node is shut down. We see this with security tests as that this is the only place where are we using transport-netty4.\r\n\r\nI think we should explore ways to clean up these resources when the transport is shut down.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/545540928","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-545540928","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":545540928,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTU0MDkyOA==","user":{"login":"tbrooks8","id":862472,"node_id":"MDQ6VXNlcjg2MjQ3Mg==","avatar_url":"https://avatars3.githubusercontent.com/u/862472?v=4","gravatar_id":"","url":"https://api.github.com/users/tbrooks8","html_url":"https://github.com/tbrooks8","followers_url":"https://api.github.com/users/tbrooks8/followers","following_url":"https://api.github.com/users/tbrooks8/following{/other_user}","gists_url":"https://api.github.com/users/tbrooks8/gists{/gist_id}","starred_url":"https://api.github.com/users/tbrooks8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tbrooks8/subscriptions","organizations_url":"https://api.github.com/users/tbrooks8/orgs","repos_url":"https://api.github.com/users/tbrooks8/repos","events_url":"https://api.github.com/users/tbrooks8/events{/privacy}","received_events_url":"https://api.github.com/users/tbrooks8/received_events","type":"User","site_admin":false},"created_at":"2019-10-23T17:02:21Z","updated_at":"2019-10-23T17:02:21Z","author_association":"CONTRIBUTOR","body":"> This means that transport-netty4 without direct buffers is unusable with our ESIntegTestCase infrastructure as it's not properly cleaning resources up after a node is shut down. We see this with security tests as that this is the only place where are we using transport-netty4.\r\n\r\nIsn't this just the scenario that we used to have in production where there is not enough heap pressure to trigger a GC? I think setting MaxDirectMemory would fix this, although maybe we don't know what to set it to because there is not as simple of a heuristic (1/2 heap size) as there is in production? Probably a combination of Armin's PR and setting MaxDirectMemory would fix this.\r\n\r\nAlso the Nio transport uses 256KB buffers opposed to the Netty transport's 1MB. So 1/4 the memory.\r\n\r\nThere is no way to manually \"clean\" direct buffers without enabling Unsafe or casting to internally JDK packages.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/545550737","html_url":"https://github.com/elastic/elasticsearch/issues/46803#issuecomment-545550737","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46803","id":545550737,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTU1MDczNw==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-10-23T17:25:53Z","updated_at":"2019-10-23T17:26:44Z","author_association":"MEMBER","body":"@ywelsch \r\n\r\n> This means that transport-netty4 without direct buffers is unusable with our ESIntegTestCase infrastructure as it's not properly cleaning resources up after a node is shut down. We see this with security tests as that this is the only place where are we using transport-netty4.\r\n\r\nI wouldn't quite put it like that. I think security just uses a bunch of instances of the transport and gets very unlucky here. The issue got super rare after I lowered the transport thread count already. I think a slight reduction in the buffer size will get us to a stable place for good.\r\n\r\n> I think we should explore ways to clean up these resources when the transport is shut down.\r\n\r\nAs Tim points out, there's no hard/deterministic way of doing that. We could somehow collect the thread-locals in the treansport threads and clear them out to have `MaxDirectMemory` kick in and not get killed by the `WeakReference` in the `ThreadLocal` holding on to the buffer instance and preventing the off-heap \"GC\" from eventually dealing with the buffer.\r\nI think the big difference between the normal NIO (which never showed this problem) and Netty is simply the use of actual `ThreadLocal` which brings the weak reference and worse cleanup behavior when in `NIO` we use our own \"effective threadlocal\" in the `NioSelector`.\r\nI'm not sure it's worth the effort though ... this really is just a test-only issue and very rare already. If we can just make it disappear via a smaller buffer size I think it's probably fine?","performed_via_github_app":null}]