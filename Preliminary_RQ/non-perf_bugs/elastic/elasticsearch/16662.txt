{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/16662","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16662/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16662/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16662/events","html_url":"https://github.com/elastic/elasticsearch/issues/16662","id":133584003,"node_id":"MDU6SXNzdWUxMzM1ODQwMDM=","number":16662,"title":"Translog recovery failure eating CPU","user":{"login":"drewr","id":6202,"node_id":"MDQ6VXNlcjYyMDI=","avatar_url":"https://avatars3.githubusercontent.com/u/6202?v=4","gravatar_id":"","url":"https://api.github.com/users/drewr","html_url":"https://github.com/drewr","followers_url":"https://api.github.com/users/drewr/followers","following_url":"https://api.github.com/users/drewr/following{/other_user}","gists_url":"https://api.github.com/users/drewr/gists{/gist_id}","starred_url":"https://api.github.com/users/drewr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/drewr/subscriptions","organizations_url":"https://api.github.com/users/drewr/orgs","repos_url":"https://api.github.com/users/drewr/repos","events_url":"https://api.github.com/users/drewr/events{/privacy}","received_events_url":"https://api.github.com/users/drewr/received_events","type":"User","site_admin":false},"labels":[{"id":836542781,"node_id":"MDU6TGFiZWw4MzY1NDI3ODE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Engine","name":":Distributed/Engine","color":"0e8a16","default":false,"description":"Anything around managing Lucene and the Translog in an open shard."},{"id":152510590,"node_id":"MDU6TGFiZWwxNTI1MTA1OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Recovery","name":":Distributed/Recovery","color":"0e8a16","default":false,"description":"Anything around constructing a new shard, either from a local or a remote source."},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null}],"state":"closed","locked":false,"assignee":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"assignees":[{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2016-02-14T21:44:47Z","updated_at":"2018-04-24T09:45:19Z","closed_at":"2018-04-24T09:45:19Z","author_association":"MEMBER","active_lock_reason":null,"body":"Found one of the nodes in my idle (other than marvel), local **2.2.0** cluster eating a chunk of CPU today.\n\n![image](https://cloud.githubusercontent.com/assets/6202/13036529/42007536-d32f-11e5-82bb-8a867af923e4.png)\n\n```\n% curl -s localhost:9200/_cat/nodes\\?h=ip,master,name\n127.0.0.1 * Graviton     \n127.0.0.1 m Blood Spider \n% curl -s localhost:9200/_cat/shards | fgrep 02.13\n.marvel-es-2016.02.13                     0 p INITIALIZING                127.0.0.1 Graviton     \n.marvel-es-2016.02.13                     0 r UNASSIGNED\n%\n```\n\nA bunch of checkpoint logs happen:\n\n```\n[2016-02-14 00:29:10,769][DEBUG][index.translog           ] [Graviton] [.marvel-es-2016.02.13][0] recovered local translog from checkpoint Checkpoint{offset=2556515, numOps=4273, translogFileGe\nneration= 3691}\n```\n\nBoth nodes load their data dir from the same `nodes/` path, and this volume is **87% full**.  So allocation of this shard keeps occurring:\n\n```\n[2016-02-14 00:29:28,147][INFO ][cluster.routing.allocation.decider] [Graviton] low disk watermark [85%] exceeded on [DG6Jls2UTrSSsrG2huoYMQ][Graviton][/home/aar/Downloads/elasticsearch-2.2.0/d\nata/elasticsearch/nodes/1] free: 66.2gb[14.8%], replicas will not be assigned to this node\n[2016-02-14 00:29:28,147][INFO ][cluster.routing.allocation.decider] [Graviton] low disk watermark [85%] exceeded on [w8hx8OmwRg6ETfhbAA6XYg][Blood Spider][/home/aar/Downloads/elasticsearch-2.2\n.0/data/elasticsearch/nodes/0] free: 66.2gb[14.8%], replicas will not be assigned to this node\n```\n\nThen the allocation is attempted:\n\n```\n[2016-02-14 00:29:29,120][DEBUG][index.translog           ] [Graviton] [.marvel-es-2016.02.13][0] translog closed\n[2016-02-14 00:29:29,120][DEBUG][index                    ] [Graviton] [.marvel-es-2016.02.13] [0] closing... (reason: [failed recovery])\n[2016-02-14 00:29:29,120][DEBUG][index.shard              ] [Graviton] [.marvel-es-2016.02.13][0] state: [RECOVERING]->[CLOSED], reason [failed recovery]\n[2016-02-14 00:29:29,120][DEBUG][index.shard              ] [Graviton] [.marvel-es-2016.02.13][0] operations counter reached 0, will not accept any further writes\n[2016-02-14 00:29:29,120][DEBUG][index.store              ] [Graviton] [.marvel-es-2016.02.13][0] store reference count on close: 0\n[2016-02-14 00:29:29,120][DEBUG][index                    ] [Graviton] [.marvel-es-2016.02.13] [0] closed (reason: [failed recovery])\n[2016-02-14 00:29:29,120][WARN ][indices.cluster          ] [Graviton] [[.marvel-es-2016.02.13][0]] marking and sending shard failed due to [failed recovery]\n[.marvel-es-2016.02.13][[.marvel-es-2016.02.13][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/1/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2556515] length: [4] end: [2556515]];\n        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:254)\n        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)\n        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [.marvel-es-2016.02.13][[.marvel-es-2016.02.13][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/1/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2556515] length: [4] end: [2556515]];\n        at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:178)\n        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)\n        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1450)\n        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1434)\n        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:925)\n        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:897)\n        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)\n        ... 5 more\nCaused by: [.marvel-es-2016.02.13][[.marvel-es-2016.02.13][0]] EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/1/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2556515] length: [4] end: [2556515]];\n        at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:254)\n        at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:175)\n        ... 11 more\nCaused by: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/1/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2556515] length: [4] end: [2556515]];\n        at org.elasticsearch.index.translog.TranslogReader.readSize(TranslogReader.java:102)\n        at org.elasticsearch.index.translog.TranslogReader.access$000(TranslogReader.java:46)\n        at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:297)\n        at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:290)\n        at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)\n        at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:240)\n        ... 12 more\nCaused by: java.io.EOFException: read past EOF. pos [2556515] length: [4] end: [2556515]\n        at org.elasticsearch.common.io.Channels.readFromFileChannelWithEofException(Channels.java:102)\n        at org.elasticsearch.index.translog.ImmutableTranslogReader.readBytes(ImmutableTranslogReader.java:84)\n        at org.elasticsearch.index.translog.TranslogReader.readSize(TranslogReader.java:91)\n        ... 17 more\n[2016-02-14 00:29:29,121][WARN ][cluster.action.shard     ] [Graviton] [.marvel-es-2016.02.13][0] received shard failed for [.marvel-es-2016.02.13][0], node[DG6Jls2UTrSSsrG2huoYMQ], [P], v[7374], s[INITIALIZING], a[id=0EodiK9gS8KIGvHSHNi1ew], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-02-14T06:29:10.518Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/0/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2343547] length: [4] end: [2343547]]; ]], indexUUID [Wl7rh0TuT1ms79hg-H8hzA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/1/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2556515] length: [4] end: [2556515]]; ]\n```\n\nThis process repeats ad infinitum.  `DEBUG` log file is each over 1g the past couple of days.\n\n`hot_threads` has this repeated section:\n\n```\n   98.7% (493.3ms out of 500ms) cpu usage by thread 'elasticsearch[Graviton][generic][T#1]'\n     10/10 snapshots sharing following 24 elements\n       sun.nio.fs.UnixNativeDispatcher.readdir(Native Method)\n       sun.nio.fs.UnixDirectoryStream$UnixDirectoryIterator.readNextEntry(UnixDirectoryStream.java:168)\n       sun.nio.fs.UnixDirectoryStream$UnixDirectoryIterator.hasNext(UnixDirectoryStream.java:201)\n       org.elasticsearch.index.translog.Translog$OnCloseRunnable.handle(Translog.java:726)\n       org.elasticsearch.index.translog.Translog$OnCloseRunnable.handle(Translog.java:713)\n       org.elasticsearch.index.translog.ChannelReference.closeInternal(ChannelReference.java:67)\n       org.elasticsearch.common.util.concurrent.AbstractRefCounted.decRef(AbstractRefCounted.java:64)\n       org.elasticsearch.index.translog.TranslogReader.close(TranslogReader.java:143)\n       org.apache.lucene.util.IOUtils.close(IOUtils.java:97)\n       org.elasticsearch.index.translog.Translog.close(Translog.java:425)\n       org.apache.lucene.util.IOUtils.closeWhileHandlingException(IOUtils.java:129)\n       org.apache.lucene.util.IOUtils.closeWhileHandlingException(IOUtils.java:118)\n       org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:183)\n       org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)\n       org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1450)\n       org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1434)\n       org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:925)\n       org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:897)\n       org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)\n       org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)\n       org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)\n       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n       java.lang.Thread.run(Thread.java:745)\n```\n","closed_by":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"performed_via_github_app":null}