[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/188882441","html_url":"https://github.com/elastic/elasticsearch/issues/16802#issuecomment-188882441","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16802","id":188882441,"node_id":"MDEyOklzc3VlQ29tbWVudDE4ODg4MjQ0MQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2016-02-25T17:11:51Z","updated_at":"2016-02-25T17:16:00Z","author_association":"MEMBER","body":"@ben-manes Thanks for the post.\n\n> First is that a r/w lock is expensive, often much more than an exclusive lock, and its usage per read is probably best replaced with a ConcurrentHashMap. Second is that the LRU lock is a throttling point, as all accesses contend on it.\n\nIt's difficult to say how impactful these are without search latency benchmarks here (to be clear since I don't want this to read like a dodge, I suspect the LRU lock is impactful under heavy search loads with large search thread pools). Unfortunately, we do not have benchmarks here that would stress the lock contention issues. These sorts of benchmarks are a work-in-progress.\n\n> Third is that computeIfAbsent is racy by allowing redundant computations with last insert wins.\n\nThis was the case for the initial commit that you linked to, but this has been [addressed](https://github.com/elastic/elasticsearch/blob/70396d4243e89dbe2194499cc942bff17bc695aa/core/src/main/java/org/elasticsearch/common/cache/Cache.java#L345-L408) and (unless I'm mistaken) is no longer the case.\n\n> Fourth is that LRU is sub-optimal for search workloads, where frequency is a more insightful metric.\n\nDo you have any literature on this specific point that you can share?\n\n> The easiest (and biased) solution is to adopt Caffeine. Alternatively a big win would come from integrating TinyLFU into the existing cache.\n\nI'll take a look. I have some [ideas regarding the LRU lock contention as well](https://github.com/elastic/elasticsearch/blob/70396d4243e89dbe2194499cc942bff17bc695aa/core/src/main/java/org/elasticsearch/common/cache/Cache.java#L51-L61), but I really don't want to go stabbing in the dark until we can get valid multi-threaded search latency benchmarks in place. One goal we had with the cache implementation was to keep it simple.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/188907811","html_url":"https://github.com/elastic/elasticsearch/issues/16802#issuecomment-188907811","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16802","id":188907811,"node_id":"MDEyOklzc3VlQ29tbWVudDE4ODkwNzgxMQ==","user":{"login":"ben-manes","id":378614,"node_id":"MDQ6VXNlcjM3ODYxNA==","avatar_url":"https://avatars3.githubusercontent.com/u/378614?v=4","gravatar_id":"","url":"https://api.github.com/users/ben-manes","html_url":"https://github.com/ben-manes","followers_url":"https://api.github.com/users/ben-manes/followers","following_url":"https://api.github.com/users/ben-manes/following{/other_user}","gists_url":"https://api.github.com/users/ben-manes/gists{/gist_id}","starred_url":"https://api.github.com/users/ben-manes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ben-manes/subscriptions","organizations_url":"https://api.github.com/users/ben-manes/orgs","repos_url":"https://api.github.com/users/ben-manes/repos","events_url":"https://api.github.com/users/ben-manes/events{/privacy}","received_events_url":"https://api.github.com/users/ben-manes/received_events","type":"User","site_admin":false},"created_at":"2016-02-25T18:07:54Z","updated_at":"2016-02-25T18:07:54Z","author_association":"NONE","body":"> It's difficult to say how impactful these are without search latency benchmarks here\n\nYep, I'd much rather have data to work from than a hypothesis too. I know from [micro-benchmarks](https://github.com/ben-manes/caffeine/wiki/Benchmarks#read-100-1) and past experiences that it can be a problem, but it is too application dependent to draw a conclusion. The hit rate generally has a bigger impact than concurrency, given how expensive the miss penalty often is. I'm more interested in seeing if we can improve that for ES.\n\n> This was the case for the initial commit that you linked to, but this has been addressed\n\nOh, nice. Sorry that I missed that when skimming over the code.\n\n> Do you have any literature on this specific point that you can share?\n\nZipf-like distributions are commonly discussed in information retrieval literature ([1](https://hughewilliams.com/tag/zipfs-law/), [2](https://books.google.com/books?id=mDI72_9-bw0C&pg=PA99&lpg=PA99&dq=search+engine+zipf&source=bl&ots=yVpgus4rCv&sig=9-LGzMmmLGZ9JGX5SH_bHqljjAw&hl=en&sa=X&ved=0ahUKEwjy6arxt5PLAhUB9WMKHcxdDuoQ6AEIPjAG#v=onepage&q=search%20engine%20zipf&f=false), [3](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.8343&rep=rep1&type=pdf), [4](http://nlp.stanford.edu/IR-book/pdf/05comp.pdf), [5](http://ils.unc.edu/~losee/multzipf.pdf)). Similarly the distribution holds as a model for caching ([6](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=0B2D54724B9411CC893B5F0097CCF426?doi=10.1.1.12.2253&rep=rep1&type=pdf), [7](http://www.cas.mcmaster.ca/~gk/papers/effcache.pdf), [8](http://ftp.cs.wisc.edu/pub/techreports/1998/TR1371.pdf)). This is fairly intuitive because the popularity of items is a good signal, but some aging based on recency is logically necessary to avoid holding stale data for too long. The [TinyLFU paper](http://arxiv.org/pdf/1512.00727.pdf) includes charts with web search traces from IBM and UMass. This policy uses recency and historic frequency (present and absent entries) to predict whether an item should be retained.\n\n> I have some ideas regarding the LRU lock contention as well\n\nThis [article](http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html) on HighScalability describes the approach that I took. The concurrency model was proven out in my prior libraries (CLHM, Guava). The former is used by Cassandra (among others) and the latter is of course popular but underperforms due to [important optimizations](https://github.com/google/guava/issues/2063#issuecomment-107169736) not being ported over. This has very predictable latencies by being O(1) and borrows from database / filesystem theory where similar approaches are used to scale writes.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/215623848","html_url":"https://github.com/elastic/elasticsearch/issues/16802#issuecomment-215623848","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16802","id":215623848,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNTYyMzg0OA==","user":{"login":"ben-manes","id":378614,"node_id":"MDQ6VXNlcjM3ODYxNA==","avatar_url":"https://avatars3.githubusercontent.com/u/378614?v=4","gravatar_id":"","url":"https://api.github.com/users/ben-manes","html_url":"https://github.com/ben-manes","followers_url":"https://api.github.com/users/ben-manes/followers","following_url":"https://api.github.com/users/ben-manes/following{/other_user}","gists_url":"https://api.github.com/users/ben-manes/gists{/gist_id}","starred_url":"https://api.github.com/users/ben-manes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ben-manes/subscriptions","organizations_url":"https://api.github.com/users/ben-manes/orgs","repos_url":"https://api.github.com/users/ben-manes/repos","events_url":"https://api.github.com/users/ben-manes/events{/privacy}","received_events_url":"https://api.github.com/users/ben-manes/received_events","type":"User","site_admin":false},"created_at":"2016-04-29T04:28:18Z","updated_at":"2016-04-29T04:28:59Z","author_association":"NONE","body":"@jasontedor \nI added ElasticSearch's cache to Caffeine's benchmark suite. The results were not positive.\n\nThe [micro-benchmarks](https://github.com/ben-manes/caffeine/wiki/Benchmarks) were run on my laptop. These results are slower than a synchronized LinkedHashMap in LRU mode. I ran Caffeine a few times since this isn't a good (isolated) testbed machine and it consumes all the available CPU resources.\n\n| Benchmark | ElasticSearch | LinkedHashMap | Caffeine |\n| :-: | :-: | :-: | :-: |\n| read-only | 2.5 M/s | 7.4 M/s | 130 - 150 M/s |\n| read-write | 2.3 M/s | 7.7 M/s | 100 - 115 M/s |\n| write-only | 1.6 M/s | 7.7 M/s | 50 - 65 M/s |\n\nThe simulator compares the hit rate of the caching policies. The current cache stays very close to a pure LRU, which validates the design. Unfortunately LRU is poor for search workloads, which are biased towards frequency instead of recency.  The [efficiency](https://github.com/ben-manes/caffeine/wiki/Efficiency) using the Univ. of Massachusetts `WebSearch1` workload was,\n\n| Benchmark | ElasticSearch | Caffeine | Optimal |\n| :-: | :-: | :-: | :-: |\n| WS1 @ 2M | 6.09 % | 23.06 % | 41.28 % |\n| WS1 @ 4M | 21.60 % | 41.32 % | 57.80 % |\n| WS1 @ 6M | 45.74 % | 55.02 % | 65.85 % |\n\nIt may very well be that the cache is not a critical bottleneck for ElasticSearch. Unfortunately it appears to leave a lot of performance on the table, which I bet a little profiling would dramatically improve. If the size of the cache is large, there is an opportunity it either achieve the same hit rate at a reduced capacity or increase the hit rate. In either case there should be reduced latencies (less GC / fewer misses).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/257199726","html_url":"https://github.com/elastic/elasticsearch/issues/16802#issuecomment-257199726","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16802","id":257199726,"node_id":"MDEyOklzc3VlQ29tbWVudDI1NzE5OTcyNg==","user":{"login":"ben-manes","id":378614,"node_id":"MDQ6VXNlcjM3ODYxNA==","avatar_url":"https://avatars3.githubusercontent.com/u/378614?v=4","gravatar_id":"","url":"https://api.github.com/users/ben-manes","html_url":"https://github.com/ben-manes","followers_url":"https://api.github.com/users/ben-manes/followers","following_url":"https://api.github.com/users/ben-manes/following{/other_user}","gists_url":"https://api.github.com/users/ben-manes/gists{/gist_id}","starred_url":"https://api.github.com/users/ben-manes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ben-manes/subscriptions","organizations_url":"https://api.github.com/users/ben-manes/orgs","repos_url":"https://api.github.com/users/ben-manes/repos","events_url":"https://api.github.com/users/ben-manes/events{/privacy}","received_events_url":"https://api.github.com/users/ben-manes/received_events","type":"User","site_admin":false},"created_at":"2016-10-31T02:11:01Z","updated_at":"2016-10-31T02:11:01Z","author_association":"NONE","body":"Congrats on 5.0! Its an impressive release and very timely for me. I was about to start migrating from Algolia to Elastic, and was considering Druid for analytical functions. The new features in Elastic are a wonderful alternative, so I'm looking forward to using them instead.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/383884738","html_url":"https://github.com/elastic/elasticsearch/issues/16802#issuecomment-383884738","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16802","id":383884738,"node_id":"MDEyOklzc3VlQ29tbWVudDM4Mzg4NDczOA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-04-24T10:34:38Z","updated_at":"2018-04-24T10:34:38Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-core-infra","performed_via_github_app":null}]