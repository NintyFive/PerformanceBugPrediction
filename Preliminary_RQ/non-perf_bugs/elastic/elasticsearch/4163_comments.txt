[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/28479600","html_url":"https://github.com/elastic/elasticsearch/issues/4163#issuecomment-28479600","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4163","id":28479600,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NDc5NjAw","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2013-11-14T12:20:55Z","updated_at":"2013-11-14T12:20:55Z","author_association":"MEMBER","body":"That is correct. There isn't really a good way to get the size in bytes of the filter (which is used as the key...), it requires enabling in Lucene (which we have already internally discussed) to make it possible. You can always use the cache key option on filter to control the key the filter is stored under, btw.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/28524331","html_url":"https://github.com/elastic/elasticsearch/issues/4163#issuecomment-28524331","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4163","id":28524331,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NTI0MzMx","user":{"login":"dspangen","id":1711045,"node_id":"MDQ6VXNlcjE3MTEwNDU=","avatar_url":"https://avatars1.githubusercontent.com/u/1711045?v=4","gravatar_id":"","url":"https://api.github.com/users/dspangen","html_url":"https://github.com/dspangen","followers_url":"https://api.github.com/users/dspangen/followers","following_url":"https://api.github.com/users/dspangen/following{/other_user}","gists_url":"https://api.github.com/users/dspangen/gists{/gist_id}","starred_url":"https://api.github.com/users/dspangen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dspangen/subscriptions","organizations_url":"https://api.github.com/users/dspangen/orgs","repos_url":"https://api.github.com/users/dspangen/repos","events_url":"https://api.github.com/users/dspangen/events{/privacy}","received_events_url":"https://api.github.com/users/dspangen/received_events","type":"User","site_admin":false},"created_at":"2013-11-14T21:26:09Z","updated_at":"2013-11-14T21:26:09Z","author_association":"NONE","body":"Understood. Just noticed that our cache was using far more memory than 20%, and digging in noticed that the keys weren't part of the size calculation. In our case, our keys were a ~5% of our heap; our values were ~22% (and the cache itself took up a non-trivial amount of space as well).\n\nI also wanted to report this in case someone else sees something similar with cache sizing that seems out of whack with their indices.cache.filter.size parameter.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/28557553","html_url":"https://github.com/elastic/elasticsearch/issues/4163#issuecomment-28557553","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4163","id":28557553,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NTU3NTUz","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2013-11-15T09:44:41Z","updated_at":"2013-11-15T09:44:41Z","author_association":"MEMBER","body":"Make sense..., it is annoying, and we should find a way to solve it one way or another. By the way, there is another more subtle complication, which is properly counting the filter (assuming we can size it) because of how segment based filter caching works. For example, a search on a shard with 8 segment comes in, the same filter instance will be used as the cache key for all 8 and should be counted once. If another segment gets created, and the same search request is executed again, then we need to count that filter again (or potentially do key based lookup and reuse the same instance..., which can prove to be expensive...)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/54610633","html_url":"https://github.com/elastic/elasticsearch/issues/4163#issuecomment-54610633","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4163","id":54610633,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjEwNjMz","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2014-09-05T10:50:47Z","updated_at":"2014-09-05T10:50:47Z","author_association":"CONTRIBUTOR","body":"It is hard for us to compute the memory cost of the keys involved as they are the complex, composite objects that are the results of the query parsing process. Rather than try and solve this complex memory reporting problem we generally advise that users provide a more efficient form of key in the first place using a custom _cache_key provided as part of the request.\nThis approach is documented here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-filters.html\n\nWhile we could have chosen to do this key-making internally and automatically by using a hashing function (we like to be helpful!) in this case it would introduce the possibility of hash collisions which is a risk we do not want to assume responsibility for. That is why we rely on clients providing us with the cache key to be associated with the request.\n","performed_via_github_app":null}]