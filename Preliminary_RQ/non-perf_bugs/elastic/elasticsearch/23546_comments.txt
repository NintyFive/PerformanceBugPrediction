[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/285962424","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-285962424","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":285962424,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NTk2MjQyNA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-03-12T18:06:26Z","updated_at":"2017-03-12T18:06:26Z","author_association":"CONTRIBUTOR","body":"While this sounds like an appealing idea there are quite a bit of caveats attached to it. Essentially what this request is about is to make opening an index reader in a lazy fashion once it's requested. Yet, if you take a step closer what that means is that we need to load the in-memory datastrucutres once the reader is accessed. Lets say you have 2k lazy indices in your cluster and you call `_search` you will hit every index in the cluster and that will in-turn cause your cluster to load all it's lazy readers which will likely end up in a out of memory (OOM) error? While I agree that this would be nice I am way to concerned about a read request killing your cluster to make this a feature.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/285964520","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-285964520","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":285964520,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NTk2NDUyMA==","user":{"login":"trevan","id":2044248,"node_id":"MDQ6VXNlcjIwNDQyNDg=","avatar_url":"https://avatars2.githubusercontent.com/u/2044248?v=4","gravatar_id":"","url":"https://api.github.com/users/trevan","html_url":"https://github.com/trevan","followers_url":"https://api.github.com/users/trevan/followers","following_url":"https://api.github.com/users/trevan/following{/other_user}","gists_url":"https://api.github.com/users/trevan/gists{/gist_id}","starred_url":"https://api.github.com/users/trevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/trevan/subscriptions","organizations_url":"https://api.github.com/users/trevan/orgs","repos_url":"https://api.github.com/users/trevan/repos","events_url":"https://api.github.com/users/trevan/events{/privacy}","received_events_url":"https://api.github.com/users/trevan/received_events","type":"User","site_admin":false},"created_at":"2017-03-12T18:38:22Z","updated_at":"2017-03-12T18:38:22Z","author_association":"NONE","body":"@s1monw, the problem is that the current way to deal with lots of cold indices is to close/open them.  So if I have a custom built auto-opener, then I can still have the issue where lots of indices are opened at the same time and run out of memory.  So I and anyone who would use this feature already have to deal with that case.  Anyone who would use this feature would have to deal with that issue and you can put \"caveats\" around it.\r\n\r\nBut the open/close mechanism is more likely to cause issues in the long run because it probably takes longer to open an index versus warm up a lazy reader, the cluster goes red while the index is opening (and might not even recover if there are allocation issues), and closed indices don't automatically close when a node goes down.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/285985585","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-285985585","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":285985585,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NTk4NTU4NQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-03-12T23:12:37Z","updated_at":"2017-03-12T23:12:37Z","author_association":"CONTRIBUTOR","body":"> So if I have a custom built auto-opener, then I can still have the issue where lots of indices are opened at the same time and run out of memory. So I and anyone who would use this feature already have to deal with that case. Anyone who would use this feature would have to deal with that issue and you can put \"caveats\" around it.\r\n\r\nI disagree on this statement. Lemme explain, when you open / close an index that is and admin like operation that isn't executed by accident and it also won't apply wildcards. It's also executed by a privileged user (I hope) such that there is more control over this in general. Yet, if we allow this to be executed by `_search` or some index patterns that expand to those lazy indices can be done by anybody and should not hit you by surprise as in letting your nodes go OOM.\r\n\r\n> But the open/close mechanism is more likely to cause issues in the long run because it probably takes longer to open an index versus warm up a lazy reader, the cluster goes red while the index is opening (and might not even recover if there are allocation issues), and closed indices don't automatically close when a node goes down.\r\n\r\nwe are currently in the process of designing a better open / close implement that is also replicated etc. that might also make it easier to allocate these indices and will prevent the cluster from blinking. (go red for a while) I think we need to design that is safe and gives you a less hard time here. Yet, we are not there yet.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/285997526","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-285997526","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":285997526,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NTk5NzUyNg==","user":{"login":"trevan","id":2044248,"node_id":"MDQ6VXNlcjIwNDQyNDg=","avatar_url":"https://avatars2.githubusercontent.com/u/2044248?v=4","gravatar_id":"","url":"https://api.github.com/users/trevan","html_url":"https://github.com/trevan","followers_url":"https://api.github.com/users/trevan/followers","following_url":"https://api.github.com/users/trevan/following{/other_user}","gists_url":"https://api.github.com/users/trevan/gists{/gist_id}","starred_url":"https://api.github.com/users/trevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/trevan/subscriptions","organizations_url":"https://api.github.com/users/trevan/orgs","repos_url":"https://api.github.com/users/trevan/repos","events_url":"https://api.github.com/users/trevan/events{/privacy}","received_events_url":"https://api.github.com/users/trevan/received_events","type":"User","site_admin":false},"created_at":"2017-03-13T01:50:48Z","updated_at":"2017-03-13T01:50:48Z","author_association":"NONE","body":"@s1monw, I think what you are missing is that for those of us that need either this feature or #10869 will implement our own open/close mechanism that can be executed by accident and can be applied to wildcards.  We need to be able to have lots of data available in ES without requiring a ton of RAM.  So, the only mechanism is to close/open the indices and to do it automatically when searches are made.\r\n\r\nIn the end, we need a way to reduce the RAM usage for really cold indices but still allow those indices to be easily searchable.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/286819790","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-286819790","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":286819790,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NjgxOTc5MA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-03-15T17:33:24Z","updated_at":"2017-03-15T17:33:24Z","author_association":"CONTRIBUTOR","body":"> We need to be able to have lots of data available in ES without requiring a ton of RAM. So, the only mechanism is to close/open the indices and to do it automatically when searches are made.\r\n\r\nI agree but we can't just add some risky feature just fix a problem. we need a sustainable solution that deals with it. This suggestion has problems that I am not willing to sign up for as a compromise. If we make compromises here they need to be safe!","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/286819918","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-286819918","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":286819918,"node_id":"MDEyOklzc3VlQ29tbWVudDI4NjgxOTkxOA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-03-15T17:33:46Z","updated_at":"2017-03-15T17:33:46Z","author_association":"CONTRIBUTOR","body":"I will reopen this to use it as a discuss area for now.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/290058768","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-290058768","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":290058768,"node_id":"MDEyOklzc3VlQ29tbWVudDI5MDA1ODc2OA==","user":{"login":"makeyang","id":13898618,"node_id":"MDQ6VXNlcjEzODk4NjE4","avatar_url":"https://avatars2.githubusercontent.com/u/13898618?v=4","gravatar_id":"","url":"https://api.github.com/users/makeyang","html_url":"https://github.com/makeyang","followers_url":"https://api.github.com/users/makeyang/followers","following_url":"https://api.github.com/users/makeyang/following{/other_user}","gists_url":"https://api.github.com/users/makeyang/gists{/gist_id}","starred_url":"https://api.github.com/users/makeyang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/makeyang/subscriptions","organizations_url":"https://api.github.com/users/makeyang/orgs","repos_url":"https://api.github.com/users/makeyang/repos","events_url":"https://api.github.com/users/makeyang/events{/privacy}","received_events_url":"https://api.github.com/users/makeyang/received_events","type":"User","site_admin":false},"created_at":"2017-03-29T11:13:39Z","updated_at":"2017-03-29T11:13:39Z","author_association":"CONTRIBUTOR","body":"how about add a breaker for in-memory datastructures when open index reader?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297095430","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-297095430","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":297095430,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzA5NTQzMA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T16:58:51Z","updated_at":"2017-04-25T16:58:51Z","author_association":"CONTRIBUTOR","body":"If we do this, I'd like to make sure we are addressing an actual problem rather than the symptom of some mis-configuration (eg. too many shards in a cluster). Also how much memory are we talking about, and where is it spent? Maybe the right fix is to make Lucene indices more memory-efficient.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297099476","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-297099476","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":297099476,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzA5OTQ3Ng==","user":{"login":"trevan","id":2044248,"node_id":"MDQ6VXNlcjIwNDQyNDg=","avatar_url":"https://avatars2.githubusercontent.com/u/2044248?v=4","gravatar_id":"","url":"https://api.github.com/users/trevan","html_url":"https://github.com/trevan","followers_url":"https://api.github.com/users/trevan/followers","following_url":"https://api.github.com/users/trevan/following{/other_user}","gists_url":"https://api.github.com/users/trevan/gists{/gist_id}","starred_url":"https://api.github.com/users/trevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/trevan/subscriptions","organizations_url":"https://api.github.com/users/trevan/orgs","repos_url":"https://api.github.com/users/trevan/repos","events_url":"https://api.github.com/users/trevan/events{/privacy}","received_events_url":"https://api.github.com/users/trevan/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T17:09:20Z","updated_at":"2017-04-25T17:09:20Z","author_association":"NONE","body":"@jpountz, improving memory usage of Lucene could work for me as well.\r\n\r\nThis is really a cost issue.  It is really cheap to add more disk for cold storage indices (we have it over iSCSI which is slow but these are indices that only used once a day at most).  But to add additional RAM, it costs a bit.\r\n\r\nhttps://discuss.elastic.co/t/why-is-my-heap-usage-always-high/45017/9 is my discuss question to see if there is something I can do in my configuration.  The last reply from Mark Walkom is that I just need to add new nodes and that increases our overhead.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297105683","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-297105683","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":297105683,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzEwNTY4Mw==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T17:29:48Z","updated_at":"2017-04-25T17:29:48Z","author_association":"CONTRIBUTOR","body":"To me the issue is that there are 17745 shards for only 5.6 billion docs. I'd recommend using the [rollover and shrink APIs](https://www.elastic.co/blog/managing-time-based-indices-efficiently) to manage indices rather than daily indices in order to better utilize resources.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297109711","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-297109711","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":297109711,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzEwOTcxMQ==","user":{"login":"trevan","id":2044248,"node_id":"MDQ6VXNlcjIwNDQyNDg=","avatar_url":"https://avatars2.githubusercontent.com/u/2044248?v=4","gravatar_id":"","url":"https://api.github.com/users/trevan","html_url":"https://github.com/trevan","followers_url":"https://api.github.com/users/trevan/followers","following_url":"https://api.github.com/users/trevan/following{/other_user}","gists_url":"https://api.github.com/users/trevan/gists{/gist_id}","starred_url":"https://api.github.com/users/trevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/trevan/subscriptions","organizations_url":"https://api.github.com/users/trevan/orgs","repos_url":"https://api.github.com/users/trevan/repos","events_url":"https://api.github.com/users/trevan/events{/privacy}","received_events_url":"https://api.github.com/users/trevan/received_events","type":"User","site_admin":false},"created_at":"2017-04-25T17:44:12Z","updated_at":"2017-04-25T17:45:06Z","author_association":"NONE","body":"@jpountz, you need to read my last comment there.  We are now doing rollover that is based on size instead of daily.  As of today, we have 7300 shards with 55 billion docs that takes up 180TB of disk.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297454026","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-297454026","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":297454026,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzQ1NDAyNg==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-04-26T15:46:43Z","updated_at":"2017-04-26T15:46:43Z","author_association":"CONTRIBUTOR","body":"180TB for 7300 shards means that shards are only 25GB on average, I think you could still aim at larger shards.\r\n\r\nThat said, I agree this is the kind of scale that makes keeping these indices open quite costly. I don't have good ideas how to improve this, but I don't think opening indices on demand is a good solution.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/297460930","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-297460930","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":297460930,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NzQ2MDkzMA==","user":{"login":"trevan","id":2044248,"node_id":"MDQ6VXNlcjIwNDQyNDg=","avatar_url":"https://avatars2.githubusercontent.com/u/2044248?v=4","gravatar_id":"","url":"https://api.github.com/users/trevan","html_url":"https://github.com/trevan","followers_url":"https://api.github.com/users/trevan/followers","following_url":"https://api.github.com/users/trevan/following{/other_user}","gists_url":"https://api.github.com/users/trevan/gists{/gist_id}","starred_url":"https://api.github.com/users/trevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/trevan/subscriptions","organizations_url":"https://api.github.com/users/trevan/orgs","repos_url":"https://api.github.com/users/trevan/repos","events_url":"https://api.github.com/users/trevan/events{/privacy}","received_events_url":"https://api.github.com/users/trevan/received_events","type":"User","site_admin":false},"created_at":"2017-04-26T16:08:10Z","updated_at":"2017-04-26T16:08:10Z","author_association":"NONE","body":"@jpountz, yeah, we've been working on that.  All of our most recent indices should be averaging around 40GB per shard.  It takes a while to combine older indices.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/316144264","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-316144264","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":316144264,"node_id":"MDEyOklzc3VlQ29tbWVudDMxNjE0NDI2NA==","user":{"login":"trevan","id":2044248,"node_id":"MDQ6VXNlcjIwNDQyNDg=","avatar_url":"https://avatars2.githubusercontent.com/u/2044248?v=4","gravatar_id":"","url":"https://api.github.com/users/trevan","html_url":"https://github.com/trevan","followers_url":"https://api.github.com/users/trevan/followers","following_url":"https://api.github.com/users/trevan/following{/other_user}","gists_url":"https://api.github.com/users/trevan/gists{/gist_id}","starred_url":"https://api.github.com/users/trevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/trevan/subscriptions","organizations_url":"https://api.github.com/users/trevan/orgs","repos_url":"https://api.github.com/users/trevan/repos","events_url":"https://api.github.com/users/trevan/events{/privacy}","received_events_url":"https://api.github.com/users/trevan/received_events","type":"User","site_admin":false},"created_at":"2017-07-18T17:54:23Z","updated_at":"2017-07-18T17:54:23Z","author_association":"NONE","body":"@s1monw, is the design for the better open / close mechanism in 5.x or 6.x?  We are trying to deal with opening/closing indices and handling rebalancing and node removal better.  That seems to be the only way to accomplish this unless there has been any more discussion about this?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/437485497","html_url":"https://github.com/elastic/elasticsearch/issues/23546#issuecomment-437485497","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23546","id":437485497,"node_id":"MDEyOklzc3VlQ29tbWVudDQzNzQ4NTQ5Nw==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2018-11-09T20:27:59Z","updated_at":"2018-11-09T20:27:59Z","author_association":"CONTRIBUTOR","body":"superseded by #34352 and #33888","performed_via_github_app":null}]