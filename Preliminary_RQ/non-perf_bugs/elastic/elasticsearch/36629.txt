{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/36629","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36629/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36629/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/36629/events","html_url":"https://github.com/elastic/elasticsearch/issues/36629","id":390991719,"node_id":"MDU6SXNzdWUzOTA5OTE3MTk=","number":36629,"title":"Check existing data for duplicate field docs in the migration assistance APIs and Migration Assistant","user":{"login":"geekpete","id":2070843,"node_id":"MDQ6VXNlcjIwNzA4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/2070843?v=4","gravatar_id":"","url":"https://api.github.com/users/geekpete","html_url":"https://github.com/geekpete","followers_url":"https://api.github.com/users/geekpete/followers","following_url":"https://api.github.com/users/geekpete/following{/other_user}","gists_url":"https://api.github.com/users/geekpete/gists{/gist_id}","starred_url":"https://api.github.com/users/geekpete/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/geekpete/subscriptions","organizations_url":"https://api.github.com/users/geekpete/orgs","repos_url":"https://api.github.com/users/geekpete/repos","events_url":"https://api.github.com/users/geekpete/events{/privacy}","received_events_url":"https://api.github.com/users/geekpete/received_events","type":"User","site_admin":false},"labels":[{"id":1142254209,"node_id":"MDU6TGFiZWwxMTQyMjU0MjA5","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Features/Features","name":":Core/Features/Features","color":"0e8a16","default":false,"description":"A catch-all label for :Core/Features that don't fit in any other bucket."},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2018-12-14T07:09:42Z","updated_at":"2019-01-02T23:36:54Z","closed_at":"2019-01-02T23:36:54Z","author_association":"MEMBER","active_lock_reason":null,"body":"**Describe the feature**:\r\n\r\nElasticsearch 5.x allows duplicate fields to be indexed into documents:\r\nhttps://github.com/elastic/elasticsearch/issues/19614\r\n\r\nThis was fixed in 6.x by enforcing strict duplicate validation:\r\nhttps://github.com/elastic/elasticsearch/pull/22073\r\n\r\n\r\nA user who upgrades a cluster containing docs with duplicate fields from 5.x to 6.x will currently have no warning that their data might become unusable (for doing any operations requiring json parsing, including `?pretty`, indexing, scripts) once upgraded to 6.x.\r\n\r\nThe error message example is:\r\n\r\n```\r\n\"caused_by\": { \"type\": \"json_parse_exception\",\r\n\"reason\" : \"Duplicate field 'FILECONTENT'\\n at [Source: org.elasticsearch.common.bytes.BytesReference$MarkSupportingStreamInputWrapper@5d3ea628; line: 99, column: 14]\"...\r\n```\r\n\r\n\r\nThis scenario can occur when a custom indexing client may be inadvertently creating documents with duplicate fields, so this might be seen as an edge case as well by the few reports of users hitting duplicate fields issues I've seen but when it does occur it's a bad situation.\r\n\r\nOnce upgraded to 6.x, there are limited options to repair the problem documents due to the inability to use reindex without hitting the duplicate field error (even with the escape hatch enabled to disable validation: `es.json.strict_duplicate_detection=false` as it doesn't disable validatin for reindex operations).\r\nFor this reason, repairs are probably best done before upgrading on the 5.x cluster.\r\n\r\nIt seems that a fix is either to update in place using the the existing _source (eg with update script `ctx._source = ctx._source;`)  or to reindex the documents which will create new documents without the duplicate fields:\r\n\r\nOne other thing to consider is that any scenario where the values for the duplicate field are different, then a more custom script or solution might be needed to be able to choose which value to keep, it gets messy then.\r\n\r\nSo users should be at least warned and if possible presented with either automatic or manual repair options.\r\n\r\n","closed_by":{"login":"geekpete","id":2070843,"node_id":"MDQ6VXNlcjIwNzA4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/2070843?v=4","gravatar_id":"","url":"https://api.github.com/users/geekpete","html_url":"https://github.com/geekpete","followers_url":"https://api.github.com/users/geekpete/followers","following_url":"https://api.github.com/users/geekpete/following{/other_user}","gists_url":"https://api.github.com/users/geekpete/gists{/gist_id}","starred_url":"https://api.github.com/users/geekpete/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/geekpete/subscriptions","organizations_url":"https://api.github.com/users/geekpete/orgs","repos_url":"https://api.github.com/users/geekpete/repos","events_url":"https://api.github.com/users/geekpete/events{/privacy}","received_events_url":"https://api.github.com/users/geekpete/received_events","type":"User","site_admin":false},"performed_via_github_app":null}