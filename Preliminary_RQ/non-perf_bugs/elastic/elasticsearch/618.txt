{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/618","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/618/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/618/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/618/events","html_url":"https://github.com/elastic/elasticsearch/issues/618","id":516219,"node_id":"MDU6SXNzdWU1MTYyMTk=","number":618,"title":"Long system-wide freezes, non-gc related","user":{"login":"mrflip","id":6128,"node_id":"MDQ6VXNlcjYxMjg=","avatar_url":"https://avatars0.githubusercontent.com/u/6128?v=4","gravatar_id":"","url":"https://api.github.com/users/mrflip","html_url":"https://github.com/mrflip","followers_url":"https://api.github.com/users/mrflip/followers","following_url":"https://api.github.com/users/mrflip/following{/other_user}","gists_url":"https://api.github.com/users/mrflip/gists{/gist_id}","starred_url":"https://api.github.com/users/mrflip/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mrflip/subscriptions","organizations_url":"https://api.github.com/users/mrflip/orgs","repos_url":"https://api.github.com/users/mrflip/repos","events_url":"https://api.github.com/users/mrflip/events{/privacy}","received_events_url":"https://api.github.com/users/mrflip/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2011-01-09T22:13:55Z","updated_at":"2011-01-27T05:45:19Z","closed_at":"2011-01-27T05:45:19Z","author_association":"NONE","active_lock_reason":null,"body":"Using Amazon EC2 17GB ram 2 core machines, ES heap size 12GB / index buffer 2GB, mlockall=true (and ulimit -l = unlimited), file handles 65k\n\nMachines will occasionally, and apropos of nothing, seize up for as long as 3 or 4 minutes.  This isn't a GC storm -- we see sequential-numbered ParNew sweeps bracketing the pauses, and they don't seem to correlate with high-memory conditions. A background process like \n    ifstat 30 &\nwill continue to give output (so there's /some/ network), but the shell is fully unresponsive and no new commands will launch. The logs show no behavior whatsoever: you'll see a line at say 1:41:02 and then the next one will be at 1:45:03.\n\nThis only happens while ES is under load. The whole cluster will block when one of these pauses happens.  I can't reproduce it at will, but it happens often enough to be a real problem.   We're beating on ES pretty hard -- under load typically means 25 writers trying to saturate the disk I/O of 16 data nodes, and with merge_factor at 30\n\nMy only ideas are:\n- an interaction with the VM\n- the system is being starved of some resource that's not making it into the logs.\n","closed_by":null,"performed_via_github_app":null}