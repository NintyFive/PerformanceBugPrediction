{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/43350","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43350/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43350/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/43350/events","html_url":"https://github.com/elastic/elasticsearch/issues/43350","id":457681806,"node_id":"MDU6SXNzdWU0NTc2ODE4MDY=","number":43350,"title":"Constraints to de-prioritize nodes from becoming shard allocation targets","user":{"login":"vigyasharma","id":869395,"node_id":"MDQ6VXNlcjg2OTM5NQ==","avatar_url":"https://avatars3.githubusercontent.com/u/869395?v=4","gravatar_id":"","url":"https://api.github.com/users/vigyasharma","html_url":"https://github.com/vigyasharma","followers_url":"https://api.github.com/users/vigyasharma/followers","following_url":"https://api.github.com/users/vigyasharma/following{/other_user}","gists_url":"https://api.github.com/users/vigyasharma/gists{/gist_id}","starred_url":"https://api.github.com/users/vigyasharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vigyasharma/subscriptions","organizations_url":"https://api.github.com/users/vigyasharma/orgs","repos_url":"https://api.github.com/users/vigyasharma/repos","events_url":"https://api.github.com/users/vigyasharma/events{/privacy}","received_events_url":"https://api.github.com/users/vigyasharma/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2019-06-18T20:31:34Z","updated_at":"2019-08-26T11:52:19Z","closed_at":"2019-08-26T11:52:18Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## The Problem\r\nIn clusters where each node has a reasonable number of shards, when a node is replaced, (or a small number of node(s) added); all shards of newly created indexes get allocated on the new empty node(s). It puts a lot of stress on the single (or few) new node, deteriorating overall cluster performance.\r\n\r\nThis happens because the `index-balance` factor in weight function is unable to offset `shard-balance` when other nodes are filled with shards. It causes the new node to always have minimum weight and get picked as the target for allocation (until the new node has approx. mean number of shards/node in the cluster).\r\n\r\nFurther, `index-balance` does kick in after the node is relatively filled (as compared to other nodes), and it moves out the recently allocated shards. Thus for the same shards, which are newly created and [usually getting indexing traffic](https://github.com/elastic/elasticsearch/issues/17213#issuecomment-199226605), we end up doing twice the work in allocation.\r\n\r\n\r\n### Current Workaround\r\nA work around for this is to set the `total-shards-per-node` index/cluster [setting](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-total-shards.html) , which limits shards of an index on a single node. \r\n\r\nThis however, is a hard limit enforced by Allocation Deciders. If breached on all nodes, the shards go unassigned causing yellow/red clusters. Configuring this setting requires careful calculation around number of nodes, and must be revised when the cluster is scaled down.\r\n\r\n\r\n## Proposal\r\nWe propose an *allocation constraint* mechanism, that de-prioritizes nodes from getting picked for allocation if they breach certain constraints. Whenever an allocation constraint is breached for a shard (or index) on a node, we add a high positive constant to the node's weight. This increased weight makes the node less preferable for allocating the shard. Unlike deciders, however, this is not a hard filter. If no other nodes are eligible to accept shards (say due to deciders like disk watermarks), the shard can still be allocated on nodes with breached constraints.\r\n\r\n![Constraint Based Weights - Step Function Diagram](https://user-images.githubusercontent.com/869395/59717122-434d3500-91cc-11e9-9b9c-75939cf1727a.jpg)\r\n\r\n\r\n### Index Shards Per Node Constraint\r\nThis constraint controls the number of shards of an index on a single node. `indexShardsPerNodeConstraint` is breached if the number of shards of an index allocated on a node, exceeds average shards per node for that index.\r\n\r\n```java\r\nlong expIndexShardsOnNode = node.numShards(index) + numAdditionalShards;\r\nlong allowedShardsPerNode = Math.round(Math.ceil(balancer.avgShardsPerNode(index)));\r\nboolean shardPerNodeLimitBreached = (expIndexShardsOnNode - allowedShardsPerNode) > 0;\r\n```\r\n\r\n\r\n`indexShardsPerNodeConstraint` getting breached, causes shards of the newly created index to get assigned to other nodes, thus preventing indexing hot spots. Post unassigned shard allocation, rebalance fills up the node with other index shards, without having to move out the already allocated shards. Since this does not prevent allocation, we do not run into unassigned shards due to breached constraints.\r\n\r\n\r\nThe allocator flow now becomes: \r\n```\r\ndeciders to filter out ineligible nodes -> constraints to de-prioritize certain nodes by increasing their weight -> node selection as allocation target.\r\n```\r\n\r\n## Extension\r\nThis framework can be extended with other rules by modeling them as boolean constraints. One possiible example is primary shard density on a node, as required by [issue #41543](https://github.com/elastic/elasticsearch/issues/41543). A constraint that requires `#primaries on a node <= avg-primaries on a node`, could prevent scenarios with all primaries on few nodes and all replicas on others.\r\n\r\n\r\n## Comparing Multiple Constraints\r\nFor every constraint breached, we add a high positive constant to the weight function. This is a step function approach where we consider all nodes on the lower step (lower weight) as more eligible than those on a higher step. The high positive constant ensures  that node weight doesn't go as high as the next step unless it breaches a constraint.\r\n\r\nSince the constant is added for every constraint breached, i.e. `c * HIGH_CONSTANT`, nodes with one constraints breached are preferred over nodes with two constraints breached and so on. All nodes with the same number of constraints breached simply resort back to the first part of weight function, which is based on shard count.\r\n\r\nWe could potentially keep different weights for different constraints with some sort of ranking among them. But this can quickly become a maintenance and extension nightmare. Adding new constraints will require going through every other constraint weight and deciding on the right weight and place in priority.\r\n\r\n\r\n### Next Steps\r\nIf this idea makes sense and seems like a useful add to the Elasticsearch community, we can raise a PR for these changes.\r\n\r\nThis change is targeted to solve problems listed in https://github.com/elastic/elasticsearch/issues/17213, https://github.com/elastic/elasticsearch/issues/37638, https://github.com/elastic/elasticsearch/issues/41543, https://github.com/elastic/elasticsearch/issues/12279, https://github.com/elastic/elasticsearch/issues/29437\r\n\r\n","closed_by":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"performed_via_github_app":null}