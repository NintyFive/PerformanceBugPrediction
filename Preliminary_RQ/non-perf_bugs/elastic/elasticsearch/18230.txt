{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18230/events","html_url":"https://github.com/elastic/elasticsearch/issues/18230","id":153930425,"node_id":"MDU6SXNzdWUxNTM5MzA0MjU=","number":18230,"title":"Elasticsearch nodes run into OOM during sustained ThreadPoolRejections","user":{"login":"mahdibh","id":602733,"node_id":"MDQ6VXNlcjYwMjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/602733?v=4","gravatar_id":"","url":"https://api.github.com/users/mahdibh","html_url":"https://github.com/mahdibh","followers_url":"https://api.github.com/users/mahdibh/followers","following_url":"https://api.github.com/users/mahdibh/following{/other_user}","gists_url":"https://api.github.com/users/mahdibh/gists{/gist_id}","starred_url":"https://api.github.com/users/mahdibh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahdibh/subscriptions","organizations_url":"https://api.github.com/users/mahdibh/orgs","repos_url":"https://api.github.com/users/mahdibh/repos","events_url":"https://api.github.com/users/mahdibh/events{/privacy}","received_events_url":"https://api.github.com/users/mahdibh/received_events","type":"User","site_admin":false},"labels":[{"id":144797810,"node_id":"MDU6TGFiZWwxNDQ3OTc4MTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Infra/Core","name":":Core/Infra/Core","color":"0e8a16","default":false,"description":"Core issues without another label"},{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":29,"created_at":"2016-05-10T06:07:20Z","updated_at":"2017-09-14T01:49:55Z","closed_at":"2016-07-05T08:45:11Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version**: 1.7.3\n**JVM version**: Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)\n**OS version**: Linux ip-XXXXXXX.ec2.internal 4.1.13-19.30.amzn1.x86_64 #1 SMP Fri Dec 11 03:42:10 -- UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n**Description of the problem including expected versus actual behavior**:\n3 data nodes (7GB heap), 3 dedicated master nodes.\n1 index with 128 primary shards, 2 replicas. Index has 600k documents.\nDefault configuration, with reduced search thread pool size (from default of 13 to 5 threads). We also changed the search queue size from 1000 to 999 (to verify that we could issue cluster level settings updates during the test).\n\n**Steps to reproduce**:\nWe were hitting this issue in production and were able to successfully reproduce it in a staging environment by doing the following:\n1. spin off 1000 threads running the following query concurrently (same query across all threads) in a tight loop\n\n`POST index_name/object/_search\n{\n  \"size\" : 5000,\n  \"query\" : {\n    \"filtered\" : {\n      \"query\" : {\n        \"filtered\" : {\n          \"query\" : {\n            \"query_string\" : {\n              \"query\" : \"(field1:value1 AND NOT field2:_PREFIX_*)\",\n              \"lowercase_expanded_terms\" : false\n            }\n          },\n          \"filter\" : {\n            \"and\" : {\n              \"filters\" : [ {\n                \"term\" : {\n                  \"field3\" : \"value2\"\n                }\n              }, {\n                \"term\" : {\n                  \"type\" : \"MTS\"\n                }\n              }, {\n                \"or\" : {\n                  \"filters\" : [ {\n                    \"term\" : {\n                      \"active\" : true\n                    }\n                  }, {\n                    \"and\" : {\n                      \"filters\" : [ {\n                        \"term\" : {\n                          \"active\" : false\n                        }\n                      }, {\n                        \"range\" : {\n                          \"lastActive\" : {\n                            \"from\" : 0,\n                            \"to\" : null,\n                            \"include_lower\" : true,\n                            \"include_upper\" : true\n                          },\n                          \"_cache\" : false\n                        }\n                      } ]\n                    }\n                  } ]\n                }\n              } ]\n            }\n          }\n        }\n      },\n      \"filter\" : {\n        \"missing\" : {\n          \"field\" : \"_deletedOnMs\",\n          \"null_value\" : true,\n          \"existence\" : true\n        }\n      }\n    }\n  },\n  \"version\" : false,\n  \"_source\" : {\n    \"includes\" : [ ],\n    \"excludes\" : [ \"_props\" ]\n  },\n  \"sort\" : [ {\n    \"createdOnMs\" : {\n      \"order\" : \"asc\"\n    }\n  } ]\n}`\n1. those queries will rapidly fill up the search queue and we get thread pool rejections.\n2. after 5mn, all 3 data nodes get into a zombie state (ie, OOM'ed) and get kicked out of the cluster\n\n**Provide logs (if relevant)**:\nthread dumps\nhttps://gist.github.com/mahdibh/025d7a909475c43f9154e661c3ef839f\nhttps://gist.github.com/mahdibh/b89a1d437467339a5c30eb416a96cfb5\n\nnode log\nhttps://gist.github.com/mahdibh/320b515788400fb1560f2da1b1f2897f\n\n**Notes**\nAll nodes in the cluster run into the same issue. We took a heap dump of one of the nodes when it was in this state. 41% of the shallow size of the heap is consumed by byte arrays. long[] comes next with 16% and then char[] with 8%. We could privately share the heap dump if it helps figuring out what's going on.\n\nWhen this happens, the clusters becomes useless. Only the master nodes respond to API calls. The head plugin shows a blank list of nodes.\n\nWe can easily reproduce this internally, if there is anything we can do to provide more details, please let us know.\n","closed_by":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"performed_via_github_app":null}