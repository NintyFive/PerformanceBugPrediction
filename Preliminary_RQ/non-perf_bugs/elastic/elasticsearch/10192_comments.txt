[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/89630647","html_url":"https://github.com/elastic/elasticsearch/issues/10192#issuecomment-89630647","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10192","id":89630647,"node_id":"MDEyOklzc3VlQ29tbWVudDg5NjMwNjQ3","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-04-04T17:57:48Z","updated_at":"2015-04-04T17:57:48Z","author_association":"CONTRIBUTOR","body":"Duplicate of #9466. Closing in favour of #8909\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/92785773","html_url":"https://github.com/elastic/elasticsearch/issues/10192#issuecomment-92785773","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10192","id":92785773,"node_id":"MDEyOklzc3VlQ29tbWVudDkyNzg1Nzcz","user":{"login":"missinglink","id":738069,"node_id":"MDQ6VXNlcjczODA2OQ==","avatar_url":"https://avatars2.githubusercontent.com/u/738069?v=4","gravatar_id":"","url":"https://api.github.com/users/missinglink","html_url":"https://github.com/missinglink","followers_url":"https://api.github.com/users/missinglink/followers","following_url":"https://api.github.com/users/missinglink/following{/other_user}","gists_url":"https://api.github.com/users/missinglink/gists{/gist_id}","starred_url":"https://api.github.com/users/missinglink/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/missinglink/subscriptions","organizations_url":"https://api.github.com/users/missinglink/orgs","repos_url":"https://api.github.com/users/missinglink/repos","events_url":"https://api.github.com/users/missinglink/events{/privacy}","received_events_url":"https://api.github.com/users/missinglink/received_events","type":"User","site_admin":false},"created_at":"2015-04-14T12:11:18Z","updated_at":"2015-04-14T12:11:18Z","author_association":"CONTRIBUTOR","body":"Sorry, late to the party on this one, I think you'll find that this is not a duplicate of #9466 but instead related to how your TokenStream expands synonyms.\n\nFor example, if you have a string which has ~26 tokens and they are expanded x10 times by synonym expansion or another method you get to a point where you have over 256 finite strings and :boom:\n\nWe have also encountered this issue in https://github.com/pelias/pelias/issues/33 and you should be able to mask/fix the error by increasing your `max_token_length` setting for the relevant analyzer.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/94443533","html_url":"https://github.com/elastic/elasticsearch/issues/10192#issuecomment-94443533","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10192","id":94443533,"node_id":"MDEyOklzc3VlQ29tbWVudDk0NDQzNTMz","user":{"login":"maximTarleckiy","id":6791248,"node_id":"MDQ6VXNlcjY3OTEyNDg=","avatar_url":"https://avatars2.githubusercontent.com/u/6791248?v=4","gravatar_id":"","url":"https://api.github.com/users/maximTarleckiy","html_url":"https://github.com/maximTarleckiy","followers_url":"https://api.github.com/users/maximTarleckiy/followers","following_url":"https://api.github.com/users/maximTarleckiy/following{/other_user}","gists_url":"https://api.github.com/users/maximTarleckiy/gists{/gist_id}","starred_url":"https://api.github.com/users/maximTarleckiy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/maximTarleckiy/subscriptions","organizations_url":"https://api.github.com/users/maximTarleckiy/orgs","repos_url":"https://api.github.com/users/maximTarleckiy/repos","events_url":"https://api.github.com/users/maximTarleckiy/events{/privacy}","received_events_url":"https://api.github.com/users/maximTarleckiy/received_events","type":"User","site_admin":false},"created_at":"2015-04-20T12:48:49Z","updated_at":"2015-04-20T14:29:37Z","author_association":"NONE","body":"clintongormley\nHow it can be related to #9466? I have not context here.\nAlso I have this error with context, but I indexed context < 256 value per document (max 111)!\nBut context have two keys:\ncontext:\n                                category_id:\n                                    type: category\n                                    default: \"default\"\n                                department_id:\n                                    type: category\n                                    default: \"default\"\ncategory_id has 111 values\ndepartment_id has 10 values\n\nmissinglink\nIf I ever remove 'op_synonyms' from token filter this error still appears.\n","performed_via_github_app":null}]