{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/20897","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20897/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20897/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20897/events","html_url":"https://github.com/elastic/elasticsearch/issues/20897","id":182627944,"node_id":"MDU6SXNzdWUxODI2Mjc5NDQ=","number":20897,"title":"New Feature Req - Throttling write speed in ES-Hadoop Connector","user":{"login":"kamaldsingh","id":15106140,"node_id":"MDQ6VXNlcjE1MTA2MTQw","avatar_url":"https://avatars2.githubusercontent.com/u/15106140?v=4","gravatar_id":"","url":"https://api.github.com/users/kamaldsingh","html_url":"https://github.com/kamaldsingh","followers_url":"https://api.github.com/users/kamaldsingh/followers","following_url":"https://api.github.com/users/kamaldsingh/following{/other_user}","gists_url":"https://api.github.com/users/kamaldsingh/gists{/gist_id}","starred_url":"https://api.github.com/users/kamaldsingh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kamaldsingh/subscriptions","organizations_url":"https://api.github.com/users/kamaldsingh/orgs","repos_url":"https://api.github.com/users/kamaldsingh/repos","events_url":"https://api.github.com/users/kamaldsingh/events{/privacy}","received_events_url":"https://api.github.com/users/kamaldsingh/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2016-10-12T20:37:54Z","updated_at":"2016-10-12T22:26:51Z","closed_at":"2016-10-12T22:26:51Z","author_association":"NONE","active_lock_reason":null,"body":"Hello,\n\nI was trying out ES - hadoop connector and I see that in case of too many parallel writes, like from a spark job, writes keep on getting dropped. \norg.elasticsearch.hadoop.EsHadoopException: Could not write all entries (maybe ES was overloaded?). Bailing out...\n\nThe workaround is to decrease number of partitions and tune the batch size and timeouts accordingly.\n\nAs mentioned in https://discuss.elastic.co/t/pushback-to-hadoop-from-es-on-bulk-load/1535/5 there's no bi-directional communication between Hadoop and the connector - the connector cannot say, there's too much data, slow down.\n\nDoes anyone think it might be a good idea to use sth. like Blocking Queues here and add acks while writing (kafka) so as to let the consumer (thread on ES) read at slower pace.\n\nElse we would have to tune the batch size, write speed, http timeouts ourself.\n\nAny other suggestions are more than welcome.\n\nI'm open to building/contributing to this its a good idea.\n\nhttps://discuss.elastic.co/t/throttling-write-speed-in-es-hadoop-connector/62528/1\n","closed_by":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"performed_via_github_app":null}