{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488/events","html_url":"https://github.com/elastic/elasticsearch/issues/29488","id":313534395,"node_id":"MDU6SXNzdWUzMTM1MzQzOTU=","number":29488,"title":"Index translogs can get stuck after node failure, doesn't get smaller when flushed anymore","user":{"login":"danopia","id":40628,"node_id":"MDQ6VXNlcjQwNjI4","avatar_url":"https://avatars2.githubusercontent.com/u/40628?v=4","gravatar_id":"","url":"https://api.github.com/users/danopia","html_url":"https://github.com/danopia","followers_url":"https://api.github.com/users/danopia/followers","following_url":"https://api.github.com/users/danopia/following{/other_user}","gists_url":"https://api.github.com/users/danopia/gists{/gist_id}","starred_url":"https://api.github.com/users/danopia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danopia/subscriptions","organizations_url":"https://api.github.com/users/danopia/orgs","repos_url":"https://api.github.com/users/danopia/repos","events_url":"https://api.github.com/users/danopia/events{/privacy}","received_events_url":"https://api.github.com/users/danopia/received_events","type":"User","site_admin":false},"labels":[{"id":836542781,"node_id":"MDU6TGFiZWw4MzY1NDI3ODE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Engine","name":":Distributed/Engine","color":"0e8a16","default":false,"description":"Anything around managing Lucene and the Translog in an open shard."},{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":16,"created_at":"2018-04-12T00:23:25Z","updated_at":"2019-01-25T07:46:07Z","closed_at":"2018-05-08T01:31:43Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version**: 6.1.2, from https://artifacts.elastic.co/packages/6.x/yum\r\n\r\n**Plugins installed**: discovery-ec2, repository-s3\r\n\r\n**JVM version**:\r\n\r\n```\r\njava version \"1.8.0_162\"\r\nJava(TM) SE Runtime Environment (build 1.8.0_162-b12)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.162-b12, mixed mode)\r\n```\r\n\r\n**OS version**: Amazon Linux 2. `Linux ip-10-2-45-153.ec2.internal 4.9.76-38.79.amzn2.x86_64 #1 SMP Mon Jan 15 23:35:15 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n**Description of the problem including expected versus actual behavior**: When a cluster isn't green, translogs have a tendency of not getting trimmed anymore. Flushes sure work, and the `uncommitted_size` of the translog goes down, but the whole translog sits there reaching into hundreds of gigabytes. I've tried the _flush API, the synced-flush API, and really any API I can find. **Expected behavior**: the translog size should match the uncommitted_size after a flush, unless there is an ongoing recovery on that shard.\r\n\r\nRelevant stats from an index I haven't written to in at least 12 hours:\r\n\r\n```\r\n      \"store\": {\r\n        \"size\": \"110.1gb\",\r\n        \"size_in_bytes\": 118257807651\r\n      },\r\n      \"translog\": {\r\n        \"operations\": 79068513,\r\n        \"size\": \"214.5gb\",\r\n        \"size_in_bytes\": 230343762719,\r\n        \"uncommitted_operations\": 0,\r\n        \"uncommitted_size\": \"387b\",\r\n        \"uncommitted_size_in_bytes\": 387\r\n      },\r\n      \"recovery\": {\r\n        \"current_as_source\": 0,\r\n        \"current_as_target\": 0,\r\n        \"throttle_time\": \"7.7s\",\r\n        \"throttle_time_in_millis\": 7719\r\n      }\r\n```\r\n\r\nFull stats from that index: https://gist.github.com/danopia/7390f5bbb898bfe4c2b3709a503539ec\r\n\r\n**Steps to reproduce**:\r\n\r\nPretty hard for this issue (I haven't _tried_ to cause this problem) but here's some general steps.\r\n\r\n1. Set up an EC2 cluster and index a lot of data into indexes with these settings. I indexed a few dozen terabytes, 50-200GB per shard.\r\n\r\n```\r\n      \"index\": {\r\n        \"refresh_interval\": \"60s\",\r\n        \"number_of_shards\": \"18\",\r\n        \"translog\": {\r\n          \"flush_threshold_size\": \"5gb\"\r\n        },\r\n        \"number_of_replicas\": \"1\",\r\n      }\r\n```\r\n\r\n2. Wait for an Elasticsearch node to die. I had two clusters exhibit this behavior, one initially caused by ES running out of file descriptors (since been addressed), the second was EC2-level instance failure.\r\n\r\n3. Create a new fully-allocated index and continue writing into it.\r\n\r\n3. Observe that while the existing data recovery is ongoing, the new indexes can get translogs that grow unbounded.\r\n\r\n4. Once an index has a translog in this state, no combination of API calls can fix it.\r\n\r\nThe worst part about the stuck translogs is even after I've moved on from indexing into those indexes, they're still in the cluster and take AGES to recover. I'm talking multiple hours per shard recovery because the entire translog must be read in order to recover from existing store, and then read AGAIN to recover a replica. So I'm not aware of any options other than deleting the impacted indexes or living with that.\r\n\r\n---\r\n\r\nSome impacted indexes have \"Too many open files\" logs, others have zero mentions in ES errors. So it's not a requirement but I'm including some such log lines for completeness.\r\n\r\n```\r\n[2018-04-11T11:15:30,421][WARN ][o.e.i.s.IndexShard       ] [i-0352bafb7594c1b75] [events-2018.04.10b][3] failed to flush index\r\n[2018-04-11T18:26:16,055][WARN ][o.e.i.c.IndicesClusterStateService] [i-0c1f657db93d5c6e3] [[events-2018.04.10b][6]] marking and sending shard failed due to [shard failure, reason [lucene commit failed]]\r\n[2018-04-11T18:26:16,056][WARN ][o.e.c.a.s.ShardStateAction] [i-07d34a9aad3760a6c] [events-2018.04.10b][6] received shard failed for shard id [[events-2018.04.10b][6]], allocation id [OWI1rj9WRUiRsHg2cRy96Q], primary term [0], message [shard failure, reason [lucene commit failed]], failure [FileSystemException[/mnt/data/nodes/0/indices/cpYZBHd3TSSMy4qxZXKEHA/6/index/_3vx.cfs: Too many open files]]\r\n```\r\n\r\n---\r\n\r\nWith this issue I'm looking for reasons why I can't fix the translogs once they grow unbounded. I'm not entirely sure on the root cause of the unbounded growth, and it's less important as long as I can fix it after... but the root cause really seems to be related to recoveries and the amount of file descriptors that recoveries require.\r\n\r\nHere's a graph of a totally healthy cluster getting hit by an EC2 failure, and then spiraling into craziness. Wednesday around 12:00 I performed a full-cluster restart to 10x the file descriptor limit.\r\n\r\n![screenshot 2018-04-11 at 5 31 01 pm](https://user-images.githubusercontent.com/40628/38650014-6ad5f756-3dae-11e8-9edb-aa1121a984f2.png)\r\n","closed_by":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"performed_via_github_app":null}