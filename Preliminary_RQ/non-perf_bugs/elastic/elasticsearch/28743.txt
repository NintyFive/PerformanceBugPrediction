{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/28743","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28743/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28743/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28743/events","html_url":"https://github.com/elastic/elasticsearch/issues/28743","id":298554558,"node_id":"MDU6SXNzdWUyOTg1NTQ1NTg=","number":28743,"title":"Data node with spinning disk crashes (OutOfMemoryError)","user":{"login":"redlus","id":7827282,"node_id":"MDQ6VXNlcjc4MjcyODI=","avatar_url":"https://avatars3.githubusercontent.com/u/7827282?v=4","gravatar_id":"","url":"https://api.github.com/users/redlus","html_url":"https://github.com/redlus","followers_url":"https://api.github.com/users/redlus/followers","following_url":"https://api.github.com/users/redlus/following{/other_user}","gists_url":"https://api.github.com/users/redlus/gists{/gist_id}","starred_url":"https://api.github.com/users/redlus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/redlus/subscriptions","organizations_url":"https://api.github.com/users/redlus/orgs","repos_url":"https://api.github.com/users/redlus/repos","events_url":"https://api.github.com/users/redlus/events{/privacy}","received_events_url":"https://api.github.com/users/redlus/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"},{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null},{"id":538455989,"node_id":"MDU6TGFiZWw1Mzg0NTU5ODk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/v5.2.2","name":"v5.2.2","color":"dddddd","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":24,"created_at":"2018-02-20T11:10:41Z","updated_at":"2018-02-22T11:28:29Z","closed_at":"2018-02-21T18:33:21Z","author_association":"NONE","active_lock_reason":null,"body":"Hi everyone.\r\n\r\nWe're experiencing a critical production issue when trying to utilize cold-nodes architecture (moving indices from SSD disk nodes to HDD disk nodes). The following the is the full report:\r\n\r\n<!-- Bug report -->\r\n\r\n**Elasticsearch version** (`bin/elasticsearch --version`):\r\n5.2.2\r\n\r\n**Plugins installed**: []\r\ningest-attachment\r\ningest-geoip\r\nmapper-murmur3\r\nmapper-size\r\nrepository-azure\r\nrepository-gcs\r\nrepository-s3\r\nx-pack\r\n\r\n**JVM version** (`java -version`):\r\nopenjdk version \"1.8.0_151\"\r\nOpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12)\r\nOpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)\r\n\r\n**OS version** (`uname -a` if on a Unix-like system):\r\nLinux 4.4.0-47-generic #68-Ubuntu SMP 2016 x86_64 GNU/Linux\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\nOur cluster has fast, local SSD disks nodes for indexing new data, and slower spinning SSD disks for older data. esdatacold nodes specification:\r\n8 cores\r\n32GB of ram (16GB allocated for es heap)\r\n10TB of disks (5 *spinning* disks, 2TB each formatted using GPT and ext4)\r\n~700 shards / 3TB of data used on each node\r\n\r\nOlder indices are backed-up using a snapshot to cold storage, and then we change their settings:\r\n```\r\n{\r\n    \"index.routing.allocation.include._name\": \"esdatacold*\",\r\n    \"index.routing.allocation.total_shards_per_node\": \"-1\"\r\n}\r\n```\r\n\r\nAdditionally, to minimize the amount of work each esdatacold handling consecutively, we've reduced cluster.routing.allocation.node_concurrent_recoveries to 1 (moving only 1 shard per node into the esdatacold nodes).\r\nHowever, the elasticsearch process in these nodes crashes with an OutOfMemory exception. Looking at the stacktrace, it seems that all threads are waiting for a lock to be released. This is the full stacktrace of the process: https://pastebin.com/6q8GtSeg\r\n\r\nI can understand if an overloaded node (e.g. high number of shards per GB heap or something similar) performs poorly, but a crash has severe performance effects on the whole cluster, and this should definitely be protected against.\r\n\r\nIs there anything we can do to prevent this crash?\r\nIs there any memory configuration we're missing?\r\n\r\n**Steps to reproduce**:\r\n\r\nPlease include a *minimal* but *complete* recreation of the problem, including\r\n(e.g.) index creation, mappings, settings, query etc.  The easier you make for\r\nus to reproduce it, the more likely that somebody will take the time to look at it.\r\n\r\nNone yet. This is a production-scale issue, so it is not easy to reproduce.\r\n\r\nI'd be happy to provide additional details, whatever is needed.\r\n\r\nThanks!\r\n","closed_by":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"performed_via_github_app":null}