{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/15835","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15835/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15835/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15835/events","html_url":"https://github.com/elastic/elasticsearch/issues/15835","id":125426202,"node_id":"MDU6SXNzdWUxMjU0MjYyMDI=","number":15835,"title":"When using a whitespace tokenizer the stop words filter doesn't work","user":{"login":"imranazad","id":1709630,"node_id":"MDQ6VXNlcjE3MDk2MzA=","avatar_url":"https://avatars1.githubusercontent.com/u/1709630?v=4","gravatar_id":"","url":"https://api.github.com/users/imranazad","html_url":"https://github.com/imranazad","followers_url":"https://api.github.com/users/imranazad/followers","following_url":"https://api.github.com/users/imranazad/following{/other_user}","gists_url":"https://api.github.com/users/imranazad/gists{/gist_id}","starred_url":"https://api.github.com/users/imranazad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imranazad/subscriptions","organizations_url":"https://api.github.com/users/imranazad/orgs","repos_url":"https://api.github.com/users/imranazad/repos","events_url":"https://api.github.com/users/imranazad/events{/privacy}","received_events_url":"https://api.github.com/users/imranazad/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2016-01-07T16:19:48Z","updated_at":"2016-01-08T17:40:35Z","closed_at":"2016-01-07T16:22:53Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"When using the whitespace tokenizer, the stop words filter doesn't work. Here are the CURL commands to replicate:\n\n```\n        PUT /my_index\n        {\n          \"settings\": {\n            \"index\": {\n              \"number_of_shards\": 1,\n              \"analysis\": {\n                  \"analyzer\": {\n                     \"fulltext\":{\n                    \"type\":\"custom\",\n                    \"tokenizer\":\"whitespace\",\n                    \"filter\": [\"english_stop\"]\n                  }\n                  },\n                 \"filter\":{\n                    \"english_stop\":{\n                       \"type\":\"stop\",\n                       \"stopwords\":\"_english_\"\n                    }\n                  }\n                 }\n              }\n            }\n          }\n        }\n```\n\nI need to be able to use the whitespace tokenizer because I'm also using the word_delim filter which turns terms like \"wi-fi\" to wifi, if I use the standard tokenizer, I will lose this ability.\n","closed_by":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"performed_via_github_app":null}