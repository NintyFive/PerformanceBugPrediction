{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/52146","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/52146/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/52146/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/52146/events","html_url":"https://github.com/elastic/elasticsearch/issues/52146","id":562565487,"node_id":"MDU6SXNzdWU1NjI1NjU0ODc=","number":52146,"title":"Long-running InternalEngine#recoverFromTranslog can block cluster state application","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"labels":[{"id":836542781,"node_id":"MDU6TGFiZWw4MzY1NDI3ODE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Engine","name":":Distributed/Engine","color":"0e8a16","default":false,"description":"Anything around managing Lucene and the Translog in an open shard."},{"id":152510590,"node_id":"MDU6TGFiZWwxNTI1MTA1OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Recovery","name":":Distributed/Recovery","color":"0e8a16","default":false,"description":"Anything around constructing a new shard, either from a local or a remote source."},{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2020-02-10T13:36:30Z","updated_at":"2020-03-02T13:50:42Z","closed_at":"2020-03-02T13:50:42Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I saw a 7.5.0 cluster with a node that was failing to apply cluster state updates due to the following chain of locks:\r\n\r\n```\r\n\"elasticsearch[REDACTED][clusterApplierService#updateTask][T#1]\" ... waiting for monitor entry\r\n   java.lang.Thread.State: BLOCKED (on object monitor)\r\n  at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyClusterState(IndicesClusterStateService.java:235)\r\n  - waiting to lock <0x0000000116e271c0> (a org.elasticsearch.indices.cluster.IndicesClusterStateService)\r\n  at org.elasticsearch.cluster.service.ClusterApplierService.lambda$callClusterStateAppliers$5(ClusterApplierService.java:517)\r\n  at org.elasticsearch.cluster.service.ClusterApplierService$$Lambda$4315/0x00000008019c6840.accept(Unknown Source)\r\n  at java.lang.Iterable.forEach(java.base@13.0.1/Iterable.java:75)\r\n  at org.elasticsearch.cluster.service.ClusterApplierService.callClusterStateAppliers(ClusterApplierService.java:514)\r\n  at org.elasticsearch.cluster.service.ClusterApplierService.applyChanges(ClusterApplierService.java:485)\r\n  at org.elasticsearch.cluster.service.ClusterApplierService.runTask(ClusterApplierService.java:432)\r\n  at org.elasticsearch.cluster.service.ClusterApplierService.access$100(ClusterApplierService.java:73)\r\n  at org.elasticsearch.cluster.service.ClusterApplierService$UpdateTask.run(ClusterApplierService.java:176)\r\n  at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:703)\r\n  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\r\n  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@13.0.1/ThreadPoolExecutor.java:1128)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@13.0.1/ThreadPoolExecutor.java:628)\r\n  at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)\r\n\r\n\"elasticsearch[REDACTED][generic][T#9]\" ... waiting on condition\r\n   java.lang.Thread.State: WAITING (parking)\r\n  at jdk.internal.misc.Unsafe.park(java.base@13.0.1/Native Method)\r\n  - parking to wait for  <0x0000000127303a98> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)\r\n  at java.util.concurrent.locks.LockSupport.park(java.base@13.0.1/LockSupport.java:194)\r\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(java.base@13.0.1/AbstractQueuedSynchronizer.java:885)\r\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(java.base@13.0.1/AbstractQueuedSynchronizer.java:917)\r\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(java.base@13.0.1/AbstractQueuedSynchronizer.java:1240)\r\n  at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(java.base@13.0.1/ReentrantReadWriteLock.java:959)\r\n  at org.elasticsearch.common.util.concurrent.ReleasableLock.acquire(ReleasableLock.java:55)\r\n  at org.elasticsearch.index.engine.Engine.close(Engine.java:1729)\r\n  at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:104)\r\n  at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:62)\r\n  at org.elasticsearch.index.shard.IndexShard.close(IndexShard.java:1333)\r\n  - locked <0x0000000127371ce8> (a java.lang.Object)\r\n  at org.elasticsearch.index.IndexService.closeShard(IndexService.java:485)\r\n  at org.elasticsearch.index.IndexService.removeShard(IndexService.java:468)\r\n  - locked <0x0000000127f05d68> (a org.elasticsearch.index.IndexService)\r\n  at org.elasticsearch.indices.cluster.IndicesClusterStateService.failAndRemoveShard(IndicesClusterStateService.java:729)\r\n  at org.elasticsearch.indices.cluster.IndicesClusterStateService.handleRecoveryFailure(IndicesClusterStateService.java:721)\r\n  - locked <0x0000000116e271c0> (a org.elasticsearch.indices.cluster.IndicesClusterStateService)\r\n  at org.elasticsearch.indices.cluster.IndicesClusterStateService.access$500(IndicesClusterStateService.java:101)\r\n  at org.elasticsearch.indices.cluster.IndicesClusterStateService$RecoveryListener.onRecoveryFailure(IndicesClusterStateService.java:716)\r\n  at org.elasticsearch.indices.recovery.RecoveryTarget.notifyListener(RecoveryTarget.java:238)\r\n  at org.elasticsearch.indices.recovery.RecoveryTarget.fail(RecoveryTarget.java:225)\r\n  at org.elasticsearch.indices.recovery.RecoveriesCollection.failRecovery(RecoveriesCollection.java:177)\r\n  at org.elasticsearch.indices.recovery.RecoveriesCollection$RecoveryMonitor.doRun(RecoveriesCollection.java:281)\r\n  at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:773)\r\n  at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@13.0.1/ThreadPoolExecutor.java:1128)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@13.0.1/ThreadPoolExecutor.java:628)\r\n  at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)\r\n\r\n\"elasticsearch[REDACTED][generic][T#17]\"\r\n   java.lang.Thread.State: RUNNABLE\r\n  at org.apache.lucene.index.FrozenBufferedUpdates.applyDocValuesUpdates(FrozenBufferedUpdates.java:568)\r\n  at org.apache.lucene.index.FrozenBufferedUpdates.applyDocValuesUpdates(FrozenBufferedUpdates.java:451)\r\n  at org.apache.lucene.index.FrozenBufferedUpdates.apply(FrozenBufferedUpdates.java:421)\r\n  at org.apache.lucene.index.FrozenBufferedUpdates.forceApply(FrozenBufferedUpdates.java:249)\r\n  at org.apache.lucene.index.FrozenBufferedUpdates.tryApply(FrozenBufferedUpdates.java:159)\r\n  at org.apache.lucene.index.IndexWriter.lambda$publishFrozenUpdates$3(IndexWriter.java:2592)\r\n  at org.apache.lucene.index.IndexWriter$$Lambda$5396/0x0000000801c78040.process(Unknown Source)\r\n  at org.apache.lucene.index.IndexWriter.processEvents(IndexWriter.java:5116)\r\n  at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1597)\r\n  at org.apache.lucene.index.IndexWriter.softUpdateDocument(IndexWriter.java:1654)\r\n  at org.elasticsearch.index.engine.InternalEngine.updateDocs(InternalEngine.java:1248)\r\n  at org.elasticsearch.index.engine.InternalEngine.indexIntoLucene(InternalEngine.java:1079)\r\n  at org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:923)\r\n  at org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:796)\r\n  at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:768)\r\n  at org.elasticsearch.index.shard.IndexShard.applyTranslogOperation(IndexShard.java:1491)\r\n  at org.elasticsearch.index.shard.IndexShard.runTranslogRecovery(IndexShard.java:1522)\r\n  at org.elasticsearch.index.shard.IndexShard.lambda$recoverLocallyUpToGlobalCheckpoint$6(IndexShard.java:1421)\r\n  at org.elasticsearch.index.shard.IndexShard$$Lambda$5353/0x0000000801c58c40.run(Unknown Source)\r\n  at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslogInternal(InternalEngine.java:474)\r\n  at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:446)\r\n  at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:124)\r\n  at org.elasticsearch.index.shard.IndexShard.recoverLocallyUpToGlobalCheckpoint(IndexShard.java:1427)\r\n  at org.elasticsearch.indices.recovery.PeerRecoveryTargetService.doRecovery(PeerRecoveryTargetService.java:178)\r\n  at org.elasticsearch.indices.recovery.PeerRecoveryTargetService.access$500(PeerRecoveryTargetService.java:79)\r\n  at org.elasticsearch.indices.recovery.PeerRecoveryTargetService$RecoveryRunner.doRun(PeerRecoveryTargetService.java:563)\r\n  at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:773)\r\n  at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@13.0.1/ThreadPoolExecutor.java:1128)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@13.0.1/ThreadPoolExecutor.java:628)\r\n  at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)\r\n```\r\n\r\nThis third thread holds `Engine#readLock` (not mentioned by `jstack` for some reason) in `InternalEngine.recoverFromTranslog` and holds it until we've finished replaying the local translog. This means the second thread cannot cancel the recovery in a timely fashion during this phase of the recovery.\r\n\r\n","closed_by":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"performed_via_github_app":null}