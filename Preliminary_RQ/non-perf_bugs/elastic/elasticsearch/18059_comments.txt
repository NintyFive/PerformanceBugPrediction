[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/215682964","html_url":"https://github.com/elastic/elasticsearch/issues/18059#issuecomment-215682964","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18059","id":215682964,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNTY4Mjk2NA==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-04-29T10:50:34Z","updated_at":"2016-04-29T10:50:34Z","author_association":"CONTRIBUTOR","body":"> can't there be dedicated queue for reindex?\n\nIt is possible but I think that'd be difficult. Reindex already has logic to backoff and retry when the bulk queue is full. Applying that logic to the search and scroll portions would be much simpler by comparison and ought to work for this. Would you mind if I repurposed this issue to track that?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/215709451","html_url":"https://github.com/elastic/elasticsearch/issues/18059#issuecomment-215709451","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18059","id":215709451,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNTcwOTQ1MQ==","user":{"login":"jrots","id":195346,"node_id":"MDQ6VXNlcjE5NTM0Ng==","avatar_url":"https://avatars1.githubusercontent.com/u/195346?v=4","gravatar_id":"","url":"https://api.github.com/users/jrots","html_url":"https://github.com/jrots","followers_url":"https://api.github.com/users/jrots/followers","following_url":"https://api.github.com/users/jrots/following{/other_user}","gists_url":"https://api.github.com/users/jrots/gists{/gist_id}","starred_url":"https://api.github.com/users/jrots/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jrots/subscriptions","organizations_url":"https://api.github.com/users/jrots/orgs","repos_url":"https://api.github.com/users/jrots/repos","events_url":"https://api.github.com/users/jrots/events{/privacy}","received_events_url":"https://api.github.com/users/jrots/received_events","type":"User","site_admin":false},"created_at":"2016-04-29T13:22:31Z","updated_at":"2016-04-29T13:22:52Z","author_association":"NONE","body":"Sure, retry logic for searches sounds good too \nThink also having the ability to bump the search/ scroll limit to f.e. 15000 instead of default 100 would help to avoid having these issues.\n","performed_via_github_app":null}]