[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164964616","html_url":"https://github.com/elastic/elasticsearch/issues/15465#issuecomment-164964616","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15465","id":164964616,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDk2NDYxNg==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2015-12-16T02:15:17Z","updated_at":"2015-12-16T02:20:09Z","author_association":"MEMBER","body":"> single node clusters\n\nFrom the bulk thread pool having `pool size = 2` (in your second exception message) we can deduce that this a very small node, a system presenting itself as having two cores it appears?\n\n> indexing into it using 16 executors\n\nFrom your description, I'm assuming that these are all operating concurrently. This is basically the perfect setup for an overwhelmed node.\n\nOn 1.3.4:\n\n> `maybe ES was overloaded?`\n\nOn 2.1.0:\n\n> `Too Many Requests(429) - [rejected execution... queue capacity = 50...queued tasks = 50...]`\n\nIn both cases, the exception messages are telling you that Elasticsearch can't keep up, and this doesn't strike me as surprising at all given that you have sixteen workers and two consumers. Elasticsearch is trying to apply backpressure to the job because its bulk queue is stuffed.\n\nGiven this, I don't see an issue to be addressed.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/165018485","html_url":"https://github.com/elastic/elasticsearch/issues/15465#issuecomment-165018485","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15465","id":165018485,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTAxODQ4NQ==","user":{"login":"hronik1","id":1770336,"node_id":"MDQ6VXNlcjE3NzAzMzY=","avatar_url":"https://avatars0.githubusercontent.com/u/1770336?v=4","gravatar_id":"","url":"https://api.github.com/users/hronik1","html_url":"https://github.com/hronik1","followers_url":"https://api.github.com/users/hronik1/followers","following_url":"https://api.github.com/users/hronik1/following{/other_user}","gists_url":"https://api.github.com/users/hronik1/gists{/gist_id}","starred_url":"https://api.github.com/users/hronik1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hronik1/subscriptions","organizations_url":"https://api.github.com/users/hronik1/orgs","repos_url":"https://api.github.com/users/hronik1/repos","events_url":"https://api.github.com/users/hronik1/events{/privacy}","received_events_url":"https://api.github.com/users/hronik1/received_events","type":"User","site_admin":false},"created_at":"2015-12-16T07:06:43Z","updated_at":"2015-12-16T07:06:43Z","author_association":"NONE","body":"Jason yes, as I pointed out earlier in the post, the purpose is to test how\nthe cluster handles when it is overwhelmed.\n\nI am aware that those messages are pointing out the fact the node is being\noverwhelmed, that is not my point of confusion. The point I am trying to\nprove is that this appears to be a regression, since 1.3.4 consistently\nappears to be doing a much better job at successfully applying back\npressure and indexing data into a node when in an overwhelmed stated, while\n2.1.0 appears to immediately just fall over.\n\nOn Tue, Dec 15, 2015 at 6:16 PM, Jason Tedor notifications@github.com\nwrote:\n\n> Closed #15465\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_elastic_elasticsearch_issues_15465&d=BQMCaQ&c=8hUWFZcy2Z-Za5rBPlktOQ&r=MXoI1K0dgBQpc4jGuqgr9Y3z8gHgOxW8KepkbfZ_AUQ&m=c72KgSl49JqX8kvbhBn_uRhB-H40_UuYIUd3BLYX564&s=s9vXNlje2trC2po-JWNS6rlMASznP6b1bVimaOLkPT8&e=\n> .\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_elastic_elasticsearch_issues_15465-23event-2D492985248&d=BQMCaQ&c=8hUWFZcy2Z-Za5rBPlktOQ&r=MXoI1K0dgBQpc4jGuqgr9Y3z8gHgOxW8KepkbfZ_AUQ&m=c72KgSl49JqX8kvbhBn_uRhB-H40_UuYIUd3BLYX564&s=nVmA2fUSBd8Ppbo10FMgBFehs4FQUIDw72mKikHbTYk&e=\n> .\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/165081429","html_url":"https://github.com/elastic/elasticsearch/issues/15465#issuecomment-165081429","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15465","id":165081429,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTA4MTQyOQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-12-16T11:50:12Z","updated_at":"2015-12-16T11:50:12Z","author_association":"CONTRIBUTOR","body":"It's not falling over. It is rejecting requests (telling the client to back off) so that it doesn't fall over and consume all resources, depriving other tasks like eg search from working.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/165204294","html_url":"https://github.com/elastic/elasticsearch/issues/15465#issuecomment-165204294","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15465","id":165204294,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTIwNDI5NA==","user":{"login":"hronik1","id":1770336,"node_id":"MDQ6VXNlcjE3NzAzMzY=","avatar_url":"https://avatars0.githubusercontent.com/u/1770336?v=4","gravatar_id":"","url":"https://api.github.com/users/hronik1","html_url":"https://github.com/hronik1","followers_url":"https://api.github.com/users/hronik1/followers","following_url":"https://api.github.com/users/hronik1/following{/other_user}","gists_url":"https://api.github.com/users/hronik1/gists{/gist_id}","starred_url":"https://api.github.com/users/hronik1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hronik1/subscriptions","organizations_url":"https://api.github.com/users/hronik1/orgs","repos_url":"https://api.github.com/users/hronik1/repos","events_url":"https://api.github.com/users/hronik1/events{/privacy}","received_events_url":"https://api.github.com/users/hronik1/received_events","type":"User","site_admin":false},"created_at":"2015-12-16T18:28:56Z","updated_at":"2015-12-16T18:28:56Z","author_association":"NONE","body":"Clinton, yes my apologies for the misuse of fallover.\n\nI am aware of why the reject requests are happening, as well as their\npurpose. My confusion, and underlying issue I am trying to get at is why\nthe older version appears to be able to index substantially more(200x, 15\nmillion vs. 85 thousand) documents before reaching a state where they\ncompletely stop indexing altogether. If this was a trivial difference, I\nwould not be so concerned/confused, however a 200x difference is not\ntrivial.\n\nOn Wed, Dec 16, 2015 at 3:51 AM, Clinton Gormley notifications@github.com\nwrote:\n\n> It's not falling over. It is rejecting requests (telling the client to\n> back off) so that it doesn't fall over and consume all resources, depriving\n> other tasks like eg search from working.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_elastic_elasticsearch_issues_15465-23issuecomment-2D165081429&d=BQMCaQ&c=8hUWFZcy2Z-Za5rBPlktOQ&r=MXoI1K0dgBQpc4jGuqgr9Y3z8gHgOxW8KepkbfZ_AUQ&m=PQQfiVwHMkmZM6gM2z7Q0PaPnrEGjxnIYjsc3nrYx0A&s=SqYeWYuzRQ_5OgEZ1ZG_kr0zlf1Da8WgkQnm5AkYdaE&e=\n> .\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/165268568","html_url":"https://github.com/elastic/elasticsearch/issues/15465#issuecomment-165268568","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15465","id":165268568,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTI2ODU2OA==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2015-12-16T22:08:49Z","updated_at":"2015-12-16T22:08:49Z","author_association":"MEMBER","body":"@hronik1 I understand what you're saying, but sixteen producers against two consumers is so far beyond the intended usage of Elasticsearch and the bulk API (which sits behind the elasticsearch-hadoop implementation) that it is not a situation to engineer for.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/269849777","html_url":"https://github.com/elastic/elasticsearch/issues/15465#issuecomment-269849777","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15465","id":269849777,"node_id":"MDEyOklzc3VlQ29tbWVudDI2OTg0OTc3Nw==","user":{"login":"AshisPF","id":22148063,"node_id":"MDQ6VXNlcjIyMTQ4MDYz","avatar_url":"https://avatars0.githubusercontent.com/u/22148063?v=4","gravatar_id":"","url":"https://api.github.com/users/AshisPF","html_url":"https://github.com/AshisPF","followers_url":"https://api.github.com/users/AshisPF/followers","following_url":"https://api.github.com/users/AshisPF/following{/other_user}","gists_url":"https://api.github.com/users/AshisPF/gists{/gist_id}","starred_url":"https://api.github.com/users/AshisPF/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AshisPF/subscriptions","organizations_url":"https://api.github.com/users/AshisPF/orgs","repos_url":"https://api.github.com/users/AshisPF/repos","events_url":"https://api.github.com/users/AshisPF/events{/privacy}","received_events_url":"https://api.github.com/users/AshisPF/received_events","type":"User","site_admin":false},"created_at":"2016-12-31T05:05:04Z","updated_at":"2016-12-31T05:06:05Z","author_association":"NONE","body":"increase the ulimit on the consumer .This problem is due to too may files opening which by default is limited to 1048 number of files\r\n\r\nUse a text editor and open the /etc/initscript file and add the following lines to the file:\r\nulimit -Hn 65535\r\nulimit -Sn 65535\r\neval exec \"$4\"\r\n\r\nSave the file.\r\nIf you start the server as a regular process\r\nUse a text editor and open the /etc/security/limits.conf file.\r\nIncrease the upper limit on the number of file descriptors by adding the following lines to the file:\r\n\r\n* soft    nofile          65535\r\n* hard    nofile          65535\r\n\r\nSave the file.\r\nRestart the Linux server for the operating system change to take effect for all processes.","performed_via_github_app":null}]