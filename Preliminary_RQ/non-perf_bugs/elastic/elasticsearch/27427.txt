{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/27427","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27427/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27427/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27427/events","html_url":"https://github.com/elastic/elasticsearch/issues/27427","id":274803628,"node_id":"MDU6SXNzdWUyNzQ4MDM2Mjg=","number":27427,"title":"Pending tasks increase with increase in number of shards in the cluster during relocation.","user":{"login":"itiyama","id":7935367,"node_id":"MDQ6VXNlcjc5MzUzNjc=","avatar_url":"https://avatars3.githubusercontent.com/u/7935367?v=4","gravatar_id":"","url":"https://api.github.com/users/itiyama","html_url":"https://github.com/itiyama","followers_url":"https://api.github.com/users/itiyama/followers","following_url":"https://api.github.com/users/itiyama/following{/other_user}","gists_url":"https://api.github.com/users/itiyama/gists{/gist_id}","starred_url":"https://api.github.com/users/itiyama/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/itiyama/subscriptions","organizations_url":"https://api.github.com/users/itiyama/orgs","repos_url":"https://api.github.com/users/itiyama/repos","events_url":"https://api.github.com/users/itiyama/events{/privacy}","received_events_url":"https://api.github.com/users/itiyama/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2017-11-17T09:33:28Z","updated_at":"2019-03-18T14:08:26Z","closed_at":"2019-03-18T14:08:26Z","author_association":"NONE","active_lock_reason":null,"body":"\r\n\r\n**Elasticsearch version** (`bin/elasticsearch --version`): 5.1.1 , although the piece of code we are are having problems with does not have any major difference with 6.0.\r\n\r\n**Plugins installed**: []\r\n\r\n**JVM version** (`java -version`): 1.8\r\n\r\n**OS version** (`uname -a` if on a Unix-like system): 4.9.38-16.35.amzn1.x86_64\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\nWe have observed that during forced relocation on a cluster with high number of shards(7K), the [shard-started event](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java#L378)(relocation complete status update task) takes a huge amount of time to execute(close to 4 seconds). With more number of relocation going on in the system due to forced relocation- this causes an increase in the pending task queue resulting in timeouts of index creation and update mappings.\r\n\r\n**What causes the time taken for the task to increase to 4 seconds?**\r\nThe ES relocation completion status update task iterates through all shards and takes a move and allocation decision for each one of them. The move decision and allocation decision is based on multiple factors: disk available on current node, excluded ips, shard limit, number of allowed shard relocations per node, shard attributes etc. In a steady state during forced relocation, all nodes will be relocating maximum number of shards based on the throttling limit. All tasks are serialized at the master. Hence, once a relocation finishes, we might have to relocate only a single shard in steady state. There is no point of iterating through all shards in the cluster. Similarly, there is no point of iterating through shards of nodes that is not excluded as move happens from excluded nodes to non-excluded nodes. Also, there is no point of iterating through shards of nodes which already have maximum number of relocations going in parallel as we already know that a “DON’T MOVE” decision will be taken for all shards of that node.\r\n \r\n_The fundamental problem with this algorithm is the abstraction that is used to make allocation and move decision._  We iterate through all shards and make a decision, while most of the decisions can be taken at a node level and all shards for that node can be skipped as ultimately a don’t move decision will anyway be taken for all shards of that node.\r\n\r\n**Optimizations that can be made:**\r\nWe changed the abstraction to make allocation and move decision to node+shard rather than shard. Hence, if a node is not excluded and it has available disk and attributes conditions and shard limit conditions are fulfilled- we do not iterate through any shard it has. Similarly, if a node is already throttled on maximum outgoing recoveries- we do not iterate through any more shards of that node. As a result, we reduced the number of shards being iterated to a very small value- 1-2 in most cases as opposed to number of shards in the cluster in the older algorithm.\r\n\r\nSimilarly, _we changed the decision to rebalance to be decided at node+shard level instead of checking shard by shard._\r\n\r\n**Steps to reproduce**:\r\nWe have a cluster with 7k shards spread over 100 data nodes. Now we wish to upgrade the cluster to a better data node configuration, for which we spin-up another 100 node cluster and exclude the IPs of older data nodes. This triggers relocation of the shards. Node incoming and outgoing recoveries is set to 6 and cluster concurrent rebalance is set to 10. The problem does not depend a lot on shard size but on number of shards in the cluster,\r\n\r\nWe checked the time it takes for [this](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java#L92) method to execute both with and without the optimizations and we saw the time taken being reduced from a range of 1s-3.5s to 60ms.\r\n\r\nFrom the code perspective, \r\nwe have basically added a new [canRemain method](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecider.java#L66) for allocation deciders that returns decision at a node level rather than at a shard level. Nodes for which (canRemain returned true for all deciders) \r\n or (if outgoing recoveries is equal to throttle limit- basically decision is THROTTLE for any decider) the node gets skipped.\r\n\r\nWe have added a canRemain method per node for [DiskThresholdDecider](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java), [FilterAllocationDecider](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java),[ShardsLimitAllocationDecider](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java),[ThrottlingAllocationDecider](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java) and  [AwarenessAllocationDecider](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java)\r\n\r\nFor awareness allocation decider, we simply iterate through all shards on that node and then see if any of it needs rebalancing. For our case, the shards were already balanced by attributes, hence we got a canRemain as true. We can add more optimizations here to cache a few values for shards that are not balanced by attributes while the shards are being added to the node\r\n\r\nThere are multiple other optimizations that can be done to reduce time, eg. \r\n\r\n1. [sizeOfRelocatingShards](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java#L86) in DiskThresholdDecider checks number of initializing and relocating shards by iterating through all shards in the list. A better way is to maintain a map as a cluster with more number of shards will have very few shards relocating and initializing at any point in time.\r\n2. Some decisions can be taken at a cluster level like if total number of relocations happening in the cluster are equal to node incoming times number of included nodes or node outgoing recoveries times number of excluded nodes, we can skip the move shards decision.","closed_by":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"performed_via_github_app":null}