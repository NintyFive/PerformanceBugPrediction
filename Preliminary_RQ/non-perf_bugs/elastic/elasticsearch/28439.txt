{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/28439","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28439/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28439/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28439/events","html_url":"https://github.com/elastic/elasticsearch/issues/28439","id":292735539,"node_id":"MDU6SXNzdWUyOTI3MzU1Mzk=","number":28439,"title":"WordDelimiterTokenFilter does not produce expected tokens with split_on_case_change and preserve_original set to true","user":{"login":"atbagga","id":11888714,"node_id":"MDQ6VXNlcjExODg4NzE0","avatar_url":"https://avatars1.githubusercontent.com/u/11888714?v=4","gravatar_id":"","url":"https://api.github.com/users/atbagga","html_url":"https://github.com/atbagga","followers_url":"https://api.github.com/users/atbagga/followers","following_url":"https://api.github.com/users/atbagga/following{/other_user}","gists_url":"https://api.github.com/users/atbagga/gists{/gist_id}","starred_url":"https://api.github.com/users/atbagga/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/atbagga/subscriptions","organizations_url":"https://api.github.com/users/atbagga/orgs","repos_url":"https://api.github.com/users/atbagga/repos","events_url":"https://api.github.com/users/atbagga/events{/privacy}","received_events_url":"https://api.github.com/users/atbagga/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2018-01-30T10:46:26Z","updated_at":"2018-01-31T05:27:47Z","closed_at":"2018-01-30T12:45:44Z","author_association":"NONE","active_lock_reason":null,"body":"<!-- Bug report -->\r\n\r\n**Elasticsearch version** (`bin/elasticsearch --version`):\r\nVersion: 5.4.1, Build: 2cfe0df/2017-05-29T16:05:51.443Z\r\n\r\n**Plugins installed**: []\r\n\r\n**JVM version** (`java -version`):\r\n JVM: 1.8.0_151\r\n\r\n**OS version** (`uname -a` if on a Unix-like system):\r\nWindows 10\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\nWhen using word delimiter token filter some expected tokens are not generated.\r\n\r\nWhen I try to analyze the text \"ElasticSearch.TestProject\"\r\n\r\nI expect the tokens elastic, search, test, project, elasticsearch, testproject, elasticsearch.testproject to be generated since I have split_on_case_change, split_on_numerics on and using a whitespace tokenizer. \r\n\r\nBut Actually I only see following tokens -\r\nelasticsearch.testproject, elastic, search, test, project\r\n\r\n**Steps to reproduce**:\r\n **1. Create index** \r\n_PUT testindex\r\n {\r\n    \"settings\" : {\r\n        \"index\" : {\r\n            \"number_of_shards\" : 2, \r\n            \"number_of_replicas\" : 2 \r\n        },\r\n    \"analysis\": {\r\n      \"filter\": {\r\n        \"wordDelimiter\": {\r\n          \"type\": \"word_delimiter\",\r\n          \"generate_word_parts\": \"true\",\r\n          \"generate_number_parts\": \"true\",\r\n          \"catenate_words\": \"false\",\r\n          \"catenate_numbers\": \"false\",\r\n          \"catenate_all\": \"false\",\r\n          \"split_on_case_change\": \"true\",\r\n          \"preserve_original\": \"true\",\r\n          \"split_on_numerics\": \"true\",\r\n          \"stem_english_possessive\": \"true\"\r\n        }\r\n      },\r\n      \"analyzer\": {\r\n\t\t\"content_analyzer\": {\r\n          \"type\": \"custom\",\r\n          \"tokenizer\": \"whitespace\",\r\n          \"filter\": [\r\n            \"asciifolding\",\r\n            \"wordDelimiter\",\r\n            \"lowercase\"\r\n          ]\r\n        }\r\n      }\r\n    }\r\n\t}\r\n }_\r\n\r\n **2. Analyze Text-**\r\n_POST testindex/_analyze \r\n{\r\n  \"analyzer\": \"content_analyzer\",\r\n  \"text\": \"ElasticSearch.TestProject\"\r\n}_\r\n\r\n**Following tokens are generated-**\r\n\r\n{\r\n\"token\": \"elasticsearch-testproject\",\r\n\"start_offset\": 0,\r\n\"end_offset\": 25,\r\n\"type\": \"word\",\r\n\"position\": 0\r\n}\r\n,\r\n{\r\n\"token\": \"elastic\",\r\n\"start_offset\": 0,\r\n\"end_offset\": 7,\r\n\"type\": \"word\",\r\n\"position\": 0\r\n}\r\n,\r\n{\r\n\"token\": \"search\",\r\n\"start_offset\": 7,\r\n\"end_offset\": 13,\r\n\"type\": \"word\",\r\n\"position\": 1\r\n}\r\n,\r\n{\r\n\"token\": \"test\",\r\n\"start_offset\": 14,\r\n\"end_offset\": 18,\r\n\"type\": \"word\",\r\n\"position\": 2\r\n}\r\n,\r\n{\r\n\"token\": \"project\",\r\n\"start_offset\": 18,\r\n\"end_offset\": 25,\r\n\"type\": \"word\",\r\n\"position\": 3\r\n}\r\n\r\n**Expected Result:**\r\nBesides the above tokens even elasticsearch and testproject should be generated. such that the phrase query \"elasticsearch testproject\" should also match.","closed_by":{"login":"romseygeek","id":1347065,"node_id":"MDQ6VXNlcjEzNDcwNjU=","avatar_url":"https://avatars0.githubusercontent.com/u/1347065?v=4","gravatar_id":"","url":"https://api.github.com/users/romseygeek","html_url":"https://github.com/romseygeek","followers_url":"https://api.github.com/users/romseygeek/followers","following_url":"https://api.github.com/users/romseygeek/following{/other_user}","gists_url":"https://api.github.com/users/romseygeek/gists{/gist_id}","starred_url":"https://api.github.com/users/romseygeek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/romseygeek/subscriptions","organizations_url":"https://api.github.com/users/romseygeek/orgs","repos_url":"https://api.github.com/users/romseygeek/repos","events_url":"https://api.github.com/users/romseygeek/events{/privacy}","received_events_url":"https://api.github.com/users/romseygeek/received_events","type":"User","site_admin":false},"performed_via_github_app":null}