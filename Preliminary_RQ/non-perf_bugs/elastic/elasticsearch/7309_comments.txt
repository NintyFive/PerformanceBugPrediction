[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/52996393","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-52996393","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":52996393,"node_id":"MDEyOklzc3VlQ29tbWVudDUyOTk2Mzkz","user":{"login":"c-a-m","id":1190407,"node_id":"MDQ6VXNlcjExOTA0MDc=","avatar_url":"https://avatars1.githubusercontent.com/u/1190407?v=4","gravatar_id":"","url":"https://api.github.com/users/c-a-m","html_url":"https://github.com/c-a-m","followers_url":"https://api.github.com/users/c-a-m/followers","following_url":"https://api.github.com/users/c-a-m/following{/other_user}","gists_url":"https://api.github.com/users/c-a-m/gists{/gist_id}","starred_url":"https://api.github.com/users/c-a-m/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/c-a-m/subscriptions","organizations_url":"https://api.github.com/users/c-a-m/orgs","repos_url":"https://api.github.com/users/c-a-m/repos","events_url":"https://api.github.com/users/c-a-m/events{/privacy}","received_events_url":"https://api.github.com/users/c-a-m/received_events","type":"User","site_admin":false},"created_at":"2014-08-21T22:57:14Z","updated_at":"2014-08-21T22:57:14Z","author_association":"NONE","body":"Ryan pointed out the BREACH vulnerability with SSL and compression.  I think it still should be on default in ES, but off by default if SSL is enabled\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/53029406","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-53029406","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":53029406,"node_id":"MDEyOklzc3VlQ29tbWVudDUzMDI5NDA2","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-08-22T07:01:52Z","updated_at":"2014-08-22T07:01:52Z","author_association":"CONTRIBUTOR","body":"@c-a-m ok, so we should leave this open then, no?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/56870625","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-56870625","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":56870625,"node_id":"MDEyOklzc3VlQ29tbWVudDU2ODcwNjI1","user":{"login":"c-a-m","id":1190407,"node_id":"MDQ6VXNlcjExOTA0MDc=","avatar_url":"https://avatars1.githubusercontent.com/u/1190407?v=4","gravatar_id":"","url":"https://api.github.com/users/c-a-m","html_url":"https://github.com/c-a-m","followers_url":"https://api.github.com/users/c-a-m/followers","following_url":"https://api.github.com/users/c-a-m/following{/other_user}","gists_url":"https://api.github.com/users/c-a-m/gists{/gist_id}","starred_url":"https://api.github.com/users/c-a-m/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/c-a-m/subscriptions","organizations_url":"https://api.github.com/users/c-a-m/orgs","repos_url":"https://api.github.com/users/c-a-m/repos","events_url":"https://api.github.com/users/c-a-m/events{/privacy}","received_events_url":"https://api.github.com/users/c-a-m/received_events","type":"User","site_admin":false},"created_at":"2014-09-25T19:28:05Z","updated_at":"2014-09-25T19:28:05Z","author_association":"NONE","body":"I just realized that the BREACH vulnerability is with a compression feature of TLS, it has nothing to do with HTTP level compression.  This is totally fine to be left on by default.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/190658238","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-190658238","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":190658238,"node_id":"MDEyOklzc3VlQ29tbWVudDE5MDY1ODIzOA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-03-01T10:42:35Z","updated_at":"2016-03-01T10:42:35Z","author_association":"CONTRIBUTOR","body":"Apparently HTTP compression was disabled originally because the LZF(?) library used by Netty had memory leaks.  Need to check if this is still the case.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/202919272","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-202919272","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":202919272,"node_id":"MDEyOklzc3VlQ29tbWVudDIwMjkxOTI3Mg==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2016-03-29T14:26:46Z","updated_at":"2016-03-29T14:26:46Z","author_association":"MEMBER","body":"I did some stress tests with es 2.3 and was not able to reproduce the leak. It seems that the http compression was disabled by default because \"many clients are buggy when it comes to supporting it.\" https://github.com/elastic/elasticsearch/issues/1482\nI've tested sending compressed data and receiving compressed data with elasticsearch-py on my local machine. Compression did not help for the performance and can also degrade some types of queries (scroll queries are 20% slower when the compression is enabled). Tough my test is not realistic at all, I am using a mac book air and all my queries are local to my machine. I guess that the compression could help if the network is congested.\nBottom line is that we can re-enable http compression by default but it will not change anything if the users do not send the appropriate header which activates the compression of the response (Accept-Encoding: gzip) in their requests.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/203579747","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-203579747","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":203579747,"node_id":"MDEyOklzc3VlQ29tbWVudDIwMzU3OTc0Nw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-03-30T18:56:00Z","updated_at":"2016-03-30T18:56:00Z","author_association":"CONTRIBUTOR","body":"@jimferenczi thanks for testing this.  Of course, testing on your local machine avoids network latency so you see the downside of compression without it really having the opportunity to shine.\n\n> Bottom line is that we can re-enable http compression by default but it will not change anything if the users do not send the appropriate header which activates the compression of the response (Accept-Encoding: gzip) in their requests.\n\nAgreed.  I think most (if not all) of the official clients have compression support, as long as the user enables it.  If we decide to enable it by default, then the client authors can make the appropriate changes.\n\n@jpountz what do you think of enabling it by default?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/203976894","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-203976894","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":203976894,"node_id":"MDEyOklzc3VlQ29tbWVudDIwMzk3Njg5NA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-03-31T15:00:43Z","updated_at":"2016-03-31T15:00:43Z","author_association":"CONTRIBUTOR","body":"The size of responses seems to be a pretty common source of complaints so I think we should try to enable it by default. I suspect that our responses have a lot of duplicated strings so even low levels of compression would already reduce the size of the data significantly. So we could try to enable it by default eg. with a compression level of 3 (currently the default level is 6, 3 is the highest compression level of DEFLATE that does not use lazy match evaluation, which tends to make compression slow). This way we would limit the potential bad performance impacts of having compression on by default?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/204297841","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-204297841","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":204297841,"node_id":"MDEyOklzc3VlQ29tbWVudDIwNDI5Nzg0MQ==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2016-04-01T07:50:26Z","updated_at":"2016-04-01T07:50:26Z","author_association":"MEMBER","body":"I've tested the full compression scheme (send and receive compressed content) with a compression level of 6. I'll check with the response compression only and a compression level of 3 if the impact is visible in terms of performance. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/212774464","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-212774464","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":212774464,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjc3NDQ2NA==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-04-21T07:04:30Z","updated_at":"2016-04-21T07:10:40Z","author_association":"MEMBER","body":"I have benchmarked this scenario with Rally against a single node cluster with default settings except for the heap size (which I set to `-Xms4G -Xmx4G`) of a recent master build of Elasticsearch (revision 6921712). I used a dedicated bare metal machine for Rally and a dedicated one for the benchmark candidate. I used compression level 9 to amplify the effect of compression as much as possible and compressed all requests and responses. The data set was the same geonames benchmark that we also use in the nightly benchmarks. Preliminary results show:\n- Network traffic is significantly lower  (around 25 vs. around 3GB sent, around 3.4GB vs. 350MB received)\n- Indexing throughput and CPU utilization during indexing is roughly equivalent\n- Query latency suffers drastically, especially in the higher percentiles (90% percentile and above). Worst are the scroll query and the term query.\n\nDetails are in the attached graphics from the Kibana dashboard which are _currently_ also available at https://b7dea5252a72b78502fc91e0462fca7e.us-east-1.aws.found.io/app/kibana#/dashboard/HTTP-Compression-Benchmark-Results (I may remove them at any time; that's why I uploaded the screenshot for reference):\n\n![http-benchmark-results](https://cloud.githubusercontent.com/assets/1699576/14700444/e81a8534-079f-11e6-8dd0-57f821cdb746.png)\n\n I'll run a few more benchmarks but so far I can confirm Jim's testing.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/212813674","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-212813674","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":212813674,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjgxMzY3NA==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2016-04-21T08:42:19Z","updated_at":"2016-04-21T08:42:19Z","author_association":"MEMBER","body":"Thanks @danielmitterdorfer. \n\n> Indexing throughput and CPU utilization during indexing is roughly equivalent\n\nThis is really a big win, most of the traffic is generated during indexing IMO we should really accept compressed request by default.\n\n> Query latency suffers drastically, especially in the higher percentiles (90% percentile and above). Worst are the scroll query and the term query.\n\nThis is the tricky part. In my tests the request and the response (and I guess it's the same here) are compressed. IMO we should never compress a body smaller than 1k.\nWhat do you think of adding a minimum body size to enable compression on both end (server and client) ? \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/212820776","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-212820776","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":212820776,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjgyMDc3Ng==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2016-04-21T09:01:03Z","updated_at":"2016-04-21T09:01:03Z","author_association":"CONTRIBUTOR","body":"Is the issue really with small bodies? If the body is small, then likely it will be very fast to compress as well? I was more under the assumption that scrolls and term queries have a performance hit because they are among the cheapest queries that you can send to elasticsearch? I would be curious to see how different the results are with a compression level of 1.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/212824706","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-212824706","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":212824706,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjgyNDcwNg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-04-21T09:16:26Z","updated_at":"2016-04-21T09:16:26Z","author_association":"CONTRIBUTOR","body":"@danielmitterdorfer you say:\n\n> we should really accept compressed request by default.\n\nIf you were using the python client, I'm pretty sure the request was not compressed, only the response.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/212832844","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-212832844","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":212832844,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjgzMjg0NA==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2016-04-21T09:41:38Z","updated_at":"2016-04-21T09:41:38Z","author_association":"MEMBER","body":"@clintongormley I think he was using a custom connection that enable the compression on the client side. In fact I am pretty sure he did because the received bytes on es side is way lower when compression is \"on\". \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/212844055","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-212844055","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":212844055,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjg0NDA1NQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-04-21T10:15:32Z","updated_at":"2016-04-21T10:15:32Z","author_association":"MEMBER","body":"> @danielmitterdorfer you say:\n> \n> >    we should really accept compressed request by default.\n\n@clintongormley This was @jimferenczi. I did not draw any conclusions yet. ;) I'll gather more data points (different compression rates) and also investigate a few issues. Btw, Jim was right: I used a custom connection in the Python client that gzips the request\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/212851507","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-212851507","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":212851507,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjg1MTUwNw==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-04-21T10:42:45Z","updated_at":"2016-04-21T10:45:01Z","author_association":"MEMBER","body":"I just checked the impact of Python 3.5 stdlib gzip compression for bulk requests (with a bulk size of 5000) and a sample query (result of 100 trials in a [microbenchmark](https://gist.github.com/danielmitterdorfer/48ff9ec8fc1e62252e67ca9e62b346f5)):\n\n| Comment | Size [bytes] | Min compression time (ms) | Mean compression time (ms) | Max compression time (ms) |\n| --- | --- | --- | --- | --- |\n| Bulk request with 5000 items | 1829194 | 103.12 | 105.10 | 114.09 |\n| Aggregation Query | 330 | 0.023 | 0.023 | 0.066 |\n\nSo the overhead on client side is negligible.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/213388282","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-213388282","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":213388282,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMzM4ODI4Mg==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-04-22T11:34:20Z","updated_at":"2016-04-22T11:35:45Z","author_association":"MEMBER","body":"I ran a couple of further experiments. Again, preliminary results but with larger scroll sizes, `org.jboss.netty.util.internal.jzlib.ZStream.deflate(int)` completely dominates the profile, i.e. it's spending more than half of its time compressing the result. With a compression level of 1, `ZStream` uses a different compression approach which does not show up that high in the profile btw.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/214711599","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-214711599","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":214711599,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNDcxMTU5OQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-04-26T11:30:53Z","updated_at":"2016-05-13T13:11:53Z","author_association":"MEMBER","body":"The relevant source in the Netty code base indicates that the same compression approach is used for compression levels from 1 to 3 (see https://github.com/netty/netty/blob/netty-3.10.5.Final/src/main/java/org/jboss/netty/util/internal/jzlib/Deflate.java#L79-L81) so I also benchmarked with a compression level of 3. In the benchmarked scenario (geonames) indicates that we can save a negligible amount of network traffic compared to level 1. Query latency also increases a little bit.\n\nI will run the benchmark results against another data set to add one data point more but I'd suggest that in the interest of query latency we reduce the default compression level either to 1 or 3 if we enable HTTP compression by default.\n\nInteractive results are available at https://elasticsearch-benchmarks.elastic.co/app/kibana#/dashboard/HTTP-Compression-Benchmark-Results\n\nBelow is a full-page screenshot of the same page:\n\n![http_comp_geonames](https://cloud.githubusercontent.com/assets/1699576/14816772/0a4cedd8-0bb3-11e6-9e62-6f9d81f13245.png)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/214712592","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-214712592","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":214712592,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNDcxMjU5Mg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-04-26T11:35:09Z","updated_at":"2016-04-26T11:35:09Z","author_association":"CONTRIBUTOR","body":"We should enable request decompression regardless of whether response decompression is enabled, ie in https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java#L547 change ESHttpContentDecompressor to HttpContentDecompressor.\n\nAlso, the comment about BREACH https://github.com/elastic/elasticsearch/issues/7309#issuecomment-56870625 appears to be incorrect (see http://breachattack.com/) so we should default compression to disabled if SSL is enabled.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/215790726","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-215790726","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":215790726,"node_id":"MDEyOklzc3VlQ29tbWVudDIxNTc5MDcyNg==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-04-29T16:20:43Z","updated_at":"2016-04-30T08:53:07Z","author_association":"MEMBER","body":"I also ran a microbenchmark of Netty's `ZlibEncoder` and `JdkZlibEncoder` (refered to as `jzlib` and `jdk` below) with a smaller JSON document (a few hundred bytes) and a larger JSON document (3.6MB) at different compression levels to see whether we should change the encoder implementation for performance reasons but the benchmark results indicate we should not change it (especially at smaller compression levels):\n\n```\nBenchmark                  (compressionLevel)  (impl)  (smallDocument)   Mode  Cnt       Score     Error  Units\nNettyZlibBenchmark.encode                   1   jzlib            false  thrpt  150      75.960 ±   0.051  ops/s\nNettyZlibBenchmark.encode                   1   jzlib             true  thrpt  150  195383.389 ± 690.821  ops/s\nNettyZlibBenchmark.encode                   1     jdk            false  thrpt  150      68.254 ±   0.154  ops/s\nNettyZlibBenchmark.encode                   1     jdk             true  thrpt  150  159102.287 ± 227.628  ops/s\nNettyZlibBenchmark.encode                   3   jzlib            false  thrpt  150      74.859 ±   0.057  ops/s\nNettyZlibBenchmark.encode                   3   jzlib             true  thrpt  150  187901.799 ± 612.592  ops/s\nNettyZlibBenchmark.encode                   3     jdk            false  thrpt  150      67.480 ±   0.042  ops/s\nNettyZlibBenchmark.encode                   3     jdk             true  thrpt  150  159002.153 ± 101.567  ops/s\nNettyZlibBenchmark.encode                   6   jzlib            false  thrpt  150      38.250 ±   0.023  ops/s\nNettyZlibBenchmark.encode                   6   jzlib             true  thrpt  150   84190.875 ± 303.414  ops/s\nNettyZlibBenchmark.encode                   6     jdk            false  thrpt  150      35.101 ±   0.179  ops/s\nNettyZlibBenchmark.encode                   6     jdk             true  thrpt  150   86632.628 ±  77.181  ops/s\nNettyZlibBenchmark.encode                   9   jzlib            false  thrpt  150      11.812 ±   0.017  ops/s\nNettyZlibBenchmark.encode                   9   jzlib             true  thrpt  150   54201.944 ±  89.032  ops/s\nNettyZlibBenchmark.encode                   9     jdk            false  thrpt  150      11.894 ±   0.021  ops/s\nNettyZlibBenchmark.encode                   9     jdk             true  thrpt  150   60536.066 ± 101.270  ops/s\n```\n\nThe benchmark was run on a silent server class machine (Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz, Linux Kernel 4.2.0-34). It was pinned to core 0 with `taskset -c 0 java -jar netty-zlib-0.1.0-all.jar -f 5 -wi 30 -i 30`. I verified (in a separate trial run) with JMH's perf profiler that we had no CPU migrations. All cores ran with the performance CPU governor at 3.4GHz.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/382322365","html_url":"https://github.com/elastic/elasticsearch/issues/7309#issuecomment-382322365","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/7309","id":382322365,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MjMyMjM2NQ==","user":{"login":"cbismuth","id":21545602,"node_id":"MDQ6VXNlcjIxNTQ1NjAy","avatar_url":"https://avatars1.githubusercontent.com/u/21545602?v=4","gravatar_id":"","url":"https://api.github.com/users/cbismuth","html_url":"https://github.com/cbismuth","followers_url":"https://api.github.com/users/cbismuth/followers","following_url":"https://api.github.com/users/cbismuth/following{/other_user}","gists_url":"https://api.github.com/users/cbismuth/gists{/gist_id}","starred_url":"https://api.github.com/users/cbismuth/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbismuth/subscriptions","organizations_url":"https://api.github.com/users/cbismuth/orgs","repos_url":"https://api.github.com/users/cbismuth/repos","events_url":"https://api.github.com/users/cbismuth/events{/privacy}","received_events_url":"https://api.github.com/users/cbismuth/received_events","type":"User","site_admin":false},"created_at":"2018-04-18T09:20:16Z","updated_at":"2018-04-18T09:26:04Z","author_association":"CONTRIBUTOR","body":"We have lost really a **lot** of time before fixing Painless really bad performance issues by enabling TCP compression ... we were consuming all our VMware bandwidth with complex aggregations on a 5 nodes cluster.\r\n\r\nWe have applied all performances guidelines, but no hint about compression.\r\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/system-config.html\r\n\r\n![download](https://user-images.githubusercontent.com/21545602/38923479-ef028110-42fa-11e8-8611-3501ccc67ed9.png)\r\n","performed_via_github_app":null}]