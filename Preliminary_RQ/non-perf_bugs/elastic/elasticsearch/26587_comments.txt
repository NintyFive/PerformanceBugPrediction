[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328775142","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-328775142","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":328775142,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODc3NTE0Mg==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T08:07:05Z","updated_at":"2017-09-12T08:07:05Z","author_association":"CONTRIBUTOR","body":"Would you be able to share a heap dump of the node upon OOME? Or if that is too sensitive, screenshots of a heap analysis tool showing the top consumers?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328811185","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-328811185","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":328811185,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODgxMTE4NQ==","user":{"login":"natelapp","id":15839796,"node_id":"MDQ6VXNlcjE1ODM5Nzk2","avatar_url":"https://avatars1.githubusercontent.com/u/15839796?v=4","gravatar_id":"","url":"https://api.github.com/users/natelapp","html_url":"https://github.com/natelapp","followers_url":"https://api.github.com/users/natelapp/followers","following_url":"https://api.github.com/users/natelapp/following{/other_user}","gists_url":"https://api.github.com/users/natelapp/gists{/gist_id}","starred_url":"https://api.github.com/users/natelapp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/natelapp/subscriptions","organizations_url":"https://api.github.com/users/natelapp/orgs","repos_url":"https://api.github.com/users/natelapp/repos","events_url":"https://api.github.com/users/natelapp/events{/privacy}","received_events_url":"https://api.github.com/users/natelapp/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T10:26:23Z","updated_at":"2017-09-12T10:26:23Z","author_association":"NONE","body":"Unfortunately, being an ES federal user, I don't have any way of getting the heap dump or screenshots to you.  I will continue working it on my end to see if I can narrow it down further.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328832282","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-328832282","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":328832282,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODgzMjI4Mg==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T12:05:40Z","updated_at":"2017-09-12T12:05:40Z","author_association":"CONTRIBUTOR","body":"Your description of the issue suggests that memory is actually used by one of the cache, but in a way that the cache is not aware of, otherwise it would have evicted entries in order to keep a bounded size. It would be nice to find out which cache has the bug, and why.\r\n\r\nYour bug description reminds me of https://issues.apache.org/jira/browse/LUCENE-7657 but this bug is expected to be fixed in 5.5.2.\r\n\r\nIs there any search activity in parallel to bulk indexing? If not, then it means that this can't be the request cache or query cache.\r\n\r\nAre you using parent/child or nested mappings? Those fields make Elasticsearch load some data-structures in memory in order to be able to resolve joins efficiently.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328837705","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-328837705","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":328837705,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODgzNzcwNQ==","user":{"login":"natelapp","id":15839796,"node_id":"MDQ6VXNlcjE1ODM5Nzk2","avatar_url":"https://avatars1.githubusercontent.com/u/15839796?v=4","gravatar_id":"","url":"https://api.github.com/users/natelapp","html_url":"https://github.com/natelapp","followers_url":"https://api.github.com/users/natelapp/followers","following_url":"https://api.github.com/users/natelapp/following{/other_user}","gists_url":"https://api.github.com/users/natelapp/gists{/gist_id}","starred_url":"https://api.github.com/users/natelapp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/natelapp/subscriptions","organizations_url":"https://api.github.com/users/natelapp/orgs","repos_url":"https://api.github.com/users/natelapp/repos","events_url":"https://api.github.com/users/natelapp/events{/privacy}","received_events_url":"https://api.github.com/users/natelapp/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T12:29:41Z","updated_at":"2017-09-12T12:29:41Z","author_association":"NONE","body":"Sure.  We're happy to work to track it down.\r\n\r\nSo, we're absolutely doing some things that are less than ideal in our design that likely fall outside of the norm.   We're currently using parent/child/grandchild.  This particular OOM issue occurs when attempting to create the grandchild records for existing documents that don't have any grandchildren.   The basic flow is:\r\n\r\n- Query for children records that don't have children of their own.  We're using \"search_after\" to page through the results.\r\n- For _each_ child doc that is found from the initial query, submit a different query to find the data that is needed to create the grandchildren docs.\r\n- Create the grandchild docs and insert them using the bulk API.\r\n\r\nSo yes, there are lots of parallel queries and inserts going on.   And yes, parent/child uses abound.  We're using nested objects in our mapping, but in my process of triaging this, I created a new index simply setting \"dynamic\":true and let ES define all of the data types automatically to rule out the possibility of our mapping causing the problem.  The OOMs still occurred.\r\n\r\nI've also been in contact with the ES Federal team this morning, in case they're in a better position to be able to work this.\r\n\r\nI've got a heap dump from an OOM this morning and will be running it through an analysis tool to figure out what the top classes are.   I'll update this ticket when I have more information.   Thanks!\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328886865","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-328886865","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":328886865,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODg4Njg2NQ==","user":{"login":"natelapp","id":15839796,"node_id":"MDQ6VXNlcjE1ODM5Nzk2","avatar_url":"https://avatars1.githubusercontent.com/u/15839796?v=4","gravatar_id":"","url":"https://api.github.com/users/natelapp","html_url":"https://github.com/natelapp","followers_url":"https://api.github.com/users/natelapp/followers","following_url":"https://api.github.com/users/natelapp/following{/other_user}","gists_url":"https://api.github.com/users/natelapp/gists{/gist_id}","starred_url":"https://api.github.com/users/natelapp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/natelapp/subscriptions","organizations_url":"https://api.github.com/users/natelapp/orgs","repos_url":"https://api.github.com/users/natelapp/repos","events_url":"https://api.github.com/users/natelapp/events{/privacy}","received_events_url":"https://api.github.com/users/natelapp/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T15:22:35Z","updated_at":"2017-09-12T15:22:35Z","author_association":"NONE","body":"The problem appears to be in `org.elasticsearch.indices.IndicesQueryCache$ElasticsearchLRUQueryCache`.  That's consuming almost all of the memory.\r\n\r\nWe're using the Eclipse Memory Analyzer, but aren't sure what type of information would be helpful to you in figuring out the cause.   Are there any other pieces of data from the analysis that we should pass along?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328906428","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-328906428","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":328906428,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODkwNjQyOA==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T16:24:18Z","updated_at":"2017-09-12T16:24:18Z","author_association":"CONTRIBUTOR","body":"Good. Most likely the cache itself is not the issue, but rather some of the queries in the cache are larger than they should be. Can you check that most memory does indeed go into `org.elasticsearch.indices.IndicesQueryCache$ElasticsearchLRUQueryCache.uniqueQueries`, and see what are the largest queries this `uniqueQueries` map stores?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329176716","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-329176716","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":329176716,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTE3NjcxNg==","user":{"login":"natelapp","id":15839796,"node_id":"MDQ6VXNlcjE1ODM5Nzk2","avatar_url":"https://avatars1.githubusercontent.com/u/15839796?v=4","gravatar_id":"","url":"https://api.github.com/users/natelapp","html_url":"https://github.com/natelapp","followers_url":"https://api.github.com/users/natelapp/followers","following_url":"https://api.github.com/users/natelapp/following{/other_user}","gists_url":"https://api.github.com/users/natelapp/gists{/gist_id}","starred_url":"https://api.github.com/users/natelapp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/natelapp/subscriptions","organizations_url":"https://api.github.com/users/natelapp/orgs","repos_url":"https://api.github.com/users/natelapp/repos","events_url":"https://api.github.com/users/natelapp/events{/privacy}","received_events_url":"https://api.github.com/users/natelapp/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T13:59:39Z","updated_at":"2017-09-13T13:59:39Z","author_association":"NONE","body":"I've done a bit more digging.  I'm not very familiar with the MAT application, so forgive me if this is a bit painful in trying to interpret what I'm seeing.\r\n\r\n- It does indeed look like it's `uniqueQueries`.  The `cache` usage is below the default 10% of heap.\r\n- Most of the rest of the top consumers within `ElasticsearchLRUQueryCache` are `BooleanQuery` objects, with the largest consumer within those objects being `globalOrds`.  For example, the largest `BooleanQuery` objects (if i'm interpreting everything correctly) is ~3.6mb.  The `GlobalOrdinalsQuery` inside of that object is 98% of that.\r\n- Inside some of the map entries, there appears to be memory taken up by the `<class>.<classloader>.<class>.scl.classes.elementData`.   Again, not sure if this is just noise, but figured I'd include it.\r\n- I noticed a small thing that confused me at the `IndicesQueryCache` level.  I see the following values:\r\n  `cacheSize`: 54550\r\n  `cacheCount`: 55424\r\n  `maxSize`: 10000\r\nI know that the configuration of the cache.count is 10000, but I wasn't sure what role `cacheSize` and `cacheCount` play and if those numbers made sense.   If those are normal/expected, that's fine.   Again, just throwing things out here that aren't obvious in case there's some issue.\r\n\r\nIf there's anything else I should be looking for, please let me know.   It's also not outside the realm of possibility that we're doing something terrible in our querying.   I'll spend some time logging the queries themselves and make sure there's not something obvious on our end.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329240786","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-329240786","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":329240786,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTI0MDc4Ng==","user":{"login":"natelapp","id":15839796,"node_id":"MDQ6VXNlcjE1ODM5Nzk2","avatar_url":"https://avatars1.githubusercontent.com/u/15839796?v=4","gravatar_id":"","url":"https://api.github.com/users/natelapp","html_url":"https://github.com/natelapp","followers_url":"https://api.github.com/users/natelapp/followers","following_url":"https://api.github.com/users/natelapp/following{/other_user}","gists_url":"https://api.github.com/users/natelapp/gists{/gist_id}","starred_url":"https://api.github.com/users/natelapp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/natelapp/subscriptions","organizations_url":"https://api.github.com/users/natelapp/orgs","repos_url":"https://api.github.com/users/natelapp/repos","events_url":"https://api.github.com/users/natelapp/events{/privacy}","received_events_url":"https://api.github.com/users/natelapp/received_events","type":"User","site_admin":false},"created_at":"2017-09-13T17:32:45Z","updated_at":"2017-09-13T17:32:45Z","author_association":"NONE","body":"There aren't any obvious problems with the queries from what I can see.  The process is as I indicated above:\r\n\r\n- From within a filter, do a `must` on a _type and a single term and a `must_not` of `has_child` (match_all).   Basically, find all docs of a given _type that don't have any children.   In my process there are a few million results that come back.\r\n- With those 100 results in memory in our app, we're collecting the distinct values from a field in the _source of the results and submitting those values as a query (term filter) to the same index.  There are at most 200 terms.   Using the results from that query, we're constructing the children documents for each of the 100 initial results.\r\n- Finally, we're using the bulk API to submit the new children, queuing up 500 index requests at a time with a max of 5 concurrent submits.\r\n- Page for the next set of 100 docs that don't have children using `search_after`.\r\n\r\nOne of the oddities I've noticed is we're re-issuing the `search_after` query on the data we're modifying.  Since `search_after` is against a \"live\" index, the results of that paging query are continually changing, due to the children being added via the bulk requests.  A scroll would likely be a better solution here, but I'd rather figure out the cause of this issue rather than finding a work-around.\r\n\r\nIf it would help to see the actual queries we're submitting, let me know and I'll transcribe them.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329415435","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-329415435","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":329415435,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTQxNTQzNQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-09-14T08:43:27Z","updated_at":"2017-09-14T08:43:27Z","author_association":"CONTRIBUTOR","body":"OK I think your issue is caused by those GlobalOrdinalsQuery objects. They have to reference global ordinals instances in order to be able to work, however those global ords are not taken into account while they can take significant memory (3.6MB in your case). Multiplied by the number of cached queries, this can be an issue.\r\n\r\nI can't think of a straightforward way to fix it, however you should be able to work around it on your end by setting `indices.queries.cache.count` to eg. 100 instead of its default value of 10000. Since your queries are ~4MB this should make sure that cached queries can't take more than 400MB (the cache might be larger than that however since it also needs to store the values that are associated with these queries). Unfortunately this is a node setting and requires nodes to be restarted in order to be taken into account.\r\n\r\n> cacheSize: 54550\r\ncacheCount: 55424\r\nmaxSize: 10000\r\n\r\n`cacheSize` is the number of (query, segment) pairs that currently exist in the cache and `cacheCount` is the cumulative number of (query, segment) pairs that have been put in the cache. These numbers are only maintained in order to give usage statistics of the cache, they are not used to maintain invariants or anything. `maxSize` is the number of _unique_ queries that are allowed to live in the cache at any instant. `cacheSize` can be larger than `maxSize` if the same query is cached on multiple segments, which is common. In order to know the number of currently cached queries, you need to look up the size of the `uniqueQueries` map.\r\n\r\nI'll think about ways to fix this issue...","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329438217","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-329438217","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":329438217,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTQzODIxNw==","user":{"login":"natelapp","id":15839796,"node_id":"MDQ6VXNlcjE1ODM5Nzk2","avatar_url":"https://avatars1.githubusercontent.com/u/15839796?v=4","gravatar_id":"","url":"https://api.github.com/users/natelapp","html_url":"https://github.com/natelapp","followers_url":"https://api.github.com/users/natelapp/followers","following_url":"https://api.github.com/users/natelapp/following{/other_user}","gists_url":"https://api.github.com/users/natelapp/gists{/gist_id}","starred_url":"https://api.github.com/users/natelapp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/natelapp/subscriptions","organizations_url":"https://api.github.com/users/natelapp/orgs","repos_url":"https://api.github.com/users/natelapp/repos","events_url":"https://api.github.com/users/natelapp/events{/privacy}","received_events_url":"https://api.github.com/users/natelapp/received_events","type":"User","site_admin":false},"created_at":"2017-09-14T10:13:54Z","updated_at":"2017-09-14T10:13:54Z","author_association":"NONE","body":"That all makes sense.  Is `globalOrds` so large because of our use of parent/child?  And will the size of `globalOrds` increase with an increase in data?  This issue is present in our development system which contains much less data than our production environment.  I want to set `indices.queries.cache.count` to something that can support our prod data set.\r\n\r\nI want to make sure I understand everything you've explained.   It seems like there are two pieces that are cached... the query itself and the query results.   Is that correct?  And according to your comment [here ](https://github.com/elastic/elasticsearch/issues/22742#issuecomment-274524333) and in this issue, while byte size _is_ accounted for with regards to the query results, the byte size of the query is not considered within the eviction strategy.  So, in our case, because the bytes of the results are small and the bytes of the query itself (due to the `globalOrds` ref) is large, the only way anything is going to be evicted is based on count (which defaults to 10000).  Unfortunately, we end up running out of memory on the nodes well before we hit the 10000 count limit of the cache.  Is that an accurate summary?\r\n\r\nI know it's probably easier said than done, but is it reasonable to consider all of the data (query and results) when determining the overall byte size of the cache?   Admittedly, we're doing things with parent/child/grandchild that [fall outside of what you guys recommend](https://www.elastic.co/guide/en/elasticsearch/guide/current/parent-child-performance.html#_multigenerations_and_concluding_thoughts), however, it does seem like an issue that will likely pop up again in the future with someone else.\r\n\r\nAlso, one other thing I'm curious about is, why didn't the `/my_index/_cache/clear` call free up the memory, yet calling `/*/_cache/clear` did?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329451142","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-329451142","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":329451142,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTQ1MTE0Mg==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-09-14T11:14:24Z","updated_at":"2017-09-14T11:14:24Z","author_association":"CONTRIBUTOR","body":"> That all makes sense. Is globalOrds so large because of our use of parent/child? And will the size of globalOrds increase with an increase in data?\r\n\r\nYou wouldn't need this GlobalOrdinalsQuery if you were not using parent/child. The size will indeed increase if you add more data, so you can try a more conservative size of the cache.\r\n\r\n>  It seems like there are two pieces that are cached... the query itself and the query results. Is that correct?\r\n\r\nCorrect, the cache has 2 levels: on the first level we have queries, and on the second level we have segments. Conceptually the cache acts like a `Map<Query, Map<Segment, DocIdSet>>` where `DocIdSet` is a set of matching docs.\r\n\r\n> And according to your comment here and in this issue, while byte size is accounted for with regards to the query results, the byte size of the query is not considered within the eviction strategy.\r\n\r\nThere are some efforts to take the query size into account, but it doesn't work all the time (which would be hard to fix) and we tend to rely on the assumption that queries are much smaller than DocIdSets, which is true for most queries, but there are exceptions like TermInSetQuery (Lucene's impl for Elasticsearch's `terms`query) or this `GlobalOrdinalsQuery`.\r\n\r\n> Unfortunately, we end up running out of memory on the nodes well before we hit the 10000 count limit of the cache. Is that an accurate summary?\r\n\r\nThis is the most likely explanation I have for the symptoms you are observing indeed.\r\n\r\n> is it reasonable to consider all of the data (query and results) when determining the overall byte size of the cache\r\n\r\nThis is what we would do ideally. The issue is that it feels a bit wrong to add an API for queries purely for `TermInSetQuery`and `GlobalOrdinalsQuery` which tend to be quite esoteric from a Lucene perspective. We could try to do `instanceof`checks for these two queries but it would not work all the time due to wrapper queries like `BooleanQuery`.\r\n\r\n> Also, one other thing I'm curious about is, why didn't the /my_index/_cache/clear call free up the memory, yet calling /*/_cache/clear did?\r\n\r\nThe reason is that the cache is shared by all indices. If we had one cache per index then applying a maximum size would be complicated. For instance if we were to exceed the maximum capacity, from which cache should we evict.\r\n\r\nSo to take back the Map analogy, when you call `clear` on the entire cache then we clear the top-level `Map<Query, Map<Segment, DocIdSet>>` but if you `clear` a single index then we only clear on the second level for segments that belong to the cleared index. So we keep queries which are stored at the top level.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329480675","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-329480675","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":329480675,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTQ4MDY3NQ==","user":{"login":"natelapp","id":15839796,"node_id":"MDQ6VXNlcjE1ODM5Nzk2","avatar_url":"https://avatars1.githubusercontent.com/u/15839796?v=4","gravatar_id":"","url":"https://api.github.com/users/natelapp","html_url":"https://github.com/natelapp","followers_url":"https://api.github.com/users/natelapp/followers","following_url":"https://api.github.com/users/natelapp/following{/other_user}","gists_url":"https://api.github.com/users/natelapp/gists{/gist_id}","starred_url":"https://api.github.com/users/natelapp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/natelapp/subscriptions","organizations_url":"https://api.github.com/users/natelapp/orgs","repos_url":"https://api.github.com/users/natelapp/repos","events_url":"https://api.github.com/users/natelapp/events{/privacy}","received_events_url":"https://api.github.com/users/natelapp/received_events","type":"User","site_admin":false},"created_at":"2017-09-14T13:24:59Z","updated_at":"2017-09-14T13:24:59Z","author_association":"NONE","body":"Thank you for the explanation.  I think the safest thing short-term is to try disabling query cache completely and hope that performance isn't _too_ bad.  We were already planning on moving away from parent/child, which seems to be our best bet long-term.\r\n\r\nThanks again for helping us figure out what was causing this.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329483821","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-329483821","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":329483821,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTQ4MzgyMQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-09-14T13:36:14Z","updated_at":"2017-09-14T13:36:14Z","author_association":"CONTRIBUTOR","body":"We used to rely a lot on the cache for performance to be good (especially pre-2.0) but this is something that we worked on fixing in recent versions, so things shouldn't be too bad with a disabled query cache.\r\n\r\nThanks for reporting this bug, it can definitely be problematic in some cases and we need to find a way to fix/avoid it.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/379290092","html_url":"https://github.com/elastic/elasticsearch/issues/26587#issuecomment-379290092","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26587","id":379290092,"node_id":"MDEyOklzc3VlQ29tbWVudDM3OTI5MDA5Mg==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2018-04-06T15:31:50Z","updated_at":"2018-04-06T15:31:50Z","author_association":"CONTRIBUTOR","body":"This was fixed in Elasticsearch 6.2 which introduced the ability for queries to opt out of caching (https://issues.apache.org/jira/browse/LUCENE-8017).","performed_via_github_app":null}]