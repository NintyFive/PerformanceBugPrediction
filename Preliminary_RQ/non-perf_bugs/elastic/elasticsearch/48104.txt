{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/48104","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/48104/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/48104/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/48104/events","html_url":"https://github.com/elastic/elasticsearch/issues/48104","id":507646542,"node_id":"MDU6SXNzdWU1MDc2NDY1NDI=","number":48104,"title":"[DOC] Clarification around low disk watermark documentation","user":{"login":"renshuki","id":7076736,"node_id":"MDQ6VXNlcjcwNzY3MzY=","avatar_url":"https://avatars2.githubusercontent.com/u/7076736?v=4","gravatar_id":"","url":"https://api.github.com/users/renshuki","html_url":"https://github.com/renshuki","followers_url":"https://api.github.com/users/renshuki/followers","following_url":"https://api.github.com/users/renshuki/following{/other_user}","gists_url":"https://api.github.com/users/renshuki/gists{/gist_id}","starred_url":"https://api.github.com/users/renshuki/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/renshuki/subscriptions","organizations_url":"https://api.github.com/users/renshuki/orgs","repos_url":"https://api.github.com/users/renshuki/repos","events_url":"https://api.github.com/users/renshuki/events{/privacy}","received_events_url":"https://api.github.com/users/renshuki/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"},{"id":23715,"node_id":"MDU6TGFiZWwyMzcxNQ==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Edocs","name":">docs","color":"db755e","default":false,"description":"General docs changes"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2019-10-16T06:37:05Z","updated_at":"2019-10-16T12:15:33Z","closed_at":"2019-10-16T12:15:33Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"<!-- Feature request -->\r\n\r\n**Documentation enhancement request**:\r\n\r\nCurrently the documentation says:\r\n\r\n> `cluster.routing.allocation.disk.watermark.low`\r\n> Controls the low watermark for disk usage. It defaults to 85%, meaning that Elasticsearch will not allocate shards to nodes that have more than 85% disk used. It can also be set to an absolute byte value (like 500mb) to prevent Elasticsearch from allocating shards if less than the specified amount of space is available. This setting has no effect on the primary shards of newly-created indices or, specifically, any shards that have never previously been allocated.\r\n\r\nThe `This setting has no effect on the primary shards of newly-created indices or, specifically, any shards that have never previously been allocated.` part is quite confusing. In fact, it has effect on replica shards as these can't be allocated when low watermark is tripped (even on newly-created indices).\r\n\r\n**To sum up:**\r\n- primary shards of newly created indices are unaffected and will be allocated.\r\n- replica shards of newly created indices are affected and won't be allocated.\r\n- re-allocated / re-balanced shards won't be allocated to the node.\r\n\r\n## Enhancement\r\n\r\nChange: \r\n`This setting has no effect on the primary shards of newly-created indices or, specifically, any shards that have never previously been allocated.`\r\n\r\nTo something like this: \r\n`This setting has no effect on the primary shards of newly-created indices but replicas won't be allocated. Existing shards can't be re-allocated or re-balanced to a node with low watermark.`\r\n\r\n## Test for a newly-created indice (shards allocation)\r\n\r\n> [instance-0000000001] low disk watermark [60gb] exceeded on [TLltk6a9SQOnjKgJndoHkQ][instance-0000000001][/app/data/nodes/0] free: 59.8gb[99.6%], replicas will not be assigned to this node\r\n\r\n```\r\nPUT replicalloc4\r\n{\r\n  \"settings\": {\r\n    \"number_of_replicas\": 1\r\n  }\r\n}\r\n\r\nGET _cat/shards?v&index=replicalloc*\r\nreplicalloc4 0     p      STARTED       0  230b 10.46.47.236 instance-0000000002\r\nreplicalloc4 0     r      UNASSIGNED\r\n```\r\n\r\n### Test for existing shard re-allocation\r\n\r\n```\r\nPOST /_cluster/reroute\r\n{\r\n  \"commands\" : [\r\n      {\r\n          \"move\" : {\r\n              \"index\" : \"replicalloc4\", \"shard\" : 0,\r\n              \"from_node\" : \"instance-0000000002\", \"to_node\" : \"instance-0000000001\"\r\n          }\r\n      }\r\n ]\r\n}\r\n```\r\n\r\noutput:\r\n> \"[move_allocation] can't move 0, from {instance-0000000002}{ec9jxAyqQpeQi55_LGhBKg}{nrzEfkTnS2muUQ9tslDOxg}{10.46.47.236}{10.46.47.236:19287}{dim}{logical_availability_zone=zone-1, server_name=instance-0000000002.b3ec28ce2c9a4c2a85e517d28973d6da, availability_zone=asia-northeast1-b, xpack.installed=true, region=unknown-region, instance_configuration=gcp.data.highio.1}, to {instance-0000000001}{TLltk6a9SQOnjKgJndoHkQ}{nPxoZShVStm_gNQ9p1WTBg}{10.46.47.234}{10.46.47.234:19763}{dim}{logical_availability_zone=zone-0, server_name=instance-0000000001.b3ec28ce2c9a4c2a85e517d28973d6da, availability_zone=asia-northeast1-a, xpack.installed=true, instance_configuration=gcp.data.highio.1, region=unknown-region}, since its not allowed, reason: [YES(shard has no previous failures)][YES(shard is primary and can be allocated)][YES(explicitly ignoring any disabling of allocation due to manual allocation commands via the reroute API)][YES(can relocate primary shard from a node with version [7.4.0] to a node with equal-or-newer version [7.4.0])][YES(no snapshots are currently running)][YES(ignored as shard is not being recovered from a snapshot)][YES(node passes include/exclude/require filters)][YES(the shard does not exist on the same node)][NO(the node is above the low watermark cluster setting [cluster.routing.allocation.disk.watermark.low=60gb], having less than the minimum required [60gb] free space, actual free: [59.7gb])][YES(below shard recovery limit of outgoing: [0 < 2] incoming: [0 < 2])][YES(total shard limits are disabled: [index: -1, cluster: -1] <= 0)][YES(node meets all awareness attribute requirements)]\"\r\n\r\n➡️ `[NO(the node is above the low watermark cluster setting [cluster.routing.allocation.disk.watermark.low=60gb], having less than the minimum required [60gb] free space, actual free: [59.7gb])]`\r\n\r\nAny thoughts on this? Or suggestion for better wording?\r\nI would be happy to make a PR for that.","closed_by":{"login":"renshuki","id":7076736,"node_id":"MDQ6VXNlcjcwNzY3MzY=","avatar_url":"https://avatars2.githubusercontent.com/u/7076736?v=4","gravatar_id":"","url":"https://api.github.com/users/renshuki","html_url":"https://github.com/renshuki","followers_url":"https://api.github.com/users/renshuki/followers","following_url":"https://api.github.com/users/renshuki/following{/other_user}","gists_url":"https://api.github.com/users/renshuki/gists{/gist_id}","starred_url":"https://api.github.com/users/renshuki/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/renshuki/subscriptions","organizations_url":"https://api.github.com/users/renshuki/orgs","repos_url":"https://api.github.com/users/renshuki/repos","events_url":"https://api.github.com/users/renshuki/events{/privacy}","received_events_url":"https://api.github.com/users/renshuki/received_events","type":"User","site_admin":false},"performed_via_github_app":null}