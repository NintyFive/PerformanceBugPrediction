{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/27911","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27911/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27911/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27911/events","html_url":"https://github.com/elastic/elasticsearch/issues/27911","id":283318308,"node_id":"MDU6SXNzdWUyODMzMTgzMDg=","number":27911,"title":"Shard's primary and replica documents inconsistency","user":{"login":"anosulchik","id":14876304,"node_id":"MDQ6VXNlcjE0ODc2MzA0","avatar_url":"https://avatars2.githubusercontent.com/u/14876304?v=4","gravatar_id":"","url":"https://api.github.com/users/anosulchik","html_url":"https://github.com/anosulchik","followers_url":"https://api.github.com/users/anosulchik/followers","following_url":"https://api.github.com/users/anosulchik/following{/other_user}","gists_url":"https://api.github.com/users/anosulchik/gists{/gist_id}","starred_url":"https://api.github.com/users/anosulchik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/anosulchik/subscriptions","organizations_url":"https://api.github.com/users/anosulchik/orgs","repos_url":"https://api.github.com/users/anosulchik/repos","events_url":"https://api.github.com/users/anosulchik/events{/privacy}","received_events_url":"https://api.github.com/users/anosulchik/received_events","type":"User","site_admin":false},"labels":[{"id":145572580,"node_id":"MDU6TGFiZWwxNDU1NzI1ODA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/CRUD","name":":Distributed/CRUD","color":"0e8a16","default":false,"description":"A catch all label for issues around indexing, updating and getting a doc by id. Not search."},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2017-12-19T17:56:10Z","updated_at":"2018-02-16T12:20:17Z","closed_at":"2018-02-16T12:20:17Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version**: 2.4.0\r\n**Plugins installed**: cloud-aws, hq, kopf, reindexing, whatson\r\n**JVM version**: 1.8.0_92\r\n**OS version**: Amazon Linux\r\n\r\n**Elasticsearch setup**:\r\n24 nodes with `-Xms10529m -Xmx10529m` for elasticsearch on each\r\n360 GB dedicated volume for elasticsearch data on each node\r\n**elasticsearch.yml**:\r\n```action.disable_delete_all_indices: true\r\nbootstrap.mlockall: true\r\ncloud.aws.access_key: xxx\r\ncloud.aws.region: us-east-1\r\ncloud.aws.secret_key: xxx\r\ncloud.node.auto_attributes: true\r\ncluster.name: es-production-cluster\r\ndiscovery.ec2.groups: sg-0123456\r\ndiscovery.ec2.host_type: private_ip\r\ndiscovery.type: ec2\r\ndiscovery.zen.minimum_master_nodes: 2\r\ndiscovery.zen.ping.multicast.enabled: false\r\ngateway.expected_nodes: 2\r\ngateway.recover_after_nodes: 1\r\ngateway.recover_after_time: 5m\r\nindex.search.slowlog.threshold.fetch.info: 800ms\r\nindex.search.slowlog.threshold.fetch.trace: 0ms\r\nindex.search.slowlog.threshold.fetch.warn: 1s\r\nindex.search.slowlog.threshold.query.info: 300ms\r\nindex.search.slowlog.threshold.query.trace: 0ms\r\nindex.search.slowlog.threshold.query.warn: 500ms\r\nindex.translog.flush_threshold_size: 500mb\r\nindices.memory.index_buffer_size: 1504m\r\nindices.memory.max_shard_index_buffer_size: 1504m\r\nindices.memory.min_shard_index_buffer_size: 1504m\r\nnetwork.host: 0.0.0.0\r\nnetwork.publish_host: 172.1.2.3\r\nnode.data: true\r\nnode.master: true\r\nnode.max_local_storage_nodes: 1\r\nnode.name: es-production-node1\r\nscript.indexed: true\r\nscript.inline: true\r\nthreadpool.bulk.queue_size: 150\r\npath.conf: /etc/elasticsearch/01\r\npath.data: /opt/elasticsearch/data/01\r\npath.work: /tmp/elasticsearch/01\r\npath.logs: /var/log/elasticsearch/01\r\npath.plugins: /usr/share/elasticsearch/plugins/01\r\n```\r\n\r\n**Index settings**:\r\n```\r\n{\r\n  \"strings\" : {\r\n    \"settings\" : {\r\n        \"mapping\" : {\r\n          \"nested_fields\" : {\r\n            \"limit\" : \"250\"\r\n          }\r\n        },\r\n        \"refresh_interval\" : \"1s\",\r\n        \"number_of_shards\" : \"48\",\r\n        \"creation_date\" : \"1475183908402\",\r\n        \"analysis\" : {\r\n          \"analyzer\" : {\r\n            \"custom_keywords_only\" : {\r\n              \"filter\" : \"lowercase\",\r\n              \"tokenizer\" : \"keyword\"\r\n            }\r\n          }\r\n        },\r\n        \"number_of_replicas\" : \"1\",\r\n        \"version\" : {\r\n          \"created\" : \"2040099\"\r\n        },\r\n        \"policy\" : {\r\n          \"max_merge_at_once\" : \"2\"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\nWe faced the problem with primary/replica shard inconsistency that correlates with node(s) rolling restart. In other words, when we restart one of production nodes, number of documents in primary and replica for one (or more) shards differs e.g. by 100 documents. We use _cat/shards to see difference between primary and replica for shards. This difference doesn't recovers itself until we re-index those documents.\r\n\r\nUsing _preference query parameter we figured out that mentioned inconsistency implies that documents are present in primary but are missing in replica. Moreover, according to our logs and `created` field in the documents, it’s clear that mentioned documents were created before restart of the node. So replica rolled back somehow to past and this triggered inconsistency after node restart.\r\n\r\nWe also noticed that arise of documents missing in replica and present in primary, correlates with spike of number of failed indexing operations (we have both metrics in our monitoring system). At the same time our application that uses bulk requests to index data doesn’t have anything in its logs despite it’s logic, according to which it examines response from elasticsearch and throws error into logs in case found non empty failed counter in response. Our assumption is that those failed indexing operations are caused by translog replay that happens every time when shards is recovered.\r\n\r\nOur process of restarting node is as follows:\r\n\r\n1. Disable shards allocation in cluster.\r\n2. Restart elasticsearch node, one at a time.\r\n3. Enable shards allocation.\r\n4. Wait until shards are recovered, relocated and cluster status becomes green.\r\n\r\nOur traffic pattern includes constant indexing and search traffic sent to elasticsearch cluster during nodes restart. \r\n\r\nPlease kindly share your thoughts, ideas why inconsistency may happen after node restart and possible ways how to avoid it but still be able to maintain cluster nodes (they needs to be restarted from time to time e.g. to upgrade OS version etc.). Upgrade to elasticsearch 6.0 currently is not possible for multiple reasons. In case you need more information e.g. logs -- please let me know.\r\n\r\nThank you!","closed_by":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"performed_via_github_app":null}