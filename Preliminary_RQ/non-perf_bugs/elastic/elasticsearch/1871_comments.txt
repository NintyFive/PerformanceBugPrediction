[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5258832","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5258832","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5258832,"node_id":"MDEyOklzc3VlQ29tbWVudDUyNTg4MzI=","user":{"login":"jprante","id":635745,"node_id":"MDQ6VXNlcjYzNTc0NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/635745?v=4","gravatar_id":"","url":"https://api.github.com/users/jprante","html_url":"https://github.com/jprante","followers_url":"https://api.github.com/users/jprante/followers","following_url":"https://api.github.com/users/jprante/following{/other_user}","gists_url":"https://api.github.com/users/jprante/gists{/gist_id}","starred_url":"https://api.github.com/users/jprante/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jprante/subscriptions","organizations_url":"https://api.github.com/users/jprante/orgs","repos_url":"https://api.github.com/users/jprante/repos","events_url":"https://api.github.com/users/jprante/events{/privacy}","received_events_url":"https://api.github.com/users/jprante/received_events","type":"User","site_admin":false},"created_at":"2012-04-21T08:00:59Z","updated_at":"2012-04-21T08:00:59Z","author_association":"CONTRIBUTOR","body":"You have a crashed index. The reason can be OOM, and the Java VM can't continue when resources are tight, even writing the index will fail (but cluster state updates might not). There should be more severe errors in the logs, stating \"OutOfMemoryException\". The situation can become worse when you kill elasticsearch with \"kill -9\", instead try stop indexing and shut it down with the shutdown API call. To avoid OOM, adjust JVM heap memory to a size as high as possible for your system. Don't rely on the default settings, they are very small, for instance for logstash.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5265820","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5265820","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5265820,"node_id":"MDEyOklzc3VlQ29tbWVudDUyNjU4MjA=","user":{"login":"arimus","id":228394,"node_id":"MDQ6VXNlcjIyODM5NA==","avatar_url":"https://avatars0.githubusercontent.com/u/228394?v=4","gravatar_id":"","url":"https://api.github.com/users/arimus","html_url":"https://github.com/arimus","followers_url":"https://api.github.com/users/arimus/followers","following_url":"https://api.github.com/users/arimus/following{/other_user}","gists_url":"https://api.github.com/users/arimus/gists{/gist_id}","starred_url":"https://api.github.com/users/arimus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arimus/subscriptions","organizations_url":"https://api.github.com/users/arimus/orgs","repos_url":"https://api.github.com/users/arimus/repos","events_url":"https://api.github.com/users/arimus/events{/privacy}","received_events_url":"https://api.github.com/users/arimus/received_events","type":"User","site_admin":false},"created_at":"2012-04-22T06:07:39Z","updated_at":"2012-04-22T06:07:39Z","author_association":"NONE","body":"So once this happens, is there any way to recover/repair the index?  I assume there is some way to get indexes happy again, rather than having to throw everything away?\n\nBtw, on restart there are no memory issues, although there may have been initially.  When elasticsearch tries to spin back up again (like during a regular restart), these exceptions are still present.  \n\np.s. I have since pushed the indexes up to 16GB, which seems to have helped some of my issues although these restart issues persist.\n\nThanks\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5377338","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5377338","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5377338,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNzczMzg=","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2012-04-27T08:51:48Z","updated_at":"2012-04-27T08:51:48Z","author_association":"MEMBER","body":"OOM will not cause the index or shards to be removed, and it seems liek they did. Did logstash that you started had an elasticsearch version 0.18.x? Was it on the same box as elasticsearch?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5382138","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5382138","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5382138,"node_id":"MDEyOklzc3VlQ29tbWVudDUzODIxMzg=","user":{"login":"arimus","id":228394,"node_id":"MDQ6VXNlcjIyODM5NA==","avatar_url":"https://avatars0.githubusercontent.com/u/228394?v=4","gravatar_id":"","url":"https://api.github.com/users/arimus","html_url":"https://github.com/arimus","followers_url":"https://api.github.com/users/arimus/followers","following_url":"https://api.github.com/users/arimus/following{/other_user}","gists_url":"https://api.github.com/users/arimus/gists{/gist_id}","starred_url":"https://api.github.com/users/arimus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arimus/subscriptions","organizations_url":"https://api.github.com/users/arimus/orgs","repos_url":"https://api.github.com/users/arimus/repos","events_url":"https://api.github.com/users/arimus/events{/privacy}","received_events_url":"https://api.github.com/users/arimus/received_events","type":"User","site_admin":false},"created_at":"2012-04-27T14:10:54Z","updated_at":"2012-04-27T14:10:54Z","author_association":"NONE","body":"Nothing appeared to be removed actually.  It's just that the indexes seemed to have issues upon restart.  I did initially have logstash start up with elasticsearch embedded and try to multicast join this instance (I thought unsuccessfully), however I built logstash with ES 0.19.2 and not 0.18.x.  You can see the patch that I applied here if you want...\n\nhttps://github.com/arimus/log-processing/blob/master/rpmbuild/SOURCES/logstash_amqp09_queuefix.patch\n\nAll that said, if an index does get into a corrupt (even partially) state, is there any way to repair it or do you just need to delete it?  It appeared that despite the log messages with errors, the index was still happily being updated and was searchable.  It's didn't seem completely foobar'd, which is why I assumed there might be something to go in and do some form of consistency check on it.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5384694","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5384694","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5384694,"node_id":"MDEyOklzc3VlQ29tbWVudDUzODQ2OTQ=","user":{"login":"jprante","id":635745,"node_id":"MDQ6VXNlcjYzNTc0NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/635745?v=4","gravatar_id":"","url":"https://api.github.com/users/jprante","html_url":"https://github.com/jprante","followers_url":"https://api.github.com/users/jprante/followers","following_url":"https://api.github.com/users/jprante/following{/other_user}","gists_url":"https://api.github.com/users/jprante/gists{/gist_id}","starred_url":"https://api.github.com/users/jprante/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jprante/subscriptions","organizations_url":"https://api.github.com/users/jprante/orgs","repos_url":"https://api.github.com/users/jprante/repos","events_url":"https://api.github.com/users/jprante/events{/privacy}","received_events_url":"https://api.github.com/users/jprante/received_events","type":"User","site_admin":false},"created_at":"2012-04-27T16:14:47Z","updated_at":"2012-04-27T16:14:47Z","author_association":"CONTRIBUTOR","body":"There is an index checker at org.apache.lucene.index.CheckIndex but it is not exposed via an Elasticsearch API yet. Nice opportunity for writing a plugin, though.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5384941","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5384941","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5384941,"node_id":"MDEyOklzc3VlQ29tbWVudDUzODQ5NDE=","user":{"login":"arimus","id":228394,"node_id":"MDQ6VXNlcjIyODM5NA==","avatar_url":"https://avatars0.githubusercontent.com/u/228394?v=4","gravatar_id":"","url":"https://api.github.com/users/arimus","html_url":"https://github.com/arimus","followers_url":"https://api.github.com/users/arimus/followers","following_url":"https://api.github.com/users/arimus/following{/other_user}","gists_url":"https://api.github.com/users/arimus/gists{/gist_id}","starred_url":"https://api.github.com/users/arimus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arimus/subscriptions","organizations_url":"https://api.github.com/users/arimus/orgs","repos_url":"https://api.github.com/users/arimus/repos","events_url":"https://api.github.com/users/arimus/events{/privacy}","received_events_url":"https://api.github.com/users/arimus/received_events","type":"User","site_admin":false},"created_at":"2012-04-27T16:27:30Z","updated_at":"2012-04-27T16:27:30Z","author_association":"NONE","body":"Good to know, thanks.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5387168","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5387168","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5387168,"node_id":"MDEyOklzc3VlQ29tbWVudDUzODcxNjg=","user":{"login":"jprante","id":635745,"node_id":"MDQ6VXNlcjYzNTc0NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/635745?v=4","gravatar_id":"","url":"https://api.github.com/users/jprante","html_url":"https://github.com/jprante","followers_url":"https://api.github.com/users/jprante/followers","following_url":"https://api.github.com/users/jprante/following{/other_user}","gists_url":"https://api.github.com/users/jprante/gists{/gist_id}","starred_url":"https://api.github.com/users/jprante/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jprante/subscriptions","organizations_url":"https://api.github.com/users/jprante/orgs","repos_url":"https://api.github.com/users/jprante/repos","events_url":"https://api.github.com/users/jprante/events{/privacy}","received_events_url":"https://api.github.com/users/jprante/received_events","type":"User","site_admin":false},"created_at":"2012-04-27T18:12:38Z","updated_at":"2012-04-27T18:12:38Z","author_association":"CONTRIBUTOR","body":"Hm. I was half in error. CheckIndex is already integrated in Elasticsearch, I just discovered a setting index.shard.check_on_startup. This parameter can be set to 'true' (default is 'false') and a node will run the checkIndex() method of org.apache.lucene.index.CheckIndex at startup time on each shard. This may take a long time to complete. But, CheckIndex also offers a fixIndex() method, and that is not used when setting index.shard.check_on_startup to 'true'. Well, in the end, I think an extra step of index repair after a successful check could be triggered by a new parameter index.shard.check_on_startup_and_fix or something - of course, only for the not so faint hearted. I'll try to set up a pull request.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5401934","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5401934","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5401934,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MDE5MzQ=","user":{"login":"arimus","id":228394,"node_id":"MDQ6VXNlcjIyODM5NA==","avatar_url":"https://avatars0.githubusercontent.com/u/228394?v=4","gravatar_id":"","url":"https://api.github.com/users/arimus","html_url":"https://github.com/arimus","followers_url":"https://api.github.com/users/arimus/followers","following_url":"https://api.github.com/users/arimus/following{/other_user}","gists_url":"https://api.github.com/users/arimus/gists{/gist_id}","starred_url":"https://api.github.com/users/arimus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arimus/subscriptions","organizations_url":"https://api.github.com/users/arimus/orgs","repos_url":"https://api.github.com/users/arimus/repos","events_url":"https://api.github.com/users/arimus/events{/privacy}","received_events_url":"https://api.github.com/users/arimus/received_events","type":"User","site_admin":false},"created_at":"2012-04-29T01:55:32Z","updated_at":"2012-04-29T01:55:32Z","author_association":"NONE","body":"Yeah, that would be pretty handy.  It might also be nice to expose a flag via the API that simply returns if any runtime index errors have occurred and/or the startup CheckIndex found any issues.  This way, a fix index could be triggered, preferably also through the management API, at the most convenient time: maintenance window, low traffic hours, etc.\n\nWhat would really be nice is if you could keep that specific index offline and repair it while everything else was up and running, then bring it back online once ready.  Of course that's much more complicated, since you probably still need to be able to receive updates for that index as soon as ES comes back online (e.g. for todays index which got corrupted, but still has more updates coming in).  Then you'd have to rename the corrupt index and either merge those entries back in once fixed (likely very heavy) or let apps include the new index in their searches (by convention, all index searches, etc.?)\n\n/foodForThought\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5414606","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5414606","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5414606,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MTQ2MDY=","user":{"login":"jprante","id":635745,"node_id":"MDQ6VXNlcjYzNTc0NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/635745?v=4","gravatar_id":"","url":"https://api.github.com/users/jprante","html_url":"https://github.com/jprante","followers_url":"https://api.github.com/users/jprante/followers","following_url":"https://api.github.com/users/jprante/following{/other_user}","gists_url":"https://api.github.com/users/jprante/gists{/gist_id}","starred_url":"https://api.github.com/users/jprante/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jprante/subscriptions","organizations_url":"https://api.github.com/users/jprante/orgs","repos_url":"https://api.github.com/users/jprante/repos","events_url":"https://api.github.com/users/jprante/events{/privacy}","received_events_url":"https://api.github.com/users/jprante/received_events","type":"User","site_admin":false},"created_at":"2012-04-30T10:02:11Z","updated_at":"2012-04-30T10:02:11Z","author_association":"CONTRIBUTOR","body":"arimus, kimchy accepted a pull request, fixIndex() is now included: https://github.com/elasticsearch/elasticsearch/pull/1890 \n\nExposing a fix method via API has some drawbacks. First, fixing the Lucene segments while the node is up is damn risky. It would impact the whole cluster performance and stability, and would not stop the cluster from being more unstable, you described the problem when new data is stil coming in. Second, node shutdown and startup API usage for checking and fixing indexes is potentially a candidate for race conditions. So, a node must leave the cluster and be stopped to perform the lengthy check and the subsequent optional fix. \n\nThe method which is implemented by kimchy is to take the defect node down (shutdown api or kill), let the ES cluster readjust to the new situation, enable a \"fix\" check setting value explicitly in the defect node index setting and start the node again. Then, the node will enter a quite lengthy check and repair mode, before it re-enables all the index structures and tries to rejoin the cluster.\n\nA warning: because fixing a broken Lucene segment can lead to data loss, this is not for the faint hearted. It should never be assumed fixIndex() should be called on a regular basis at every startup. There is no guarantee at all that fixIndex() will cure all defects. It is a last resort for the advanced adaministrator if nothing else helps and data loss is accepted.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5421871","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5421871","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5421871,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjE4NzE=","user":{"login":"arimus","id":228394,"node_id":"MDQ6VXNlcjIyODM5NA==","avatar_url":"https://avatars0.githubusercontent.com/u/228394?v=4","gravatar_id":"","url":"https://api.github.com/users/arimus","html_url":"https://github.com/arimus","followers_url":"https://api.github.com/users/arimus/followers","following_url":"https://api.github.com/users/arimus/following{/other_user}","gists_url":"https://api.github.com/users/arimus/gists{/gist_id}","starred_url":"https://api.github.com/users/arimus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arimus/subscriptions","organizations_url":"https://api.github.com/users/arimus/orgs","repos_url":"https://api.github.com/users/arimus/repos","events_url":"https://api.github.com/users/arimus/events{/privacy}","received_events_url":"https://api.github.com/users/arimus/received_events","type":"User","site_admin":false},"created_at":"2012-04-30T16:56:25Z","updated_at":"2012-04-30T16:56:25Z","author_association":"NONE","body":"Great, thanks for adding that!  And yes, I understand the difficulties in trying to do any type of repairs while any of the cluster is online.  That was more of a theoretical nice to have :)  With a multi-node ES cluster, it would probably fairly pointless anyways.\n\nQuestion though.  If you have a couple nodes, fix that node's corrupt indexes (results in some data loss), then re-add back into the cluster...will the ES cluster detect and replicate data from the good node back over to the repaired node or is replication only handled at the time that data was initially added?  Basically, what practical affects does this have on the overal cluster state and health?  If there is already documentation on this somewhere, please advise...I don't want to eat up more of your time.\n\nThanks again.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5422325","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5422325","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5422325,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjIzMjU=","user":{"login":"jprante","id":635745,"node_id":"MDQ6VXNlcjYzNTc0NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/635745?v=4","gravatar_id":"","url":"https://api.github.com/users/jprante","html_url":"https://github.com/jprante","followers_url":"https://api.github.com/users/jprante/followers","following_url":"https://api.github.com/users/jprante/following{/other_user}","gists_url":"https://api.github.com/users/jprante/gists{/gist_id}","starred_url":"https://api.github.com/users/jprante/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jprante/subscriptions","organizations_url":"https://api.github.com/users/jprante/orgs","repos_url":"https://api.github.com/users/jprante/repos","events_url":"https://api.github.com/users/jprante/events{/privacy}","received_events_url":"https://api.github.com/users/jprante/received_events","type":"User","site_admin":false},"created_at":"2012-04-30T17:18:48Z","updated_at":"2012-04-30T17:18:48Z","author_association":"CONTRIBUTOR","body":"If you had a good index before, well, the bad index should already have been replaced with good data by the ES recovery even without the fixIndex() method. Fixing the index should only be a method for ultima ratio, such as the recovery fails badly for a non-replicated index and ES finds no way to continue with a working index, with weird exceptions showing up in the log file, and there is no source data backup available so re-indexing is not an option.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/5422520","html_url":"https://github.com/elastic/elasticsearch/issues/1871#issuecomment-5422520","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/1871","id":5422520,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjI1MjA=","user":{"login":"arimus","id":228394,"node_id":"MDQ6VXNlcjIyODM5NA==","avatar_url":"https://avatars0.githubusercontent.com/u/228394?v=4","gravatar_id":"","url":"https://api.github.com/users/arimus","html_url":"https://github.com/arimus","followers_url":"https://api.github.com/users/arimus/followers","following_url":"https://api.github.com/users/arimus/following{/other_user}","gists_url":"https://api.github.com/users/arimus/gists{/gist_id}","starred_url":"https://api.github.com/users/arimus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arimus/subscriptions","organizations_url":"https://api.github.com/users/arimus/orgs","repos_url":"https://api.github.com/users/arimus/repos","events_url":"https://api.github.com/users/arimus/events{/privacy}","received_events_url":"https://api.github.com/users/arimus/received_events","type":"User","site_admin":false},"created_at":"2012-04-30T17:26:47Z","updated_at":"2012-04-30T17:26:47Z","author_association":"NONE","body":"Sounds completely reasonable.  Thanks!\n","performed_via_github_app":null}]