{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/19598","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19598/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19598/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19598/events","html_url":"https://github.com/elastic/elasticsearch/issues/19598","id":167556134,"node_id":"MDU6SXNzdWUxNjc1NTYxMzQ=","number":19598,"title":"Inefficient FVhighlighting when set many HighlightedField.","user":{"login":"dohykim","id":11845306,"node_id":"MDQ6VXNlcjExODQ1MzA2","avatar_url":"https://avatars2.githubusercontent.com/u/11845306?v=4","gravatar_id":"","url":"https://api.github.com/users/dohykim","html_url":"https://github.com/dohykim","followers_url":"https://api.github.com/users/dohykim/followers","following_url":"https://api.github.com/users/dohykim/following{/other_user}","gists_url":"https://api.github.com/users/dohykim/gists{/gist_id}","starred_url":"https://api.github.com/users/dohykim/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dohykim/subscriptions","organizations_url":"https://api.github.com/users/dohykim/orgs","repos_url":"https://api.github.com/users/dohykim/repos","events_url":"https://api.github.com/users/dohykim/events{/privacy}","received_events_url":"https://api.github.com/users/dohykim/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2016-07-26T09:03:50Z","updated_at":"2016-07-26T13:38:15Z","closed_at":"2016-07-26T13:38:15Z","author_association":"NONE","active_lock_reason":null,"body":"First it may related with Lucene.  but I heard ES guys contribute lucene so.\n\nwhen each searchDoc's highlightPhase, It calls highlighter's(In this case, FVH) highlight(highlighterContext).\nIn FastVectorHighlighter.java, loop for each requested 'HighlightedField' and getBestFragments.\ngetBestFragments method receive parameter[2] hitContext.docId() that uses for getting termVector of doc.\nIt finally reach org.apache.lucene.search.vectorhighlight.FieldTermStack.java and get termVector of doc.\n\nThe problem is \n'every highlightedField's getBestFragments method' call\n`\nfinal Fields vectors = reader.getTermVectors(docId);\n`  \nand it seems Inefficiently slow when search highlight result include (big document && many highlightedField). read whole doc's termvector with every highlightedField.\n\nmy testing machine:\nquad 1.87 ghz,\n8Gb memory,\nspinning disk.\nES-1.5.2 (relevent code not changed, when I saw)\n\nExample,\nmy query :\n`\n{\n \"from\" : 0,\n  \"size\" : 20,\n\n  \"query\" : {\n \"query about highlighted field and more\"},\n  \"explain\" : false,\n  \"fields\" : [ \"highlight\", \"fileRevision\", \"ownerNameUnigram\", \"ownerName\", \"ownerId\", \"timeLastModified\", \"size\" ],\n  \"sort\" : [ {\n    \"_score\" : { }\n  } ],\n  \"highlight\" : {\n    \"pre_tags\" : [ \"<strong>\" ],\n    \"post_tags\" : [ \"</strong>\" ],\n    \"order\" : \"score\",\n    \"fragment_size\" : 128,\n    \"number_of_fragments\" : 10,\n    \"require_field_match\" : true,\n    \"type\" : \"fvh\",\n    \"fields\" : [ {\n      \"ownerName\" : { }\n    }, {\n      \"fileName_ko\" : { }\n    }, {\n      \"fileName_en\" : { }\n    }, {\n      \"fileName_id\" : { }\n    }, {\n      \"fileName_es\" : { }\n    }, {\n      \"fileName_zh\" : { }\n    }, {\n      \"fileName_ja\" : { }\n    }, {\n      \"fileName_it\" : { }\n    }, {\n      \"fileName_ru\" : { }\n    }, {\n      \"fileName_pt\" : { }\n    }, {\n      \"fileName_hi\" : { }\n    }, {\n      \"fileName_etc\" : { }\n    }, {\n      \"contents_ko\" : { }\n    }, {\n      \"contents_ko.ngram\" : { }\n    }, {\n      \"contents_en\" : { }\n    }, {\n      \"contents_en.ngram\" : { }\n    }, {\n      \"contents_id\" : { }\n    }, {\n      \"contents_id.ngram\" : { }\n    }, {\n      \"contents_es\" : { }\n    }, {\n      \"contents_es.ngram\" : { }\n    }, {\n      \"contents_zh\" : { }\n    }, {\n      \"contents_zh.ngram\" : { }\n    }, {\n      \"contents_ja\" : { }\n    }, {\n      \"contents_ja.ngram\" : { }\n    }, {\n      \"contents_it\" : { }\n    }, {\n      \"contents_it.ngram\" : { }\n    }, {\n      \"contents_ru\" : { }\n    }, {\n      \"contents_ru.ngram\" : { }\n    }, {\n      \"contents_pt\" : { }\n    }, {\n      \"contents_pt.ngram\" : { }\n    }, {\n      \"contents_hi\" : { }\n    }, {\n      \"contents_hi.ngram\" : { }\n    }, {\n      \"contents_etc\" : { }\n    }, {\n      \"contents_etc.ngram\" : { }\n    } ]\n  }\n}\n`\n\nTest\n[tookTime in millis, getBestFragments]\nmy doc 12538's every field getBestFragments took about 20ms. \nand total highlight phase tooks 705 ms.\n\nI have a sparse mapping field. that means\n 'doc 12538' field fileName_*\n[fileName_id , fileName_es, fileName_zh, fileName_ja, fileName_it, fileName_ru, fileName_pt, fileName_hi, fileName_etc]\nonly one field is filled with data among this array.\nIt's same to contents_\\* field.\n\ndangerous doc 12538 - \n[CONVOCADOS_TALLER_SALUDMENTAL_JULIO2014.xlsx.txt](https://github.com/elastic/elasticsearch/files/383450/CONVOCADOS_TALLER_SALUDMENTAL_JULIO2014.xlsx.txt)\n\n[2016-07-26 16:57:04,043][INFO ][root                     ] [4][FastVectorHighlighter.highlight] [22], filedName : fileName_id, docId : 12538\n[23], filedName : fileName_id, docId : 12538\n[20], filedName : fileName_es, docId : 12538\n[21], filedName : fileName_zh, docId : 12538\n[21], filedName : fileName_ja, docId : 12538\n[28], filedName : fileName_it, docId : 12538\n[26], filedName : fileName_ru, docId : 12538\n[24], filedName : fileName_pt, docId : 12538\n[22], filedName : fileName_hi, docId : 12538\n[22], filedName : fileName_etc, docId : 12538\n[22], filedName : contents_ko, docId : 12538\n[20], filedName : contents_ko.ngram, docId : 12538\n[19], filedName : contents_en, docId : 12538\n[19], filedName : contents_en.ngram, docId : 12538\n[20], filedName : contents_id, docId : 12538\n[20], filedName : contents_id.ngram, docId : 12538\n[19], filedName : contents_es, docId : 12538\n[18], filedName : contents_es.ngram, docId : 12538\n[19], filedName : contents_zh, docId : 12538\n[19], filedName : contents_zh.ngram, docId : 12538\n[19], filedName : contents_ja, docId : 12538\n[19], filedName : contents_ja.ngram, docId : 12538\n[18], filedName : contents_it, docId : 12538\n[18], filedName : contents_it.ngram, docId : 12538\n[18], filedName : contents_ru, docId : 12538\n[18], filedName : contents_ru.ngram, docId : 12538\n[20], filedName : contents_pt.ngram, docId : 12538\n[18], filedName : contents_hi, docId : 12538\n[18], filedName : contents_hi.ngram, docId : 12538\n[18], filedName : contents_etc, docId : 12538\n[19], filedName : contents_etc.ngram, docId : 12538\n[2016-07-26 16:57:04,654][INFO ][root                     ] highlight tooks : 705, docId : 12538\n\nand...\nreader.getTermVectors(docId) tooks.\nI didn't log sync with getBestFragments took. but i can see rough sequence and how it tooks.\nI think heavy analyzed doc (have big termvectors) impact my query. (around 20ms sequence.)\n\nlong tTime = System.currentTimeMillis();\nfinal Fields vectors = reader.getTermVectors(docId);\ntermVectorTimeLogging(\"tVectorTime : \"+(System.currentTimeMillis() - tTime));\n\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 24\ntVectorTime : 24\ntVectorTime : 23\ntVectorTime : 21\ntVectorTime : 20\ntVectorTime : 19\ntVectorTime : 19\ntVectorTime : 48\ntVectorTime : 19\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 19\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\n\nit may read once and pass through parameter I think.\n","closed_by":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"performed_via_github_app":null}