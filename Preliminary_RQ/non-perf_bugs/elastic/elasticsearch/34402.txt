{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/34402","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34402/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34402/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/34402/events","html_url":"https://github.com/elastic/elasticsearch/issues/34402","id":369144763,"node_id":"MDU6SXNzdWUzNjkxNDQ3NjM=","number":34402,"title":"Support for token filter/char filter that cleanups word boundaries (remove or replace with space) according to Unicode Text Segmentation algorithm","user":{"login":"Tiuser4567","id":15355667,"node_id":"MDQ6VXNlcjE1MzU1NjY3","avatar_url":"https://avatars2.githubusercontent.com/u/15355667?v=4","gravatar_id":"","url":"https://api.github.com/users/Tiuser4567","html_url":"https://github.com/Tiuser4567","followers_url":"https://api.github.com/users/Tiuser4567/followers","following_url":"https://api.github.com/users/Tiuser4567/following{/other_user}","gists_url":"https://api.github.com/users/Tiuser4567/gists{/gist_id}","starred_url":"https://api.github.com/users/Tiuser4567/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Tiuser4567/subscriptions","organizations_url":"https://api.github.com/users/Tiuser4567/orgs","repos_url":"https://api.github.com/users/Tiuser4567/repos","events_url":"https://api.github.com/users/Tiuser4567/events{/privacy}","received_events_url":"https://api.github.com/users/Tiuser4567/received_events","type":"User","site_admin":false},"labels":[{"id":142001965,"node_id":"MDU6TGFiZWwxNDIwMDE5NjU=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Analysis","name":":Search/Analysis","color":"0e8a16","default":false,"description":"How text is split into tokens"},{"id":929267538,"node_id":"MDU6TGFiZWw5MjkyNjc1Mzg=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/team-discuss","name":"team-discuss","color":"fbca04","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2018-10-11T14:19:29Z","updated_at":"2018-10-24T11:19:55Z","closed_at":"2018-10-24T11:19:54Z","author_association":"NONE","active_lock_reason":null,"body":"**Describe the feature**:\r\nVersion used: Elasticsearch 6.0\r\n\r\nHello,\r\nI would like to ask if it is possible to provide support for a token filter/char filter that would clean-up (remove or replace with space) word boundaries (according to the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29, the one used by the standard tokenizer) \r\nwhich can be used in a normalizer for the keyword data type.\r\n\r\nScenario:\r\nWe have 2 types of fields\r\n1. full text search (type: text + standard analyzer)\r\n2. exact match (type: keyword + keyword analyzer)\r\n\r\ne.g. given\r\n'can't see you' (enclosing single quotes are part of the text content)\r\nthe ff is currently tokenized as the ff:\r\ntype:text (standard analyzer) -> can't, see, you\r\ntype:keyword (keyword analyzer) -> 'can't see you'\r\n\r\nWhat we would like is for the exact match field to also not consider the apostrophes that were considered as word boundaries in the output tokens in the full text search (still includes the apostrophe that is not considered as word boundary e.g in \"can't\") \r\nso that we can search \"can't see you\" (without enclosing single quotes) in the full text search and exact match and get the same result.\r\n\r\ne.g.\r\ntype:keyword (keyword analyzer + normalizer w/ \"standard\" char filter) -> can't see you\r\n\r\nI know there is a pattern_replace character filter but I think it is not straightforward and easy to implement the algorithm as the condition for the word boundaries is diverse which will lead to a long regular expression and also need to consider the possible performance impact (due to using regular expression).\r\n\r\nThanks","closed_by":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"performed_via_github_app":null}