[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329850980","html_url":"https://github.com/elastic/elasticsearch/issues/26669#issuecomment-329850980","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26669","id":329850980,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTg1MDk4MA==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2017-09-15T17:44:04Z","updated_at":"2017-09-15T17:44:04Z","author_association":"MEMBER","body":"> When this happened, after a ~12 hour attempt/timeout/failure,\r\n\r\nDo you have any logs from this attempt/timeout/failure (stacktraces would be great to see where it timed out)\r\n\r\n> ElasticSearch would remove ALL Logstash indexes at or older than the index that it was trying to move to the cold nodes (i.e. all Logstash indexes that were in good shape on the cold nodes [about 30 indexes in my case], in addition to the Logstash index that it couldn't move because of the filesystem errors).\r\n\r\nThis sounds suspiciously like something like `curator` is running in a cron job that might be deleting all indices older than a certain date. Can you confirm whether you have a process like that in your environment?\r\n\r\nAny other relevant logs would be much appreciated, there should be logs at least of which indices were deleted and when.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/329876379","html_url":"https://github.com/elastic/elasticsearch/issues/26669#issuecomment-329876379","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26669","id":329876379,"node_id":"MDEyOklzc3VlQ29tbWVudDMyOTg3NjM3OQ==","user":{"login":"Daniel314","id":1891142,"node_id":"MDQ6VXNlcjE4OTExNDI=","avatar_url":"https://avatars0.githubusercontent.com/u/1891142?v=4","gravatar_id":"","url":"https://api.github.com/users/Daniel314","html_url":"https://github.com/Daniel314","followers_url":"https://api.github.com/users/Daniel314/followers","following_url":"https://api.github.com/users/Daniel314/following{/other_user}","gists_url":"https://api.github.com/users/Daniel314/gists{/gist_id}","starred_url":"https://api.github.com/users/Daniel314/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Daniel314/subscriptions","organizations_url":"https://api.github.com/users/Daniel314/orgs","repos_url":"https://api.github.com/users/Daniel314/repos","events_url":"https://api.github.com/users/Daniel314/events{/privacy}","received_events_url":"https://api.github.com/users/Daniel314/received_events","type":"User","site_admin":false},"created_at":"2017-09-15T19:22:11Z","updated_at":"2017-09-15T19:22:11Z","author_association":"NONE","body":"Lee,\n\nAside from one log that was a report of a failure to forcemerge (and a bunch of indicies disappearing right after that), I have no logs with anything interesting in them that I've seen that correlate with the disappearance of indexes.  I _think_ the first time I observed this issue (with the silent index deletion) was on Aug 23rd (filesystem timestamps indicate the directories were removed around 7:40am that morning).  Here is a link to a tarball of the logs for the past 30 days from the warm node (where the forcemege failed, and where the filesystem problems were happening): https://mesa5.coloradomesa.edu/~dan/0f48740bc7251ae2c2a661a3f928f05760c1dc31.tar.gz\n\n\nI have yet to see any ElasticSearch log file mention the removal of an index.  If you're curious or interested, here are the logs from one of the cold ('crypt') nodes for the past 30 days.  This node would have had shards from at least 10 indicies on it that disappeared the day that most of the indexes went missing:  https://mesa5.coloradomesa.edu/~dan/10d7714affb5cc21747601320fb15cbfb49a0b1a.tar.gz\n\nI wondered if curator might be the issue as well, but I'm 99% certain that it isn't: the index disappearance issue didn't stop until I fixed the filesystem problem on the warm node (it was happening daily for about 2 weeks).  Also, for the days that I checked closely, the curator delete did not correlate with the time that the indicies disappeared.  The curator delete happens in the evening (~7pm), but the index disappearance always happened around 12 hours later, with a variance of 1-3 hours from that).  I'm the only person who manages this system, so it wouldn't be anyone else using curator or deleting it, and the firewall rules are restrictive enough that only a few people can even access the system.  FWIW: the curator program is run from a master node, not the warm node.\n\nWhile the forcemerge failure might have resulted in the initial deletion of the indicies, I stopped running that process (which is done via curator_cli) a few days later and the index disappearance issue continued to happen.  In the daily script that does the deletion of old indexes and forcemerges, the forcemerge is one of the last steps in the script.  The only other things that might have run afterwards were a couple of index routing changes.  Before I completely commented out the forcemerge, it was the final step in that script.\n\nHere are the relevant lines from the script about curator deletes:\n   DAYSTOKEEP=29\n   echo \"Deleting indicies older than ${DAYSTOKEEP} days... (`date`)\"\n   curator_cli delete_indices --ignore_empty_list --filter_list '[\n       {\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"logstash-\"},\n       {\"filtertype\":\"age\", \"source\":\"name\", \"direction\":\"older\", \"timestring\":\"%Y.%m.%d\", \"unit\":\"days\", \"unit_count\":'${DAYSTOKEEP}'},\n       {\"filtertype\":\"none\"}]'\n\nThe current version of curator_cli is 5.2.0.  When the initial index disappearance happened I was running ElasticSearch 2.4.1 and an older version of curator_cli; I upgraded ES to 5.4.3 on Sep 6th (and curator_cli 5.2.0 a few days later).  The upgrade was done initially because of the issues I was seeing in the ES logfile with the forcemerge failures, and I thought the forcemerge errors and index disappearance issue might be related to that.  I didn't realize the warm node filesystem itself was the issue until later (because a full fsck -f run on the volume would come back clean) -- it was only later when I was seeing drive timeout messages from 'dmesg' that I got to the bottom of the filesystem issue.\n\nI realize that this is just a corner case that ElasticSearch isn't handling well, but how it is reacting to the issue is serious enough to warrant the bug report.\n\nThanks,\n\n\t- Daniel\n\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330224523","html_url":"https://github.com/elastic/elasticsearch/issues/26669#issuecomment-330224523","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26669","id":330224523,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDIyNDUyMw==","user":{"login":"Daniel314","id":1891142,"node_id":"MDQ6VXNlcjE4OTExNDI=","avatar_url":"https://avatars0.githubusercontent.com/u/1891142?v=4","gravatar_id":"","url":"https://api.github.com/users/Daniel314","html_url":"https://github.com/Daniel314","followers_url":"https://api.github.com/users/Daniel314/followers","following_url":"https://api.github.com/users/Daniel314/following{/other_user}","gists_url":"https://api.github.com/users/Daniel314/gists{/gist_id}","starred_url":"https://api.github.com/users/Daniel314/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Daniel314/subscriptions","organizations_url":"https://api.github.com/users/Daniel314/orgs","repos_url":"https://api.github.com/users/Daniel314/repos","events_url":"https://api.github.com/users/Daniel314/events{/privacy}","received_events_url":"https://api.github.com/users/Daniel314/received_events","type":"User","site_admin":false},"created_at":"2017-09-18T13:40:33Z","updated_at":"2017-09-18T13:40:33Z","author_association":"NONE","body":"Okay, egg on my face on this issue: my test environment had a cron script to trim indicies, and after an IP shuffle it was sending commands to the production system instead of itself.\nSorry for the noise and time wasted on this!\n\n\t- Daniel\n\n","performed_via_github_app":null}]