[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/95564938","html_url":"https://github.com/elastic/elasticsearch/issues/10719#issuecomment-95564938","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10719","id":95564938,"node_id":"MDEyOklzc3VlQ29tbWVudDk1NTY0OTM4","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2015-04-23T12:15:14Z","updated_at":"2015-04-23T12:15:14Z","author_association":"CONTRIBUTOR","body":"@jpountz , can you confirm my assumption: the parent bucket IDs aggs are asked to collect on are compact and ascending (0,1,2,3...) or do I have to allow for very sparse values (7,10342,...)?\nThis dictates if I use a map or an array in my sampler collection and also if I in turn should rebase IDs of the buckets that survive the \"best docs\" selection process.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/95570842","html_url":"https://github.com/elastic/elasticsearch/issues/10719#issuecomment-95570842","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10719","id":95570842,"node_id":"MDEyOklzc3VlQ29tbWVudDk1NTcwODQy","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2015-04-23T12:33:56Z","updated_at":"2015-04-23T12:33:56Z","author_association":"CONTRIBUTOR","body":"@markharwood Indeed they are fine to use as array indices. However I'm confused why you are mentioning \"surviving\" bucket as the sampler aggregator should not filter buckets? My assumption was that it would just compute a different sample on each bucket?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/95574249","html_url":"https://github.com/elastic/elasticsearch/issues/10719#issuecomment-95574249","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10719","id":95574249,"node_id":"MDEyOklzc3VlQ29tbWVudDk1NTc0MjQ5","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2015-04-23T12:45:38Z","updated_at":"2015-04-23T12:45:48Z","author_association":"CONTRIBUTOR","body":"> My assumption was that it would just compute a different sample on each bucket?\n\nMy bad. You are correct.\nOn a separate point - when replaying the deferred collection(s) I need to replay collects in docId order along with the choice of bucket ID. There may be more than one bucket per doc id. A convenient way of doing this which avoids extra object allocations is to take the ScoreDocs produced from each of the samples and sneak the bucketID into the \"shardIndex\" int value they hold and then sort them for replay. A bit hacky (casting long bucket ids to ints) but should be OK?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/95575518","html_url":"https://github.com/elastic/elasticsearch/issues/10719#issuecomment-95575518","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10719","id":95575518,"node_id":"MDEyOklzc3VlQ29tbWVudDk1NTc1NTE4","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2015-04-23T12:51:45Z","updated_at":"2015-04-23T12:51:45Z","author_association":"CONTRIBUTOR","body":"This hack sounds ok to me, if you use more than Integer.MAX_VALUE buckets to collect such an aggregator, you will have other issues anyway.\n","performed_via_github_app":null}]