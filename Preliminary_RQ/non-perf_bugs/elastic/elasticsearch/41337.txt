{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/41337","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/41337/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/41337/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/41337/events","html_url":"https://github.com/elastic/elasticsearch/issues/41337","id":434734100,"node_id":"MDU6SXNzdWU0MzQ3MzQxMDA=","number":41337,"title":"Nodes failing with OutOfMemoryError after about a week of uptime","user":{"login":"bra-fsn","id":820331,"node_id":"MDQ6VXNlcjgyMDMzMQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820331?v=4","gravatar_id":"","url":"https://api.github.com/users/bra-fsn","html_url":"https://github.com/bra-fsn","followers_url":"https://api.github.com/users/bra-fsn/followers","following_url":"https://api.github.com/users/bra-fsn/following{/other_user}","gists_url":"https://api.github.com/users/bra-fsn/gists{/gist_id}","starred_url":"https://api.github.com/users/bra-fsn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bra-fsn/subscriptions","organizations_url":"https://api.github.com/users/bra-fsn/orgs","repos_url":"https://api.github.com/users/bra-fsn/repos","events_url":"https://api.github.com/users/bra-fsn/events{/privacy}","received_events_url":"https://api.github.com/users/bra-fsn/received_events","type":"User","site_admin":false},"labels":[{"id":145572580,"node_id":"MDU6TGFiZWwxNDU1NzI1ODA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/CRUD","name":":Distributed/CRUD","color":"0e8a16","default":false,"description":"A catch all label for issues around indexing, updating and getting a doc by id. Not search."},{"id":146832564,"node_id":"MDU6TGFiZWwxNDY4MzI1NjQ=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Search","name":":Search/Search","color":"0e8a16","default":false,"description":"Search-related issues that do not fall into other categories"},{"id":111053151,"node_id":"MDU6TGFiZWwxMTEwNTMxNTE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/resiliency","name":"resiliency","color":"009800","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":25,"created_at":"2019-04-18T12:00:20Z","updated_at":"2019-05-02T08:21:06Z","closed_at":"2019-04-19T15:14:17Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version** (`bin/elasticsearch --version`): 6.6.2-oss\r\n\r\n**Plugins installed**: analysis-icu\r\n\r\n**JVM version** (`java -version`): 1.8.0_202\r\n\r\n**OS version** (`uname -a` if on a Unix-like system): FreeBSD 11.2-STABLE\r\n\r\n**Description of the problem including expected versus actual behavior**: I have a relatively stable cluster of 56 (40 data) nodes. My problem is that after some days/weeks of uptime some nodes start to fail with OOM. These tend to happen during the period, when some scrolls and more updates/deletes are done, but it's not rare outside that window as well.\r\nI've already tried to raise the heap from 8G to 12G, without success.\r\nWhen I restart the cluster, the nodes work fine for another week/days, so I guess if the heap size would be the sole problem, it should cause instabilities sooner.\r\n\r\n**Steps to reproduce**: I don't have a way to reproduce it programmatically. It seems to happen with elapsed time. Mostly, the cluster members are (re)started together and when one node falls out with an OOM, there are cases when several nodes follow it. I could observe these OOMs only after a given uptime (mostly weeks, but one week (10-12 days) without any problems is quite usual).\r\nThe cluster usage is quite consistent over time, repeating the same patterns, I can't see any spikes in any monitored metrics before the OOM.\r\nLast month, one node had OOM at 2019-03-19T06:14:15 then another at 2019-03-19T08:30:47. All of them had an uptime of 10.4 days.\r\nI have heap dumps from a number of these occasions (currently 19, some with bigger, some with smaller heaps).\r\nI can provide any of these privately if requested.\r\nOpening the last one in Eclipse MAT and running memory leak suspects yields the following:\r\n\r\n> Problem Suspect 1\r\n> \r\n> 27 instances of \"org.apache.lucene.index.IndexWriter\", loaded by \"sun.misc.Launcher$AppClassLoader @ 0x87c901df8\" occupy 1,444,023,192 (17.99%) bytes.\r\n> Biggest instances:\r\n> \r\n> org.apache.lucene.index.IndexWriter @ 0x8e322cd30 - 354,902,448 (4.42%) bytes.\r\n> org.apache.lucene.index.IndexWriter @ 0x896cc48a8 - 194,821,848 (2.43%) bytes.\r\n> org.apache.lucene.index.IndexWriter @ 0x8dfc52008 - 168,491,352 (2.10%) bytes.\r\n> org.apache.lucene.index.IndexWriter @ 0x8f59fdef8 - 149,564,464 (1.86%) bytes.\r\n> org.apache.lucene.index.IndexWriter @ 0x9157db0b0 - 138,265,992 (1.72%) bytes.\r\n> org.apache.lucene.index.IndexWriter @ 0x89e62dec0 - 137,079,136 (1.71%) bytes.\r\n> org.apache.lucene.index.IndexWriter @ 0x8f424cdc0 - 120,118,608 (1.50%) bytes.\r\n> \r\n\r\n> Problem Suspect 2\r\n> \r\n> 75,375 instances of \"org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader\", loaded by \"sun.misc.Launcher$AppClassLoader @ 0x87c901df8\" occupy 1,335,024,544 (16.63%) bytes. \r\n\r\n> Problem Suspect 3\r\n> \r\n> 5,010 instances of \"byte[]\", loaded by \"<system class loader>\" occupy 992,431,728 (12.36%) bytes. \r\n\r\nOutside of these, 4 GiB is reported in other stuff.\r\n\r\n\r\nUsed heap dump | 7.5 GB\r\n-- | --\r\nNumber of objects | 21,550,603\r\nNumber of classes | 14,541\r\nNumber of class loaders | 75\r\nNumber of GC roots | 3,418\r\n\r\nBiggest objects:\r\n\r\nClass Name | Shallow Heap | Retained Heap\r\n-- | -- | --\r\norg.elasticsearch.indices.IndicesQueryCache$ElasticsearchLRUQueryCache @ 0x8879c3208 » | 88 | 754,786,360\r\norg.apache.lucene.index.IndexWriter @ 0x8e322cd30 » | 208 | 354,902,448\r\norg.apache.lucene.index.IndexWriter @ 0x896cc48a8 » | 208 | 194,821,848\r\norg.elasticsearch.search.SearchService @ 0x8b5c48798 » | 104 | 170,363,752\r\norg.apache.lucene.index.IndexWriter @ 0x8dfc52008 » | 208 | 168,491,352\r\norg.elasticsearch.common.util.PageCacheRecycler @ 0x887b88ac8 » | 32 | 161,909,848\r\norg.apache.lucene.index.IndexWriter @ 0x8f59fdef8 » | 208 | 149,564,464\r\norg.apache.lucene.index.IndexWriter @ 0x9157db0b0 » | 208 | 138,265,992\r\norg.apache.lucene.index.IndexWriter @ 0x89e62dec0 » | 208 | 137,079,136\r\norg.apache.lucene.index.IndexWriter @ 0x8f424cdc0 » | 208 | 120,118,608\r\norg.elasticsearch.index.IndexService @ 0x8b0ed27e8 » | 152 | 91,669,488\r\nTotal: 11 entries\r\n\r\nPackages:\r\n\r\nPackage | Retained Heap | Retained Heap, % | # Top Dominators\r\n-- | -- | -- | --\r\n<all>First 10 of 635,805 objects | 8,027,986,976 | 100.00% | 635,805\r\norgFirst 10 of 472,651 objects | 5,969,889,520 | 74.36% | 472,651\r\napacheFirst 10 of 130,992 objects | 4,066,876,496 | 50.66% | 130,992\r\nluceneFirst 10 of 129,804 objects | 4,066,572,360 | 50.65% | 129,804\r\nindexFirst 10 of 31,273 objects | 1,875,677,160 | 23.36% | 31,273\r\nIndexWriterFirst 10 of 28 objects | 1,444,048,200 | 17.99% | 28\r\nSegmentReaderFirst 10 of 4,445 objects | 391,440,960 | 4.88% | 4,445\r\nTotal: 2 entries | 1,835,489,160 |   | 4,473\r\ncodecsFirst 10 of 86,643 objects | 1,845,488,368 | 22.99% | 86,643\r\ncompressingFirst 10 of 79,011 objects | 1,424,856,096 | 17.75% | 79,011\r\nCompressingStoredFieldsReaderFirst 10 of 75,376 objects | 1,335,025,368 | 16.63% | 75,376\r\nCompressingStoredFieldsIndexReaderFirst 10 of 3,610 objects | 89,664,576 | 1.12% | 3,610\r\nTotal: 2 entries | 1,424,689,944 |   | 78,986\r\nblocktreeFirst 10 of 6,240 objects | 417,828,792 | 5.20% | 6,240\r\nBlockTreeTermsReaderFirst 10 of 2,933 objects | 416,353,448 | 5.19% | 2,933\r\nTotal: 2 entries | 1,842,684,888 |   | 85,251\r\nutilFirst 10 of 5,607 objects | 339,374,808 | 4.23% | 5,607\r\nFixedBitsFirst 10 of 1,591 objects | 339,206,320 | 4.23% | 1,591\r\nTotal: 3 entries | 4,060,540,336 |   | 123,523\r\nelasticsearchFirst 10 of 341,504 objects | 1,902,894,456 | 23.70% | 341,504\r\nindicesFirst 10 of 158,478 objects | 803,637,920 | 10.01% | 158,478\r\nIndicesQueryCache$ElasticsearchLRUQueryCacheAll 2 objects | 754,786,360 | 9.40% | 2\r\ncommonFirst 10 of 167,769 objects | 599,292,256 | 7.47% | 167,769\r\nrecyclerFirst 10 of 48 objects | 241,483,832 | 3.01% | 48\r\nDequeRecyclerFirst 10 of 20 objects | 241,482,928 | 3.01% | 20\r\nutilFirst 10 of 412 objects | 162,170,040 | 2.02% | 412\r\nPageCacheRecyclerAll 2 objects | 161,910,216 | 2.02% | 2\r\ncacheFirst 10 of 158,152 objects | 115,113,352 | 1.43% | 158,152\r\nCache$EntryFirst 10 of 158,138 objects | 105,003,680 | 1.31% | 158,138\r\nTotal: 3 entries | 518,767,224 |   | 158,612\r\nindexFirst 10 of 3,285 objects | 288,017,520 | 3.59% | 3,285\r\nIndexServiceFirst 10 of 61 objects | 199,436,968 | 2.48% | 61\r\nsearchFirst 10 of 781 objects | 170,820,352 | 2.13% | 781\r\nSearchServiceAll 2 objects | 170,370,840 | 2.12% | 2\r\nTotal: 4 entries | 1,861,768,048 |   | 330,313\r\nTotal: 2 entries | 5,969,770,952 |   | 472,496\r\nbyte[]First 10 of 5,011 objects | 992,431,728 | 12.36% | 5,011\r\njavaFirst 10 of 126,404 objects | 675,098,264 | 8.41% | 126,404\r\nlangFirst 10 of 101,112 objects | 664,370,256 | 8.28% | 101,112\r\nThreadFirst 10 of 338 objects | 653,106,128 | 8.14% | 338\r\nioFirst 10 of 11,102 objects | 384,385,200 | 4.79% | 11,102\r\nnettyFirst 10 of 11,102 objects | 384,385,200 | 4.79% | 11,102\r\nbufferFirst 10 of 7,420 objects | 379,483,704 | 4.73% | 7,420\r\nPoolChunkFirst 10 of 129 objects | 354,472,384 | 4.42% | 129\r\nTotal: 4 entries | 8,021,804,712 |   | 615,168\r\n\r\nClass histogram:\r\nClass Name\tObjects\tShallow Heap\tRetained Heap\r\nbyte[]\r\nAll objects\t3,481,815\t3,936,315,992\t>= 3,936,315,992\r\nlong[]\r\nAll objects\t737,541\t3,205,004,560\t>= 3,205,004,560\r\norg.apache.lucene.util.FixedBits\r\nAll objects\t8,033\t192,792\t>= 2,444,792,464\r\njava.util.HashMap\r\nAll objects\t269,808\t12,950,784\t>= 1,721,627,000\r\njava.util.HashMap$Node[]\r\nAll objects\t101,124\t11,569,696\t>= 1,716,176,752\r\norg.apache.lucene.index.SegmentReader\r\nAll objects\t18,531\t1,630,728\t>= 1,676,086,048\r\norg.apache.lucene.index.IndexWriter\r\nAll objects\t151\t31,408\t>= 1,511,363,088\r\norg.apache.lucene.index.ReaderPool\r\nAll objects\t151\t8,456\t>= 1,472,692,568\r\norg.apache.lucene.codecs.compressing.CompressingStoredFieldsReader\r\nAll objects\t84,957\t6,796,560\t>= 1,386,803,376\r\norg.apache.lucene.codecs.compressing.CompressingStoredFieldsReader$BlockState\r\nAll objects\t84,957\t4,757,592\t>= 1,359,573,648\r\norg.apache.lucene.util.BytesRef\r\nAll objects\t636,774\t15,282,576\t>= 1,347,417,024\r\njava.lang.Object[]\r\nAll objects\t128,979\t27,684,648\t>= 1,225,122,704\r\njava.util.WeakHashMap\r\nAll objects\t52,307\t2,510,736\t>= 917,978,536\r\njava.util.Collections$SynchronizedSet\r\nAll objects\t47,912\t1,149,888\t>= 915,486,680\r\njava.util.Collections$SetFromMap\r\nAll objects\t44,749\t1,073,976\t>= 914,223,392\r\njava.util.WeakHashMap$Entry[]\r\nAll objects\t52,310\t4,717,744\t>= 913,484,528\r\njava.util.HashMap$Node\r\nAll objects\t882,020\t28,224,640\t>= 812,033,048\r\norg.elasticsearch.indices.IndicesQueryCache$ElasticsearchLRUQueryCache\r\nAll 1 objects\t1\t88\t>= 754,786,360\r\njava.util.IdentityHashMap\r\nAll objects\t1,032\t41,280\t>= 752,369,864\r\norg.apache.lucene.search.LRUQueryCache$LeafCache\r\nAll objects\t395\t12,640\t>= 752,069,536\r\norg.apache.lucene.util.BitDocIdSet\r\nAll objects\t2,347\t56,328\t>= 656,515,592\r\norg.apache.lucene.util.FixedBitSet\r\nAll objects\t2,478\t59,472\t>= 656,478,864\r\njava.lang.Thread\r\nAll objects\t339\t40,680\t>= 653,118,440\r\njava.lang.ThreadLocal$ThreadLocalMap\r\nAll objects\t673\t16,152\t>= 647,854,104\r\njava.lang.ThreadLocal$ThreadLocalMap$Entry[]\r\nAll objects\t673\t4,461,968\t>= 647,837,944\r\n\r\nTotal: 25 of 14,541 entries; 14,516 more\r\n21,550,603\t8,027,986,976\t\r\n\r\n\r\n\r\nCould this be a memory leak somewhere (or\r\n\r\n**Provide logs (if relevant)**:\r\n```\r\n[2019-04-08T12:41:52,956][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][young][1688189][55387] duration [1.7s], collections [1]/[2.6s], total [1.7s]/[1.6h], memory [17.5gb]->[16.5gb]/[20gb],\r\n all_pools {[young] [6.7gb]->[48mb]/[0b]}{[survivor] [216mb]->[104mb]/[0b]}{[old] [10.6gb]->[16.3gb]/[20gb]}\r\n[2019-04-08T12:41:53,003][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][1688189] overhead, spent [1.7s] collecting in the last [2.6s]\r\n[2019-04-08T12:46:24,726][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][young][1688452][55400] duration [8.1s], collections [1]/[9s], total [8.1s]/[1.6h], memory [17.5gb]->[17.2gb]/[20gb], a\r\nll_pools {[young] [9.8gb]->[120mb]/[0b]}{[survivor] [216mb]->[16mb]/[0b]}{[old] [7.5gb]->[17.1gb]/[20gb]}\r\n[2019-04-08T12:46:24,802][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][1688452] overhead, spent [8.1s] collecting in the last [9s]\r\n[2019-04-08T12:50:55,367][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][young][1688715][55413] duration [7.2s], collections [1]/[7.7s], total [7.2s]/[1.6h], memory [17.5gb]->[16.9gb]/[20gb],\r\n all_pools {[young] [9.7gb]->[8mb]/[0b]}{[survivor] [192mb]->[24mb]/[0b]}{[old] [7.6gb]->[16.8gb]/[20gb]}\r\n[2019-04-08T12:50:55,371][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][1688715] overhead, spent [7.2s] collecting in the last [7.7s]\r\n[2019-04-08T12:53:28,724][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][young][1688860][55428] duration [7.8s], collections [1]/[8.8s], total [7.8s]/[1.6h], memory [17.3gb]->[16.8gb]/[20gb],\r\n all_pools {[young] [9.2gb]->[192mb]/[0b]}{[survivor] [160mb]->[0b]/[0b]}{[old] [7.9gb]->[16.7gb]/[20gb]}\r\n[2019-04-08T12:53:28,743][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][1688860] overhead, spent [7.8s] collecting in the last [8.8s]\r\n[2019-04-08T12:57:20,689][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][young][1689078][55442] duration [13s], collections [1]/[14s], total [13s]/[1.6h], memory [17.4gb]->[16.9gb]/[20gb], al\r\nl_pools {[young] [9.6gb]->[64mb]/[0b]}{[survivor] [168mb]->[0b]/[0b]}{[old] [7.5gb]->[16.9gb]/[20gb]}\r\n[2019-04-08T12:57:20,754][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][1689078] overhead, spent [13s] collecting in the last [14s]\r\n[2019-04-08T13:04:17,007][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][young][1689485][55459] duration [8.8s], collections [1]/[9.2s], total [8.8s]/[1.6h], memory [17.6gb]->[17.2gb]/[20gb],\r\n all_pools {[young] [9.8gb]->[136mb]/[0b]}{[survivor] [208mb]->[0b]/[0b]}{[old] [7.6gb]->[17gb]/[20gb]}\r\n[2019-04-08T13:04:17,104][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][1689485] overhead, spent [8.8s] collecting in the last [9.2s]\r\n[2019-04-08T13:16:09,037][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][young][1690186][55482] duration [9.3s], collections [1]/[9.8s], total [9.3s]/[1.6h], memory [17.6gb]->[17.3gb]/[20gb],\r\n all_pools {[young] [9.9gb]->[112mb]/[0b]}{[survivor] [232mb]->[0b]/[0b]}{[old] [7.4gb]->[17.1gb]/[20gb]}\r\n[2019-04-08T13:16:09,352][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][1690186] overhead, spent [9.3s] collecting in the last [9.8s]\r\n[2019-04-08T13:20:53,549][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][young][1690468][55495] duration [1.8s], collections [1]/[1.9s], total [1.8s]/[1.6h], memory [17.6gb]->[16gb]/[20gb], a\r\nll_pools {[young] [9.9gb]->[80mb]/[0b]}{[survivor] [176mb]->[96mb]/[0b]}{[old] [7.5gb]->[15.9gb]/[20gb]}\r\n[2019-04-08T13:20:53,578][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][1690468] overhead, spent [1.8s] collecting in the last [1.9s]\r\n[2019-04-08T13:22:54,467][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][young][1690581][55505] duration [8.3s], collections [1]/[8.6s], total [8.3s]/[1.6h], memory [17.5gb]->[16.8gb]/[20gb],\r\n all_pools {[young] [9.4gb]->[144mb]/[0b]}{[survivor] [152mb]->[0b]/[0b]}{[old] [7.8gb]->[16.6gb]/[20gb]}\r\n[2019-04-08T13:22:54,505][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][1690581] overhead, spent [8.3s] collecting in the last [8.6s]\r\n[2019-04-08T13:25:49,479][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][young][1690755][55521] duration [1.2s], collections [1]/[1.4s], total [1.2s]/[1.6h], memory [17.6gb]->[13gb]/[20gb], a\r\nll_pools {[young] [10gb]->[88mb]/[0b]}{[survivor] [168mb]->[96mb]/[0b]}{[old] [7.4gb]->[12.9gb]/[20gb]}\r\n[2019-04-08T13:25:49,499][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25.] [gc][1690755] overhead, spent [1.2s] collecting in the last [1.4s]\r\n[2019-04-08T13:27:30,648][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25] [gc][young][1690842][55526] duration [14.1s], collections [1]/[14.9s], total [14.1s]/[1.6h], memory [17.5gb]->[17gb]/[20gb], all_pools {[young] [9.7gb]->[16mb]/[0b]}{[survivor] [112mb]->[0b]/[0b]}{[old] [7.6gb]->[17gb]/[20gb]}\r\n[2019-04-08T13:27:30,649][WARN ][o.e.m.j.JvmGcMonitorService] [fmfe25] [gc][1690842] overhead, spent [14.1s] collecting in the last [14.9s]\r\njava.lang.OutOfMemoryError: Java heap space\r\nDumping heap to /data/elasticsearch/heapdumps/java_pid82750.hprof ...\r\nHeap dump file created [8530677372 bytes in 60.215 secs]\r\nTerminating due to java.lang.OutOfMemoryError: Java heap space\r\n```\r\n","closed_by":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"performed_via_github_app":null}