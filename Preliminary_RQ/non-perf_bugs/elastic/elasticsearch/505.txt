{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/505","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/505/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/505/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/505/events","html_url":"https://github.com/elastic/elasticsearch/issues/505","id":409102,"node_id":"MDU6SXNzdWU0MDkxMDI=","number":505,"title":"Add maximum token count for tokenized fields","user":{"login":"apatrida","id":182340,"node_id":"MDQ6VXNlcjE4MjM0MA==","avatar_url":"https://avatars3.githubusercontent.com/u/182340?v=4","gravatar_id":"","url":"https://api.github.com/users/apatrida","html_url":"https://github.com/apatrida","followers_url":"https://api.github.com/users/apatrida/followers","following_url":"https://api.github.com/users/apatrida/following{/other_user}","gists_url":"https://api.github.com/users/apatrida/gists{/gist_id}","starred_url":"https://api.github.com/users/apatrida/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apatrida/subscriptions","organizations_url":"https://api.github.com/users/apatrida/orgs","repos_url":"https://api.github.com/users/apatrida/repos","events_url":"https://api.github.com/users/apatrida/events{/privacy}","received_events_url":"https://api.github.com/users/apatrida/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2010-11-09T16:55:00Z","updated_at":"2013-04-04T18:33:50Z","closed_at":"2013-04-04T18:33:50Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Add a maximum token count for tokenized fields so that they are indexed up until N tokens and then the rest is discarded.  This is a typical use case where some people consider the first part of a document to contain the important aspects, and the rest might be repetition.  But, there are the opposite case where everything is important.  So allow the truncation for the tokenized terms.  If not by token count, then by size (depth into the source material that the tokenizer is reading from).\n\nNote, this is not for the stored version, that should be set independently by size.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}