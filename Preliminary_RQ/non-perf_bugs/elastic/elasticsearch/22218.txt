{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/22218","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22218/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22218/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22218/events","html_url":"https://github.com/elastic/elasticsearch/issues/22218","id":196006269,"node_id":"MDU6SXNzdWUxOTYwMDYyNjk=","number":22218,"title":"Unbalanced use of multiple data paths which eventually leads to cluster outage","user":{"login":"kjelle","id":598905,"node_id":"MDQ6VXNlcjU5ODkwNQ==","avatar_url":"https://avatars2.githubusercontent.com/u/598905?v=4","gravatar_id":"","url":"https://api.github.com/users/kjelle","html_url":"https://github.com/kjelle","followers_url":"https://api.github.com/users/kjelle/followers","following_url":"https://api.github.com/users/kjelle/following{/other_user}","gists_url":"https://api.github.com/users/kjelle/gists{/gist_id}","starred_url":"https://api.github.com/users/kjelle/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kjelle/subscriptions","organizations_url":"https://api.github.com/users/kjelle/orgs","repos_url":"https://api.github.com/users/kjelle/repos","events_url":"https://api.github.com/users/kjelle/events{/privacy}","received_events_url":"https://api.github.com/users/kjelle/received_events","type":"User","site_admin":false},"labels":[{"id":836504707,"node_id":"MDU6TGFiZWw4MzY1MDQ3MDc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Distributed","name":":Distributed/Distributed","color":"0e8a16","default":false,"description":"A catch all label for anything in the Distributed Area. If you aren't sure, use this one."},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2016-12-16T08:35:32Z","updated_at":"2018-02-13T19:27:10Z","closed_at":"2017-03-31T14:12:26Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version**: 2.2.0\r\n**Plugins installed**: [marvel]\r\n**JVM version**: 8u51\r\n**OS version**: docker 1.10.3 (20f81dd) on centos7.2.1511\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\nEach server have 8 SSDs (/grid/1-8) and it runs 2 ES instances. First instance uses 4 disks, /grid/1-4, last instance use the rest, /grid/5-8. The number of shards per index varies, and so does the size of each shard. We run mostly default configs, only tampered with translog and indices.memory/indices.recovery stuff.\r\n\r\nOf course there isnt anything else using these disks.\r\n\r\nThe problem is the insane variation of disk usage (see logs).  As an example we had a server with two disks at up to 100% (how is that even possible with disk watermarks?) while the other data paths are \"almost unused\" (se logs, \"server1\").  At one point these disks hit 100% usage.\r\n\r\nNormally our shards are sized around 60-80gb (per shard). We use 10 shards per index as we have 10 SSD nodes. The \"bad\" indices was one day of log data which generated 4-5x times more data than normal.  The biggest troublesome index had shards around 400gb per shard, while others varied from 100-150gb.\r\n\r\nPlacing 400gb of data on one SSD of 881GB size while maintaining high watermark which already have a decent load is a problem in it self.\r\n\r\nAfter we cleaned up this mess (aka deleted the bad indices ...), we still see a data skew problem. On one particular server we now have disk use of grid 1-4 on 39%, 22%, 39% and 13%.  We expect Elasticsearch to place shards on the disk with least disk-usage inside the node - to maintain an even/balanced disk usage.\r\n\r\n**Steps to reproduce**:\r\n 1.\r\n 2.\r\n 3.\r\n\r\n**Provide logs (if relevant)**:\r\nDisk usage of 4 of our SSD nodes when having issues\r\n```\r\nserver1:\r\n/dev/sdb1                    881G  186G  650G  23% /grid/1\r\n/dev/sdc1                    881G  818G   19G  98% /grid/2\r\n/dev/sdd1                    881G  787G   50G  95% /grid/3\r\n/dev/sde1                    881G  166G  671G  20% /grid/4 \r\n/dev/sdf1                    881G  251G  586G  30% /grid/5\r\n/dev/sdg1                    881G  709G  128G  85% /grid/6\r\n/dev/sdh1                    881G  803G   33G  97% /grid/7\r\n/dev/sdi1                    881G  254G  583G  31% /grid/8\r\n\r\nserver2\r\n/dev/sdb1                    881G  805G   31G  97% /grid/1\r\n/dev/sdc1                    881G  541G  295G  65% /grid/2\r\n/dev/sdd1                    881G  407G  429G  49% /grid/3\r\n/dev/sde1                    881G  518G  318G  62% /grid/4\r\n/dev/sdf1                    881G  576G  260G  69% /grid/5\r\n/dev/sdg1                    881G  550G  286G  66% /grid/6\r\n/dev/sdh1                    881G  591G  246G  71% /grid/7\r\n/dev/sdi1                    881G  526G  311G  63% /grid/8\r\n\r\nserver3\r\n/dev/sdb1                    881G  608G  228G  73% /grid/1\r\n/dev/sdc1                    881G  261G  575G  32% /grid/2\r\n/dev/sdd1                    881G  796G   40G  96% /grid/3\r\n/dev/sde1                    881G  694G  143G  83% /grid/4\r\n/dev/sdf1                    881G  503G  333G  61% /grid/5\r\n/dev/sdg1                    881G  487G  349G  59% /grid/6\r\n/dev/sdh1                    881G  744G   93G  89% /grid/7\r\n/dev/sdi1                    881G  558G  278G  67% /grid/8\r\n\r\nserver4\r\n/dev/sdb1                    881G  624G  213G  75% /grid/1\r\n/dev/sdc1                    881G  267G  569G  32% /grid/2\r\n/dev/sdd1                    881G  835G  1,2G 100% /grid/3\r\n/dev/sde1                    881G  694G  143G  83% /grid/4\r\n/dev/sdf1                    881G  509G  328G  61% /grid/5\r\n/dev/sdg1                    881G  503G  334G  61% /grid/6\r\n/dev/sdh1                    881G  778G   58G  94% /grid/7\r\n/dev/sdi1                    881G  551G  286G  66% /grid/8\r\n```\r\n\r\nDisk usage of all the 5 SSD nodes after cleanup, still uneven use of disk;\r\n```\r\n/dev/sdb1                    881G  323G  513G  39% /grid/1\r\n/dev/sdc1                    881G  176G  660G  22% /grid/2\r\n/dev/sdd1                    881G  320G  517G  39% /grid/3\r\n/dev/sde1                    881G  105G  731G  13% /grid/4\r\n/dev/sdf1                    881G  280G  557G  34% /grid/5\r\n/dev/sdg1                    881G  102G  734G  13% /grid/6\r\n/dev/sdh1                    881G  246G  590G  30% /grid/7\r\n/dev/sdi1                    881G  376G  461G  45% /grid/8\r\n\r\n/dev/sdb1                    881G  206G  630G  25% /grid/1\r\n/dev/sdc1                    881G  297G  539G  36% /grid/2\r\n/dev/sdd1                    881G   98G  739G  12% /grid/3\r\n/dev/sde1                    881G  184G  653G  22% /grid/4\r\n/dev/sdf1                    881G  234G  602G  28% /grid/5\r\n/dev/sdg1                    881G  206G  630G  25% /grid/6\r\n/dev/sdh1                    881G  180G  657G  22% /grid/7\r\n/dev/sdi1                    881G  124G  712G  15% /grid/8\r\n\r\n/dev/sdb1                    881G  158G  678G  19% /grid/1\r\n/dev/sdc1                    881G  295G  542G  36% /grid/2\r\n/dev/sdd1                    881G  174G  662G  21% /grid/3\r\n/dev/sde1                    881G  158G  678G  19% /grid/4\r\n/dev/sdf1                    881G  131G  705G  16% /grid/5\r\n/dev/sdg1                    881G  186G  650G  23% /grid/6\r\n/dev/sdh1                    881G  233G  604G  28% /grid/7\r\n/dev/sdi1                    881G  157G  680G  19% /grid/8\r\n\r\n/dev/sdb1                    881G  175G  661G  21% /grid/1\r\n/dev/sdc1                    881G  213G  623G  26% /grid/2\r\n/dev/sdd1                    881G  182G  655G  22% /grid/3\r\n/dev/sde1                    881G  191G  645G  23% /grid/4\r\n/dev/sdf1                    881G  164G  673G  20% /grid/5\r\n/dev/sdg1                    881G  138G  699G  17% /grid/6\r\n/dev/sdh1                    881G  217G  619G  26% /grid/7\r\n/dev/sdi1                    881G  176G  660G  22% /grid/8\r\n\r\n/dev/sdb1                    881G  157G  679G  19% /grid/1\r\n/dev/sdc1                    881G  196G  641G  24% /grid/2\r\n/dev/sdd1                    881G  140G  697G  17% /grid/3\r\n/dev/sde1                    881G   67G  770G   8% /grid/4\r\n/dev/sdf1                    881G  201G  635G  24% /grid/5\r\n/dev/sdg1                    881G  242G  594G  29% /grid/6\r\n/dev/sdh1                    881G  174G  663G  21% /grid/7\r\n/dev/sdi1                    881G  271G  566G  33% /grid/8\r\n```","closed_by":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"performed_via_github_app":null}