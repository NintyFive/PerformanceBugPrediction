[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/237342179","html_url":"https://github.com/elastic/elasticsearch/issues/19789#issuecomment-237342179","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19789","id":237342179,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNzM0MjE3OQ==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2016-08-03T19:17:16Z","updated_at":"2016-08-03T19:17:16Z","author_association":"MEMBER","body":"Duplicates #18417, relates #18467\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/237484493","html_url":"https://github.com/elastic/elasticsearch/issues/19789#issuecomment-237484493","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19789","id":237484493,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNzQ4NDQ5Mw==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-08-04T08:15:25Z","updated_at":"2016-08-04T08:15:25Z","author_association":"CONTRIBUTOR","body":"There is another issue at play here:\n\nThe issue is that the index-creation context is lost after the primary shard fails to initialize on node 2 (which is a variation of #15241, only fixed by allocation ids in v5.0.0). This means that after the first failed attempt to initialize the primary shard, it is treated as an existing shard copy to be recovered instead of a new one (I've added some notes below how to detect this situation). This also means that the primary shard allocator searches for a copy of the data on the nodes to allocate the shard to, which triggers `async_shard_fetch`. As there are no on-disk copies of this shard, the shard can never be allocated. The only way to get the shard un-stuck is to either delete the index or use the reroute allocation command to force a fresh primary (which is implemented by restoring the index-creation context for that shard, see [here](https://github.com/elastic/elasticsearch/blob/2.3/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java#L231). In v5.0.0, the index creation context is preserved and the primary allocates to node 1 after the failed attempt to node 2.\n\n**How to detect that index-creation context is lost in v1.x/v2.x:**\n\nThis can be seen by looking at the unassignedInfo on the unassigned primary shard, which shows something like this:\n\n```\n ({\"state\":\"UNASSIGNED\",\"primary\":true,\"node\":null,\"relocating_node\":null,\"shard\":5,\"index\":\"some_index\",\"version\":2,\"unassigned_info\":{\"reason\":\"ALLOCATION_FAILED\",\"at\":\"2016-08-01T00:00:28.958Z\",\"details\":\"failed to create shard, ... \"}}\n```\n\nThe index-creation context is correctly set if the unassigned_info object has `\"reason\":\"INDEX_CREATED\"`.\n\nIn v5.0.0, the decision whether a fresh shard copy is expected (like after index creation) or an existing copy is required (when recovering a previously started shard after cluster restart) is not based on the unassigned_info anymore but based on allocation ids. I have already opened a PR where it will be easier to see in v5.0.0 whether a primary is going to recover as a fresh initial shard copy or requires an existing on-disk shard copy (#19516).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/237490657","html_url":"https://github.com/elastic/elasticsearch/issues/19789#issuecomment-237490657","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19789","id":237490657,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNzQ5MDY1Nw==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-08-04T08:42:39Z","updated_at":"2016-08-04T08:42:39Z","author_association":"CONTRIBUTOR","body":"This also shows another subtle issue (even in current master) which is related to async_shard_fetch:\n\nAssume for simplicity an index with 1 primary shard and no replicas. Primary shard was successfully started at some point on data node X, but now we have done a full cluster restart. Data node X has some disk permission issues for the shard directory after restart. When master tries to allocate the primary after cluster restart, it first does an async_shard_fetch, which fails hard on node X as async_shard_fetch (`TransportNodesListGatewayStartedShards#nodeOperation`) treats the `AccessDeniedException` when trying to list shard directory contents as a hard failure on the node. The async_shard_fetch code marks this node as failed and queues a delayed reroute (essentially another allocation attempt) to be executed after the current one. This means however that if the primary cannot be allocated in this round, another round is triggered that leads to shard_fetching again (and again and again). Even #18467 does not help in this scenario, as we don't count these as failed attempts: What the code in PrimaryShardAllocator does is after shard fetching to see if there is a node that has any data (node X is disregarded because it failed), and in case no data was found it just ignores allocation of this primary shard:\n\n```\nif (enoughAllocationsFound == false){\n   // we can't really allocate, so ignore it and continue\n   changed |= unassignedIterator.removeAndIgnore(AllocationStatus.NO_VALID_SHARD_COPY);\n}\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/237677348","html_url":"https://github.com/elastic/elasticsearch/issues/19789#issuecomment-237677348","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19789","id":237677348,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNzY3NzM0OA==","user":{"login":"ppf2","id":7216393,"node_id":"MDQ6VXNlcjcyMTYzOTM=","avatar_url":"https://avatars0.githubusercontent.com/u/7216393?v=4","gravatar_id":"","url":"https://api.github.com/users/ppf2","html_url":"https://github.com/ppf2","followers_url":"https://api.github.com/users/ppf2/followers","following_url":"https://api.github.com/users/ppf2/following{/other_user}","gists_url":"https://api.github.com/users/ppf2/gists{/gist_id}","starred_url":"https://api.github.com/users/ppf2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ppf2/subscriptions","organizations_url":"https://api.github.com/users/ppf2/orgs","repos_url":"https://api.github.com/users/ppf2/repos","events_url":"https://api.github.com/users/ppf2/events{/privacy}","received_events_url":"https://api.github.com/users/ppf2/received_events","type":"User","site_admin":false},"created_at":"2016-08-04T20:42:43Z","updated_at":"2016-08-04T20:42:49Z","author_association":"MEMBER","body":"To clarify per discussion with @ywelsch (thx):\n\nThe first issue where the shard loses its index creation context after first failed allocation attempt is solved in v5.0.0 based on allocation ids (elastic/elasticsearch#14739). \n\nThe second issue is that async shard fetching can in a certain situation be triggered again and again when no existing shard copy can be found to allocate as primary. The situation where this occurs is if just doing a listFiles on the shard directory during shard fetching on a data node already throws an exception.  We are keeping this issue open to track this particular PR.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/323764513","html_url":"https://github.com/elastic/elasticsearch/issues/19789#issuecomment-323764513","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19789","id":323764513,"node_id":"MDEyOklzc3VlQ29tbWVudDMyMzc2NDUxMw==","user":{"login":"gmoskovicz","id":1675411,"node_id":"MDQ6VXNlcjE2NzU0MTE=","avatar_url":"https://avatars3.githubusercontent.com/u/1675411?v=4","gravatar_id":"","url":"https://api.github.com/users/gmoskovicz","html_url":"https://github.com/gmoskovicz","followers_url":"https://api.github.com/users/gmoskovicz/followers","following_url":"https://api.github.com/users/gmoskovicz/following{/other_user}","gists_url":"https://api.github.com/users/gmoskovicz/gists{/gist_id}","starred_url":"https://api.github.com/users/gmoskovicz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gmoskovicz/subscriptions","organizations_url":"https://api.github.com/users/gmoskovicz/orgs","repos_url":"https://api.github.com/users/gmoskovicz/repos","events_url":"https://api.github.com/users/gmoskovicz/events{/privacy}","received_events_url":"https://api.github.com/users/gmoskovicz/received_events","type":"User","site_admin":false},"created_at":"2017-08-21T14:53:22Z","updated_at":"2017-08-21T14:53:22Z","author_association":"CONTRIBUTOR","body":"It would be great to do something when a disk goes to read-only. This seems to be the default in some linux OSs when there are issues (such as corruption or problems with the mounted disk).\r\n\r\nAlso, to avoid this we could mention (in the documentation) that RAID 0 could be helpful?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374569806","html_url":"https://github.com/elastic/elasticsearch/issues/19789#issuecomment-374569806","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19789","id":374569806,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDU2OTgwNg==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T11:50:58Z","updated_at":"2018-03-20T11:50:58Z","author_association":"MEMBER","body":"@ywelsch This seems to be still an issue where failing to read a disk on a data node can lead to endless shard fetching. I tend to open an issue which is dedicated to that. Do you agree?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374678970","html_url":"https://github.com/elastic/elasticsearch/issues/19789#issuecomment-374678970","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19789","id":374678970,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDY3ODk3MA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T17:08:19Z","updated_at":"2018-03-20T17:08:19Z","author_association":"CONTRIBUTOR","body":"This could also be seen as falling under the umbrella of #18417, even if the issue technically happens before the shard is even allocated to the broken FS / node. How about closing this one and adding a comment to the linked issue?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374946165","html_url":"https://github.com/elastic/elasticsearch/issues/19789#issuecomment-374946165","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19789","id":374946165,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDk0NjE2NQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-03-21T13:59:37Z","updated_at":"2018-03-21T13:59:37Z","author_association":"MEMBER","body":"works for me. Added a comment to #18417","performed_via_github_app":null}]