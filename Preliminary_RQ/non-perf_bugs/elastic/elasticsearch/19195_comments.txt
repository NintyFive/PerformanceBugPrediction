[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/229752392","html_url":"https://github.com/elastic/elasticsearch/issues/19195#issuecomment-229752392","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19195","id":229752392,"node_id":"MDEyOklzc3VlQ29tbWVudDIyOTc1MjM5Mg==","user":{"login":"MaineC","id":70953,"node_id":"MDQ6VXNlcjcwOTUz","avatar_url":"https://avatars3.githubusercontent.com/u/70953?v=4","gravatar_id":"","url":"https://api.github.com/users/MaineC","html_url":"https://github.com/MaineC","followers_url":"https://api.github.com/users/MaineC/followers","following_url":"https://api.github.com/users/MaineC/following{/other_user}","gists_url":"https://api.github.com/users/MaineC/gists{/gist_id}","starred_url":"https://api.github.com/users/MaineC/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MaineC/subscriptions","organizations_url":"https://api.github.com/users/MaineC/orgs","repos_url":"https://api.github.com/users/MaineC/repos","events_url":"https://api.github.com/users/MaineC/events{/privacy}","received_events_url":"https://api.github.com/users/MaineC/received_events","type":"User","site_admin":false},"created_at":"2016-06-30T18:46:16Z","updated_at":"2016-06-30T18:46:16Z","author_association":"CONTRIBUTOR","body":"As a bit of real world validation of the above suggestion here's what Erik Bernhardson from Wikimedia, thanks @nik9000 for the introduction, told us about how Wikimedia's search team is doing ranking QA:\n\n> Real world data sets are a bit hard. We do collect a reasonable (0.5%) amount of click through \n> and dwell time data from which to try and derive SAT (satisfied) clicks. Unfortunately the \n> individual queries are considered [PII](https://en.wikipedia.org/wiki/Personally_identifiable_information) which means the data isn't able to be shared publicly. \n> This data gets fed into our [relevance forge](https://github.com/wikimedia/wikimedia-discovery-relevanceforge) software, currently we use a calculation nominally \n> called the [Paul Score](https://github.com/wikimedia/wikimedia-discovery-relevanceForge/blob/master/engineScore.py#L176-L208) after [Paul Nelson from Search Technologies](https://www.elastic.co/elasticon/conf/2016/sf/engine-scoring-and-predictive-analytics-to-boost-search-accuracy) that presented the idea at \n> elasticon.\n> \n> The basic concept is to take all queries and clicks from a user session and consider any click to \n> be a vote for all queries in that session. A form of DCG is then calculated per-session based on \n> this, and all the session scores are averaged together. This is a very basic method of handling \n> query reformulation, along with not over weighting heavy users. It is somewhat useful, but has a \n> very heavy preference for links that already show up in the top of the search rankings. For us we \n> already see ~85% of clicks are to the top 3 [results](https://phabricator.wikimedia.org/T126522#2027738), so it's really only optimizing for the other \n> 15% of clicks. We haven't started using this with autocomplete but it might be more useful there \n> as there are more reformulations to work with.\n> \n> The only thing i could offer up is the results from our labeling platform ([discernatron](https://discernatron.wmflabs.org/), [see also](https://github.com/wikimedia/wikimedia-discovery-discernatron) In \n> Discernatron results sourced from our own engine, google, bing and duck duck go are labeled on \n> a 4 point scale by human graders. This is a brand new platform that has very minimal usage so \n> the data set is currently too small to be much use to anyone. The queries used here have been \n> hand filtered to remove PII (10 to 20% of queries end up getting filtered) so are possible to be \n> released as a public dataset. Sadly it only has ~70 sets of scores against 32 unique queries as of \n> today. We are still iterating on the code behind the labeling platform before really pushing for it's \n> usage. If the data might be interesting a relatively recent TSV export is [available](https://phabricator.wikimedia.org/P3255), but i doubt \n> it's enough for you to do much. Once we get a reasonable amount of data here we will be using it \n> for nDCG calculations in relevance forge, and possibly other algorithms (Yandex's [pFound](http://romip.ru/romip2009/15_yandex.pdf) is \n> an interesting algo i've been looking at recently). In our ideal world we would be getting around \n> 500 new queries graded this way every month, but we are a ways away from that reality.\n> \n> I don't know if you are aware, but the presenter from NY Times at elasticon spent about 5 \n> minutes of his presentation [complaining that elastic didn't currently have anything like this built in](https://www.elastic.co/elasticon/conf/2016/sf/all-the-data-thats-fit-to-find-search-at-the-new-york-times). \n> His name escapes me at the moment, but might be someone worth looking up and pinging as \n> well.\n\n What metrics are you currently using?\n\n> Currently using Paul score and nDCG as mentioned. Considering pFound, but it's mostly an \n> nDCG reformulation using binary judgements (probably click throughs) that takes into account \n> how likely a user is to continue progressing down the search results. We will possibly find more \n> as we have time to look around and evaluate things. There are plenty of different ways to \n> generate a score, but most are quite similar to what we already calculate. Will have to figure out \n> what it is we want to measure that isn't covered by the above three. We've only done initial steps \n> with our relevance forge project so far, but we finally got some dedicated hardware to clone \n> elasticsearch indicies to and run it on. Will be using it more over the coming months as we \n> evaluate the change from tf/idf to bm25. If we come up with new requirements that lead us to \n> investigating more algo's I'll ping you.\n\nCan I make our conversation public on github so it gets tracked with the rank evaluation ticket?\n\n> No restrictions, it can all be replicated to the public issue.\n> \n> One thing that would be important, at least for us, is for any kind of API to not just take search \n> terms with a templated query, but entire generated queries. We have a decent bit of pre-\n> processing that happens on user queries before they get sent off to elasticsearch. Mostly things \n> like custom boost's, but also special grammar, feature escaping (ex. we don't allow user specified \n> boosts with ^, or grouping with parens), etc. Perhaps not directly useful to you, but our raw \n> generated queries are available by attaching a cirrusDumpQuery query parameter to any [fulltext](https://en.wikipedia.org/wiki/Special:Search?search=morelike:California&fulltext=1&cirrusDumpQuery) \n> [search](https://en.wikipedia.org/wiki/Special:Search?search=elasticsearch&fulltext=1&cirrusDumpQuery).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/247965004","html_url":"https://github.com/elastic/elasticsearch/issues/19195#issuecomment-247965004","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19195","id":247965004,"node_id":"MDEyOklzc3VlQ29tbWVudDI0Nzk2NTAwNA==","user":{"login":"MaineC","id":70953,"node_id":"MDQ6VXNlcjcwOTUz","avatar_url":"https://avatars3.githubusercontent.com/u/70953?v=4","gravatar_id":"","url":"https://api.github.com/users/MaineC","html_url":"https://github.com/MaineC","followers_url":"https://api.github.com/users/MaineC/followers","following_url":"https://api.github.com/users/MaineC/following{/other_user}","gists_url":"https://api.github.com/users/MaineC/gists{/gist_id}","starred_url":"https://api.github.com/users/MaineC/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MaineC/subscriptions","organizations_url":"https://api.github.com/users/MaineC/orgs","repos_url":"https://api.github.com/users/MaineC/repos","events_url":"https://api.github.com/users/MaineC/events{/privacy}","received_events_url":"https://api.github.com/users/MaineC/received_events","type":"User","site_admin":false},"created_at":"2016-09-19T11:05:02Z","updated_at":"2016-09-19T11:05:02Z","author_association":"CONTRIBUTOR","body":"Keeping here for the record: After giving a talk on how to build personalisation features step by step starting with basic ES features last Thursday at BEDCon which included talks on how to evaluate search results there was quite some interest in this functionality from attendees. Was great being able to point them to the feature branch. Though unlikely it would be great to get some early feedback.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/248180435","html_url":"https://github.com/elastic/elasticsearch/issues/19195#issuecomment-248180435","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19195","id":248180435,"node_id":"MDEyOklzc3VlQ29tbWVudDI0ODE4MDQzNQ==","user":{"login":"softwaredoug","id":629060,"node_id":"MDQ6VXNlcjYyOTA2MA==","avatar_url":"https://avatars0.githubusercontent.com/u/629060?v=4","gravatar_id":"","url":"https://api.github.com/users/softwaredoug","html_url":"https://github.com/softwaredoug","followers_url":"https://api.github.com/users/softwaredoug/followers","following_url":"https://api.github.com/users/softwaredoug/following{/other_user}","gists_url":"https://api.github.com/users/softwaredoug/gists{/gist_id}","starred_url":"https://api.github.com/users/softwaredoug/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/softwaredoug/subscriptions","organizations_url":"https://api.github.com/users/softwaredoug/orgs","repos_url":"https://api.github.com/users/softwaredoug/repos","events_url":"https://api.github.com/users/softwaredoug/events{/privacy}","received_events_url":"https://api.github.com/users/softwaredoug/received_events","type":"User","site_admin":false},"created_at":"2016-09-20T01:54:36Z","updated_at":"2016-09-20T02:10:20Z","author_association":"CONTRIBUTOR","body":"I have a question, in part because I'm trying to figure out how a product of mine ([Quepid](http://quepid.com)) would integrate with this API\n\nMy understanding is the following\n- This feature logs query keyword strings (whatever is plugged in from a search template)\n- This feature logs basically per-doc judgements or quality metrics, probably based on analytics. But also possibly based on manual human judgments.\n- Per query, I can have a historic sense of NDCG, MRR, and other classic relevance metrics\n\nIs it planned that I could use this feature to do test-driven relevancy? Let's say I want to tweak my query, analysis, (or really anything) and measure the impact by computing a new NDCG over the new results. Will that be a supported feature? ie a worbench mode? Or is this more about logging and reporting (and tweaking boosts automatically)? (I ask because the workbench thing is what Quepid does, and anything that can help me what's planned here would be helpful to me).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/248335399","html_url":"https://github.com/elastic/elasticsearch/issues/19195#issuecomment-248335399","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19195","id":248335399,"node_id":"MDEyOklzc3VlQ29tbWVudDI0ODMzNTM5OQ==","user":{"login":"MaineC","id":70953,"node_id":"MDQ6VXNlcjcwOTUz","avatar_url":"https://avatars3.githubusercontent.com/u/70953?v=4","gravatar_id":"","url":"https://api.github.com/users/MaineC","html_url":"https://github.com/MaineC","followers_url":"https://api.github.com/users/MaineC/followers","following_url":"https://api.github.com/users/MaineC/following{/other_user}","gists_url":"https://api.github.com/users/MaineC/gists{/gist_id}","starred_url":"https://api.github.com/users/MaineC/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MaineC/subscriptions","organizations_url":"https://api.github.com/users/MaineC/orgs","repos_url":"https://api.github.com/users/MaineC/repos","events_url":"https://api.github.com/users/MaineC/events{/privacy}","received_events_url":"https://api.github.com/users/MaineC/received_events","type":"User","site_admin":false},"created_at":"2016-09-20T15:23:30Z","updated_at":"2016-09-20T15:23:30Z","author_association":"CONTRIBUTOR","body":"Hi Doug,\n\nthanks for getting into this conversation. As this API really is in the early stages we are very much looking for input on how to improve it and make it work better with software like Quepid.\n\n> This feature logs query keyword strings (whatever is plugged in from a search template)\n\nLogging is currently left to the user.\n\n> This feature logs basically per-doc judgements or quality metrics, probably based on analytics. But also possibly based on manual human judgments.\n\nIn it's current stage we assume that the user of this API provides human judgments. Reading through the Quepid tour I would imagine that this is equal to the query ratings you collect from your users.\n\n> Per query, I can have a historic sense of NDCG, MRR, and other classic relevance metrics\n\nYou'd execute a request against an Elasticsearch index that provides information on the queries to run for testing as well as the quality level of documents returned. As a result you'd get a relevance score. This would give you a sense of NDCG/MRR/etc. wrt. specific queries and documents that have been annotated. The way I would imagine someone to use this API would be to first execute the queries as they are against the index to get a sense of current NDCG/MRR/etc. Then change the queries executed (either by using another template but still the same parameters, or in a very simple setup generating new queries with existing parameters (semi-)automatically and re-executing these. Together with the quality level the API would give you a list of documents returned by your search queries that don't have any ratings yet so you can easily ship them off to manual inspection.\n\nYou can take a look at the current API request/response format here: https://github.com/elastic/elasticsearch/blob/feature/rank-eval/modules/rank-eval/src/main/java/org/elasticsearch/index/rankeval/RestRankEvalAction.java#L46\n\nHope this helps clear some confusion. Please don't hesitate to ask for further details. Also if you have any input on how to improve the API, on which other extensions would be helpful for you let us know. Would be great to receive feedback from you as soon as you start playing with this. If you want to get your hands dirty feel free to check out [our rank-eval module on the feature/rank-eval branch](https://github.com/elastic/elasticsearch/tree/feature/rank-eval/modules/rank-eval/src/main/java/org/elasticsearch/index/rankeval).\n\nAgain thanks for your feedback,\nIsabel\n","performed_via_github_app":null}]