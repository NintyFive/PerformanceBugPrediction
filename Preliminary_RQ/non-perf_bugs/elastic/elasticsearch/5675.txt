{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/5675","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5675/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5675/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5675/events","html_url":"https://github.com/elastic/elasticsearch/issues/5675","id":30777433,"node_id":"MDU6SXNzdWUzMDc3NzQzMw==","number":5675,"title":"Transport client infinite retry","user":{"login":"magnhaug","id":477436,"node_id":"MDQ6VXNlcjQ3NzQzNg==","avatar_url":"https://avatars0.githubusercontent.com/u/477436?v=4","gravatar_id":"","url":"https://api.github.com/users/magnhaug","html_url":"https://github.com/magnhaug","followers_url":"https://api.github.com/users/magnhaug/followers","following_url":"https://api.github.com/users/magnhaug/following{/other_user}","gists_url":"https://api.github.com/users/magnhaug/gists{/gist_id}","starred_url":"https://api.github.com/users/magnhaug/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/magnhaug/subscriptions","organizations_url":"https://api.github.com/users/magnhaug/orgs","repos_url":"https://api.github.com/users/magnhaug/repos","events_url":"https://api.github.com/users/magnhaug/events{/privacy}","received_events_url":"https://api.github.com/users/magnhaug/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2014-04-03T14:00:39Z","updated_at":"2014-04-07T07:34:01Z","closed_at":"2014-04-07T07:34:01Z","author_association":"NONE","active_lock_reason":null,"body":"Firstly, the `onFailure` method in `TransportClientNodesService.RetryListener` has a weak if-check:\n`if (i == nodes.size()) { /*...*/ }`  \nIf the variable `i` is allowed to progress _higher_ than nodes.size(), we can enter a endlessly recursive loop. This is reproducible in unit tests if we are throwing `ConnectTransportException` from every node, as the code can re-enter onFailure after \"terminating\".\n\nSecondly, the reason we discovered this, is that the Java client might (and will, quite consistently) end up _forking out infinite threads_ that loop on the `onFailure` method.\nIn our app this happens most frequently when we hit a major GC on the filter cache, while simultaneously maintaining about 40 threads of heavy read- and write- operations. We've also seen it happening when one of our two test nodes is shutting down.\n\nI've attached a sample from a stack trace below. This is pulled from a _live_ system, right as it suffered this error and started spawning threads spinning on RetryListener.\nPay attention to three things:\n1. This is a stack trace from the _client_, when the _server_ is irresponsive due to GC.\n2. The thread numbers. This is just a tiny sample, there are thousands of these threads (as many as the client can allocate before hitting ulimit). We are doing a max of 40 simultaneous _synchronous_ searches/writes, and would not expect any more simultaneous retries.\n3. The number of recursive calls to `onFailure` in the second stacktrace. This is from a cluster with 2 nodes and 2 client apps. It should not recurse 10 times.\n\nAlas, the problem is two-fold as I see it:\n- Why does the client fork infinite threads upon a `ConnectTransportException`?\n- Why does each loop potentially recurse deeper than the number of nodes?\n\nAlso, as a sidenote, the prefix increment of the volatile `i` variable is not threadsafe, you would probably use an AtomicInteger? Although I do not understand why several threads would want to re-use the same RetryListener instance.\n\nThis is on ElasticSearch 0.90.7, on Java 7u25 or 7u45\n\n```\n\"elasticsearch[Matt Murdock][generic][T#2023]\" daemon prio=10 tid=0x00007f1fe5551800 nid=0x62fe waiting for monitor entry [0x00007f1f1d198000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at java.lang.ThreadLocal.createInheritedMap(ThreadLocal.java:236)\n        at java.lang.Thread.init(Thread.java:415)\n        at java.lang.Thread.init(Thread.java:349)\n        at java.lang.Thread.<init>(Thread.java:674)\n        at org.elasticsearch.common.util.concurrent.EsExecutors$EsThreadFactory.newThread(EsExecutors.java:102)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:610)\n        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:924)\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)\n        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:203)\n        at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:68)\n        at org.elasticsearch.client.transport.support.InternalTransportClient$2.doWithNode(InternalTransportClient.java:109)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:259)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.action.TransportActionNodeProxy$1.handleException(TransportActionNodeProxy.java:89)\n        at org.elasticsearch.transport.TransportService$2.run(TransportService.java:206)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n\n   Locked ownable synchronizers:\n        - <0x0000000791ef8570> (a java.util.concurrent.ThreadPoolExecutor$Worker)\n\n\"elasticsearch[Matt Murdock][generic][T#2506]\" daemon prio=10 tid=0x00007f1fe554f800 nid=0x62fd runnable [0x00007f1f1d299000]\n   java.lang.Thread.State: RUNNABLE\n        at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:68)\n        at org.elasticsearch.client.transport.support.InternalTransportClient$2.doWithNode(InternalTransportClient.java:109)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:259)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)\n        at org.elasticsearch.action.TransportActionNodeProxy$1.handleException(TransportActionNodeProxy.java:89)\n        at org.elasticsearch.transport.TransportService$2.run(TransportService.java:206)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n\n   Locked ownable synchronizers:\n        - <0x0000000793b97d08> (a java.util.concurrent.ThreadPoolExecutor$Worker)\n\n\netc, etc, etc, etc..\n```\n","closed_by":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"performed_via_github_app":null}