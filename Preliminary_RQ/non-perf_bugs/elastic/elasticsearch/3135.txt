{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/3135","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3135/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3135/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3135/events","html_url":"https://github.com/elastic/elasticsearch/issues/3135","id":15143533,"node_id":"MDU6SXNzdWUxNTE0MzUzMw==","number":3135,"title":"OOM causes data loss on 0.20.6","user":{"login":"MagmaRules","id":32690,"node_id":"MDQ6VXNlcjMyNjkw","avatar_url":"https://avatars0.githubusercontent.com/u/32690?v=4","gravatar_id":"","url":"https://api.github.com/users/MagmaRules","html_url":"https://github.com/MagmaRules","followers_url":"https://api.github.com/users/MagmaRules/followers","following_url":"https://api.github.com/users/MagmaRules/following{/other_user}","gists_url":"https://api.github.com/users/MagmaRules/gists{/gist_id}","starred_url":"https://api.github.com/users/MagmaRules/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MagmaRules/subscriptions","organizations_url":"https://api.github.com/users/MagmaRules/orgs","repos_url":"https://api.github.com/users/MagmaRules/repos","events_url":"https://api.github.com/users/MagmaRules/events{/privacy}","received_events_url":"https://api.github.com/users/MagmaRules/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2013-06-04T21:57:03Z","updated_at":"2014-08-08T12:15:29Z","closed_at":"2014-08-08T12:15:29Z","author_association":"NONE","active_lock_reason":null,"body":"Hi there,\n\nLast week we had a major crash in our production cluster. Due to some faulty configuration the machines when out of memory and the whole cluster crashed.\n\nAfter a reconfiguration and a restart we discovered that shard 8 wasn't recovering. We checked the data in disk and found out that the shard had half of the expected size.\nWe ran the command: \"java -cp lucene-core-3.6.1.jar -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex /chroot/pr/elasticsearch/data/PR/nodes/0/indices/spc/8/index/ -fix\" but no data was recovered. Both the master and the replica of shard 8 had the exact same size.\n\nWe are running ES 0.20.6. Our index has 20 shards, each one with a replica. We have 6 machines and our cluster size is nearing the 250GB (500GB with the replicas). Our configuration is this: http://pastie.org/8006818 . \n\nI published our logs in dropbox: https://dl.dropboxusercontent.com/u/53354942/PR-logs.zip . Let me know if you need more logs.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}