[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/349155783","html_url":"https://github.com/elastic/elasticsearch/issues/27622#issuecomment-349155783","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27622","id":349155783,"node_id":"MDEyOklzc3VlQ29tbWVudDM0OTE1NTc4Mw==","user":{"login":"morphers82","id":1844753,"node_id":"MDQ6VXNlcjE4NDQ3NTM=","avatar_url":"https://avatars0.githubusercontent.com/u/1844753?v=4","gravatar_id":"","url":"https://api.github.com/users/morphers82","html_url":"https://github.com/morphers82","followers_url":"https://api.github.com/users/morphers82/followers","following_url":"https://api.github.com/users/morphers82/following{/other_user}","gists_url":"https://api.github.com/users/morphers82/gists{/gist_id}","starred_url":"https://api.github.com/users/morphers82/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/morphers82/subscriptions","organizations_url":"https://api.github.com/users/morphers82/orgs","repos_url":"https://api.github.com/users/morphers82/repos","events_url":"https://api.github.com/users/morphers82/events{/privacy}","received_events_url":"https://api.github.com/users/morphers82/received_events","type":"User","site_admin":false},"created_at":"2017-12-05T00:38:14Z","updated_at":"2017-12-05T00:38:14Z","author_association":"NONE","body":"I completely agree, memory pressure is why I had to stop auto reallocation and manually move around shards to keep memory pressure equal on nodes.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/349204182","html_url":"https://github.com/elastic/elasticsearch/issues/27622#issuecomment-349204182","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27622","id":349204182,"node_id":"MDEyOklzc3VlQ29tbWVudDM0OTIwNDE4Mg==","user":{"login":"antonpious","id":15054021,"node_id":"MDQ6VXNlcjE1MDU0MDIx","avatar_url":"https://avatars3.githubusercontent.com/u/15054021?v=4","gravatar_id":"","url":"https://api.github.com/users/antonpious","html_url":"https://github.com/antonpious","followers_url":"https://api.github.com/users/antonpious/followers","following_url":"https://api.github.com/users/antonpious/following{/other_user}","gists_url":"https://api.github.com/users/antonpious/gists{/gist_id}","starred_url":"https://api.github.com/users/antonpious/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/antonpious/subscriptions","organizations_url":"https://api.github.com/users/antonpious/orgs","repos_url":"https://api.github.com/users/antonpious/repos","events_url":"https://api.github.com/users/antonpious/events{/privacy}","received_events_url":"https://api.github.com/users/antonpious/received_events","type":"User","site_admin":false},"created_at":"2017-12-05T05:58:00Z","updated_at":"2017-12-05T05:58:00Z","author_association":"NONE","body":"Attaching a Node which has Mj2K which has used 7.6 GB of 7.9 GB while node G4QZ has 2.8 GB of 7.9 GB\r\n![elastic](https://user-images.githubusercontent.com/15054021/33592120-e83c0150-d94e-11e7-9117-f29d8b79e9a2.PNG)\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/349378746","html_url":"https://github.com/elastic/elasticsearch/issues/27622#issuecomment-349378746","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27622","id":349378746,"node_id":"MDEyOklzc3VlQ29tbWVudDM0OTM3ODc0Ng==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2017-12-05T17:30:25Z","updated_at":"2017-12-05T17:30:25Z","author_association":"MEMBER","body":"@antonpious I'd be interested to know what is using the memory on those nodes, is it fielddata? segment memory?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/349480146","html_url":"https://github.com/elastic/elasticsearch/issues/27622#issuecomment-349480146","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27622","id":349480146,"node_id":"MDEyOklzc3VlQ29tbWVudDM0OTQ4MDE0Ng==","user":{"login":"antonpious","id":15054021,"node_id":"MDQ6VXNlcjE1MDU0MDIx","avatar_url":"https://avatars3.githubusercontent.com/u/15054021?v=4","gravatar_id":"","url":"https://api.github.com/users/antonpious","html_url":"https://github.com/antonpious","followers_url":"https://api.github.com/users/antonpious/followers","following_url":"https://api.github.com/users/antonpious/following{/other_user}","gists_url":"https://api.github.com/users/antonpious/gists{/gist_id}","starred_url":"https://api.github.com/users/antonpious/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/antonpious/subscriptions","organizations_url":"https://api.github.com/users/antonpious/orgs","repos_url":"https://api.github.com/users/antonpious/repos","events_url":"https://api.github.com/users/antonpious/events{/privacy}","received_events_url":"https://api.github.com/users/antonpious/received_events","type":"User","site_admin":false},"created_at":"2017-12-05T23:47:47Z","updated_at":"2017-12-05T23:56:34Z","author_association":"NONE","body":"Unfortunately on the load we did not have the monitoring enabled so couldn't get the actual memory used while the load was going on.  We did exactly as morphers82 said, to be safe we had to stop processing move the shard manually around, check the memory has come down on the node and then start the processing. We continued to do this till the load finished.\r\n\r\nOnce the load finished we backed up the disk and restored on another environment to find where the memory was used. Most of the memory according to X-Pack Monitoring was used by Fixed BitSets ranged from 1 to 1.2 GB and Terms 400 to 450 MB in each node surprisingly Field data was 2-3 KB.\r\n\r\nWe are simulating these loads should have these as we keep running.\r\n\r\nWhat was baffling was when we restarted one node there was not much memory released but when we restarted the entire cluster the memory came down drastically. So not able to understand what is stored when data is at rest and  only search is happening and when data is being indexed and search is happening. (Query Cache, Request Cache are less than 100 MB)\r\n\r\nIf on the other hand if we did not restart, the memory did not come down so would be monitoring this too to check if the memory in heap comes down without restart.\r\n\r\n![elasticafterrestart](https://user-images.githubusercontent.com/15054021/33636868-b3f867a4-d9e3-11e7-9c51-cc05a0e47fba.png)\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/350279526","html_url":"https://github.com/elastic/elasticsearch/issues/27622#issuecomment-350279526","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27622","id":350279526,"node_id":"MDEyOklzc3VlQ29tbWVudDM1MDI3OTUyNg==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-12-08T14:42:25Z","updated_at":"2017-12-08T14:42:25Z","author_association":"CONTRIBUTOR","body":"I think I understand your idea and there is nothing wrong with it. We discussed it and we are all struggling a bit how we would implement something like this. Is it something like this. We really need to figure out what is consuming memory ie. in your case using nested documents (that's what I guess from your FixedBitSets). I mean we can go and say we accumulate some defined counts like segment memory and fixedBitSets but even this is tricky it might change pretty quickly due to segment merges etc. I think we need to keep this open and continue brainstorming how we can help along those lines since I think it's a valid ask. \r\n\r\non the CPU side I think it's tricky since 100% CPU isn't a bad thing necessarily... I guess people pay for this eventually :)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/351665443","html_url":"https://github.com/elastic/elasticsearch/issues/27622#issuecomment-351665443","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27622","id":351665443,"node_id":"MDEyOklzc3VlQ29tbWVudDM1MTY2NTQ0Mw==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2017-12-14T10:05:39Z","updated_at":"2017-12-14T10:05:39Z","author_association":"CONTRIBUTOR","body":"Echoing @simonw, there's a timescale issue here: memory and CPU pressure can be a lot more transient than disk space pressure, and it'd be a bad idea to start reallocating shards because of a transient issue.\r\n\r\nOn the other hand if there's a _persistent_ imbalance in memory or CPU usage across nodes then it seems like a better idea to shuffle things around to improve the situation. This raises the tricky question of how to tell if an imbalance is persistent or not, but I feel that this question is at the heart of this issue.\r\n\r\n> 100% CPU isn't a bad thing necessarily\r\n\r\nTrue, but OTOH being bottlenecked on a few hot nodes while the rest of your cluster sits idle seems suboptimal.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/357397708","html_url":"https://github.com/elastic/elasticsearch/issues/27622#issuecomment-357397708","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27622","id":357397708,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NzM5NzcwOA==","user":{"login":"antonpious","id":15054021,"node_id":"MDQ6VXNlcjE1MDU0MDIx","avatar_url":"https://avatars3.githubusercontent.com/u/15054021?v=4","gravatar_id":"","url":"https://api.github.com/users/antonpious","html_url":"https://github.com/antonpious","followers_url":"https://api.github.com/users/antonpious/followers","following_url":"https://api.github.com/users/antonpious/following{/other_user}","gists_url":"https://api.github.com/users/antonpious/gists{/gist_id}","starred_url":"https://api.github.com/users/antonpious/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/antonpious/subscriptions","organizations_url":"https://api.github.com/users/antonpious/orgs","repos_url":"https://api.github.com/users/antonpious/repos","events_url":"https://api.github.com/users/antonpious/events{/privacy}","received_events_url":"https://api.github.com/users/antonpious/received_events","type":"User","site_admin":false},"created_at":"2018-01-13T01:16:23Z","updated_at":"2018-01-22T05:51:41Z","author_association":"NONE","body":"The cluster entered into this state by a mix of search request, id search request and records which were created and updated. I have pointed out the logic change required on the Id in #28206 \r\n\r\nWith both these issues as well as the limit on the memory heap of max 31 GB, the current architecture design is safe till one of the node reaches 31 GB of heap space or the max CPU allocation on a node.\r\n\r\nSo the only logical way out of this situation is to first vertically scale all nodes to max cpu and max 31 GB of  heap memory and then do the horizontal scale of nodes with these vertically scaled limits.\r\nOnce any one of these limits are reached the cluster would enter into a single node throttled state with no solution even if we can increase the cpu we would be stuck on the heap max of 31 GB\r\n\r\nThis seems to be a significant design gap for mixed workloads on the same cluster.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/373384383","html_url":"https://github.com/elastic/elasticsearch/issues/27622#issuecomment-373384383","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27622","id":373384383,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MzM4NDM4Mw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-15T13:57:50Z","updated_at":"2018-03-15T13:57:50Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/382314610","html_url":"https://github.com/elastic/elasticsearch/issues/27622#issuecomment-382314610","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27622","id":382314610,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MjMxNDYxMA==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-04-18T08:53:36Z","updated_at":"2018-04-18T08:53:36Z","author_association":"CONTRIBUTOR","body":"We can divide this into two parts:\r\n\r\n1. assessing how resource-intensive each shard allocation is, and\r\n2. allocating shards so as not to overwhelm the resources of any individual node.\r\n\r\nIt might be possible to perform point 1 automatically, although it will take quite some research to come up with reliable ways to measure things like the CPU and memory load of an individual shard. That said, in the situation described here it seems that it'd be possible to manually specify the resource needs of each index's shards, avoiding the need for an automatic process.\r\n\r\nThe second point is covered by the idea mentioned in https://github.com/elastic/elasticsearch/issues/17213#issuecomment-382310838.\r\n\r\nOur preference would be to close this issue and leave #17213 open to track the second point, but if we see compelling evidence that it's important for Elasticsearch to _automatically_ determine the resource needs of each shard then we'll follow that up too.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/385338395","html_url":"https://github.com/elastic/elasticsearch/issues/27622#issuecomment-385338395","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27622","id":385338395,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NTMzODM5NQ==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-04-30T08:27:51Z","updated_at":"2018-04-30T08:27:51Z","author_association":"CONTRIBUTOR","body":"There were no objections to closing this in favour of #17213 so I'll do so. Please continue to feel free to leave comments indicating support (or otherwise) for this idea and we may revisit it in future.","performed_via_github_app":null}]