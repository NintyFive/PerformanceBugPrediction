{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/20657","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20657/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20657/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20657/events","html_url":"https://github.com/elastic/elasticsearch/issues/20657","id":179112597,"node_id":"MDU6SXNzdWUxNzkxMTI1OTc=","number":20657,"title":"Running ES cluster via Docker with adjusted MTU - secondary node breaks cluster","user":{"login":"cookandy","id":6495064,"node_id":"MDQ6VXNlcjY0OTUwNjQ=","avatar_url":"https://avatars0.githubusercontent.com/u/6495064?v=4","gravatar_id":"","url":"https://api.github.com/users/cookandy","html_url":"https://github.com/cookandy","followers_url":"https://api.github.com/users/cookandy/followers","following_url":"https://api.github.com/users/cookandy/following{/other_user}","gists_url":"https://api.github.com/users/cookandy/gists{/gist_id}","starred_url":"https://api.github.com/users/cookandy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cookandy/subscriptions","organizations_url":"https://api.github.com/users/cookandy/orgs","repos_url":"https://api.github.com/users/cookandy/repos","events_url":"https://api.github.com/users/cookandy/events{/privacy}","received_events_url":"https://api.github.com/users/cookandy/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":21,"created_at":"2016-09-25T22:17:28Z","updated_at":"2016-10-06T00:41:56Z","closed_at":"2016-10-05T19:54:50Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\nGitHub is reserved for bug reports and feature requests. The best place\nto ask a general question is at the Elastic Discourse forums at\nhttps://discuss.elastic.co. If you are in fact posting a bug report or\na feature request, please include one and only one of the below blocks\nin your new issue. Note that whether you're filing a bug report or a\nfeature request, ensure that your submission is for an\n[OS that we support](https://www.elastic.co/support/matrix#show_os).\nBug reports on an OS that we do not support or feature requests\nspecific to an OS that we do not support will be closed.\n-->\n\n<!--\nIf you are filing a bug report, please remove the below feature\nrequest block and provide responses for all of the below items.\n-->\n\n**Elasticsearch version**:  `Docker Image 2.4` https://hub.docker.com/_/elasticsearch/\n\n**Plugins installed**: `none`\n\n**JVM version**:  `openjdk-8`\n\n**OS version**: `Ubuntu 16.04`\n\n**Description of the problem including expected versus actual behavior**:\n\nWhen running an ES cluster in Docker via an IPSec network with adjusted MTU, cluster stops responding after secondary node is started.  \n\n**Background:**\n\nI am running Docker on an IPSec network in `transport` mode.  In order to get docker working correctly via IPSec, the MTU must be adjusted from `1500` to `1460` (to account for the 40 byte IPSec overhead).  Due to some [known issues](https://github.com/docker/docker/issues/22297) setting the the MTU on the `docker0` interface, I have used `iptables` to adjust the MTU.  Without the iptables rule, most all Docker communication fails over IPSec (including the initial load of ES as described later).\n\nWhat I am noticing is, once the primary node of ES starts, I am able to load sample data into the DB repeatedly without error.  However, as soon as the second node of the cluster starts, I can no longer load data, and the cluster health changes.\n\nI believe this is due to the \n\n**Steps to reproduce**:\n- Establish IPSec network between two docker hosts (I used `racoon` and `setkey`)\n- Adjust MTU to `1460` via iptables on both nodes:  `iptables -t mangle -A POSTROUTING -p tcp --tcp-flags SYN,RST SYN -o eth1 -j TCPMSS --set-mss 1460 --clamp-mss-to-pmtu`\n- Start ES on host 1 (replace your IPs): \n\n`docker run -it --rm --name es -p 9200:9200 -p 9300:9300 elasticsearch:2.4 elasticsearch --node.name=elastic-search-1 --cluster.name=elastic-search --network.publish_host=10.138.160.10 --discovery.zen.ping.multicast.enabled=false --discovery.zen.ping.unicast.hosts=10.138.160.10:9300,10.138.160.210:9300 --discovery.zen.ping_timeout=30s --discovery.zen.join_timeout=300s --discovery.zen.publish_timeout=300s --http.port=9200 --transport.tcp.port=9300 --transport.publish_port=9300`\n-  Verify cluster health is good:\n\n```\nroot@sf2-paas-slave-03:~# curl 'http://10.138.160.10:9200/_cluster/health?pretty'\n{\n  \"cluster_name\" : \"elastic-search\",\n  \"status\" : \"green\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 1,\n  \"number_of_data_nodes\" : 1,\n  \"active_primary_shards\" : 0,\n  \"active_shards\" : 0,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 0,\n  \"delayed_unassigned_shards\" : 0,\n  \"number_of_pending_tasks\" : 0,\n  \"number_of_in_flight_fetch\" : 0,\n  \"task_max_waiting_in_queue_millis\" : 0,\n  \"active_shards_percent_as_number\" : 100.0\n}\n```\n- [Load sample data](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/_exploring_your_data.html)\n\n```\ncurl -XPOST 'http://10.138.160.10:9200/bank/account/_bulk?pretty' --data-binary \"@accounts.json\"\n```\n\n**Note:**  at this point if you haven't adjusted your MTU using `iptables`, the curl will hang indefinitely and you'll see this in the ES logs:\n\n```\n[2016-09-25 21:20:44,282][INFO ][cluster.metadata         ] [elastic-search-1] [bank] creating index, cause [auto(bulk api)], templates [], shards [5]/[1], mappings []\n[2016-09-25 21:20:44,802][INFO ][cluster.routing.allocation] [elastic-search-1] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[bank][1], [bank][2], [bank][3], [bank][1], [bank][2], [bank][3], [bank][4]] ...]).\n[2016-09-25 21:20:44,924][INFO ][cluster.metadata         ] [elastic-search-1] [bank] create_mapping [account]\n```\n\nBut since we **DID** update iptables, the data load should happen successfully and we can query the data (and re-load it over and over) without any issues.  \n- Now, start ES on node 2:\n\n`docker run -it --rm --name es -p 9200:9200 -p 9300:9300 elasticsearch:2.4 elasticsearch --node.name=elastic-search-2 --cluster.name=elastic-search --network.publish_host=10.138.160.210 --discovery.zen.ping.multicast.enabled=false --discovery.zen.ping.unicast.hosts=10.138.160.10:9300,10.138.160.210:9300 --discovery.zen.ping_timeout=30s --discovery.zen.join_timeout=300s --discovery.zen.publish_timeout=300s --http.port=9200 --transport.tcp.port=9300 --transport.publish_port=9300`\n\nAt this point, the cluster health changes to yellow (and will never change):\n\n```\nroot@sf2-paas-slave-03:~# curl 'http://10.138.160.10:9200/_cluster/health?pretty'\n{\n  \"cluster_name\" : \"elastic-search\",\n  \"status\" : \"yellow\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 2,\n  \"number_of_data_nodes\" : 2,\n  \"active_primary_shards\" : 5,\n  \"active_shards\" : 5,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 2,\n  \"unassigned_shards\" : 3,\n  \"delayed_unassigned_shards\" : 0,\n  \"number_of_pending_tasks\" : 0,\n  \"number_of_in_flight_fetch\" : 0,\n  \"task_max_waiting_in_queue_millis\" : 0,\n  \"active_shards_percent_as_number\" : 50.0\n}\n```\n- Try to load the data again.  It hangs indefinitely (the same way it would have on Node 1 without the MTU change).\n\nIt appears that the communication between the ES nodes is breaking the cluster.  \n\nI believe this to be a problem with the MTU, as I can load a very small sample file into ES over and over, even once the secondary node has joined.  \n\n**Provide logs (if relevant)**:\n\nInitial startup of Node 1:\n\n```\n[2016-09-25 21:30:50,818][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: your kernel is buggy and you should upgrade\n[2016-09-25 21:30:50,992][INFO ][node                     ] [elastic-search-1] version[2.4.0], pid[1], build[ce9f0c7/2016-08-29T09:14:17Z]\n[2016-09-25 21:30:50,992][INFO ][node                     ] [elastic-search-1] initializing ...\n[2016-09-25 21:30:51,632][INFO ][plugins                  ] [elastic-search-1] modules [reindex, lang-expression, lang-groovy], plugins [], sites []\n[2016-09-25 21:30:51,656][INFO ][env                      ] [elastic-search-1] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/vda1)]], net usable_space [72.7gb], net total_space [78.6gb], spins? [possibly], types [ext4]\n[2016-09-25 21:30:51,656][INFO ][env                      ] [elastic-search-1] heap size [990.7mb], compressed ordinary object pointers [true]\n[2016-09-25 21:30:53,435][INFO ][node                     ] [elastic-search-1] initialized\n[2016-09-25 21:30:53,436][INFO ][node                     ] [elastic-search-1] starting ...\n[2016-09-25 21:30:53,497][INFO ][transport                ] [elastic-search-1] publish_address {10.138.160.10:9300}, bound_addresses {[::]:9300}\n[2016-09-25 21:30:53,502][INFO ][discovery                ] [elastic-search-1] elastic-search/a9I7xvZwQvOVZ5tvWTlGJQ\n[2016-09-25 21:31:23,505][WARN ][discovery                ] [elastic-search-1] waited for 30s and no initial state was set by the discovery\n[2016-09-25 21:31:23,538][INFO ][cluster.service          ] [elastic-search-1] new_master {elastic-search-1}{a9I7xvZwQvOVZ5tvWTlGJQ}{10.138.160.10}{10.138.160.10:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\n[2016-09-25 21:31:23,543][INFO ][http                     ] [elastic-search-1] publish_address {10.138.160.10:9200}, bound_addresses {[::]:9200}\n[2016-09-25 21:31:23,543][INFO ][node                     ] [elastic-search-1] started\n[2016-09-25 21:31:23,582][INFO ][gateway                  ] [elastic-search-1] recovered [0] indices into cluster_state\n```\n\nNode 1 when sample data is being loaded:\n\n```\n[2016-09-25 21:31:57,089][INFO ][cluster.metadata         ] [elastic-search-1] [bank] creating index, cause [auto(bulk api)], templates [], shards [5]/[1], mappings []\n[2016-09-25 21:31:57,450][INFO ][cluster.routing.allocation] [elastic-search-1] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[bank][1], [bank][2], [bank][3], [bank][3], [bank][2], [bank][1], [bank][4]] ...]).\n[2016-09-25 21:31:57,593][INFO ][cluster.metadata         ] [elastic-search-1] [bank] create_mapping [account]\n```\n\nNode 2 startup logs:\n\n```\n[2016-09-25 21:35:05,883][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: your kernel is buggy and you should upgrade\n[2016-09-25 21:35:06,048][INFO ][node                     ] [elastic-search-2] version[2.4.0], pid[1], build[ce9f0c7/2016-08-29T09:14:17Z]\n[2016-09-25 21:35:06,048][INFO ][node                     ] [elastic-search-2] initializing ...\n[2016-09-25 21:35:06,720][INFO ][plugins                  ] [elastic-search-2] modules [reindex, lang-expression, lang-groovy], plugins [], sites []\n[2016-09-25 21:35:06,752][INFO ][env                      ] [elastic-search-2] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/vda1)]], net usable_space [72.9gb], net total_space [78.6gb], spins? [possibly], types [ext4]\n[2016-09-25 21:35:06,752][INFO ][env                      ] [elastic-search-2] heap size [990.7mb], compressed ordinary object pointers [true]\n[2016-09-25 21:35:08,503][INFO ][node                     ] [elastic-search-2] initialized\n[2016-09-25 21:35:08,504][INFO ][node                     ] [elastic-search-2] starting ...\n[2016-09-25 21:35:08,566][INFO ][transport                ] [elastic-search-2] publish_address {10.138.160.210:9300}, bound_addresses {[::]:9300}\n[2016-09-25 21:35:08,575][INFO ][discovery                ] [elastic-search-2] elastic-search/aMeYDsBBST2p3EK6vHzIYA\n[2016-09-25 21:35:38,578][WARN ][discovery                ] [elastic-search-2] waited for 30s and no initial state was set by the discovery\n[2016-09-25 21:35:38,608][INFO ][http                     ] [elastic-search-2] publish_address {10.138.160.210:9200}, bound_addresses {[::]:9200}\n[2016-09-25 21:35:38,610][INFO ][node                     ] [elastic-search-2] started\n[2016-09-25 21:35:38,693][INFO ][cluster.service          ] [elastic-search-2] detected_master {elastic-search-1}{a9I7xvZwQvOVZ5tvWTlGJQ}{10.138.160.10}{10.138.160.10:9300}, added {{elastic-search-1}{a9I7xvZwQvOVZ5tvWTlGJQ}{10.138.160.10}{10.138.160.10:9300},}, reason: zen-disco-receive(from master [{elastic-search-1}{a9I7xvZwQvOVZ5tvWTlGJQ}{10.138.160.10}{10.138.160.10:9300}])\n```\n\nOccasionally in the Node 2 startup logs I will see the following.  **This might be the problem:**\n\n```\n[2016-09-25 22:11:09,957][INFO ][cluster.service          ] [elastic-search-2] detected_master {elastic-search-1}{8WJwu5ZnR9mDcXxBzVhC6A}{10.138.160.10}{10.138.160.10:9300}, added {{elastic-search-1}{8WJwu5ZnR9mDcXxBzVhC6A}{10.138.160.10}{10.138.160.10:9300},}, reason: zen-disco-receive(from master [{elastic-search-1}{8WJwu5ZnR9mDcXxBzVhC6A}{10.138.160.10}{10.138.160.10:9300}])\n[2016-09-25 22:11:10,271][WARN ][transport                ] [elastic-search-2] Transport response handler not found of id [10]\n[2016-09-25 22:11:10,273][WARN ][transport                ] [elastic-search-2] Transport response handler not found of id [11]\n```\n\nNode 1, when Node 2 starts and joins:\n\n```\n[2016-09-25 21:35:38,684][INFO ][cluster.service          ] [elastic-search-1] added {{elastic-search-2}{aMeYDsBBST2p3EK6vHzIYA}{10.138.160.210}{10.138.160.210:9300},}, reason: zen-disco-join(join from node[{elastic-search-2}{aMeYDsBBST2p3EK6vHzIYA}{10.138.160.210}{10.138.160.210:9300}])\n```\n\n<!--\nIf you are filing a feature request, please remove the above bug\nreport block and provide responses for all of the below items.\n-->\n\n**Describe the feature**:\n","closed_by":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"performed_via_github_app":null}