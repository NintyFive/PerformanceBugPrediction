[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/355457620","html_url":"https://github.com/elastic/elasticsearch/issues/28090#issuecomment-355457620","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28090","id":355457620,"node_id":"MDEyOklzc3VlQ29tbWVudDM1NTQ1NzYyMA==","user":{"login":"tvernum","id":2244393,"node_id":"MDQ6VXNlcjIyNDQzOTM=","avatar_url":"https://avatars0.githubusercontent.com/u/2244393?v=4","gravatar_id":"","url":"https://api.github.com/users/tvernum","html_url":"https://github.com/tvernum","followers_url":"https://api.github.com/users/tvernum/followers","following_url":"https://api.github.com/users/tvernum/following{/other_user}","gists_url":"https://api.github.com/users/tvernum/gists{/gist_id}","starred_url":"https://api.github.com/users/tvernum/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tvernum/subscriptions","organizations_url":"https://api.github.com/users/tvernum/orgs","repos_url":"https://api.github.com/users/tvernum/repos","events_url":"https://api.github.com/users/tvernum/events{/privacy}","received_events_url":"https://api.github.com/users/tvernum/received_events","type":"User","site_admin":false},"created_at":"2018-01-05T02:34:49Z","updated_at":"2018-01-05T02:34:49Z","author_association":"CONTRIBUTOR","body":"_Courtesy of Google Translate_:\r\n\r\nIn spark use elasticsearch 5.5.3 version of the client, multi-task operation Bulk write data, one of the task to send a deadlock.\r\nCode:\r\n\r\n```java\r\nBulkRequestBuilder bulkRequest = client.prepareBulk();\r\nbulkRequest.request().setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE);\r\nfor (FMEyeChainLog log : logList) {\r\n     // added by add batch\r\n    bulkRequest.add(client.prepareIndex(getIndexName(indexName, date), indexType).setSource(getXContentBuilderByChain(log)));\r\n}\r\nBulkResponse bulkResponse = bulkRequest.execute().actionGet();\r\n// if it fails\r\nif (bulkResponse.hasFailures()) {\r\n    // process failures by iterating through each bulk response item\r\n    System.out.println(\"buildFailureMessage:\" + bulkResponse.buildFailureMessage());\r\n}\r\n```\r\n\r\n\r\nAbnormal thread stack:\r\n\r\nThread ID | Thread Name | Thread State | Thread Locks\r\n-- | -- | -- | --\r\n102 | Executor task launch worker-1 | WAITING | Lock(java.util.concurrent.ThreadPoolExecutor$Worker@1595623431})\r\n\r\n\r\nsun.misc.Unsafe.park(Native Method) java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836) java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997) java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:248) org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:91) org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:42) com.dazong.eye.analysis.repository.elasticsearch.ElasticsearchRepository.saveChain(ElasticsearchRepository.java:183) com.dazong.eye.analysis.repository.elasticsearch.ElasticsearchRepository.saveChainByBatch(ElasticsearchRepository.java:150) com.dazong.eye.analysis.repository.elasticsearch.ElasticsearchRepository.saveToES(ElasticsearchRepository.java:118) com.dazong.eye.analysis.core.SparkChainAndLog4Kafka$2$1.call(SparkChainAndLog4Kafka.java:198) com.dazong.eye.analysis.core.SparkChainAndLog4Kafka$2$1.call(SparkChainAndLog4Kafka.java:141) org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartitionAsync$1.apply(JavaRDDLike.scala:741) org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartitionAsync$1.apply(JavaRDDLike.scala:741) org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:2021) org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:2021) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) org.apache.spark.scheduler.Task.run(Task.scala:99) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) java.lang.Thread.run(Thread.java:745)\r\n\r\n","performed_via_github_app":null}]