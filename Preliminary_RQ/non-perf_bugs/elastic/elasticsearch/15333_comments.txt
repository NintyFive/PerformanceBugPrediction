[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163191243","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163191243","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163191243,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzE5MTI0Mw==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-09T11:13:22Z","updated_at":"2015-12-09T11:13:22Z","author_association":"CONTRIBUTOR","body":"maybe you can tell us what prevented you from starting up again?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163200034","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163200034","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163200034,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzIwMDAzNA==","user":{"login":"kpcool","id":2868966,"node_id":"MDQ6VXNlcjI4Njg5NjY=","avatar_url":"https://avatars3.githubusercontent.com/u/2868966?v=4","gravatar_id":"","url":"https://api.github.com/users/kpcool","html_url":"https://github.com/kpcool","followers_url":"https://api.github.com/users/kpcool/followers","following_url":"https://api.github.com/users/kpcool/following{/other_user}","gists_url":"https://api.github.com/users/kpcool/gists{/gist_id}","starred_url":"https://api.github.com/users/kpcool/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpcool/subscriptions","organizations_url":"https://api.github.com/users/kpcool/orgs","repos_url":"https://api.github.com/users/kpcool/repos","events_url":"https://api.github.com/users/kpcool/events{/privacy}","received_events_url":"https://api.github.com/users/kpcool/received_events","type":"User","site_admin":false},"created_at":"2015-12-09T11:53:33Z","updated_at":"2015-12-09T11:55:56Z","author_association":"NONE","body":"There were 15 indices on the node. Of those 15 indices, 9 indices had issues with their shards and ES status was red.\n\nIssue curl -XGET http://localhost:9200/_cat/shards command, listed 52 shards UNASSIGNED and 4 shards in INITIALIZING status.\n\nI issued reroute command (localhost:9200/_cluster/reroute) to move UNASSIGNED to force shard allocation. \n\nHowever, the shards that were in INITIALIZING status stay there. The CPU usage was 100% (8-cores busy) for more than 4 hours, before I gave up and started deleting all indices that were causing the problem. Data was about 50GB and 6 Million records.\n\nEven issuing systemctl stop elasticsearch.service took forever.\n\nIs this what you were looking for, if not let me know what you are looking for and I will reply ASAP\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163296271","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163296271","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163296271,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzI5NjI3MQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-09T15:40:08Z","updated_at":"2015-12-09T15:40:08Z","author_association":"CONTRIBUTOR","body":"there are lots of open questions, do you have some logs telling why the shards where unassigned? did you just upgrade? Why do you force them to allocate? did you run into any disk space issues?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163303480","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163303480","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163303480,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzMwMzQ4MA==","user":{"login":"kpcool","id":2868966,"node_id":"MDQ6VXNlcjI4Njg5NjY=","avatar_url":"https://avatars3.githubusercontent.com/u/2868966?v=4","gravatar_id":"","url":"https://api.github.com/users/kpcool","html_url":"https://github.com/kpcool","followers_url":"https://api.github.com/users/kpcool/followers","following_url":"https://api.github.com/users/kpcool/following{/other_user}","gists_url":"https://api.github.com/users/kpcool/gists{/gist_id}","starred_url":"https://api.github.com/users/kpcool/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpcool/subscriptions","organizations_url":"https://api.github.com/users/kpcool/orgs","repos_url":"https://api.github.com/users/kpcool/repos","events_url":"https://api.github.com/users/kpcool/events{/privacy}","received_events_url":"https://api.github.com/users/kpcool/received_events","type":"User","site_admin":false},"created_at":"2015-12-09T15:59:51Z","updated_at":"2015-12-09T15:59:51Z","author_association":"NONE","body":"Yes, the disk got full and then after the issue started happening as ES\nstopped responding\n\nRegards,\nKetan\n\nOn Dec 9, 2015, at 9:11 PM, Simon Willnauer notifications@github.com\nwrote:\n\nthere are lots of open questions, do you have some logs telling why the\nshards where unassigned? did you just upgrade? Why do you force them to\nallocate? did you run into any disk space issues?\n\nâ€”\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/elasticsearch/issues/15333#issuecomment-163296271\n.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163599063","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163599063","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163599063,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzU5OTA2Mw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-12-10T12:42:43Z","updated_at":"2015-12-10T12:42:43Z","author_association":"CONTRIBUTOR","body":"@kpcool Please could you provide the logs and also answers for the questions asked by @s1monw .  The information you have provided up until now provides no clues at to why the shards were not reassigned, etc.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163604699","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163604699","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163604699,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzYwNDY5OQ==","user":{"login":"kpcool","id":2868966,"node_id":"MDQ6VXNlcjI4Njg5NjY=","avatar_url":"https://avatars3.githubusercontent.com/u/2868966?v=4","gravatar_id":"","url":"https://api.github.com/users/kpcool","html_url":"https://github.com/kpcool","followers_url":"https://api.github.com/users/kpcool/followers","following_url":"https://api.github.com/users/kpcool/following{/other_user}","gists_url":"https://api.github.com/users/kpcool/gists{/gist_id}","starred_url":"https://api.github.com/users/kpcool/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpcool/subscriptions","organizations_url":"https://api.github.com/users/kpcool/orgs","repos_url":"https://api.github.com/users/kpcool/repos","events_url":"https://api.github.com/users/kpcool/events{/privacy}","received_events_url":"https://api.github.com/users/kpcool/received_events","type":"User","site_admin":false},"created_at":"2015-12-10T13:02:02Z","updated_at":"2015-12-10T13:08:59Z","author_association":"NONE","body":"Here's the log around that time.\n\n```\n[2015-12-09 00:00:18,560][ERROR][index.engine             ] [Mister Jip] [topbeat-2015.12.09][3] failed to merge\njava.io.IOException: No space left on device\n        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n        at sun.nio.ch.IOUtil.write(IOUtil.java:65)\n        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n        at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n        at java.nio.channels.Channels.writeFully(Channels.java:101)\n        at java.nio.channels.Channels.access$000(Channels.java:61)\n        at java.nio.channels.Channels$1.write(Channels.java:174)\n        at org.apache.lucene.store.FSDirectory$FSIndexOutput$1.write(FSDirectory.java:271)\n        at java.util.zip.CheckedOutputStream.write(CheckedOutputStream.java:73)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n        at org.apache.lucene.store.OutputStreamIndexOutput.writeBytes(OutputStreamIndexOutput.java:53)\n        at org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:73)\n        at org.apache.lucene.store.DataOutput.writeBytes(DataOutput.java:52)\n        at org.apache.lucene.util.packed.DirectWriter.flush(DirectWriter.java:86)\n        at org.apache.lucene.util.packed.DirectWriter.add(DirectWriter.java:78)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:218)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:80)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addSortedNumericField(Lucene50DocValuesConsumer.java:470)\n        at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addSortedNumericField(PerFieldDocValuesFormat.java:126)\n        at org.apache.lucene.codecs.DocValuesConsumer.mergeSortedNumericField(DocValuesConsumer.java:417)\n        at org.apache.lucene.codecs.DocValuesConsumer.merge(DocValuesConsumer.java:236)\n        at org.apache.lucene.index.SegmentMerger.mergeDocValues(SegmentMerger.java:150)\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:105)\n        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4089)\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3664)\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)\n        at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:94)\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)\n[2015-12-09 00:00:18,997][WARN ][index.engine             ] [Mister Jip] [topbeat-2015.12.09][3] failed engine [already closed by tragic event]\njava.io.IOException: No space left on device\n        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n        at sun.nio.ch.IOUtil.write(IOUtil.java:65)\n        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n        at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n        at java.nio.channels.Channels.writeFully(Channels.java:101)\n        at java.nio.channels.Channels.access$000(Channels.java:61)\n        at java.nio.channels.Channels$1.write(Channels.java:174)\n        at org.apache.lucene.store.FSDirectory$FSIndexOutput$1.write(FSDirectory.java:271)\n        at java.util.zip.CheckedOutputStream.write(CheckedOutputStream.java:73)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n        at org.apache.lucene.store.OutputStreamIndexOutput.writeBytes(OutputStreamIndexOutput.java:53)\n        at org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:73)\n        at org.apache.lucene.store.DataOutput.writeBytes(DataOutput.java:52)\n        at org.apache.lucene.util.packed.DirectWriter.flush(DirectWriter.java:86)\n        at org.apache.lucene.util.packed.DirectWriter.add(DirectWriter.java:78)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:218)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:80)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addSortedNumericField(Lucene50DocValuesConsumer.java:470)\n        at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addSortedNumericField(PerFieldDocValuesFormat.java:126)\n        at org.apache.lucene.codecs.DocValuesConsumer.mergeSortedNumericField(DocValuesConsumer.java:417)\n        at org.apache.lucene.codecs.DocValuesConsumer.merge(DocValuesConsumer.java:236)\n        at org.apache.lucene.index.SegmentMerger.mergeDocValues(SegmentMerger.java:150)\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:105)\n        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4089)\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3664)\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)\n        at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:94)\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)\n[2015-12-09 00:00:19,015][WARN ][indices.cluster          ] [Mister Jip] [[topbeat-2015.12.09][3]] marking and sending shard failed due to [engine failure, reason [already closed by tragic event]]\njava.io.IOException: No space left on device\n        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n        at sun.nio.ch.IOUtil.write(IOUtil.java:65)\n        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n        at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n        at java.nio.channels.Channels.writeFully(Channels.java:101)\n        at java.nio.channels.Channels.access$000(Channels.java:61)\n        at java.nio.channels.Channels$1.write(Channels.java:174)\n        at org.apache.lucene.store.FSDirectory$FSIndexOutput$1.write(FSDirectory.java:271)\n        at java.util.zip.CheckedOutputStream.write(CheckedOutputStream.java:73)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n        at org.apache.lucene.store.OutputStreamIndexOutput.writeBytes(OutputStreamIndexOutput.java:53)\n        at org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:73)\n        at org.apache.lucene.store.DataOutput.writeBytes(DataOutput.java:52)\n        at org.apache.lucene.util.packed.DirectWriter.flush(DirectWriter.java:86)\n        at org.apache.lucene.util.packed.DirectWriter.add(DirectWriter.java:78)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:218)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:80)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addSortedNumericField(Lucene50DocValuesConsumer.java:470)\n        at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addSortedNumericField(PerFieldDocValuesFormat.java:126)\n        at org.apache.lucene.codecs.DocValuesConsumer.mergeSortedNumericField(DocValuesConsumer.java:417)\n        at org.apache.lucene.codecs.DocValuesConsumer.merge(DocValuesConsumer.java:236)\n        at org.apache.lucene.index.SegmentMerger.mergeDocValues(SegmentMerger.java:150)\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:105)\n        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4089)\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3664)\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)\n        at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:94)\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)\n[2015-12-09 00:00:19,015][WARN ][cluster.action.shard     ] [Mister Jip] [topbeat-2015.12.09][3] received shard failed for [topbeat-2015.12.09][3], node[HmS7B_CdRFqPFT1UeUZEfA], [P], v[5], s[INITIALIZING], a[id=3Yn-3bO6QtClvHUDwYnClw], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-09T04:58:37.185Z], details[engine failure, reason [merge failed], failure MergeException[java.io.IOException: No space left on device]; nested: IOException[No space left on device]; ]], indexUUID [rvUixkXqTty2osh3-PMubw], message [engine failure, reason [already closed by tragic event]], failure [IOException[No space left on device]]\njava.io.IOException: No space left on device\n        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n        at sun.nio.ch.IOUtil.write(IOUtil.java:65)\n        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n        at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n        at java.nio.channels.Channels.writeFully(Channels.java:101)\n        at java.nio.channels.Channels.access$000(Channels.java:61)\n        at java.nio.channels.Channels$1.write(Channels.java:174)\n        at org.apache.lucene.store.FSDirectory$FSIndexOutput$1.write(FSDirectory.java:271)\n        at java.util.zip.CheckedOutputStream.write(CheckedOutputStream.java:73)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n        at org.apache.lucene.store.OutputStreamIndexOutput.writeBytes(OutputStreamIndexOutput.java:53)\n        at org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:73)\n        at org.apache.lucene.store.DataOutput.writeBytes(DataOutput.java:52)\n        at org.apache.lucene.util.packed.DirectWriter.flush(DirectWriter.java:86)\n        at org.apache.lucene.util.packed.DirectWriter.add(DirectWriter.java:78)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:218)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:80)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addSortedNumericField(Lucene50DocValuesConsumer.java:470)\n        at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addSortedNumericField(PerFieldDocValuesFormat.java:126)\n        at org.apache.lucene.codecs.DocValuesConsumer.mergeSortedNumericField(DocValuesConsumer.java:417)\n        at org.apache.lucene.codecs.DocValuesConsumer.merge(DocValuesConsumer.java:236)\n        at org.apache.lucene.index.SegmentMerger.mergeDocValues(SegmentMerger.java:150)\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:105)\n        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4089)\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3664)\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)\n        at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:94)\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)\n\n[2015-12-09 00:00:19,887][WARN ][index.translog           ] [Mister Jip] [topbeat-2015.12.09][0] failed to delete temp file /var/lib/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/0/translog/translog-6857015315422195400.tlog\njava.nio.file.NoSuchFileException: /var/lib/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/0/translog/translog-6857015315422195400.tlog\n        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)\n        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)\n        at java.nio.file.Files.delete(Files.java:1126)\n        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)\n        at org.elasticsearch.index.translog.Translog.<init>(Translog.java:166)\n        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)\n        at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:152)\n        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)\n        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)\n        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)\n        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)\n        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)\n        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)\n        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)\n        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-12-09 00:00:24,760][WARN ][cluster.routing.allocation.decider] [Mister Jip] high disk watermark [90%] exceeded on [HmS7B_CdRFqPFT1UeUZEfA][Mister Jip][/var/lib/elasticsearch/DC_Reports/nodes/0] free: 1.3mb[0%], shards will be relocated away from this node\n[2015-12-09 00:00:24,760][INFO ][cluster.routing.allocation.decider] [Mister Jip] rerouting shards: [high disk watermark exceeded on one or more nodes]\n\n[2015-12-09 00:00:24,851][INFO ][rest.suppressed          ] /dealscornerin-50 Params: {index=dealscornerin-50}\n[dealscornerin-50] IndexAlreadyExistsException[already exists]\n        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:168)\n        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validate(MetaDataCreateIndexService.java:520)\n        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.access$200(MetaDataCreateIndexService.java:97)\n        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:241)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-12-09 00:00:54,241][ERROR][marvel.agent             ] [Mister Jip] background thread had an uncaught exception\nElasticsearchException[failed to flush exporter bulks]\n        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:104)\n        at org.elasticsearch.marvel.agent.exporter.ExportBulk.close(ExportBulk.java:53)\n        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:201)\n        at java.lang.Thread.run(Thread.java:745)\n        Suppressed: ElasticsearchException[failed to flush [default_local] exporter bulk]; nested: ElasticsearchException[failure in bulk execution, only the first 100 failures are printed:\n[0]: index [.marvel-es-2015.12.09], type [index_recovery], id [AVGFHCn_dr-UG15JaoIa], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[1]: index [.marvel-es-2015.12.09], type [indices_stats], id [AVGFHCn_dr-UG15JaoIb], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[2]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.04:3:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[3]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.04:3:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[4]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.04:1:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[5]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.04:1:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[6]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.04:2:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[7]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.04:2:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[8]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.04:4:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[9]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.04:4:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[10]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.04:0:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[11]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.04:0:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[12]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.03:3:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[13]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.03:3:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[14]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.03:1:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[15]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.03:1:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[16]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.03:2:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[17]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.03:2:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[18]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.03:4:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[19]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.03:4:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n--------------Similar logs--------------\n[98]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:dealscornerin-49:0:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]\n[99]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:packetbeat-2015.12.03:3:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]]\n                at org.elasticsearch.marvel.agent.exporter.local.LocalBulk.flush(LocalBulk.java:114)\n                at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:101)\n                ... 3 more\n[2015-12-09 00:00:55,213][WARN ][cluster.routing.allocation.decider] [Mister Jip] high disk watermark [90%] exceeded on [HmS7B_CdRFqPFT1UeUZEfA][Mister Jip][/var/lib/elasticsearch/DC_Reports/nodes/0] free: 20kb[3.8E-5%], shards will be relocated away from this node\n[2015-12-09 00:01:04,257][DEBUG][action.admin.indices.stats] [Mister Jip] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][4], node[HmS7B_CdRFqPFT1UeUZEfA], [P], v[5], s[INITIALIZING], a[id=n5bBcfxdS7ey8IpgEyxwzA], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-09T04:58:37.227Z], details[engine failure, reason [merge failed], failure MergeException[java.io.IOException: No space left on device]; nested: IOException[No space left on device]; ]]]\n[topbeat-2015.12.09][[topbeat-2015.12.09][4]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [topbeat-2015.12.09][[topbeat-2015.12.09][4]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n[2015-12-09 00:01:04,257][DEBUG][action.admin.indices.stats] [Mister Jip] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][2], node[HmS7B_CdRFqPFT1UeUZEfA], [P], v[33], s[INITIALIZING], a[id=hsMorSXnRYCQa28IlkksYQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-09T04:58:37.227Z], details[engine failure, reason [already closed by tragic event], failure IOException[No space left on device]]]]\n[topbeat-2015.12.09][[topbeat-2015.12.09][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [topbeat-2015.12.09][[topbeat-2015.12.09][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163609853","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163609853","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163609853,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzYwOTg1Mw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-12-10T13:09:34Z","updated_at":"2015-12-10T13:09:34Z","author_association":"CONTRIBUTOR","body":"OK, so here the disk is full.  What happened in the logs after you cleared out space on the disk?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163836623","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163836623","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163836623,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzgzNjYyMw==","user":{"login":"kpcool","id":2868966,"node_id":"MDQ6VXNlcjI4Njg5NjY=","avatar_url":"https://avatars3.githubusercontent.com/u/2868966?v=4","gravatar_id":"","url":"https://api.github.com/users/kpcool","html_url":"https://github.com/kpcool","followers_url":"https://api.github.com/users/kpcool/followers","following_url":"https://api.github.com/users/kpcool/following{/other_user}","gists_url":"https://api.github.com/users/kpcool/gists{/gist_id}","starred_url":"https://api.github.com/users/kpcool/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpcool/subscriptions","organizations_url":"https://api.github.com/users/kpcool/orgs","repos_url":"https://api.github.com/users/kpcool/repos","events_url":"https://api.github.com/users/kpcool/events{/privacy}","received_events_url":"https://api.github.com/users/kpcool/received_events","type":"User","site_admin":false},"created_at":"2015-12-11T04:30:37Z","updated_at":"2015-12-11T04:32:51Z","author_association":"NONE","body":"Here's the log when I tried to start the ES after shutting it down:\n\n```\n[2015-12-09 01:53:35,386][WARN ][bootstrap                ] If you are logged in interactively, you will have to re-login for the new limits to take effect.\n[2015-12-09 01:53:35,632][INFO ][node                     ] [Maxam] version[2.1.0], pid[5742], build[72cd1f1/2015-11-18T22:40:03Z]\n[2015-12-09 01:53:35,632][INFO ][node                     ] [Maxam] initializing ...\n[2015-12-09 01:53:36,167][INFO ][plugins                  ] [Maxam] loaded [license, marvel-agent], sites [kopf]\n[2015-12-09 01:53:36,219][INFO ][env                      ] [Maxam] using [1] data paths, mounts [[/home (/dev/mapper/centos-home)]], net usable_space [826.2gb], net total_space [872.6gb], spins? [possibly], types [xfs]\n[2015-12-09 01:53:38,666][INFO ][node                     ] [Maxam] initialized\n[2015-12-09 01:53:38,666][INFO ][node                     ] [Maxam] starting ...\n[2015-12-09 01:53:38,877][INFO ][transport                ] [Maxam] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}\n[2015-12-09 01:53:38,897][INFO ][discovery                ] [Maxam] DC_Reports/ywHqZlB2Ty6FKboZPgRoZQ\n[2015-12-09 01:53:41,926][INFO ][cluster.service          ] [Maxam] new_master {Maxam}{ywHqZlB2Ty6FKboZPgRoZQ}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\n[2015-12-09 01:53:41,939][INFO ][http                     ] [Maxam] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}\n[2015-12-09 01:53:41,940][INFO ][node                     ] [Maxam] started\n[2015-12-09 01:53:46,499][INFO ][license.plugin.core      ] [Maxam] license [07b70bf8-cc41-45d7-900c-67a16d05b960] - valid\n[2015-12-09 01:53:46,500][ERROR][license.plugin.core      ] [Maxam]\n#\n# License will expire on [Thursday, December 31, 2015]. If you have a new license, please update it.\n# Otherwise, please reach out to your support contact.\n#\n# Commercial plugins operate with reduced functionality on license expiration:\n# - marvel\n#  - The agent will stop collecting cluster and indices metrics\n[2015-12-09 01:53:47,706][INFO ][gateway                  ] [Maxam] recovered [27] indices into cluster_state\n[2015-12-09 01:53:48,164][WARN ][index.translog           ] [Maxam] [topbeat-2015.12.09][3] failed to delete temp file /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/3/translog/translog-4819800625171304865.tlog\njava.nio.file.NoSuchFileException: /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/3/translog/translog-4819800625171304865.tlog\n        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)\n        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)\n        at java.nio.file.Files.delete(Files.java:1126)\n        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)\n        at org.elasticsearch.index.translog.Translog.<init>(Translog.java:166)\n        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)\n        at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:152)\n        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)\n        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)\n        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)\n        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)\n        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)\n        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)\n        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)\n        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-12-09 01:53:48,172][WARN ][index.translog           ] [Maxam] [topbeat-2015.12.09][2] failed to delete temp file /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/2/translog/translog-4092628000966967177.tlog\njava.nio.file.NoSuchFileException: /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/2/translog/translog-4092628000966967177.tlog\n        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)\n        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)\n        at java.nio.file.Files.delete(Files.java:1126)\n        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)\n        at org.elasticsearch.index.translog.Translog.<init>(Translog.java:166)\n        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)\n        at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:152)\n        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)\n        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)\n        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)\n        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)\n        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)\n        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)\n        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)\n        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-12-09 01:53:48,172][WARN ][index.translog           ] [Maxam] [topbeat-2015.12.09][1] failed to delete temp file /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/1/translog/translog-1515358772559515929.tlog\njava.nio.file.NoSuchFileException: /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/1/translog/translog-1515358772559515929.tlog\n        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)\n        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)\n        at java.nio.file.Files.delete(Files.java:1126)\n        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)\n        at org.elasticsearch.index.translog.Translog.<init>(Translog.java:166)\n        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)\n        at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:152)\n        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)\n        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)\n        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)\n        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)\n        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)\n        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)\n        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)\n        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-12-09 01:53:48,164][WARN ][index.translog           ] [Maxam] [topbeat-2015.12.09][4] failed to delete temp file /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/4/translog/translog-7914277937547324566.tlog\njava.nio.file.NoSuchFileException: /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/4/translog/translog-7914277937547324566.tlog\n        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)\n        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)\n        at java.nio.file.Files.delete(Files.java:1126)\n        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)\n        at org.elasticsearch.index.translog.Translog.<init>(Translog.java:166)\n        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)\n        at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:152)\n        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)\n        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)\n        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)\n        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)\n        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)\n        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)\n        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)\n        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-12-09 01:53:48,819][DEBUG][action.admin.indices.stats] [Maxam] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][1], node[ywHqZlB2Ty6FKboZPgRoZQ], [P], v[3], s[INITIALIZING], a[id=beI3ZtSZRLSjmp382hpjGA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-09T06:53:42.007Z]]]\n[topbeat-2015.12.09][[topbeat-2015.12.09][1]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [topbeat-2015.12.09][[topbeat-2015.12.09][1]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n[2015-12-09 01:53:48,827][DEBUG][action.admin.indices.stats] [Maxam] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][4], node[ywHqZlB2Ty6FKboZPgRoZQ], [P], v[3], s[INITIALIZING], a[id=ibydvhMyTG-8uFU_y2Gx1g], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-09T06:53:42.007Z]]]\n[topbeat-2015.12.09][[topbeat-2015.12.09][4]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [topbeat-2015.12.09][[topbeat-2015.12.09][4]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n[2015-12-09 01:53:48,835][DEBUG][action.admin.indices.stats] [Maxam] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][3], node[ywHqZlB2Ty6FKboZPgRoZQ], [P], v[3], s[INITIALIZING], a[id=bOgSHC15TWakUYPIMhEz7A], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-09T06:53:42.007Z]]]\n[topbeat-2015.12.09][[topbeat-2015.12.09][3]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [topbeat-2015.12.09][[topbeat-2015.12.09][3]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n[2015-12-09 01:53:48,839][DEBUG][action.admin.indices.stats] [Maxam] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][2], node[ywHqZlB2Ty6FKboZPgRoZQ], [P], v[3], s[INITIALIZING], a[id=QqLsO7WMRn-P24LXF1LuiQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-09T06:53:42.007Z]]]\n[topbeat-2015.12.09][[topbeat-2015.12.09][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [topbeat-2015.12.09][[topbeat-2015.12.09][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n[2015-12-09 01:53:49,020][DEBUG][action.admin.indices.stats] [Maxam] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][2], node[ywHqZlB2Ty6FKboZPgRoZQ], [P], v[3], s[INITIALIZING], a[id=QqLsO7WMRn-P24LXF1LuiQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-09T06:53:42.007Z]]]\n[topbeat-2015.12.09][[topbeat-2015.12.09][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [topbeat-2015.12.09][[topbeat-2015.12.09][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n[2015-12-09 01:54:49,046][ERROR][marvel.agent             ] [Maxam] background thread had an uncaught exception\nElasticsearchException[failed to flush exporter bulks]\n        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:104)\n        at org.elasticsearch.marvel.agent.exporter.ExportBulk.close(ExportBulk.java:53)\n        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:201)\n        at java.lang.Thread.run(Thread.java:745)\n        Suppressed: ElasticsearchException[failed to flush [default_local] exporter bulk]; nested: ElasticsearchException[failure in bulk execution, only the first 100 failures are printed:\n[0]: index [.marvel-es-2015.12.09], type [index_recovery], id [AVGFhHRp_6d18XbNgIBf], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[1]: index [.marvel-es-2015.12.09], type [indices_stats], id [AVGFhHRp_6d18XbNgIBg], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[2]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:1:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[3]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:1:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[4]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:4:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[5]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:4:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[6]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:3:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[7]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:3:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[8]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:2:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[9]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:2:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[10]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:0:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[11]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:0:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[12]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:1:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[13]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:1:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[14]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:4:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[15]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:4:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[16]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:3:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[17]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:3:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[18]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:2:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[19]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:2:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[20]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:0:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[21]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:0:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[22]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.08:1:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[23]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.08:1:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[24]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.08:4:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n[25]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.08:4:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163903835","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163903835","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163903835,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzkwMzgzNQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-11T10:28:51Z","updated_at":"2015-12-11T10:28:51Z","author_association":"CONTRIBUTOR","body":"with your last log I can't see exceptions that indicate that recovery failed. Does the cluster come back up or do you get stuck in recoveries? The `failed to delete temp file /home/elkuser/elasticsearch/...` warn logs are annoying but harmless and fixed already. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163922412","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163922412","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163922412,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzkyMjQxMg==","user":{"login":"kpcool","id":2868966,"node_id":"MDQ6VXNlcjI4Njg5NjY=","avatar_url":"https://avatars3.githubusercontent.com/u/2868966?v=4","gravatar_id":"","url":"https://api.github.com/users/kpcool","html_url":"https://github.com/kpcool","followers_url":"https://api.github.com/users/kpcool/followers","following_url":"https://api.github.com/users/kpcool/following{/other_user}","gists_url":"https://api.github.com/users/kpcool/gists{/gist_id}","starred_url":"https://api.github.com/users/kpcool/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpcool/subscriptions","organizations_url":"https://api.github.com/users/kpcool/orgs","repos_url":"https://api.github.com/users/kpcool/repos","events_url":"https://api.github.com/users/kpcool/events{/privacy}","received_events_url":"https://api.github.com/users/kpcool/received_events","type":"User","site_admin":false},"created_at":"2015-12-11T12:00:36Z","updated_at":"2015-12-11T12:00:36Z","author_association":"NONE","body":"Here's the log where there's unhandled exception.\n\n```\n[2015-12-09 01:54:49,046][ERROR][marvel.agent             ] [Maxam] background thread had an uncaught exception\nElasticsearchException[failed to flush exporter bulks]\n        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:104)\n        at org.elasticsearch.marvel.agent.exporter.ExportBulk.close(ExportBulk.java:53)\n        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:201)\n        at java.lang.Thread.run(Thread.java:745)\n        Suppressed: ElasticsearchException[failed to flush [default_local] exporter bulk]; nested: ElasticsearchException[failure in bulk execution, only the first 100 failures are printed:\n[0]: index [.marvel-es-2015.12.09], type [index_recovery], id [AVGFhHRp_6d18XbNgIBf], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]\n```\n\nAlso, the cluster stuck in recovery for more than 4 hours, before I gave up and started removing indices giving problem. Basically, shards were in UNASSIGNED state and some were in INITIALIZING state.\n\nI can upload the whole log file which about 42MB if that would help (for entire day).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/163930894","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163930894","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":163930894,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzkzMDg5NA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-11T12:56:30Z","updated_at":"2015-12-11T12:56:30Z","author_association":"CONTRIBUTOR","body":"> I can upload the whole log file which about 42MB if that would help (for entire day).\n\nplease\n\n> Also, the cluster stuck in recovery for more than 4 hours, before I gave up and started removing indices giving problem. Basically, shards were in UNASSIGNED state and some were in INITIALIZING state.\n\nI can't see why this is happening. so the logs would be awesome\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164107977","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-164107977","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":164107977,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDEwNzk3Nw==","user":{"login":"kpcool","id":2868966,"node_id":"MDQ6VXNlcjI4Njg5NjY=","avatar_url":"https://avatars3.githubusercontent.com/u/2868966?v=4","gravatar_id":"","url":"https://api.github.com/users/kpcool","html_url":"https://github.com/kpcool","followers_url":"https://api.github.com/users/kpcool/followers","following_url":"https://api.github.com/users/kpcool/following{/other_user}","gists_url":"https://api.github.com/users/kpcool/gists{/gist_id}","starred_url":"https://api.github.com/users/kpcool/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpcool/subscriptions","organizations_url":"https://api.github.com/users/kpcool/orgs","repos_url":"https://api.github.com/users/kpcool/repos","events_url":"https://api.github.com/users/kpcool/events{/privacy}","received_events_url":"https://api.github.com/users/kpcool/received_events","type":"User","site_admin":false},"created_at":"2015-12-12T03:59:06Z","updated_at":"2015-12-12T04:07:54Z","author_association":"NONE","body":"[dc20151208.tar.gz](https://github.com/elastic/elasticsearch/files/60052/dc20151208.tar.gz)\n[dc.tar.gz](https://github.com/elastic/elasticsearch/files/60050/dc.tar.gz)\n\ndc20151208 - was when the disk was not full but about to get full\ndc.tar.gz - is when the disk was full and es couldn't initialize all shards.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164199971","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-164199971","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":164199971,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDE5OTk3MQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-12T22:48:27Z","updated_at":"2015-12-12T22:48:27Z","author_association":"CONTRIBUTOR","body":"the interesting exceptions are here:\n\n```\nCaused by: [packetbeat-2015.12.09][[packetbeat-2015.12.09][1]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x203a77f9, got: 0x2c22706f];\n    at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:254)\n    at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:175)\n    ... 11 more\nCaused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x203a77f9, got: 0x2c22706f];\n    at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1636)\n    at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)\n    at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:299)\n    at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:290)\n    at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)\n    at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:240)\n    ... 12 more\nCaused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x203a77f9, got: 0x2c22706f]\n    at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1593)\n    at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1626)\n    ... 17 more\n```\n\nI still need to investigate what's going on but can you tell me what system you are running this on? Is this a local machine or a cloud machine? I am also curious what filesystem you are using?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164351444","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-164351444","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":164351444,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDM1MTQ0NA==","user":{"login":"kpcool","id":2868966,"node_id":"MDQ6VXNlcjI4Njg5NjY=","avatar_url":"https://avatars3.githubusercontent.com/u/2868966?v=4","gravatar_id":"","url":"https://api.github.com/users/kpcool","html_url":"https://github.com/kpcool","followers_url":"https://api.github.com/users/kpcool/followers","following_url":"https://api.github.com/users/kpcool/following{/other_user}","gists_url":"https://api.github.com/users/kpcool/gists{/gist_id}","starred_url":"https://api.github.com/users/kpcool/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpcool/subscriptions","organizations_url":"https://api.github.com/users/kpcool/orgs","repos_url":"https://api.github.com/users/kpcool/repos","events_url":"https://api.github.com/users/kpcool/events{/privacy}","received_events_url":"https://api.github.com/users/kpcool/received_events","type":"User","site_admin":false},"created_at":"2015-12-14T06:07:28Z","updated_at":"2015-12-14T06:08:39Z","author_association":"NONE","body":"Its a standalone node. \nMachine Info: 16GB DDR3, 1TB HDD , Intel(R) Xeon(R) CPU E3-1245 V2 @ 3.40GHz. \nFileSystem: xfs\nCentOS: 7.0 64Bit\nJava : \"1.8.0_65\"\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164383710","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-164383710","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":164383710,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDM4MzcxMA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-14T09:08:18Z","updated_at":"2015-12-14T09:08:18Z","author_association":"CONTRIBUTOR","body":"alright I think I found the issue here @kpcool your logfiles brought the conclusion thanks you very much. This is actually a serious issue with our transaction log which basically corrupts itself when you hit a disk-full exception. I will keep you posted on this issue. Thanks for baring with me and helping to figure this out.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164384660","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-164384660","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":164384660,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDM4NDY2MA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-12-14T09:12:30Z","updated_at":"2015-12-14T09:12:30Z","author_association":"CONTRIBUTOR","body":"What happens here is that when we hit a disk full expection while we are flushing the transaction log we might be able to write a portion of the data but we will try to flush the entire data block over and over again. Yet, in the most of the scenarios the disk-full happens during a merge and that merge will fail and release disk-space. Once that is done we might be able to flush the translog again but we already wrote big chunks of data to disk which are now 1. corrupted and 2. treated as non-existing since our internal offsets haven't advanced. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/236119173","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-236119173","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":236119173,"node_id":"MDEyOklzc3VlQ29tbWVudDIzNjExOTE3Mw==","user":{"login":"robcza","id":11228982,"node_id":"MDQ6VXNlcjExMjI4OTgy","avatar_url":"https://avatars3.githubusercontent.com/u/11228982?v=4","gravatar_id":"","url":"https://api.github.com/users/robcza","html_url":"https://github.com/robcza","followers_url":"https://api.github.com/users/robcza/followers","following_url":"https://api.github.com/users/robcza/following{/other_user}","gists_url":"https://api.github.com/users/robcza/gists{/gist_id}","starred_url":"https://api.github.com/users/robcza/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/robcza/subscriptions","organizations_url":"https://api.github.com/users/robcza/orgs","repos_url":"https://api.github.com/users/robcza/repos","events_url":"https://api.github.com/users/robcza/events{/privacy}","received_events_url":"https://api.github.com/users/robcza/received_events","type":"User","site_admin":false},"created_at":"2016-07-29T08:02:01Z","updated_at":"2016-07-29T08:02:01Z","author_association":"NONE","body":"Encountered this issue on one of the older clusters after disk full issue. Is there any way to recover the index? Losing something from the translog is not a big issue for me.\n\nHere is the expection while starting the node:\n\n```\n[2016-07-29 07:48:15,265][WARN ][indices.cluster          ] [Recorder] [[myindex][0]] marking and sending shard failed due to [failed recovery]\n[myindex][[myindex][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x88b7b1d6, got: 0x2c202266];\n    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)\n    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)\n    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: [myindex][[myindex][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x88b7b1d6, got: 0x2c202266];\n    at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:177)\n    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)\n    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1509)\n    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1493)\n    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:966)\n    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:938)\n    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:241)\n    ... 5 more\nCaused by: [myindex][[myindex][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x88b7b1d6, got: 0x2c202266];\n    at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:240)\n    at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:174)\n    ... 11 more\nCaused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x88b7b1d6, got: 0x2c202266];\n    at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1717)\n    at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)\n    at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)\n    at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)\n    at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)\n    at org.elasticsearch.index.shard.TranslogRecoveryPerformer.recoveryFromSnapshot(TranslogRecoveryPerformer.java:105)\n    at org.elasticsearch.index.shard.IndexShard$1.recoveryFromSnapshot(IndexShard.java:1578)\n    at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:238)\n    ... 12 more\nCaused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x88b7b1d6, got: 0x2c202266]\n    at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1675)\n    at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1707)\n    ... 19 more\n```\n\nThese are the contents of the index translog directory:\n\n```\nelasticsearch/nodes/0/indices/myindex/0/translog# ls\ntranslog-1445516620591.ckp  translog-1445516623424.ckp   translog-1445516631010.tlog  translog-1445516631019.tlog  translog-1445516631028.tlog  translog-1445516631037.tlog  translog-1445516631046.tlog\ntranslog-1445516620592.ckp  translog-1445516624605.ckp   translog-1445516631011.ckp   translog-1445516631020.ckp   translog-1445516631029.ckp   translog-1445516631038.ckp   translog-1445516631047.ckp\ntranslog-1445516620593.ckp  translog-1445516624606.ckp   translog-1445516631011.tlog  translog-1445516631020.tlog  translog-1445516631029.tlog  translog-1445516631038.tlog  translog-1445516631047.tlog\ntranslog-1445516620594.ckp  translog-1445516624607.ckp   translog-1445516631012.ckp   translog-1445516631021.ckp   translog-1445516631030.ckp   translog-1445516631039.ckp   translog-1445516631048.ckp\ntranslog-1445516620595.ckp  translog-1445516624608.ckp   translog-1445516631012.tlog  translog-1445516631021.tlog  translog-1445516631030.tlog  translog-1445516631039.tlog  translog-1445516631048.tlog\ntranslog-1445516620596.ckp  translog-1445516624609.ckp   translog-1445516631013.ckp   translog-1445516631022.ckp   translog-1445516631031.ckp   translog-1445516631040.ckp   translog-1445516631049.ckp\ntranslog-1445516620790.ckp  translog-1445516624610.ckp   translog-1445516631013.tlog  translog-1445516631022.tlog  translog-1445516631031.tlog  translog-1445516631040.tlog  translog-1445516631049.tlog\ntranslog-1445516621043.ckp  translog-1445516624611.ckp   translog-1445516631014.ckp   translog-1445516631023.ckp   translog-1445516631032.ckp   translog-1445516631041.ckp   translog-1445516631050.ckp\ntranslog-1445516621044.ckp  translog-1445516624612.ckp   translog-1445516631014.tlog  translog-1445516631023.tlog  translog-1445516631032.tlog  translog-1445516631041.tlog  translog-1445516631050.tlog\ntranslog-1445516621237.ckp  translog-1445516625986.ckp   translog-1445516631015.ckp   translog-1445516631024.ckp   translog-1445516631033.ckp   translog-1445516631042.ckp   translog-1445516631051.ckp\ntranslog-1445516621238.ckp  translog-1445516628096.ckp   translog-1445516631015.tlog  translog-1445516631024.tlog  translog-1445516631033.tlog  translog-1445516631042.tlog  translog-1445516631051.tlog\ntranslog-1445516621239.ckp  translog-1445516628097.ckp   translog-1445516631016.ckp   translog-1445516631025.ckp   translog-1445516631034.ckp   translog-1445516631043.ckp   translog-1445516631052.ckp\ntranslog-1445516621240.ckp  translog-1445516628624.ckp   translog-1445516631016.tlog  translog-1445516631025.tlog  translog-1445516631034.tlog  translog-1445516631043.tlog  translog-1445516631052.tlog\ntranslog-1445516621380.ckp  translog-1445516628625.ckp   translog-1445516631017.ckp   translog-1445516631026.ckp   translog-1445516631035.ckp   translog-1445516631044.ckp   translog-1445516631053.tlog\ntranslog-1445516623082.ckp  translog-1445516629747.ckp   translog-1445516631017.tlog  translog-1445516631026.tlog  translog-1445516631035.tlog  translog-1445516631044.tlog  translog.ckp\ntranslog-1445516623417.ckp  translog-1445516631009.ckp   translog-1445516631018.ckp   translog-1445516631027.ckp   translog-1445516631036.ckp   translog-1445516631045.ckp\ntranslog-1445516623418.ckp  translog-1445516631009.tlog  translog-1445516631018.tlog  translog-1445516631027.tlog  translog-1445516631036.tlog  translog-1445516631045.tlog\ntranslog-1445516623423.ckp  translog-1445516631010.ckp   translog-1445516631019.ckp   translog-1445516631028.ckp   translog-1445516631037.ckp   translog-1445516631046.ckp\n```\n\n@s1monw Is there any theoretical chance to fix this? Maybe removing part of the translog a persuading elasticsearch it is complete?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/351177372","html_url":"https://github.com/elastic/elasticsearch/issues/15333#issuecomment-351177372","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/15333","id":351177372,"node_id":"MDEyOklzc3VlQ29tbWVudDM1MTE3NzM3Mg==","user":{"login":"jamshid","id":8549,"node_id":"MDQ6VXNlcjg1NDk=","avatar_url":"https://avatars0.githubusercontent.com/u/8549?v=4","gravatar_id":"","url":"https://api.github.com/users/jamshid","html_url":"https://github.com/jamshid","followers_url":"https://api.github.com/users/jamshid/followers","following_url":"https://api.github.com/users/jamshid/following{/other_user}","gists_url":"https://api.github.com/users/jamshid/gists{/gist_id}","starred_url":"https://api.github.com/users/jamshid/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jamshid/subscriptions","organizations_url":"https://api.github.com/users/jamshid/orgs","repos_url":"https://api.github.com/users/jamshid/repos","events_url":"https://api.github.com/users/jamshid/events{/privacy}","received_events_url":"https://api.github.com/users/jamshid/received_events","type":"User","site_admin":false},"created_at":"2017-12-12T20:03:50Z","updated_at":"2017-12-12T20:03:50Z","author_association":"NONE","body":"FYI I saw this same error in an Elasticsearch 2.3.3 environment that ran out of disk space. I was surprised I had to restart Elasticsearch in order to recover. \r\nI was hoping this bug was fixed by #15420 but looks like that fix *is* in 2.3.3, so there must be additional bugs. \r\nHopefully latest Elasticsearch is automatically recovers from full disk problems.","performed_via_github_app":null}]