[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/77780376","html_url":"https://github.com/elastic/elasticsearch/issues/9971#issuecomment-77780376","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9971","id":77780376,"node_id":"MDEyOklzc3VlQ29tbWVudDc3NzgwMzc2","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-03-08T22:55:33Z","updated_at":"2015-03-08T22:55:33Z","author_association":"CONTRIBUTOR","body":"Hi @BHSPitMonkey \n\nI think you mean the edge-ngram tokenizer, rather than the ngram tokenizer.  When using `token_chars' to break up the string into words, you can just make the max_ngram as big as you need, which will serve your purpose.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/78178947","html_url":"https://github.com/elastic/elasticsearch/issues/9971#issuecomment-78178947","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9971","id":78178947,"node_id":"MDEyOklzc3VlQ29tbWVudDc4MTc4OTQ3","user":{"login":"BHSPitMonkey","id":33672,"node_id":"MDQ6VXNlcjMzNjcy","avatar_url":"https://avatars2.githubusercontent.com/u/33672?v=4","gravatar_id":"","url":"https://api.github.com/users/BHSPitMonkey","html_url":"https://github.com/BHSPitMonkey","followers_url":"https://api.github.com/users/BHSPitMonkey/followers","following_url":"https://api.github.com/users/BHSPitMonkey/following{/other_user}","gists_url":"https://api.github.com/users/BHSPitMonkey/gists{/gist_id}","starred_url":"https://api.github.com/users/BHSPitMonkey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/BHSPitMonkey/subscriptions","organizations_url":"https://api.github.com/users/BHSPitMonkey/orgs","repos_url":"https://api.github.com/users/BHSPitMonkey/repos","events_url":"https://api.github.com/users/BHSPitMonkey/events{/privacy}","received_events_url":"https://api.github.com/users/BHSPitMonkey/received_events","type":"User","site_admin":false},"created_at":"2015-03-11T00:37:44Z","updated_at":"2015-03-11T00:37:44Z","author_association":"NONE","body":"1. Actually, no; I meant the ngram tokenizer.\n2. Raising the `max_gram` (and/or lowering the `min_gram`) setting is outside the question. The problem is that, once you've decided on a min/max which suit your needs (a decision based on the specific partial-matching needs and taking storage complexity into account), you still want words outside of this window to match in their entirety.\n\nFor example: Let's say we've decided to decided to restrict partial text searching to substrings from 3 to 10 characters in length. (Going below 3 would add a considerable amount of additional tokens, and we've decided it's not very useful below that threshold.)\n- You want to search for a title with the (whole) word `Me`, but you can't; `Me` won't produce any token at all, despite being a full word (which the standard tokenizer would pick up).\n- You want to search for `incandescent`, but you can't; However, you can search for `candescent`, or any other substring of length [3,10]. But what good does that do if the user provided the whole word in their query?\n\nSo, in order to accommodate these two seemingly reasonable scenarios, we currently have to index our text into two separate (identical) fields, using a standard tokenizer on the other one. This is a hack that comes with added storage/bandwidth costs, makes filtering on these fields harder, and adds complexity to queries (for instance, now you can't easily query by field anymore without transforming the query and introducing an `OR` subquery). An option like the one I'm describing would be really helpful to have.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/89585732","html_url":"https://github.com/elastic/elasticsearch/issues/9971#issuecomment-89585732","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9971","id":89585732,"node_id":"MDEyOklzc3VlQ29tbWVudDg5NTg1NzMy","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-04-04T14:23:23Z","updated_at":"2015-04-04T14:23:23Z","author_association":"CONTRIBUTOR","body":"Hi @BHSPitMonkey \n\nOK - I understand the requirement to include whole terms which are shorter than the `min_gram` (which are currently dropped from the token stream) but the same doesn't apply to words that are longer, because they will have ngrams applied anyway, for instance:\n\n```\nDELETE t\n\nPUT t\n{\n  \"settings\": {\n    \"analysis\": {\n      \"tokenizer\": {\n        \"my_ngrams\": {\n          \"type\": \"ngram\",\n          \"min_gram\": 3,\n          \"max_gram\": 4,\n          \"token_chars\": [\n            \"letter\"\n          ]\n        }\n      },\n      \"analyzer\": {\n        \"my_ngrams\": {\n          \"tokenizer\": \"my_ngrams\"\n        }\n      }\n    }\n  }\n}\n\nGET /t/_analyze?text=my tests&analyzer=my_ngrams\n```\n\nThe above returns:\n\n```\n{\n   \"tokens\": [\n      {\n         \"token\": \"tes\",\n         \"start_offset\": 3,\n         \"end_offset\": 6,\n         \"type\": \"word\",\n         \"position\": 1\n      },\n      {\n         \"token\": \"test\",\n         \"start_offset\": 3,\n         \"end_offset\": 7,\n         \"type\": \"word\",\n         \"position\": 2\n      },\n      {\n         \"token\": \"est\",\n         \"start_offset\": 4,\n         \"end_offset\": 7,\n         \"type\": \"word\",\n         \"position\": 3\n      },\n      {\n         \"token\": \"ests\",\n         \"start_offset\": 4,\n         \"end_offset\": 8,\n         \"type\": \"word\",\n         \"position\": 4\n      },\n      {\n         \"token\": \"sts\",\n         \"start_offset\": 5,\n         \"end_offset\": 8,\n         \"type\": \"word\",\n         \"position\": 5\n      }\n   ]\n}\n```\n\nSo yes, the word `my` has just been dropped, but `tests` still produces all of the tokens you need.\n\nI agree that we should probably add an option to include the whole token if it is shorter than `min_gram`    \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/89586644","html_url":"https://github.com/elastic/elasticsearch/issues/9971#issuecomment-89586644","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9971","id":89586644,"node_id":"MDEyOklzc3VlQ29tbWVudDg5NTg2NjQ0","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-04-04T14:28:05Z","updated_at":"2015-04-04T14:28:05Z","author_association":"CONTRIBUTOR","body":"Oh I think I've just got your point about including the whole token if it is above `max_gram`.   \n\nHmm, the typical approach to this would be just to index the field twice: once with standard analyzer and once with ngrams, then to use a `multi_match` query in `most_fields` mode to query both fields at once.\n\n@rmuir what's your take on this?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/89589856","html_url":"https://github.com/elastic/elasticsearch/issues/9971#issuecomment-89589856","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9971","id":89589856,"node_id":"MDEyOklzc3VlQ29tbWVudDg5NTg5ODU2","user":{"login":"rmuir","id":504194,"node_id":"MDQ6VXNlcjUwNDE5NA==","avatar_url":"https://avatars1.githubusercontent.com/u/504194?v=4","gravatar_id":"","url":"https://api.github.com/users/rmuir","html_url":"https://github.com/rmuir","followers_url":"https://api.github.com/users/rmuir/followers","following_url":"https://api.github.com/users/rmuir/following{/other_user}","gists_url":"https://api.github.com/users/rmuir/gists{/gist_id}","starred_url":"https://api.github.com/users/rmuir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rmuir/subscriptions","organizations_url":"https://api.github.com/users/rmuir/orgs","repos_url":"https://api.github.com/users/rmuir/repos","events_url":"https://api.github.com/users/rmuir/events{/privacy}","received_events_url":"https://api.github.com/users/rmuir/received_events","type":"User","site_admin":false},"created_at":"2015-04-04T14:37:15Z","updated_at":"2015-04-04T14:37:15Z","author_association":"CONTRIBUTOR","body":"I don't understand the aversion to multi fields approach here. with n=3/4 you already have two analysis fields stuck inside of one IMO (one at n=3, one at n=4). If you have another field with standardanalyzer, you have a lot of flexibility, e.g. you can query both at the same time, or just the standardanalyzer field for a more 'exact' query (e.g., user puts term in quotes).\n\nOf course, this thing seems to be complicated by a few issues. First of all, IMO n-grams shouldnt really pre-tokenize much at all. This doesn't make a lot of sense to me since n-grams is a different tokenization technique, just let it be different than tokenizing on words (so don't do it in n-grams!). \n\nIf we try to be too fancy and handle everything just within n-grams, i'm afraid the code will become super-complicated and stagnate and become unmaintainable. For a real example of this, look at CJKBigramFilter.java in lucene (it has some of the logic proposed here, in its miniature world of n=1/n=2). I wrote it from scratch and really tried to make it simple and easy. Its hairy as hell :(\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/89589909","html_url":"https://github.com/elastic/elasticsearch/issues/9971#issuecomment-89589909","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9971","id":89589909,"node_id":"MDEyOklzc3VlQ29tbWVudDg5NTg5OTA5","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-04-04T14:38:30Z","updated_at":"2015-04-04T14:38:30Z","author_association":"CONTRIBUTOR","body":"@rmuir many thanks.  Closing\n","performed_via_github_app":null}]