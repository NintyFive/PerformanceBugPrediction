{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/21241","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21241/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21241/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21241/events","html_url":"https://github.com/elastic/elasticsearch/issues/21241","id":186556839,"node_id":"MDU6SXNzdWUxODY1NTY4Mzk=","number":21241,"title":"Add \"token_filters\" array option to keyword type mapping","user":{"login":"synhershko","id":212252,"node_id":"MDQ6VXNlcjIxMjI1Mg==","avatar_url":"https://avatars2.githubusercontent.com/u/212252?v=4","gravatar_id":"","url":"https://api.github.com/users/synhershko","html_url":"https://github.com/synhershko","followers_url":"https://api.github.com/users/synhershko/followers","following_url":"https://api.github.com/users/synhershko/following{/other_user}","gists_url":"https://api.github.com/users/synhershko/gists{/gist_id}","starred_url":"https://api.github.com/users/synhershko/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/synhershko/subscriptions","organizations_url":"https://api.github.com/users/synhershko/orgs","repos_url":"https://api.github.com/users/synhershko/repos","events_url":"https://api.github.com/users/synhershko/events{/privacy}","received_events_url":"https://api.github.com/users/synhershko/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2016-11-01T15:03:11Z","updated_at":"2016-11-02T03:39:33Z","closed_at":"2016-11-01T15:13:25Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The separation of string to text and keyword is awesome. The only remaining item in my opinion is the not-tokenized-but-lowercased case - it is common enough but will still require some rigorous configuration. It probably makes sense now to allow specifying \"token-filters\" to execute on \"keyword\" fields directly in that field's mapping (because different approaches to normalization can be taken - ascii folding, etc). I'm aware of tokenizing tokenfilters (e.g. word-delimiter) but think we can ignore that edge-case for this purpose. WDYT?","closed_by":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"performed_via_github_app":null}