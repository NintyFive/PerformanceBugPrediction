{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/23185","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23185/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23185/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/23185/events","html_url":"https://github.com/elastic/elasticsearch/issues/23185","id":207805115,"node_id":"MDU6SXNzdWUyMDc4MDUxMTU=","number":23185,"title":"Default network buffer size causes higher GC pressure than necessary","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"labels":[{"id":146854632,"node_id":"MDU6TGFiZWwxNDY4NTQ2MzI=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Network","name":":Distributed/Network","color":"0e8a16","default":false,"description":"Http and internode communication implementations"},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null},{"id":334286612,"node_id":"MDU6TGFiZWwzMzQyODY2MTI=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/v6.0.0-alpha1","name":"v6.0.0-alpha1","color":"dddddd","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":16,"created_at":"2017-02-15T13:33:42Z","updated_at":"2017-03-28T15:09:53Z","closed_at":"2017-03-12T01:32:36Z","author_association":"MEMBER","active_lock_reason":null,"body":"`Netty4HttpServerTransport` uses the settings `http.netty.receive_predictor_min` and `http.netty.receive_predictor_max` to provide a properly configured `RecvByteBufAllocator` implementation to Netty. Their default value is controlled by the setting `transport.netty.receive_predictor_size` which varies between 64 kb and 512 kb (per allocated buffer). \r\n\r\nThe before-mentioned allocator is responsible for allocating memory buffers when handling incoming network packets and Netty will allocate one buffer per network packet.\r\n\r\nWe have run comparative benchmarks ([nyc_taxis](https://github.com/elastic/rally-tracks/tree/master/nyc_taxis)) track, once locally (i.e. via loopback) and once distributed (i.e. via a Ethernet) and analyzed the allocation behavior of Elasticsearch.\r\n\r\n|   Network Connection Type | Bytes allocated outside of TLABs on network layer |\r\n|----------------------------:|---------------------------------:|\r\n|   Loopback | ~ 78 GB  |\r\n|   Ethernet | ~ 2.13 TB  |\r\n\r\n_Note: On this particular machine `transport.netty.receive_predictor_size` was 512kb._\r\n\r\nThe root cause seems to be related to MTU (which differs greatly between loopback and regular network devices (65536 vs. 1500)). A smaller MTU leads to more network packets (but the buffer size stays the same) thus leading to more GC pressure.\r\n\r\nIn a custom build of Elasticsearch we set `http.netty.receive_predictor_min` to 5kb and `http.netty.receive_predictor_max` to 64kb and got comparable allocation behavior between local and distributed benchmarks.\r\n\r\n_Note: Our analysis focused only `Netty4HttpServerTransport` for a single Elasticsearch node. It is expected that `Netty4Transport` exhibits similar behavior and we should change the buffer size there too._\r\n\r\n","closed_by":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"performed_via_github_app":null}