{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/10766","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10766/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10766/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10766/events","html_url":"https://github.com/elastic/elasticsearch/issues/10766","id":70598012,"node_id":"MDU6SXNzdWU3MDU5ODAxMg==","number":10766,"title":"Hanging threads with TransportClient","user":{"login":"nilsga","id":42919,"node_id":"MDQ6VXNlcjQyOTE5","avatar_url":"https://avatars0.githubusercontent.com/u/42919?v=4","gravatar_id":"","url":"https://api.github.com/users/nilsga","html_url":"https://github.com/nilsga","followers_url":"https://api.github.com/users/nilsga/followers","following_url":"https://api.github.com/users/nilsga/following{/other_user}","gists_url":"https://api.github.com/users/nilsga/gists{/gist_id}","starred_url":"https://api.github.com/users/nilsga/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nilsga/subscriptions","organizations_url":"https://api.github.com/users/nilsga/orgs","repos_url":"https://api.github.com/users/nilsga/repos","events_url":"https://api.github.com/users/nilsga/events{/privacy}","received_events_url":"https://api.github.com/users/nilsga/received_events","type":"User","site_admin":false},"labels":[{"id":146829143,"node_id":"MDU6TGFiZWwxNDY4MjkxNDM=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Infra/Transport%20API","name":":Core/Infra/Transport API","color":"0e8a16","default":false,"description":"Transport client API"}],"state":"closed","locked":false,"assignee":{"login":"spinscale","id":667544,"node_id":"MDQ6VXNlcjY2NzU0NA==","avatar_url":"https://avatars2.githubusercontent.com/u/667544?v=4","gravatar_id":"","url":"https://api.github.com/users/spinscale","html_url":"https://github.com/spinscale","followers_url":"https://api.github.com/users/spinscale/followers","following_url":"https://api.github.com/users/spinscale/following{/other_user}","gists_url":"https://api.github.com/users/spinscale/gists{/gist_id}","starred_url":"https://api.github.com/users/spinscale/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spinscale/subscriptions","organizations_url":"https://api.github.com/users/spinscale/orgs","repos_url":"https://api.github.com/users/spinscale/repos","events_url":"https://api.github.com/users/spinscale/events{/privacy}","received_events_url":"https://api.github.com/users/spinscale/received_events","type":"User","site_admin":false},"assignees":[{"login":"spinscale","id":667544,"node_id":"MDQ6VXNlcjY2NzU0NA==","avatar_url":"https://avatars2.githubusercontent.com/u/667544?v=4","gravatar_id":"","url":"https://api.github.com/users/spinscale","html_url":"https://github.com/spinscale","followers_url":"https://api.github.com/users/spinscale/followers","following_url":"https://api.github.com/users/spinscale/following{/other_user}","gists_url":"https://api.github.com/users/spinscale/gists{/gist_id}","starred_url":"https://api.github.com/users/spinscale/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spinscale/subscriptions","organizations_url":"https://api.github.com/users/spinscale/orgs","repos_url":"https://api.github.com/users/spinscale/repos","events_url":"https://api.github.com/users/spinscale/events{/privacy}","received_events_url":"https://api.github.com/users/spinscale/received_events","type":"User","site_admin":false}],"milestone":null,"comments":17,"created_at":"2015-04-24T06:13:01Z","updated_at":"2016-08-02T12:28:17Z","closed_at":"2015-06-04T12:24:36Z","author_association":"NONE","active_lock_reason":null,"body":"We have had some instability issues with our ES cluster lately, where our application using the Java TransportClient stops responding to requests towards the cluster. Our setup consists of 3 master nodes and 4 data nodes, and our application consists of 5 nodes. The symptoms we are experiencing is that after some time, a situation occurs that freezes most or all ES requests. We have been profiling and digging through thread dumps, and when the freeze occurs we see a lot of these (typically 20+):\n\n```\n\"AkkaSystem-akka.actor.default-dispatcher-445\" - Thread t@540\n   java.lang.Thread.State: WAITING\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for <2f9d8510> (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:279)\n    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:118)\n```\n\nThe number of these seems to grow steadily but slowly, which might indicate that not all the requests that are coming in are blocked, but maybe certain requests that reach one particular node in the cluster? What is also interesting is that in the freeze situation, there is alway at least one of these threads hanging:\n\n```\n\"elasticsearch[Quentin Beck][transport_client_worker][T#1]{New I/O worker #1}\" - Thread t@34\n   java.lang.Thread.State: WAITING\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for <44a58cd8> (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:279)\n    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:118)\n    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)\n```\n\nSo it appears that all our application threads that are blocked are waiting for this single `transport_client` thread to complete. I suspect there are two issues here really. One is that one of the `transport_client` thread is never timed out. And the second is that there seems to be some synchronization going on between all (or many) of the other requests that are blocking on `BaseFuture$Sync`\n\nWe are running ES 1.5.0 with OpenJDK 1.8.0_31 on both server and client.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}