{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/11221","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11221/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11221/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11221/events","html_url":"https://github.com/elastic/elasticsearch/issues/11221","id":78030446,"node_id":"MDU6SXNzdWU3ODAzMDQ0Ng==","number":11221,"title":"Allow reallocation on node where freediskthreshold is hit if shard size is the same","user":{"login":"nellicus","id":8770097,"node_id":"MDQ6VXNlcjg3NzAwOTc=","avatar_url":"https://avatars2.githubusercontent.com/u/8770097?v=4","gravatar_id":"","url":"https://api.github.com/users/nellicus","html_url":"https://github.com/nellicus","followers_url":"https://api.github.com/users/nellicus/followers","following_url":"https://api.github.com/users/nellicus/following{/other_user}","gists_url":"https://api.github.com/users/nellicus/gists{/gist_id}","starred_url":"https://api.github.com/users/nellicus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nellicus/subscriptions","organizations_url":"https://api.github.com/users/nellicus/orgs","repos_url":"https://api.github.com/users/nellicus/repos","events_url":"https://api.github.com/users/nellicus/events{/privacy}","received_events_url":"https://api.github.com/users/nellicus/received_events","type":"User","site_admin":false},"labels":[{"id":836504707,"node_id":"MDU6TGFiZWw4MzY1MDQ3MDc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Distributed","name":":Distributed/Distributed","color":"0e8a16","default":false,"description":"A catch all label for anything in the Distributed Area. If you aren't sure, use this one."},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2015-05-19T10:30:49Z","updated_at":"2018-02-13T19:28:22Z","closed_at":"2016-01-18T19:29:40Z","author_association":"MEMBER","active_lock_reason":null,"body":"From the below repro it looks like cluster.routing.allocation.decider just look at disk space usage, but doesn't take into account the size of the shards already existing into a node coming back online?\nIf a node has its shards all allocated, hits free disk threshold, then get restarted, all the existing shards will not be reallocated again.\n\nThe above behaviour does make fully sense to me if there is data continuously coming into the cluster, hence there will be a delta of new data between the nodes, once the node which hit the free disk threshold will come back online, and it will be safe and wise not to allocate shards there.\n\nThough If there is no data coming and if it can be measured that shard have same sized (delta is 0), it should be safer to reallocate the shards to the node which hit the free disk threshold. Not sure if allocation.decider just checks sigar output or actually goes deeper and look at the size of the shards on node.\n\nMost likely this is easier to be said than implemented, though I just wanted to raise the discussion, should there will be a way in the future to handle this specific scenario in a better way. \n\n2 nodes cluster , v1.5.2, Marvel installed (2 separate VMs: node-1 192.168.0.15, node-2 192.168.0.16)\n\nnode 1 - config\n\n```\n[elk@localhost elasticsearch-1.5.2]$ /sbin/ifconfig | egrep 192\n        inet 192.168.0.15  netmask 255.255.255.0  broadcast 192.168.0.255\n\n[elk@localhost elasticsearch-1.5.2]$ tail -5 config/elasticsearch.yml \ncluster.name: \"test\"\nnode.name: \"node-1\"\ndiscovery.zen.ping.multicast.enabled: false\ndiscovery.zen.ping.unicast.hosts: [\"192.168.0.16\"]\n\n[elk@localhost elasticsearch-1.5.2]$ df -h\nFilesystem               Size  Used Avail Use% Mounted on\n/dev/mapper/centos-root   41G  4.2G   36G  11% /\ndevtmpfs                 2.0G     0  2.0G   0% /dev\ntmpfs                    2.0G   88K  2.0G   1% /dev/shm\ntmpfs                    2.0G  8.8M  2.0G   1% /run\ntmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/mapper/centos-home   20G   64M   20G   1% /home\n/dev/sda1                497M  143M  354M  29% /boot\n```\n\nnode 2 - config\n\n```\n[elk@localhost elasticsearch-1.5.2]$ /sbin/ifconfig | egrep 192\n        inet 192.168.0.16  netmask 255.255.255.0  broadcast 192.168.0.255\n\n[elk@localhost elasticsearch-1.5.2]$ tail -6 config/elasticsearch.yml \ncluster.name: \"test\"\nnode.name: \"node-1\"\ndiscovery.zen.ping.multicast.enabled: false\ndiscovery.zen.ping.unicast.hosts: [\"192.168.0.15\"]\n\n[elk@localhost elasticsearch-1.5.2]$ df -h\nFilesystem               Size  Used Avail Use% Mounted on\n/dev/mapper/centos-root   41G  4.2G   36G  11% /\ndevtmpfs                 2.0G     0  2.0G   0% /dev\ntmpfs                    2.0G   88K  2.0G   1% /dev/shm\ntmpfs                    2.0G  8.7M  2.0G   1% /run\ntmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/mapper/centos-home   20G   64M   20G   1% /home\n/dev/sda1                497M  143M  354M  29% /boot\n```\n\nnode1 - start\n\n```\n[elk@localhost elasticsearch-1.5.2]$ date && ./bin/elasticsearch -Des.logger.level=DEBUG -d \nMon May 18 14:12:27 CEST 2015\n```\n\nnode2 - start\n\n```\n[elk@localhost elasticsearch-1.5.2]$ date && ./bin/elasticsearch -Des.logger.level=DEBUG -d\nMon May 18 14:12:28 CEST 2015\n```\n\nnode 1 master\n\n```\n[elk@localhost elasticsearch-1.5.2]$ curl localhost:9200/_cat/nodes?v\nhost                  ip        heap.percent ram.percent load node.role master name   \nlocalhost.localdomain 127.0.0.1            7          27 0.00 d         *      node-1 \nlocalhost.localdomain 127.0.0.1            8          24 0.07 d         m      node-2\n```\n\nstart logstash and index data\n\n```\n[elk@localhost elasticsearch-1.5.2]$ date && curl -XGET \"localhost:9200/logstash*/_search?search_type=count&pretty\"\nMon May 18 14:22:15 CEST 2015\n{\n  \"took\" : 13,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 64294,\n    \"max_score\" : 0.0,\n    \"hits\" : [ ]\n  }\n}\n```\n\nstop logstash - no more data coming in from now on\n\nnode 1 - index created\n\n```\n[2015-05-18 14:18:08,823][DEBUG][cluster.service          ] [node-1] processing [create-index [logstash-2015.05.18], cause [auto(bulk api)]]: execute\n[2015-05-18 14:18:08,825][DEBUG][indices                  ] [node-1] creating Index [logstash-2015.05.18], shards [5]/[1]\n[2015-05-18 14:18:08,848][DEBUG][index.mapper             ] [node-1] [logstash-2015.05.18] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[jar:file:/opt/elk/elasticsearch-1.5.2/lib/elasticsearch-1.5.2.jar!/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null] [node-1] [.marvel-kibana][0] updating index_buffer_size from [7.8mb] to [4.6mb]\n```\n\nnode 2 index created\n\n```\n[2015-05-18 14:18:08,873][DEBUG][discovery.zen.publish    ] [node-2] received cluster state version 7\n[2015-05-18 14:18:08,874][DEBUG][cluster.service          ] [node-2] processing [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]: execute\n[2015-05-18 14:18:08,874][DEBUG][cluster.service          ] [node-2] cluster state updated, version [7], source [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]\n[2015-05-18 14:18:08,874][DEBUG][cluster.service          ] [node-2] set local cluster state to version 7\n[2015-05-18 14:18:08,875][DEBUG][indices.cluster          ] [node-2] [logstash-2015.05.18] creating index\n[2015-05-18 14:18:08,875][DEBUG][indices                  ] [node-2] creating Index [logstash-2015.05.18], shards [5]/[1]\n[2015-05-18 14:18:08,899][DEBUG][index.mapper             ] [node-2] [logstash-2015.05.18] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[jar:file:/opt/elk/elasticsearch-1.5.2/lib/elasticsearch-1.5.2.jar!/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null]\n```\n\ncluster is green, all shards STARTED\n\n```\n[elk@localhost elasticsearch-1.5.2]$ date && curl -XGET \"localhost:9200/_cluster/health?pretty\" \nMon May 18 14:22:33 CEST 2015\n{\n  \"cluster_name\" : \"test\",\n  \"status\" : \"green\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 2,\n  \"number_of_data_nodes\" : 2,\n  \"active_primary_shards\" : 6,\n  \"active_shards\" : 12,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 0,\n  \"number_of_pending_tasks\" : 0\n}\n\n[elk@localhost elasticsearch-1.5.2]$ date && curl -XGET \"localhost:9200/_cat/shards?pretty\" \nMon May 18 14:23:25 CEST 2015\nlogstash-2015.05.18 2 p STARTED 12871 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 2 r STARTED 12871 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 0 p STARTED 12868 1.8mb 127.0.0.1 node-2 \nlogstash-2015.05.18 0 r STARTED 12868 1.8mb 127.0.0.1 node-1 \nlogstash-2015.05.18 3 r STARTED 12825 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 3 p STARTED 12825 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 1 r STARTED 12852 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 1 p STARTED 12852 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 4 p STARTED 12878 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 4 r STARTED 12878 1.9mb 127.0.0.1 node-1 \n.marvel-2015.05.18  0 r STARTED   479 1.7mb 127.0.0.1 node-2 \n.marvel-2015.05.18  0 p STARTED   479 1.7mb 127.0.0.1 node-1 \n```\n\nnode 2 - create large file to hit disk threshold\n\n```\n[elk@localhost elasticsearch-1.5.2]$ date && df -h\nMon May 18 14:23:54 CEST 2015\nFilesystem               Size  Used Avail Use% Mounted on\n/dev/mapper/centos-root   41G  4.2G   36G  11% /\ndevtmpfs                 2.0G     0  2.0G   0% /dev\ntmpfs                    2.0G   88K  2.0G   1% /dev/shm\ntmpfs                    2.0G  8.8M  2.0G   1% /run\ntmpfs                    2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/mapper/centos-home   20G   64M   20G   1% /home\n/dev/sda1                497M  143M  354M  29% /boot\n[elk@localhost elasticsearch-1.5.2]$ date && fallocate -l 30G /tmp/largefile.img\nMon May 18 14:24:14 CEST 2015\n```\n\nnode 1(master) sees the free disk space threshold hit\n\n```\n[2015-05-18 14:24:34,830][INFO ][cluster.routing.allocation.decider] [node-1] low disk watermark [15%] exceeded on [l4OMm3XtReStyBfRFtTZwA][node-2] free: 5.8gb[14.6%], replicas will not be assigned to this node\n```\n\nEverything still fine health/shard wise\n\n```\n[elk@localhost elasticsearch-1.5.2]$ date && curl -XGET \"localhost:9200/_cluster/health?pretty\" \nMon May 18 14:25:15 CEST 2015\n{\n  \"cluster_name\" : \"test\",\n  \"status\" : \"green\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 2,\n  \"number_of_data_nodes\" : 2,\n  \"active_primary_shards\" : 6,\n  \"active_shards\" : 12,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 0,\n  \"number_of_pending_tasks\" : 0\n}\n\n[elk@localhost elasticsearch-1.5.2]$ date && curl -XGET \"localhost:9200/_cat/shards?pretty\" \nMon May 18 14:25:26 CEST 2015\nlogstash-2015.05.18 2 p STARTED 12871 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 2 r STARTED 12871 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 0 p STARTED 12868 1.8mb 127.0.0.1 node-2 \nlogstash-2015.05.18 0 r STARTED 12868 1.8mb 127.0.0.1 node-1 \nlogstash-2015.05.18 3 r STARTED 12825 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 3 p STARTED 12825 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 1 r STARTED 12852 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 1 p STARTED 12852 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 4 p STARTED 12878 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 4 r STARTED 12878 1.9mb 127.0.0.1 node-1 \n.marvel-2015.05.18  0 r STARTED   551 1.8mb 127.0.0.1 node-2 \n.marvel-2015.05.18  0 p STARTED   551 2.5mb 127.0.0.1 node-1 \n```\n\ndisable allocation\n\n```\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.disable_allocation\": true\n  }\n}\n```\n\nnode 1 updates cluster state\n\n```\n[2015-05-18 14:26:31,698][DEBUG][cluster.service          ] [node-1] processing [cluster_update_settings]: execute\n[2015-05-18 14:26:31,699][DEBUG][cluster.service          ] [node-1] cluster state updated, version [17], source [cluster_update_settings]\n[2015-05-18 14:26:31,699][DEBUG][cluster.service          ] [node-1] publishing cluster state version 17\n[2015-05-18 14:26:31,721][DEBUG][cluster.service          ] [node-1] set local cluster state to version 17\n[2015-05-18 14:26:31,723][INFO ][cluster.routing.allocation.decider] [node-1] updating [cluster.routing.allocation.disable_allocation] from [false] to [true]\n```\n\nnode 2 receives new state\n\n```\n[2015-05-18 14:26:31,709][INFO ][cluster.routing.allocation.decider] [node-2] updating [cluster.routing.allocation.disable_allocation] from [false] to [true]\n[2015-05-18 14:26:31,721][DEBUG][cluster.service          ] [node-2] processing [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]: done applying updated cluster_state (version: 17)\n```\n\neverything still ok health/shards wise\n\n```\n[elk@localhost elasticsearch-1.5.2]$ date && curl -XGET \"localhost:9200/_cluster/health?pretty\" \nMon May 18 14:28:02 CEST 2015\n{\n  \"cluster_name\" : \"test\",\n  \"status\" : \"green\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 2,\n  \"number_of_data_nodes\" : 2,\n  \"active_primary_shards\" : 6,\n  \"active_shards\" : 12,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 0,\n  \"number_of_pending_tasks\" : 0\n}\n[elk@localhost elasticsearch-1.5.2]$ date && curl -XGET \"localhost:9200/_cat/shards?pretty\" \nMon May 18 14:28:04 CEST 2015\nlogstash-2015.05.18 2 p STARTED 12871 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 2 r STARTED 12871 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 0 p STARTED 12868 1.8mb 127.0.0.1 node-2 \nlogstash-2015.05.18 0 r STARTED 12868 1.8mb 127.0.0.1 node-1 \nlogstash-2015.05.18 3 r STARTED 12825 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 3 p STARTED 12825 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 1 r STARTED 12852 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 1 p STARTED 12852 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 4 p STARTED 12878 1.9mb 127.0.0.1 node-2 \nlogstash-2015.05.18 4 r STARTED 12878 1.9mb 127.0.0.1 node-1 \n.marvel-2015.05.18  0 r STARTED   648 2.3mb 127.0.0.1 node-2 \n.marvel-2015.05.18  0 p STARTED   648 2.2mb 127.0.0.1 node-1 \n```\n\nnode 2 - Shutdown\n\n```\n[2015-05-18 14:29:19,622][DEBUG][indices                  ] [node-2] [logstash-2015.05.18] closing index service (reason [shutdown])\n[2015-05-18 14:29:19,623][DEBUG][indices                  ] [node-2] [logstash-2015.05.18] closed... (reason [shutdown])\n[2015-05-18 14:29:19,629][INFO ][node                     ] [node-2] stopped\n[2015-05-18 14:29:19,629][INFO ][node                     ] [node-2] closing ...\n[2015-05-18 14:29:19,635][INFO ][node                     ] [node-2] closed\n```\n\nnode 1 (master) sees node 2 shutdown \n\n```\n[2015-05-18 14:29:19,394][INFO ][cluster.service          ] [node-1] removed {[node-2][l4OMm3XtReStyBfRFtTZwA][localhost.localdomain][inet[192.168.0.16/192.168.0.16:9300]],}, reason: zen-disco-node_left([node-2][l4OMm3XtReStyBfRFtTZwA][localhost.localdomain][inet[/192.168.0.16:9300]])\n[2015-05-18 14:29:19,394][DEBUG][cluster.service          ] [node-1] publishing cluster state version 18\n[2015-05-18 14:29:19,394][DEBUG][cluster.service          ] [node-1] set local cluster state to version 18\n```\n\nnode 2 - Restart\n\n```\n[elk@localhost elasticsearch-1.5.2]$ date && ./bin/elasticsearch -Des.logger.level=DEBUG -d\nMon May 18 14:30:16 CEST 2015\n\n[2015-05-18 14:30:23,388][DEBUG][discovery.zen            ] [node-2] got first state from fresh master [60ojafViSJCoo3pt60ihmw]\n[2015-05-18 14:30:23,388][DEBUG][cluster.service          ] [node-2] cluster state updated, version [19], source [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]\n[2015-05-18 14:30:23,389][INFO ][cluster.service          ] [node-2] detected_master [node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]], added {[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]],}, reason: zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])\n[2015-05-18 14:30:23,389][DEBUG][cluster.service          ] [node-2] set local cluster state to version 19\n[2015-05-18 14:30:23,393][INFO ][cluster.routing.allocation.decider] [node-2] updating [cluster.routing.allocation.disable_allocation] from [false] to [true]\n[2015-05-18 14:30:23,506][DEBUG][cluster.service          ] [node-2] processing [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]: done applying updated cluster_state (version: 19)\n[2015-05-18 14:30:23,518][DEBUG][cluster.service          ] [node-2] processing [finalize_join ([node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]])]: execute\n[2015-05-18 14:30:23,518][DEBUG][cluster.service          ] [node-2] processing [finalize_join ([node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]])]: no change in cluster_state\n[2015-05-18 14:30:23,526][INFO ][http                     ] [node-2] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.0.16:9200]}\n[2015-05-18 14:30:23,527][INFO ][node                     ] [node-2] started\n[2015-05-18 14:30:30,218][DEBUG][marvel.agent.exporter    ] [node-2] deriving host setting from httpServer\n[2015-05-18 14:30:30,288][DEBUG][marvel.agent.exporter    ] [node-2] accepting existing index template (version [6], needed [6])\n```\n\nnode 1 - Sees again node 2 - however given disk threshold is hit , it will prevent the existing shards on node 2 to be re-allocated\n\n```\n[2015-05-18 14:30:23,356][DEBUG][transport.netty          ] [node-1] connected to node [[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]]]\n[2015-05-18 14:30:23,362][DEBUG][cluster.service          ] [node-1] processing [zen-disco-receive(join from node[[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]]])]: execute\n[2015-05-18 14:30:23,362][DEBUG][cluster.service          ] [node-1] cluster state updated, version [19], source [zen-disco-receive(join from node[[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]]])]\n[2015-05-18 14:30:23,363][INFO ][cluster.service          ] [node-1] added {[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]],}, reason: zen-disco-receive(join from node[[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]]])\n[2015-05-18 14:30:23,363][DEBUG][cluster.service          ] [node-1] publishing cluster state version 19\n[2015-05-18 14:30:23,506][DEBUG][cluster.service          ] [node-1] set local cluster state to version 19\n[2015-05-18 14:30:23,507][DEBUG][cluster                  ] [node-1] data node was added, retrieving new cluster info\n[2015-05-18 14:30:23,507][DEBUG][cluster.service          ] [node-1] processing [zen-disco-receive(join from node[[node-2][Vobb_8yRQ5WZJKmJyAGWfw][localhost.localdomain][inet[/192.168.0.16:9300]]])]: done applying updated cluster_state (version: 19)\n[2015-05-18 14:30:23,508][DEBUG][river.cluster            ] [node-1] processing [reroute_rivers_node_changed]: execute\n[2015-05-18 14:30:23,508][DEBUG][river.cluster            ] [node-1] processing [reroute_rivers_node_changed]: no change in cluster_state\n[2015-05-18 14:30:23,523][INFO ][cluster.routing.allocation.decider] [node-1] low disk watermark [15%] exceeded on [Vobb_8yRQ5WZJKmJyAGWfw][node-2] free: 5.8gb[14.6%], replicas will not be assigned to this node\n[2015-05-18 14:30:24,818][DEBUG][cluster.service          ] [node-1] processing [routing-table-updater]: execute\n[2015-05-18 14:30:24,820][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)\n[2015-05-18 14:30:24,820][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)\n[2015-05-18 14:30:24,821][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)\n[2015-05-18 14:30:24,821][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)\n[2015-05-18 14:30:24,821][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)\n[2015-05-18 14:30:24,821][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54194233932893% free disk (38495502336 bytes)\n[2015-05-18 14:30:24,822][DEBUG][cluster.service          ] [node-1] processing [routing-table-updater]: no change in cluster_state\n[2015-05-18 14:30:34,834][INFO ][cluster.routing.allocation.decider] [node-1] low disk watermark [15%] exceeded on [Vobb_8yRQ5WZJKmJyAGWfw][node-2] free: 5.8gb[14.6%], replicas will not be assigned to this node\n```\n\nreenable allocation\n\n```\n{\n   \"acknowledged\": true,\n   \"persistent\": {\n      \"cluster\": {\n         \"routing\": {\n            \"allocation\": {\n               \"disable_allocation\": \"false\"\n            }\n         }\n      }\n   },\n   \"transient\": {}\n}\n```\n\nnode 1\n\n```\n2015-05-18 14:31:36,142][INFO ][cluster.routing.allocation.decider] [node-1] updating [cluster.routing.allocation.disable_allocation] from [true] to [false]\n.\n.\n.\n[2015-05-18 14:31:36,156][DEBUG][cluster.routing.allocation.decider] [node-1] Less than the required 15% free disk threshold (14.6% free) on node [Vobb_8yRQ5WZJKmJyAGWfw], preventing allocation\n[2015-05-18 14:31:36,156][DEBUG][cluster.routing.allocation.decider] [node-1] Node [60ojafViSJCoo3pt60ihmw] has 89.54326665423653% free disk (38496071680 bytes)\n[2015-05-18 14:31:36,158][DEBUG][cluster.routing.allocation.decider] [node-1] Less than the required 15% free disk threshold (14.6% free) on node [Vobb_8yRQ5WZJKmJyAGWfw], preventing allocation\n[2015-05-18 14:31:36,158][DEBUG][cluster.service          ] [node-1] processing [reroute_after_cluster_update_settings]: no change in cluster_state\n```\n\nnode 2\n\n```\n[2015-05-18 14:31:36,088][DEBUG][discovery.zen.publish    ] [node-2] received cluster state version 20\n[2015-05-18 14:31:36,089][DEBUG][cluster.service          ] [node-2] processing [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]: execute\n[2015-05-18 14:31:36,090][DEBUG][cluster.service          ] [node-2] cluster state updated, version [20], source [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]\n[2015-05-18 14:31:36,090][DEBUG][cluster.service          ] [node-2] set local cluster state to version 20\n[2015-05-18 14:31:36,091][INFO ][cluster.routing.allocation.decider] [node-2] updating [cluster.routing.allocation.disable_allocation] from [true] to [false]\n[2015-05-18 14:31:36,140][DEBUG][cluster.service          ] [node-2] processing [zen-disco-receive(from master [[node-1][60ojafViSJCoo3pt60ihmw][localhost.localdomain][inet[/192.168.0.15:9300]]])]: done applying updated cluster_state (version: 20)\n```\n\nnode 2 has all unassigned shards\n\n```\n[elk@localhost elasticsearch-1.5.2]$ date && curl -XGET \"localhost:9200/_cluster/health?pretty\" \nMon May 18 14:32:29 CEST 2015\n{\n  \"cluster_name\" : \"test\",\n  \"status\" : \"yellow\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 2,\n  \"number_of_data_nodes\" : 2,\n  \"active_primary_shards\" : 6,\n  \"active_shards\" : 6,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 6,\n  \"number_of_pending_tasks\" : 0\n}\n[elk@localhost elasticsearch-1.5.2]$ date && curl -XGET \"localhost:9200/_cat/shards?pretty\" \nMon May 18 14:32:31 CEST 2015\nlogstash-2015.05.18 4 p STARTED    12878 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 4 r UNASSIGNED                              \nlogstash-2015.05.18 0 p STARTED    12868 1.8mb 127.0.0.1 node-1 \nlogstash-2015.05.18 0 r UNASSIGNED                              \nlogstash-2015.05.18 3 p STARTED    12825 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 3 r UNASSIGNED                              \nlogstash-2015.05.18 1 p STARTED    12852 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 1 r UNASSIGNED                              \nlogstash-2015.05.18 2 p STARTED    12871 1.9mb 127.0.0.1 node-1 \nlogstash-2015.05.18 2 r UNASSIGNED                              \n.marvel-2015.05.18  0 p STARTED      810 2.7mb 127.0.0.1 node-1 \n.marvel-2015.05.18  0 r UNASSIGNED                              \n```\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}