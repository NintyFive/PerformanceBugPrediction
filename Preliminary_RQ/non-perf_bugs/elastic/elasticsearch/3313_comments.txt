[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/20939811","html_url":"https://github.com/elastic/elasticsearch/issues/3313#issuecomment-20939811","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3313","id":20939811,"node_id":"MDEyOklzc3VlQ29tbWVudDIwOTM5ODEx","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2013-07-14T17:18:26Z","updated_at":"2013-07-14T17:18:26Z","author_association":"MEMBER","body":"Confirmed, this is indeed the case. There is simple workaround for this issue though - just make sure that the index gets to at least to yellow state before trying to close it. I am curious is there a particular use case that led to this being a problem?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/20942464","html_url":"https://github.com/elastic/elasticsearch/issues/3313#issuecomment-20942464","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3313","id":20942464,"node_id":"MDEyOklzc3VlQ29tbWVudDIwOTQyNDY0","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2013-07-14T19:48:42Z","updated_at":"2013-07-14T19:48:42Z","author_association":"CONTRIBUTOR","body":"On Sun, Jul 14, 2013 at 1:18 PM, Igor Motov notifications@github.comwrote:\n\n> Confirmed, this is indeed the case. There is simple workaround for this\n> issue though - just make sure that the index gets to at least to yellow\n> state before trying to close it. I am curious is there a particular use\n> case that led to this being a problem?\n\nI was building a script that can build an index from scratch as well as\nupdate it portions of its configuration are out of date and being lazy\nabout how I implemented it figuring that an empty index would be cheap to\nopen and close.  After I stopped being lazy and specifying the analysers\nduring index creation my problem went away.  It'd probably have been good\nenough to add a note to\nhttp://www.elasticsearch.org/guide/reference/api/admin-indices-open-close/and\nI wouldn't have tried it.\n\nAre there other non-index-creation cases that put the index in this state?\nIf so it might be worth implementing something stops the close action.\n\nNik\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/20942731","html_url":"https://github.com/elastic/elasticsearch/issues/3313#issuecomment-20942731","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3313","id":20942731,"node_id":"MDEyOklzc3VlQ29tbWVudDIwOTQyNzMx","user":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"created_at":"2013-07-14T20:04:47Z","updated_at":"2013-07-14T20:04:47Z","author_association":"MEMBER","body":"@imotov I believe as you mentioned that we can really open an index only after all the primary shards have been allocation at least once. This is because we can't recreate the `primaryAllocatedPostApi` flag (we could potentially, but its not supported now).\n\nI suggest that a simple fix for now is to reject a close index request if one of its index shard routing info has the primaryAllocatedPostApi set to false.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/21176419","html_url":"https://github.com/elastic/elasticsearch/issues/3313#issuecomment-21176419","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3313","id":21176419,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMTc2NDE5","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2013-07-18T11:07:11Z","updated_at":"2013-07-18T11:07:11Z","author_association":"CONTRIBUTOR","body":"While working on a patch for the issue kimchy mentioned I noticed a look alike issue:  https://gist.github.com/nik9000/6028478\n\nI'll post another github issue after some more investigation.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/21284580","html_url":"https://github.com/elastic/elasticsearch/issues/3313#issuecomment-21284580","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3313","id":21284580,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjg0NTgw","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2013-07-20T00:13:52Z","updated_at":"2013-07-20T00:13:52Z","author_association":"MEMBER","body":"Hi @nik9000. Thanks for the PR. I am just thinking maybe we can remove `assertRed();` and `assert false;` from the test. This way we are still testing fast closing, but even if closing is not fast enough test wouldn't fail. I also feel that 128 shards might be a bit excessive. Maybe reduce it to 50 or even 20?\n\nI looked at the related problem with quorum as well. Not really sure what we should do about it. Should we even allow creation of an index with 3 replicas and `index.recovery.initial_shards=quorum` on a single node? On the other side, even if we have 4 nodes, it's not always obvious if we can fulfill `index.recovery.initial_shards` requirements or not. So, we could store some flag in index metadata that would indicate that this index wasn't fully allocated at least once yet. And if this flag is set, LocalGatewayAllocator would ignore `requiredAllocation` or as in case of prematurely closed index create missing shards as needed. We could even use this flag to block any operations on such index, so it would be really obvious that this index is not in a proper state. @kimchy what do you think?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/21285340","html_url":"https://github.com/elastic/elasticsearch/issues/3313#issuecomment-21285340","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3313","id":21285340,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjg1MzQw","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2013-07-20T00:51:43Z","updated_at":"2013-07-20T00:51:43Z","author_association":"CONTRIBUTOR","body":"On Fri, Jul 19, 2013 at 8:14 PM, Igor Motov notifications@github.comwrote:\n\n> Hi @nik9000 https://github.com/nik9000. Thanks for the PR.\n\nThanks for taking the time to read it!\n\n> I am just thinking maybe we can remove assertRed(); and assert false;from the test. This way we are still testing fast closing, but even if\n> closing is not fast enough test wouldn't fail. I also feel that 128 shards\n> might be a bit excessive. Maybe reduce it to 50 or even 20?\n> \n> I'm not sure it'd be a good test if sometimes it didn't verify anything.\n> If we were in JUnit I'd say we could use the Assume api but I'm not really\n> sure what the right thing is in TestNG.\n\nAs to the 128 shards it was just a number that seemed to trigger the\nbehavior.  IIRC 20 wouldn't have consistently triggered the problem on my\nlaptop.  50 probably would but I didn't want to risk someone having a\nfaster machine than mine and getting an unexpectedly useless/failing test.\n\n> I looked at the related problem with quorum as well. Not really sure what\n> we should do about it. Should we even allow creation of an index with 3\n> replicas and index.recovery.initial_shards=quorum on a single node? On\n> the other side, even if we have 4 nodes, it's not always obvious if we can\n> fulfill index.recovery.initial_shards requirements or not. So, we could\n> store some flag in index metadata that would indicate that this index\n> wasn't fully allocated at least once yet. And if this flag is set,\n> LocalGatewayAllocator would ignore requiredAllocation or as in case of\n> prematurely closed index create missing shards as needed. We could even use\n> this flag to block any operations on such index, so it would be really\n> obvious that this index is not in a proper state. @kimchyhttps://github.com/kimchywhat do you think?\n> \n> For my book stopping people when they ask for a configuration that just\n> isn't going to fully allocate sounds like the right thing to do.  It'd\n> probably make sense to have a force flag that gets the unchecked behavior\n> but with a warning that things might not work properly if you don't bring\n> those nodes online.\n\nWhat about the case where when you create the index everything makes sense\nand allocates properly but then you lose a node?  Without that node you can\nclose the index but it won't open again until you bring that node back\nonline.  At least, that is what I saw when I was playing with\nhttps://github.com/elasticsearch/elasticsearch/issues/3354.\n\n> —\n> Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3313#issuecomment-21284580\n> .\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/21285363","html_url":"https://github.com/elastic/elasticsearch/issues/3313#issuecomment-21285363","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3313","id":21285363,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjg1MzYz","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2013-07-20T00:52:59Z","updated_at":"2013-07-20T00:52:59Z","author_association":"CONTRIBUTOR","body":"I could work the test so it used a small number of shards and just tried\nagain if it didn't hit the problem.  That wouldn't be too tough.  I'll have\na look at that sometime in the next few days.\n\nOn Fri, Jul 19, 2013 at 8:51 PM, Nikolas Everett nik9000@gmail.com wrote:\n\n> On Fri, Jul 19, 2013 at 8:14 PM, Igor Motov notifications@github.comwrote:\n> \n> > Hi @nik9000 https://github.com/nik9000. Thanks for the PR.\n> \n> Thanks for taking the time to read it!\n> \n> > I am just thinking maybe we can remove assertRed(); and assert false;from the test. This way we are still testing fast closing, but even if\n> > closing is not fast enough test wouldn't fail. I also feel that 128 shards\n> > might be a bit excessive. Maybe reduce it to 50 or even 20?\n> > \n> > I'm not sure it'd be a good test if sometimes it didn't verify anything.\n> > If we were in JUnit I'd say we could use the Assume api but I'm not really\n> > sure what the right thing is in TestNG.\n> \n> As to the 128 shards it was just a number that seemed to trigger the\n> behavior.  IIRC 20 wouldn't have consistently triggered the problem on my\n> laptop.  50 probably would but I didn't want to risk someone having a\n> faster machine than mine and getting an unexpectedly useless/failing test.\n> \n> > I looked at the related problem with quorum as well. Not really sure what\n> > we should do about it. Should we even allow creation of an index with 3\n> > replicas and index.recovery.initial_shards=quorum on a single node? On\n> > the other side, even if we have 4 nodes, it's not always obvious if we can\n> > fulfill index.recovery.initial_shards requirements or not. So, we could\n> > store some flag in index metadata that would indicate that this index\n> > wasn't fully allocated at least once yet. And if this flag is set,\n> > LocalGatewayAllocator would ignore requiredAllocation or as in case of\n> > prematurely closed index create missing shards as needed. We could even use\n> > this flag to block any operations on such index, so it would be really\n> > obvious that this index is not in a proper state. @kimchyhttps://github.com/kimchywhat do you think?\n> > \n> > For my book stopping people when they ask for a configuration that just\n> > isn't going to fully allocate sounds like the right thing to do.  It'd\n> > probably make sense to have a force flag that gets the unchecked behavior\n> > but with a warning that things might not work properly if you don't bring\n> > those nodes online.\n> \n> What about the case where when you create the index everything makes sense\n> and allocates properly but then you lose a node?  Without that node you can\n> close the index but it won't open again until you bring that node back\n> online.  At least, that is what I saw when I was playing with\n> https://github.com/elasticsearch/elasticsearch/issues/3354.\n> \n> > —\n> > Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3313#issuecomment-21284580\n> > .\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/21285609","html_url":"https://github.com/elastic/elasticsearch/issues/3313#issuecomment-21285609","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3313","id":21285609,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjg1NjA5","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2013-07-20T01:04:01Z","updated_at":"2013-07-20T01:04:01Z","author_association":"MEMBER","body":"Interesting, I was able to consistently reproduce it with 10 shards (and even 5 in most cases), hence the suggested number. I was thinking about retrying logic as well, but you would still need to remove assertRed() to remove race condition between checking index health and closing the index and then do clean up of index that failed to create the issue. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/21298231","html_url":"https://github.com/elastic/elasticsearch/issues/3313#issuecomment-21298231","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/3313","id":21298231,"node_id":"MDEyOklzc3VlQ29tbWVudDIxMjk4MjMx","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2013-07-20T18:36:48Z","updated_at":"2013-07-20T18:36:48Z","author_association":"CONTRIBUTOR","body":"I've updated the pull request with a retry logic and it looks like I can\nreproduce it with two shards!  I suppose I should have tried ratcheting\ndown the number rather than watching my logs.  Anyway I feel better with\nthe retry logic making sure the test actually does something but runs more\nquickly if it can get away with it.\n\nNik\n\nOn Fri, Jul 19, 2013 at 9:04 PM, Igor Motov notifications@github.comwrote:\n\n> Interesting, I was able to consistently reproduce it with 10 shards (and\n> even 5 in most cases), hence the suggested number. I was thinking about\n> retrying logic as well, but you would still need to remove assertRed() to\n> remove race condition between checking index health and closing the index\n> and then do clean up of index that failed to create the issue.\n> \n> —\n> Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3313#issuecomment-21285609\n> .\n","performed_via_github_app":null}]