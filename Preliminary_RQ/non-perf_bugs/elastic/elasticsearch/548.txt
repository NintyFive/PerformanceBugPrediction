{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/548","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/548/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/548/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/548/events","html_url":"https://github.com/elastic/elasticsearch/issues/548","id":448521,"node_id":"MDU6SXNzdWU0NDg1MjE=","number":548,"title":"Use multiple data dirs (allows striping IO across multiple disks)","user":{"login":"mrflip","id":6128,"node_id":"MDQ6VXNlcjYxMjg=","avatar_url":"https://avatars0.githubusercontent.com/u/6128?v=4","gravatar_id":"","url":"https://api.github.com/users/mrflip","html_url":"https://github.com/mrflip","followers_url":"https://api.github.com/users/mrflip/followers","following_url":"https://api.github.com/users/mrflip/following{/other_user}","gists_url":"https://api.github.com/users/mrflip/gists{/gist_id}","starred_url":"https://api.github.com/users/mrflip/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mrflip/subscriptions","organizations_url":"https://api.github.com/users/mrflip/orgs","repos_url":"https://api.github.com/users/mrflip/repos","events_url":"https://api.github.com/users/mrflip/events{/privacy}","received_events_url":"https://api.github.com/users/mrflip/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2010-12-01T02:46:00Z","updated_at":"2011-09-23T14:29:50Z","closed_at":"2011-09-23T14:29:50Z","author_association":"NONE","active_lock_reason":null,"body":"Elasticsearch is often IO-bound during indexing: on EC2 elasticsearch would tap out at ~12-14k wreq/s to one disk. By using multiple independent data_esnode processes on each machine, we were able to get 10k-12k wreqs/s on each of 3 disks, almost tripling throughput.\n\nIf I could give ES a list of dirs for path.data, we could adjust the processes per machine independently of the drives per machine. Using RAID might also address this, but adds significant complexity and risk to the process of instantiating a machine in an elastic cloud.\n\nFeature request:\n- allow path.data to be an array:\n  \n  path:\n    data: [\"/es1/data\", \"/es2/data\", \"/es3/data\"]\n- when Lucene allocates a new set of index files, choose the dir with the highest available space, and put it at [data_dir]/nodes/[node number]/indices/[index_name]/[shard_number]\n- This will give a significant performance increase even over RAID as merges will draw from distinct disks\n- Cassandra uses this approach; I believe they are even clever about making compactions in general go from one set of disks to a distinct one.\n\nRamifications (via kimchy on IRC): need to get between Lucene and how it handles files; and \"join\" based operations, like \"list files under shard dir\" will need to be aggregated across mount points.\n","closed_by":{"login":"kimchy","id":41300,"node_id":"MDQ6VXNlcjQxMzAw","avatar_url":"https://avatars1.githubusercontent.com/u/41300?v=4","gravatar_id":"","url":"https://api.github.com/users/kimchy","html_url":"https://github.com/kimchy","followers_url":"https://api.github.com/users/kimchy/followers","following_url":"https://api.github.com/users/kimchy/following{/other_user}","gists_url":"https://api.github.com/users/kimchy/gists{/gist_id}","starred_url":"https://api.github.com/users/kimchy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kimchy/subscriptions","organizations_url":"https://api.github.com/users/kimchy/orgs","repos_url":"https://api.github.com/users/kimchy/repos","events_url":"https://api.github.com/users/kimchy/events{/privacy}","received_events_url":"https://api.github.com/users/kimchy/received_events","type":"User","site_admin":false},"performed_via_github_app":null}