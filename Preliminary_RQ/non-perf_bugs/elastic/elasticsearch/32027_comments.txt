[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/404768150","html_url":"https://github.com/elastic/elasticsearch/issues/32027#issuecomment-404768150","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32027","id":404768150,"node_id":"MDEyOklzc3VlQ29tbWVudDQwNDc2ODE1MA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-07-13T08:42:29Z","updated_at":"2018-07-13T08:42:29Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/414640656","html_url":"https://github.com/elastic/elasticsearch/issues/32027#issuecomment-414640656","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32027","id":414640656,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNDY0MDY1Ng==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2018-08-21T11:21:01Z","updated_at":"2018-08-21T11:21:01Z","author_association":"CONTRIBUTOR","body":"The logs are unfortunately lost on this one, so we will have to wait for another failure of this. Closing until such a failure happens again.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/467828849","html_url":"https://github.com/elastic/elasticsearch/issues/32027#issuecomment-467828849","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32027","id":467828849,"node_id":"MDEyOklzc3VlQ29tbWVudDQ2NzgyODg0OQ==","user":{"login":"tlrx","id":642733,"node_id":"MDQ6VXNlcjY0MjczMw==","avatar_url":"https://avatars1.githubusercontent.com/u/642733?v=4","gravatar_id":"","url":"https://api.github.com/users/tlrx","html_url":"https://github.com/tlrx","followers_url":"https://api.github.com/users/tlrx/followers","following_url":"https://api.github.com/users/tlrx/following{/other_user}","gists_url":"https://api.github.com/users/tlrx/gists{/gist_id}","starred_url":"https://api.github.com/users/tlrx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tlrx/subscriptions","organizations_url":"https://api.github.com/users/tlrx/orgs","repos_url":"https://api.github.com/users/tlrx/repos","events_url":"https://api.github.com/users/tlrx/events{/privacy}","received_events_url":"https://api.github.com/users/tlrx/received_events","type":"User","site_admin":false},"created_at":"2019-02-27T11:35:01Z","updated_at":"2019-02-27T11:35:32Z","author_association":"MEMBER","body":"`RecoveryIT.testRecoverSyncedFlushIndex` failed again today on CI:\r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+6.7+bwc-tests/86/consoleFull\r\n\r\nIt reproduces using:\r\n```\r\n./gradlew :qa:rolling-upgrade:v6.6.0#oneThirdUpgradedTestRunner -Dtests.seed=7C050A5AAA4A67D1 -Dtests.class=org.elasticsearch.upgrades.RecoveryIT -Dtests.method=\"testRecoverSyncedFlushIndex\" -Dtests.security.manager=true -Dtests.locale=vi -Dtests.timezone=America/Indiana/Petersburg -Dcompiler.java=11 -Druntime.java=8\r\n```\r\n<details>\r\n10:10:16   1> [2019-02-27T05:09:03,606][INFO ][o.e.u.RecoveryIT         ] [testRecoverSyncedFlushIndex] before test\r\n10:10:16   1> [2019-02-27T05:10:13,634][INFO ][o.e.u.RecoveryIT         ] [testRecoverSyncedFlushIndex] after test\r\n10:10:16   2> REPRODUCE WITH: ./gradlew :qa:rolling-upgrade:v6.6.0#oneThirdUpgradedTestRunner -Dtests.seed=7C050A5AAA4A67D1 -Dtests.class=org.elasticsearch.upgrades.RecoveryIT -Dtests.method=\"testRecoverSyncedFlushIndex\" -Dtests.security.manager=true -Dtests.locale=vi -Dtests.timezone=America/Indiana/Petersburg -Dcompiler.java=11 -Druntime.java=8\r\n10:10:16 ERROR   70.0s | RecoveryIT.testRecoverSyncedFlushIndex <<< FAILURES!\r\n10:10:16   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/workspace/elastic+elasticsearch+6.7+bwc-tests/qa/rolling-upgrade/build/testrun/v6.6.0#oneThirdUpgradedTestRunner/J0/temp/org.elasticsearch.upgrades.RecoveryIT_7C050A5AAA4A67D1-001\r\n10:10:16    > Throwable #1: org.elasticsearch.client.ResponseException: method [GET], host [http://[::1]:34493], URI [/_cluster/health/recover_synced_flush_index?wait_for_no_relocating_shards=true&level=shards&timeout=70s&wait_for_status=green], status line [HTTP/1.1 408 Request Timeout]\r\n10:10:16    > {\"cluster_name\":\"rolling-upgrade\",\"status\":\"yellow\",\"timed_out\":true,\"number_of_nodes\":3,\"number_of_data_nodes\":3,\"active_primary_shards\":1,\"active_shards\":1,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":1,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":96.0,\"indices\":{\"recover_synced_flush_index\":{\"status\":\"yellow\",\"number_of_shards\":1,\"number_of_replicas\":1,\"active_primary_shards\":1,\"active_shards\":1,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":1,\"shards\":{\"0\":{\"status\":\"yellow\",\"primary_active\":true,\"active_shards\":1,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":1}}}}}\r\n10:10:16   2> NOTE: test params are: codec=Asserting(Lucene70), sim=RandomSimilarity(queryNorm=true): {}, locale=vi, timezone=America/Indiana/Petersburg\r\n10:10:16   2> NOTE: Linux 4.15.0-1027-gcp amd64/Oracle Corporation 1.8.0_202 (64-bit)/cpus=16,threads=1,free=383006912,total=514850816\r\n10:10:16    > \tat __randomizedtesting.SeedInfo.seed([7C050A5AAA4A67D1:5734D78D0D1D1298]:0)\r\n10:10:16    > \tat org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:936)\r\n10:10:16    > \tat org.elasticsearch.client.RestClient.performRequest(RestClient.java:233)\r\n10:10:16   2> NOTE: All tests run in this JVM: [IndexingIT, RecoveryIT]\r\n10:10:16    > \tat org.elasticsearch.test.rest.ESRestTestCase.ensureGreen(ESRestTestCase.java:775)\r\n10:10:16    > \tat org.elasticsearch.upgrades.RecoveryIT.testRecoverSyncedFlushIndex(RecoveryIT.java:318)\r\n10:10:16    > \tat java.lang.Thread.run(Thread.java:748)\r\n10:10:16    > Caused by: org.elasticsearch.client.ResponseException: method [GET], host [http://[::1]:34493], URI [/_cluster/health/recover_synced_flush_index?wait_for_no_relocating_shards=true&level=shards&timeout=70s&wait_for_status=green], status line [HTTP/1.1 408 Request Timeout]\r\n10:10:16    > {\"cluster_name\":\"rolling-upgrade\",\"status\":\"yellow\",\"timed_out\":true,\"number_of_nodes\":3,\"number_of_data_nodes\":3,\"active_primary_shards\":1,\"active_shards\":1,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":1,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":96.0,\"indices\":{\"recover_synced_flush_index\":{\"status\":\"yellow\",\"number_of_shards\":1,\"number_of_replicas\":1,\"active_primary_shards\":1,\"active_shards\":1,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":1,\"shards\":{\"0\":{\"status\":\"yellow\",\"primary_active\":true,\"active_shards\":1,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":1}}}}}\r\n10:10:16    > \tat org.elasticsearch.client.RestClient$1.completed(RestClient.java:552)\r\n10:10:16    > \tat org.elasticsearch.client.RestClient$1.completed(RestClient.java:537)\r\n10:10:16    > \tat org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:119)\r\n10:10:16    > \tat org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:177)\r\n10:10:16    > \tat org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:436)\r\n10:10:16    > \tat org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:326)\r\n10:10:16    > \tat org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265)\r\n10:10:16    > \tat org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81)\r\n10:10:16    > \tat org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39)\r\n10:10:16    > \tat org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)\r\n10:10:16    > \tat org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)\r\n10:10:16    > \tat org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)\r\n10:10:16    > \tat org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)\r\n10:10:16    > \tat org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)\r\n10:10:16    > \tat org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)\r\n10:10:16    > \tat org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:588)\r\n10:10:16    > \t... 1 more\r\n</details>\r\n\r\nSadly I can't connect to the worker to grab the logs.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/470638338","html_url":"https://github.com/elastic/elasticsearch/issues/32027#issuecomment-470638338","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32027","id":470638338,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3MDYzODMzOA==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2019-03-07T18:22:28Z","updated_at":"2019-03-07T18:22:28Z","author_association":"MEMBER","body":"I am on it today.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/472080259","html_url":"https://github.com/elastic/elasticsearch/issues/32027#issuecomment-472080259","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32027","id":472080259,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3MjA4MDI1OQ==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2019-03-12T16:38:44Z","updated_at":"2019-03-12T16:38:44Z","author_association":"MEMBER","body":"I suspect this is a bug in allocation but could not figure it out after spending on some good time. Some important events in the test.\r\n\r\n1. `node-0` is the master, the index `recover_synced_flush_index` has primary on `node-1`, and replica on `node-2`.\r\n\r\n2. Before we shut down node-0 to upgrade, we relocate the primary of `recover_synced_flush_index` from `node-1` to `node-0`. And `node-0` is stopped before we can move the relocating target to started\r\n\r\n3. `node-1` is elected as the master and failing the relocating target on `node-0`, then move the relocating source on `node-1` back to `started`.\r\n\r\n````\r\n[2019-02-27T10:08:48,612][DEBUG][o.e.c.r.a.AllocationService] [node-1] [recover_synced_flush_index][0] failing shard [recover_synced_flush_index][0], node[HZ9A5r6xT2Cewlb3YqIg8g], relocating [W0zYyliTQsmieW42pX6mYw], [P], recovery_source[peer recovery], s[INITIALIZING], a[id=sJILiyxiTlWMYj70-z5Irw, rId=KATt9QvpSIOykrBXyfEJjA] with unassigned info ([reason=NODE_LEFT], at[2019-02-27T10:08:48.612Z], delayed=true, details[node_left[HZ9A5r6xT2Cewlb3YqIg8g]], allocation_status[no_attempt])\r\n````\r\n\r\n4. The relocating source on `node-1` is marked already as `relocated`; it has to fail itself because it can't move back to started.\r\n\r\n```\r\n[2019-02-27T10:08:48,802][WARN ][o.e.i.c.IndicesClusterStateService] [node-1] [[recover_synced_flush_index][0]] marking and sending shard failed due to [failed updating shard routing entry]\r\norg.elasticsearch.index.shard.IndexShardRelocatedException: CurrentState[STARTED] Shard is marked as relocated, cannot safely move to state STARTED\r\n```\r\n\r\n5. The relocating source on `node-1` sends another shard-failure request.\r\n\r\n```\r\n[2019-02-27T10:08:48,935][DEBUG][o.e.c.a.s.ShardStateAction] [node-1] [recover_synced_flush_index][0] received shard failed for shard id [[recover_synced_flush_index][0]], allocation id [KATt9QvpSIOykrBXyfEJjA], primary term [0], message [master {node-1}{W0zYyliTQsmieW42pX6mYw}{V6CIni9vSFOGaBNzNP-2jw}{127.0.0.1}{127.0.0.1:36807}{gen=old, testattr=test} has not removed previously failed shard. resending shard failure], markAsStale [true]\r\n[2019-02-27T10:08:48,935][DEBUG][o.e.i.c.IndicesClusterStateService] [node-1] [[recover_with_soft_deletes/X_XCi9RpQpKeKpdGj5MhoA]] creating index\r\n```\r\n\r\n6. The master fails the relocating source\r\n\r\n```\r\n[2019-02-27T10:08:48,969][DEBUG][o.e.c.r.a.AllocationService] [node-1] [recover_synced_flush_index][0] failing shard [recover_synced_flush_index][0], node[W0zYyliTQsmieW42pX6mYw], [P], s[STARTED], a[id=KATt9QvpSIOykrBXyfEJjA] with unassigned info ([reason=ALLOCATION_FAILED], at[2019-02-27T10:08:48.968Z], failed_attempts[1], delayed=false, details[failed shard on node [W0zYyliTQsmieW42pX6mYw]: failed updating shard routing entry, failure IndexShardRelocatedException[CurrentState[STARTED] Shard is marked as relocated, cannot safely move to state STARTED]], allocation_status[no_attempt])\r\n```\r\n\r\n7. We execute several reroutes afterward, but the index `recover_synced_flush_index` remains unassigned.\r\n\r\nI looked at RoutingNodes, AllocationService and Gateway code but yield no fruit. Perhaps it's a timeout issue, but this failure looks suspicious to me. @ywelsch Can you please continue the investigation?\r\n\r\nLogs: \r\n[node-0.log](https://github.com/elastic/elasticsearch/files/2957827/node-0.log)\r\n[node-1.log](https://github.com/elastic/elasticsearch/files/2957828/node-1.log)\r\n[node-2.log](https://github.com/elastic/elasticsearch/files/2957829/node-2.log)\r\n[upgraded-node-0.log](https://github.com/elastic/elasticsearch/files/2957831/upgraded-node-0.log)\r\n[recovery_with_synced_flush.txt](https://github.com/elastic/elasticsearch/files/2957839/recovery_with_synced_flush.txt)\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/472324295","html_url":"https://github.com/elastic/elasticsearch/issues/32027#issuecomment-472324295","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32027","id":472324295,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3MjMyNDI5NQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2019-03-13T08:22:45Z","updated_at":"2019-03-13T08:22:45Z","author_association":"CONTRIBUTOR","body":"I checked all the logs on this and cannot see why the replica is not being allocated, even after 70 seconds. Beside running this with TRACE logging (which is tricky as it means running all tests with TRACE logging), I wonder if we should extend `ensureGreen` in `ESTestTestCase` similar to how we do it in `ESIntegTestCase`, by querying the cluster state and possibly the allocation explain API for that index to get a better understanding of why the shard is not getting allocated.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/472351081","html_url":"https://github.com/elastic/elasticsearch/issues/32027#issuecomment-472351081","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32027","id":472351081,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3MjM1MTA4MQ==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2019-03-13T09:46:59Z","updated_at":"2019-03-13T09:46:59Z","author_association":"MEMBER","body":">  I wonder if we should extend ensureGreen in ESTestTestCase similar to how we do it in ESIntegTestCase, by querying the cluster state and possibly the allocation explain API for that index to get a better understanding of why the shard is not getting allocated.\r\n\r\n+1. I will do this.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/473676081","html_url":"https://github.com/elastic/elasticsearch/issues/32027#issuecomment-473676081","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32027","id":473676081,"node_id":"MDEyOklzc3VlQ29tbWVudDQ3MzY3NjA4MQ==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2019-03-17T15:26:00Z","updated_at":"2019-03-17T15:26:00Z","author_association":"MEMBER","body":"There's not much that we can do with the existing log. I have integrated #40133 which will dump the cluster state if ensureGreen is timed out.","performed_via_github_app":null}]