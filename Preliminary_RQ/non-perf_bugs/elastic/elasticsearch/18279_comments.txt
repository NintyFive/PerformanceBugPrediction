[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218714389","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-218714389","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":218714389,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODcxNDM4OQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-05-12T10:06:16Z","updated_at":"2016-05-12T10:06:16Z","author_association":"CONTRIBUTOR","body":"Related to #18217\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218714548","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-218714548","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":218714548,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODcxNDU0OA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-05-12T10:06:58Z","updated_at":"2016-05-12T10:06:58Z","author_association":"CONTRIBUTOR","body":"While I think there may be improvements that can be made when a disk dies, if you want hot swapping etc I think you need a proper RAID system or LVS\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218994716","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-218994716","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":218994716,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODk5NDcxNg==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-05-13T09:25:45Z","updated_at":"2016-05-13T09:25:45Z","author_association":"CONTRIBUTOR","body":"I think we need to add some resiliency here:\n- we should check if we can write on the datapath before we allocate\n- we should fail the engine if we hit an IOException in any case it's really crazy that we don't do that. There should not be any IOException here \n\nI will take care of this\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218995048","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-218995048","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":218995048,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODk5NTA0OA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-05-13T09:27:14Z","updated_at":"2016-05-13T09:27:14Z","author_association":"CONTRIBUTOR","body":"yeah I am torn on the hot-swapping. I think we can potentially take things out of the loop internally but if you are pluggin in a new disk and we should auto-detect that a datapath is good again I think you should restart the node instead?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219038755","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-219038755","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":219038755,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTAzODc1NQ==","user":{"login":"PhaedrusTheGreek","id":4387023,"node_id":"MDQ6VXNlcjQzODcwMjM=","avatar_url":"https://avatars0.githubusercontent.com/u/4387023?v=4","gravatar_id":"","url":"https://api.github.com/users/PhaedrusTheGreek","html_url":"https://github.com/PhaedrusTheGreek","followers_url":"https://api.github.com/users/PhaedrusTheGreek/followers","following_url":"https://api.github.com/users/PhaedrusTheGreek/following{/other_user}","gists_url":"https://api.github.com/users/PhaedrusTheGreek/gists{/gist_id}","starred_url":"https://api.github.com/users/PhaedrusTheGreek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/PhaedrusTheGreek/subscriptions","organizations_url":"https://api.github.com/users/PhaedrusTheGreek/orgs","repos_url":"https://api.github.com/users/PhaedrusTheGreek/repos","events_url":"https://api.github.com/users/PhaedrusTheGreek/events{/privacy}","received_events_url":"https://api.github.com/users/PhaedrusTheGreek/received_events","type":"User","site_admin":false},"created_at":"2016-05-13T13:12:41Z","updated_at":"2016-05-13T13:12:41Z","author_association":"CONTRIBUTOR","body":"Definitely we don't want to _introduce_ any resiliency issues.    Some manual intervention makes sense, but restarting a node can sometimes take a long time.    Should there be something like delayed allocation on marking a path.data as failed? - there is the case of something like NFS, where a network problem might make the drive appear to come and go.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219249138","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-219249138","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":219249138,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTI0OTEzOA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-05-14T20:12:58Z","updated_at":"2016-05-14T20:12:58Z","author_association":"CONTRIBUTOR","body":"I think if you loose a disk you need to restart the node. I can totally improve along the lines of failing shards quicker but we shouldn't try to be fancy here. I think we should take the node out of the cluster somehow but that's something that needs more thought.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/219759176","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-219759176","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":219759176,"node_id":"MDEyOklzc3VlQ29tbWVudDIxOTc1OTE3Ng==","user":{"login":"PhaedrusTheGreek","id":4387023,"node_id":"MDQ6VXNlcjQzODcwMjM=","avatar_url":"https://avatars0.githubusercontent.com/u/4387023?v=4","gravatar_id":"","url":"https://api.github.com/users/PhaedrusTheGreek","html_url":"https://github.com/PhaedrusTheGreek","followers_url":"https://api.github.com/users/PhaedrusTheGreek/followers","following_url":"https://api.github.com/users/PhaedrusTheGreek/following{/other_user}","gists_url":"https://api.github.com/users/PhaedrusTheGreek/gists{/gist_id}","starred_url":"https://api.github.com/users/PhaedrusTheGreek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/PhaedrusTheGreek/subscriptions","organizations_url":"https://api.github.com/users/PhaedrusTheGreek/orgs","repos_url":"https://api.github.com/users/PhaedrusTheGreek/repos","events_url":"https://api.github.com/users/PhaedrusTheGreek/events{/privacy}","received_events_url":"https://api.github.com/users/PhaedrusTheGreek/received_events","type":"User","site_admin":false},"created_at":"2016-05-17T15:40:54Z","updated_at":"2016-05-17T15:40:54Z","author_association":"CONTRIBUTOR","body":"Multiple disks on `path.data` offers some added benefit over RAID0, in that IO is spread out over all disks, theoretically matching RAID0 performance, but while not causing a total volume failure on a single disk loss.    \n\nRestarting a node is much easier than re-building a logical volume, and much less data is lost, so either way we are ahead.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/238870117","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-238870117","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":238870117,"node_id":"MDEyOklzc3VlQ29tbWVudDIzODg3MDExNw==","user":{"login":"neuroticnetworks","id":3965137,"node_id":"MDQ6VXNlcjM5NjUxMzc=","avatar_url":"https://avatars0.githubusercontent.com/u/3965137?v=4","gravatar_id":"","url":"https://api.github.com/users/neuroticnetworks","html_url":"https://github.com/neuroticnetworks","followers_url":"https://api.github.com/users/neuroticnetworks/followers","following_url":"https://api.github.com/users/neuroticnetworks/following{/other_user}","gists_url":"https://api.github.com/users/neuroticnetworks/gists{/gist_id}","starred_url":"https://api.github.com/users/neuroticnetworks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/neuroticnetworks/subscriptions","organizations_url":"https://api.github.com/users/neuroticnetworks/orgs","repos_url":"https://api.github.com/users/neuroticnetworks/repos","events_url":"https://api.github.com/users/neuroticnetworks/events{/privacy}","received_events_url":"https://api.github.com/users/neuroticnetworks/received_events","type":"User","site_admin":false},"created_at":"2016-08-10T13:42:29Z","updated_at":"2016-08-10T13:44:07Z","author_association":"NONE","body":"> I think if you loose a disk you need to restart the node. I can totally improve along the lines of failing shards quicker but we shouldn't try to be fancy here. I think we should take the node out of the cluster somehow but that's something that needs more thought.\n\nIn general this makes sense but it would be nice if you could apply something like a transient setting to tell that node that a disk has died and to temporarily stop trying to perform I/O on it. That would still require manual intervention, but it would allow to apply a temporary hotfix if a node restart is not immediately feasible.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/255874842","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-255874842","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":255874842,"node_id":"MDEyOklzc3VlQ29tbWVudDI1NTg3NDg0Mg==","user":{"login":"neuroticnetworks","id":3965137,"node_id":"MDQ6VXNlcjM5NjUxMzc=","avatar_url":"https://avatars0.githubusercontent.com/u/3965137?v=4","gravatar_id":"","url":"https://api.github.com/users/neuroticnetworks","html_url":"https://github.com/neuroticnetworks","followers_url":"https://api.github.com/users/neuroticnetworks/followers","following_url":"https://api.github.com/users/neuroticnetworks/following{/other_user}","gists_url":"https://api.github.com/users/neuroticnetworks/gists{/gist_id}","starred_url":"https://api.github.com/users/neuroticnetworks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/neuroticnetworks/subscriptions","organizations_url":"https://api.github.com/users/neuroticnetworks/orgs","repos_url":"https://api.github.com/users/neuroticnetworks/repos","events_url":"https://api.github.com/users/neuroticnetworks/events{/privacy}","received_events_url":"https://api.github.com/users/neuroticnetworks/received_events","type":"User","site_admin":false},"created_at":"2016-10-24T21:42:20Z","updated_at":"2016-10-24T21:43:19Z","author_association":"NONE","body":"Had this issue come up against last night.\n\nOur logging nodes have 4 SSDs. We've passed an array to the `path.data` in elasticsearch.yaml. \n\nOver the weekend, one of the file systems on one of the disks one one of the ES servers became corrupt. Over the next 12 hours, ES spewed 500GB of errors like the following into the logs, filling up the root partition and eventually alerting us (because we alert on disk usage but we didn't at the time have alerts on ES log file size / growth)\n\n```\n[2016-10-22 00:00:04,017][WARN ][cluster.action.shard     ] [deliverability_master02-es02] [logstash-delivery-2016.10.14.09][0] received shard failed for target shard [[logstash-delivery-2016.10.14.09][0], node[J_Wws-cKQPKPJjIE7lEacw], relocating [IIKJ3BHGRlG0IYmZ3GLeNA], [R], v[8192], s[INITIALIZING], a[id=HwzksPLITruZz94vsNTMvg, rId=6DS2pI5FS3uih0a1yvRJFw], expected_shard_size[25697352067]], indexUUID [RL1zWoD6SN6_ZmpjPGM0Yw], message [failed to create shard], failure [ElasticsearchException[failed to create shard]; nested: NotSerializableExceptionWrapper[file_system_exception: /storage/sdd1/deliverability/nodes/0/indices/logstash-delivery-2016.10.14.09/0/_state: Input/output error]; ]\n[logstash-delivery-2016.10.14.09][[logstash-delivery-2016.10.14.09][0]] ElasticsearchException[failed to create shard]; nested: NotSerializableExceptionWrapper[file_system_exception: /storage/sdd1/deliverability/nodes/0/indices/logstash-delivery-2016.10.14.09/0/_state: Input/output error];\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:620)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:520)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:177)\n    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: NotSerializableExceptionWrapper[file_system_exception: /storage/sdd1/deliverability/nodes/0/indices/logstash-delivery-2016.10.14.09/0/_state: Input/output error]\n    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n    at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:427)\n    at java.nio.file.Files.newDirectoryStream(Files.java:457)\n    at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:257)\n    at org.elasticsearch.index.shard.ShardPath.loadShardPath(ShardPath.java:122)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 10 more\n```\n\nThere are 12 data nodes in this cluster with 4 SSDs, 3 dedicated masters, and we run a replication factor of 2 using hourly indices with 2 primary shards.\n\nDuring the time that this happened, Elasticsearch continued to place primary shards on the failed `storage/sdd1` drive. Because the writes to the primary failed, and because we were only alerted of the problem (interesting to note as well that the cluster remained green the entire time and none of our monitoring and alerting of /_cluster and _nodes stats caught it.. which is our fault, but still important to note) because the errors in the logs filled up the root disk. \n\nAs a result of Elasticsearch continuing to place primary shards on the failed disk, we lost half of the log data for 9 out of the 12 hours that this disk was unreachable (because 9 out of 12 times it attempted to place at least one of each hour's primary shards on the unreachable disk; the writes to primary failed, the primary was never moved elsewhere).\n\nI suspect, although I did not dig into it or write a test case to prove it, that the process whereby Elasticsearch determines which nodes are eligible to get a write and which disk to write to once it gets there might also bias further writes towards the drive that failed. In our case, we had 9 data nodes that were eligible to accept writes, each having 4 eligible disks that had not exceeded any water marks or otherwise were unwritable. Over 12 hours, 9 of the 24 primary shards created were allocated to the node with the disk failure and it routed them to the unreachable disk. As a result of being unwritable for several hours, that disk also was less full than the other disks on the cluster. Again, I don't know that a disk failure like the one we had biases shard placement in favor of writing to the unreachable disk. But we did see an abnormally high number of shards placed on one machine, and on one disk on one machine.... abnormal enough to make me wonder if that wasn't just a coincidence. \n\nAll of which to say.... I think this issue is extremely important.  I also think @s1monw is right to suggest that ensuring a filepath is writable before placing a shard (especially a primary shard) will go a long way towards adding resiliency. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/372580753","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-372580753","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":372580753,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MjU4MDc1Mw==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-03-13T08:11:27Z","updated_at":"2018-03-13T08:11:27Z","author_association":"MEMBER","body":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-218994716 describe two things that need to happen to resolve this issue. The first has been done in https://github.com/elastic/elasticsearch/pull/16745  . The second (failing the shard) is very easy. I opened https://github.com/elastic/elasticsearch/issues/29008 to highlight it as an adopt me and a low hanging fruit. Closing this one as superseded by these two issues. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/372853599","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-372853599","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":372853599,"node_id":"MDEyOklzc3VlQ29tbWVudDM3Mjg1MzU5OQ==","user":{"login":"neuroticnetworks","id":3965137,"node_id":"MDQ6VXNlcjM5NjUxMzc=","avatar_url":"https://avatars0.githubusercontent.com/u/3965137?v=4","gravatar_id":"","url":"https://api.github.com/users/neuroticnetworks","html_url":"https://github.com/neuroticnetworks","followers_url":"https://api.github.com/users/neuroticnetworks/followers","following_url":"https://api.github.com/users/neuroticnetworks/following{/other_user}","gists_url":"https://api.github.com/users/neuroticnetworks/gists{/gist_id}","starred_url":"https://api.github.com/users/neuroticnetworks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/neuroticnetworks/subscriptions","organizations_url":"https://api.github.com/users/neuroticnetworks/orgs","repos_url":"https://api.github.com/users/neuroticnetworks/repos","events_url":"https://api.github.com/users/neuroticnetworks/events{/privacy}","received_events_url":"https://api.github.com/users/neuroticnetworks/received_events","type":"User","site_admin":false},"created_at":"2018-03-13T23:29:09Z","updated_at":"2018-03-13T23:29:09Z","author_association":"NONE","body":"@bleskes would you consider reopening this ticket as a high hanging fruit, as per https://github.com/elastic/elasticsearch/issues/29008#issuecomment-372852685? Or, if you feel it should remain closed, can you share a bit more of your thinking about why? I don't feel like https://github.com/elastic/elasticsearch/pull/16745 and https://github.com/elastic/elasticsearch/issues/18279#issuecomment-218994716 are talking about the same thing","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/373343816","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-373343816","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":373343816,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MzM0MzgxNg==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-03-15T11:23:01Z","updated_at":"2018-03-15T12:52:55Z","author_association":"MEMBER","body":"@evanv I agree it's not the same thing. As the discussion above indicates, we feel adding hot swappiness on the path level will come at a too high of a price. Elasticsearch currently works on the level of a node - shard copies are spread up across nodes and if a shard fails the master will try to assign it to another node. We can do better there and start tracking failures per node so we can stop allocating to it (we don't do that now) but adding another conceptual layer isn't worth it. LVM or RAID are much more mature solutions to achieve that part. That said, there were a few things we can do that came out of the discussion. One is done and the other is tracked by the another issue, which is why I closed this one.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/373486576","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-373486576","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":373486576,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MzQ4NjU3Ng==","user":{"login":"neuroticnetworks","id":3965137,"node_id":"MDQ6VXNlcjM5NjUxMzc=","avatar_url":"https://avatars0.githubusercontent.com/u/3965137?v=4","gravatar_id":"","url":"https://api.github.com/users/neuroticnetworks","html_url":"https://github.com/neuroticnetworks","followers_url":"https://api.github.com/users/neuroticnetworks/followers","following_url":"https://api.github.com/users/neuroticnetworks/following{/other_user}","gists_url":"https://api.github.com/users/neuroticnetworks/gists{/gist_id}","starred_url":"https://api.github.com/users/neuroticnetworks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/neuroticnetworks/subscriptions","organizations_url":"https://api.github.com/users/neuroticnetworks/orgs","repos_url":"https://api.github.com/users/neuroticnetworks/repos","events_url":"https://api.github.com/users/neuroticnetworks/events{/privacy}","received_events_url":"https://api.github.com/users/neuroticnetworks/received_events","type":"User","site_admin":false},"created_at":"2018-03-15T18:57:10Z","updated_at":"2018-03-15T19:40:40Z","author_association":"NONE","body":"Thank you for explaining. I see what you're saying. \r\n\r\nI feel like this ticket shouldn't be called \"Hot swappable data paths\" and instead be a bug report along the lines of \"ES shouldn't allocate shards to dead disks.\" I think the later is still true, albeit far more complicated, to your point. I also feel like the docs recommending multiple file paths should be caveated that RAID0 might be a better option, depending on your needs (I'm happy to submit an update to the docs along these lines, if you'd be open to accepting it). \r\n\r\nYou're definitely right that ES shouldn't be responsible for replacing RAID or LVM.  Focusing on the issues you did makes sense as a better solution than currently exists. Not to beat a dead horse, but I do feel that ES should be capable of not trying to allocate shards to dead disks. That is how I viewed this original issue, and it sounds like we both agree that https://github.com/elastic/elasticsearch/issues/29008 doesn't quite cover that. Would you be open to adding an issue along the lines of \"ES Shouldn't Allocate Shard to Dead Disks\" and/or renaming this one and orienting the scope of it around that, not hot swappable disks? ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/373487968","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-373487968","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":373487968,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MzQ4Nzk2OA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-03-15T19:01:52Z","updated_at":"2018-03-15T19:01:52Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-core-infra","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/373642310","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-373642310","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":373642310,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MzY0MjMxMA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-03-16T08:41:02Z","updated_at":"2018-03-16T08:41:02Z","author_association":"MEMBER","body":">  I also feel like the docs recommending multiple file paths should be caveated that RAID0 might be a better option, depending on your needs (I'm happy to submit an update to the docs along these lines, if you'd be open to accepting it).\r\n\r\nYes please, though I tried to find what you meant and couldn't. \r\n\r\n> Would you be open to adding an issue along the lines of \"ES Shouldn't Allocate Shard to Dead Disks\" and/or renaming this one and orienting the scope of it around that, not hot swappable disks?\r\n\r\nI think this one https://github.com/elastic/elasticsearch/issues/18417 covers it? If you agree, feel free to comment there.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/373838327","html_url":"https://github.com/elastic/elasticsearch/issues/18279#issuecomment-373838327","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18279","id":373838327,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MzgzODMyNw==","user":{"login":"neuroticnetworks","id":3965137,"node_id":"MDQ6VXNlcjM5NjUxMzc=","avatar_url":"https://avatars0.githubusercontent.com/u/3965137?v=4","gravatar_id":"","url":"https://api.github.com/users/neuroticnetworks","html_url":"https://github.com/neuroticnetworks","followers_url":"https://api.github.com/users/neuroticnetworks/followers","following_url":"https://api.github.com/users/neuroticnetworks/following{/other_user}","gists_url":"https://api.github.com/users/neuroticnetworks/gists{/gist_id}","starred_url":"https://api.github.com/users/neuroticnetworks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/neuroticnetworks/subscriptions","organizations_url":"https://api.github.com/users/neuroticnetworks/orgs","repos_url":"https://api.github.com/users/neuroticnetworks/repos","events_url":"https://api.github.com/users/neuroticnetworks/events{/privacy}","received_events_url":"https://api.github.com/users/neuroticnetworks/received_events","type":"User","site_admin":false},"created_at":"2018-03-16T20:38:46Z","updated_at":"2018-03-16T20:38:46Z","author_association":"NONE","body":"> Yes please, though I tried to find what you meant and couldn't.\r\n\r\nI may be recalling incorrectly, or it may have been a blog post. In any event, I'll poke around and add a note to the docs on \"things to watch out for\" vis a vis multiple data paths.\r\n\r\nhttps://github.com/elastic/elasticsearch/issues/18417 does cover my concern yes. Thanks for taking the time to explain your reasoning on this one. I wasn't following you at first, but it's very clear now what you're thinking and how you're breaking down the work on this task. Much appreciated. ","performed_via_github_app":null}]