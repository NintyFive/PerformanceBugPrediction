{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/10066","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10066/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10066/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10066/events","html_url":"https://github.com/elastic/elasticsearch/issues/10066","id":60741500,"node_id":"MDU6SXNzdWU2MDc0MTUwMA==","number":10066,"title":"OOM causes index corruption on replicas","user":{"login":"avleen","id":539525,"node_id":"MDQ6VXNlcjUzOTUyNQ==","avatar_url":"https://avatars1.githubusercontent.com/u/539525?v=4","gravatar_id":"","url":"https://api.github.com/users/avleen","html_url":"https://github.com/avleen","followers_url":"https://api.github.com/users/avleen/followers","following_url":"https://api.github.com/users/avleen/following{/other_user}","gists_url":"https://api.github.com/users/avleen/gists{/gist_id}","starred_url":"https://api.github.com/users/avleen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/avleen/subscriptions","organizations_url":"https://api.github.com/users/avleen/orgs","repos_url":"https://api.github.com/users/avleen/repos","events_url":"https://api.github.com/users/avleen/events{/privacy}","received_events_url":"https://api.github.com/users/avleen/received_events","type":"User","site_admin":false},"labels":[{"id":152510590,"node_id":"MDU6TGFiZWwxNTI1MTA1OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Recovery","name":":Distributed/Recovery","color":"0e8a16","default":false,"description":"Anything around constructing a new shard, either from a local or a remote source."},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2015-03-11T22:33:53Z","updated_at":"2015-12-05T20:22:50Z","closed_at":"2015-12-05T20:22:50Z","author_association":"NONE","active_lock_reason":null,"body":"v 1.4.3\n\nOur cluster started doing very heavy garbage collection, probably due to an expensive query.\nIn the process of this, all nodes in the cluster OOM'd.\nAfter restarting the entire cluster, we found that a number of replicas were giving errors when trying to bring shards up. This is the log from the master during recovery:\n\n```\n[2015-03-11 22:19:39,311][WARN ][gateway.local            ] [logdbmaster04] [logstash-2015.03.01][1]: failed to list shard stores on node [3-sHwrfRQ4O-GEiloWFyRA]\norg.elasticsearch.action.FailedNodeException: Failed node [3-sHwrfRQ4O-GEiloWFyRA]\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onFailure(TransportNodesOperationAction.java:206)\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$1000(TransportNodesOperationAction.java:97)\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$4.handleException(TransportNodesOperationAction.java:178)\n        at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:185)\n        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:175)\n        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\nCaused by: org.elasticsearch.transport.RemoteTransportException: [logdb26-2][inet[/10.x.x.x:9301]][internal:cluster/nodes/indices/shard/store[n]]\nCaused by: org.elasticsearch.ElasticsearchException: Failed to list store metadata for shard [[logstash-2015.03.01][1]]\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:143)\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:62)\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:278)\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:269)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\nCaused by: org.apache.lucene.index.CorruptIndexException: codec footer mismatch: actual footer=-1593622416 vs expected footer=-1071082520 (resource: SimpleFSIndexInput(path=\"/data/elasticsearch/elasticsearch/nodes/1/indices/logstash-2015.03.01/1/index/_1fq7_es090_0.doc\"))\n        at org.apache.lucene.codecs.CodecUtil.validateFooter(CodecUtil.java:235)\n        at org.apache.lucene.codecs.CodecUtil.retrieveChecksum(CodecUtil.java:228)\n        at org.elasticsearch.index.store.Store$MetadataSnapshot.checksumFromLuceneFile(Store.java:717)\n        at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:613)\n        at org.elasticsearch.index.store.Store$MetadataSnapshot.<init>(Store.java:596)\n        at org.elasticsearch.index.store.Store.readMetadataSnapshot(Store.java:317)\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.listStoreMetaData(TransportNodesListShardStoreMetaData.java:189)\n        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:141)\n        ... 7 more\n```\n\nWhen the node OOM'd, this is the stack trace we saw:\n\n```\n[2015-03-11 21:53:19,001][WARN ][transport.netty          ] [logdb26-2] exception caught on transport layer [[id: 0xef397851, /10.x.x.x:51786 => /10.x.x.x:9300]], closing connection\njava.lang.OutOfMemoryError: Java heap space\n        at org.elasticsearch.common.compress.BufferRecycler.allocDecodeBuffer(BufferRecycler.java:137)\n        at org.elasticsearch.common.compress.lzf.LZFCompressedStreamInput.<init>(LZFCompressedStreamInput.java:46)\n        at org.elasticsearch.common.compress.lzf.LZFCompressor.streamInput(LZFCompressor.java:121)\n        at org.elasticsearch.common.io.stream.CachedStreamInput.cachedHandlesCompressed(CachedStreamInput.java:69)\n        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:104)\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n[2015-03-11 21:57:38,951][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\n```\n\nMy understanding is that when an OOM happens, Elasticsearch shouldn't be writing out data to a live file.\nI think there was a resiliency improvement in 1.4.0 which only switched in files to the live index once they were fully written? This knowledge was gleaned from https://github.com/elastic/elasticsearch/issues/8707.\nThe interesting thing here is that we optimise our indices nightly, and this happened on 241 out of ~3250 shards. \n\nThe cluster is currently recovering all replica shards, which is good (recovery works!) but also not great (the replica shards should not have been corrupt).\nWe also optimize \n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}