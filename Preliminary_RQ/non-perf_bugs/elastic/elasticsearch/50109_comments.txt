[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/564898679","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-564898679","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":564898679,"node_id":"MDEyOklzc3VlQ29tbWVudDU2NDg5ODY3OQ==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-12-12T08:15:44Z","updated_at":"2019-12-12T08:15:44Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-analytics-geo (:Analytics/Aggregations)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/565088288","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-565088288","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":565088288,"node_id":"MDEyOklzc3VlQ29tbWVudDU2NTA4ODI4OA==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2019-12-12T16:47:01Z","updated_at":"2019-12-12T16:47:01Z","author_association":"MEMBER","body":"/cc @not-napoleon mind taking a look at this when you get a chance?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/565101648","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-565101648","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":565101648,"node_id":"MDEyOklzc3VlQ29tbWVudDU2NTEwMTY0OA==","user":{"login":"not-napoleon","id":979663,"node_id":"MDQ6VXNlcjk3OTY2Mw==","avatar_url":"https://avatars0.githubusercontent.com/u/979663?v=4","gravatar_id":"","url":"https://api.github.com/users/not-napoleon","html_url":"https://github.com/not-napoleon","followers_url":"https://api.github.com/users/not-napoleon/followers","following_url":"https://api.github.com/users/not-napoleon/following{/other_user}","gists_url":"https://api.github.com/users/not-napoleon/gists{/gist_id}","starred_url":"https://api.github.com/users/not-napoleon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/not-napoleon/subscriptions","organizations_url":"https://api.github.com/users/not-napoleon/orgs","repos_url":"https://api.github.com/users/not-napoleon/repos","events_url":"https://api.github.com/users/not-napoleon/events{/privacy}","received_events_url":"https://api.github.com/users/not-napoleon/received_events","type":"User","site_admin":false},"created_at":"2019-12-12T17:19:35Z","updated_at":"2019-12-12T17:19:35Z","author_association":"CONTRIBUTOR","body":"Yeah, I definitely see why that would happen.  It's a little unclear to me what the correct behavior would be though.  \r\n\r\nIn this case, it seems like we want to default the end date to `now()`, but I'm not sure that's a good general case behavior.  What happens if the start point is null, do we default that to `now()` as well? What happens if the default generates an invalid range, do we just skip that document?  What about open ended Numeric ranges, or IPs (although, if there's a sensible solution for Dates that doesn't generalize to other ranges, I have no problem just fixing this for `DateHistogram`)\r\n\r\nMissing doesn't currently let you specify a range, but this seems almost like missing behavior.  Just thinking out loud, if we let users specify a Range value for missing, and then partially applied it if the start or end point was null (and used the whole range if the value was actually missing), that might give users some ability to control this.  We'd still need to deal with invalid ranges in the aggregator though.  Unfortunately, getting missing to support complex types would be a pretty major change.  It's something I'd like to do, but I don't have a plan for it right now.\r\n\r\nI'm hesitant to add an explicit option (two options really, for start and end) for this case.  That creates a situation where we have options on the aggregation that only apply sometimes, or worse we end up needing a new aggregation entirely.   One of our goals with adding range support was to minimize API footprint as much as possible, so in that spirit, I'd like to keep adding a new option as a last resort.\r\n\r\n**TL;DR** - if there's a sensible default value, this is a relatively straightforward fix, but I'm not convinced there's a sensible default value.  At an absolute minimum, we should at least just skip these docs instead of trying to create infinite buckets.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/565158133","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-565158133","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":565158133,"node_id":"MDEyOklzc3VlQ29tbWVudDU2NTE1ODEzMw==","user":{"login":"not-napoleon","id":979663,"node_id":"MDQ6VXNlcjk3OTY2Mw==","avatar_url":"https://avatars0.githubusercontent.com/u/979663?v=4","gravatar_id":"","url":"https://api.github.com/users/not-napoleon","html_url":"https://github.com/not-napoleon","followers_url":"https://api.github.com/users/not-napoleon/followers","following_url":"https://api.github.com/users/not-napoleon/following{/other_user}","gists_url":"https://api.github.com/users/not-napoleon/gists{/gist_id}","starred_url":"https://api.github.com/users/not-napoleon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/not-napoleon/subscriptions","organizations_url":"https://api.github.com/users/not-napoleon/orgs","repos_url":"https://api.github.com/users/not-napoleon/repos","events_url":"https://api.github.com/users/not-napoleon/events{/privacy}","received_events_url":"https://api.github.com/users/not-napoleon/received_events","type":"User","site_admin":false},"created_at":"2019-12-12T19:49:09Z","updated_at":"2019-12-12T19:49:09Z","author_association":"CONTRIBUTOR","body":"@jcrapuchettes I ran your test case locally, and it hit a `CircuitBreakerException`, not an actual `OutOfMemoryError` - can you confirm this is what you're seeing?  To be clear,  the circuit breaker should kick in before it actually runs out of memory, and is a well-handled error case.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/565246851","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-565246851","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":565246851,"node_id":"MDEyOklzc3VlQ29tbWVudDU2NTI0Njg1MQ==","user":{"login":"jcrapuchettes","id":4846109,"node_id":"MDQ6VXNlcjQ4NDYxMDk=","avatar_url":"https://avatars3.githubusercontent.com/u/4846109?v=4","gravatar_id":"","url":"https://api.github.com/users/jcrapuchettes","html_url":"https://github.com/jcrapuchettes","followers_url":"https://api.github.com/users/jcrapuchettes/followers","following_url":"https://api.github.com/users/jcrapuchettes/following{/other_user}","gists_url":"https://api.github.com/users/jcrapuchettes/gists{/gist_id}","starred_url":"https://api.github.com/users/jcrapuchettes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jcrapuchettes/subscriptions","organizations_url":"https://api.github.com/users/jcrapuchettes/orgs","repos_url":"https://api.github.com/users/jcrapuchettes/repos","events_url":"https://api.github.com/users/jcrapuchettes/events{/privacy}","received_events_url":"https://api.github.com/users/jcrapuchettes/received_events","type":"User","site_admin":false},"created_at":"2019-12-13T00:21:48Z","updated_at":"2019-12-13T00:21:48Z","author_association":"NONE","body":"@not-napoleon I do get a `CircuitBreakerException`.\r\n\r\nHaving a default value of `now()` for missing would be great for my case, but I can understand the difficulty of dealing with all cases. Is there any way to use a script or something else to mimic the default value in current versions? Our queries are really big and ugly since we can't use the date_histogram.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/565485150","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-565485150","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":565485150,"node_id":"MDEyOklzc3VlQ29tbWVudDU2NTQ4NTE1MA==","user":{"login":"not-napoleon","id":979663,"node_id":"MDQ6VXNlcjk3OTY2Mw==","avatar_url":"https://avatars0.githubusercontent.com/u/979663?v=4","gravatar_id":"","url":"https://api.github.com/users/not-napoleon","html_url":"https://github.com/not-napoleon","followers_url":"https://api.github.com/users/not-napoleon/followers","following_url":"https://api.github.com/users/not-napoleon/following{/other_user}","gists_url":"https://api.github.com/users/not-napoleon/gists{/gist_id}","starred_url":"https://api.github.com/users/not-napoleon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/not-napoleon/subscriptions","organizations_url":"https://api.github.com/users/not-napoleon/orgs","repos_url":"https://api.github.com/users/not-napoleon/repos","events_url":"https://api.github.com/users/not-napoleon/events{/privacy}","received_events_url":"https://api.github.com/users/not-napoleon/received_events","type":"User","site_admin":false},"created_at":"2019-12-13T15:33:34Z","updated_at":"2019-12-13T15:33:34Z","author_association":"CONTRIBUTOR","body":"Scripting would be a good solution, but we don't currently have support for getting range values from scripts in aggregations.  I've opened an issue for this (#50190 ), if you want to follow the discussion there.  Unfortunately, I don't think there's a scripting based work around in 7.4","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/576800078","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-576800078","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":576800078,"node_id":"MDEyOklzc3VlQ29tbWVudDU3NjgwMDA3OA==","user":{"login":"bloche","id":2810059,"node_id":"MDQ6VXNlcjI4MTAwNTk=","avatar_url":"https://avatars3.githubusercontent.com/u/2810059?v=4","gravatar_id":"","url":"https://api.github.com/users/bloche","html_url":"https://github.com/bloche","followers_url":"https://api.github.com/users/bloche/followers","following_url":"https://api.github.com/users/bloche/following{/other_user}","gists_url":"https://api.github.com/users/bloche/gists{/gist_id}","starred_url":"https://api.github.com/users/bloche/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bloche/subscriptions","organizations_url":"https://api.github.com/users/bloche/orgs","repos_url":"https://api.github.com/users/bloche/repos","events_url":"https://api.github.com/users/bloche/events{/privacy}","received_events_url":"https://api.github.com/users/bloche/received_events","type":"User","site_admin":false},"created_at":"2020-01-21T17:50:42Z","updated_at":"2020-01-21T18:04:04Z","author_association":"NONE","body":"I've been experiencing a very similar issue.\r\n\r\nI have ~180M documents spanning the past 4 years or so. Each document has a `date_range` field that could be anywhere from a few days to a few years. When I perform a `date_histogram` on the date range I always get buckets for _every_ date in the range the documents reside in and there isn't a way to restrict to a subset of those buckets.\r\n\r\nWhen querying by month it isn't so bad, I just get more data back than I need (all months between the minimum month in the matching documents and the maximum month). A real problem comes up, however, when I aggregate on days. One query where I'm trying to see data for say 30 days, will actually bucket for _all_ days in the range, which could be upward of 1500 buckets, one for each day in the matching document's range (not my query range). Most of the time this results in a timeout in Elasticsearch and I never actually get data back.\r\n\r\nThis issue is preventing me from adopting the `date_histogram` aggregation for `date_range` fields. If there were something like `extended_bounds` that could restrict the aggregation from looking at buckets that don't fall in the bounded range, that would solve both my problem and the OP's problem, I believe.\r\n\r\n#### Example\r\n\r\nAn example query I would be running (using the same field name as the OP) would be something like this:\r\n\r\n```json\r\n{\r\n    \"track_total_hits\": true,\r\n    \"query\": {\r\n        \"range\": {\r\n            \"active_range\": {\r\n                \"gte\": \"2016-11-01\",\r\n                \"lte\": \"2016-11-30\"\r\n            }\r\n        }\r\n    },\r\n    \"size\": 0,\r\n    \"aggs\": {\r\n        \"timeseries\": {\r\n            \"date_histogram\": {\r\n                \"field\": \"active_range\",\r\n                \"calendar_interval\": \"day\",\r\n                \"min_doc_count\": 1,\r\n                \"extended_bounds\": {\r\n                    \"min\": \"2016-11-01\",\r\n                    \"max\": \"2016-11-30\"\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nSince documents back in 2016 could be \"active\" today the ES query is generating all buckets from whatever the first day in the document range is all the way to today.\r\n\r\nIt _feels_ like `extended_bounds` could tell Elasticsearch not to aggregate buckets outside the specified range, but the documentation says it explicitly does not do that. So either `extended_bounds` could start doing that (breaking change) or we could introduce another field `bounds` or something like that, which would be a hard enforcement at aggregation time.\r\n\r\nA script might be a good solution for this, but it feels like a field similar to `extended_bounds` would make a bit more sense for this type of aggregation. Or if there were a way to tell bucket aggregations to ignore certain buckets at aggregation time (like a bucket selector aggregation, only operating before aggregation instead of after), which could be useful in more than just these kind of range histogram aggregations.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/578264809","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-578264809","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":578264809,"node_id":"MDEyOklzc3VlQ29tbWVudDU3ODI2NDgwOQ==","user":{"login":"not-napoleon","id":979663,"node_id":"MDQ6VXNlcjk3OTY2Mw==","avatar_url":"https://avatars0.githubusercontent.com/u/979663?v=4","gravatar_id":"","url":"https://api.github.com/users/not-napoleon","html_url":"https://github.com/not-napoleon","followers_url":"https://api.github.com/users/not-napoleon/followers","following_url":"https://api.github.com/users/not-napoleon/following{/other_user}","gists_url":"https://api.github.com/users/not-napoleon/gists{/gist_id}","starred_url":"https://api.github.com/users/not-napoleon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/not-napoleon/subscriptions","organizations_url":"https://api.github.com/users/not-napoleon/orgs","repos_url":"https://api.github.com/users/not-napoleon/repos","events_url":"https://api.github.com/users/not-napoleon/events{/privacy}","received_events_url":"https://api.github.com/users/not-napoleon/received_events","type":"User","site_admin":false},"created_at":"2020-01-24T19:20:59Z","updated_at":"2020-01-24T19:20:59Z","author_association":"CONTRIBUTOR","body":"I discussed this with @polyfractal today, and we're not opposed to adding a flag to make the `extended_bounds` values hard limits on what buckets are computed.  Something like:\r\n\r\n```             \r\n \"extended_bounds\": {\r\n                    \"min\": \"2016-11-01\",\r\n                    \"max\": \"2016-11-30\",\r\n                    \"hard_limit\": true\r\n                }\r\n```\r\n(exact option name still TBD) Which would return exactly all buckets between the given min and max.  This would still include the current behavior of adding empty buckets to fill out the range,  but also serve to clip the results that fall outside of that range.  This is not entirely useless for non-range cases either, as it will be a cleaner syntax than the current recommendation of adding a filter query.  Would something along those lines solve your difficulty?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/579033597","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-579033597","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":579033597,"node_id":"MDEyOklzc3VlQ29tbWVudDU3OTAzMzU5Nw==","user":{"login":"bloche","id":2810059,"node_id":"MDQ6VXNlcjI4MTAwNTk=","avatar_url":"https://avatars3.githubusercontent.com/u/2810059?v=4","gravatar_id":"","url":"https://api.github.com/users/bloche","html_url":"https://github.com/bloche","followers_url":"https://api.github.com/users/bloche/followers","following_url":"https://api.github.com/users/bloche/following{/other_user}","gists_url":"https://api.github.com/users/bloche/gists{/gist_id}","starred_url":"https://api.github.com/users/bloche/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bloche/subscriptions","organizations_url":"https://api.github.com/users/bloche/orgs","repos_url":"https://api.github.com/users/bloche/repos","events_url":"https://api.github.com/users/bloche/events{/privacy}","received_events_url":"https://api.github.com/users/bloche/received_events","type":"User","site_admin":false},"created_at":"2020-01-28T01:14:51Z","updated_at":"2020-01-28T01:14:51Z","author_association":"NONE","body":"@not-napoleon that solution would fit my use case perfectly. Thanks for taking the time to discuss this, it'll simplify what I'm currently doing significantly.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/604300580","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-604300580","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":604300580,"node_id":"MDEyOklzc3VlQ29tbWVudDYwNDMwMDU4MA==","user":{"login":"krlm","id":1184144,"node_id":"MDQ6VXNlcjExODQxNDQ=","avatar_url":"https://avatars1.githubusercontent.com/u/1184144?v=4","gravatar_id":"","url":"https://api.github.com/users/krlm","html_url":"https://github.com/krlm","followers_url":"https://api.github.com/users/krlm/followers","following_url":"https://api.github.com/users/krlm/following{/other_user}","gists_url":"https://api.github.com/users/krlm/gists{/gist_id}","starred_url":"https://api.github.com/users/krlm/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krlm/subscriptions","organizations_url":"https://api.github.com/users/krlm/orgs","repos_url":"https://api.github.com/users/krlm/repos","events_url":"https://api.github.com/users/krlm/events{/privacy}","received_events_url":"https://api.github.com/users/krlm/received_events","type":"User","site_admin":false},"created_at":"2020-03-26T08:39:16Z","updated_at":"2020-03-26T08:39:16Z","author_association":"NONE","body":"@not-napoleon would it still count in values with open end in all buckets valid for given start date or filter them out? ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/604421944","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-604421944","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":604421944,"node_id":"MDEyOklzc3VlQ29tbWVudDYwNDQyMTk0NA==","user":{"login":"polyfractal","id":1224228,"node_id":"MDQ6VXNlcjEyMjQyMjg=","avatar_url":"https://avatars1.githubusercontent.com/u/1224228?v=4","gravatar_id":"","url":"https://api.github.com/users/polyfractal","html_url":"https://github.com/polyfractal","followers_url":"https://api.github.com/users/polyfractal/followers","following_url":"https://api.github.com/users/polyfractal/following{/other_user}","gists_url":"https://api.github.com/users/polyfractal/gists{/gist_id}","starred_url":"https://api.github.com/users/polyfractal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/polyfractal/subscriptions","organizations_url":"https://api.github.com/users/polyfractal/orgs","repos_url":"https://api.github.com/users/polyfractal/repos","events_url":"https://api.github.com/users/polyfractal/events{/privacy}","received_events_url":"https://api.github.com/users/polyfractal/received_events","type":"User","site_admin":false},"created_at":"2020-03-26T13:10:22Z","updated_at":"2020-03-26T13:10:36Z","author_association":"MEMBER","body":"@krlm: @not-napoleon can correct me if I get this wrong, but I believe any unbounded interval on the \"edge\" will still be included in the final bucket on that side.  It will just stop the aggregation from continuing to \"fill out\" the histogram with buckets that match the unbounded, infinite interval.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/604439149","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-604439149","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":604439149,"node_id":"MDEyOklzc3VlQ29tbWVudDYwNDQzOTE0OQ==","user":{"login":"not-napoleon","id":979663,"node_id":"MDQ6VXNlcjk3OTY2Mw==","avatar_url":"https://avatars0.githubusercontent.com/u/979663?v=4","gravatar_id":"","url":"https://api.github.com/users/not-napoleon","html_url":"https://github.com/not-napoleon","followers_url":"https://api.github.com/users/not-napoleon/followers","following_url":"https://api.github.com/users/not-napoleon/following{/other_user}","gists_url":"https://api.github.com/users/not-napoleon/gists{/gist_id}","starred_url":"https://api.github.com/users/not-napoleon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/not-napoleon/subscriptions","organizations_url":"https://api.github.com/users/not-napoleon/orgs","repos_url":"https://api.github.com/users/not-napoleon/repos","events_url":"https://api.github.com/users/not-napoleon/events{/privacy}","received_events_url":"https://api.github.com/users/not-napoleon/received_events","type":"User","site_admin":false},"created_at":"2020-03-26T13:44:38Z","updated_at":"2020-03-26T13:44:38Z","author_association":"CONTRIBUTOR","body":"Yeah, that's more or less what I was thinking.  Currently it just adds buckets until it adds a bucket that contains the end point.  If a hard limit extended bounds were set, it would stop at that limit instead.  Basically, you'd set the hard limit and get exactly those buckets, even if your ranges fell outside of them.  You'd still have the existing extend_bounds behavior where we add in empty buckets as needed to fit that range, too.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/605286358","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-605286358","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":605286358,"node_id":"MDEyOklzc3VlQ29tbWVudDYwNTI4NjM1OA==","user":{"login":"krlm","id":1184144,"node_id":"MDQ6VXNlcjExODQxNDQ=","avatar_url":"https://avatars1.githubusercontent.com/u/1184144?v=4","gravatar_id":"","url":"https://api.github.com/users/krlm","html_url":"https://github.com/krlm","followers_url":"https://api.github.com/users/krlm/followers","following_url":"https://api.github.com/users/krlm/following{/other_user}","gists_url":"https://api.github.com/users/krlm/gists{/gist_id}","starred_url":"https://api.github.com/users/krlm/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krlm/subscriptions","organizations_url":"https://api.github.com/users/krlm/orgs","repos_url":"https://api.github.com/users/krlm/repos","events_url":"https://api.github.com/users/krlm/events{/privacy}","received_events_url":"https://api.github.com/users/krlm/received_events","type":"User","site_admin":false},"created_at":"2020-03-27T19:49:46Z","updated_at":"2020-03-27T19:49:46Z","author_association":"NONE","body":"@polyfractal @not-napoleon,  thanks for the info. I forgot that by default there's no upper and lower boundaries (which I've defined in the filter part of my query) for date histogram aggregation. This should do the job then.  ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/638942515","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-638942515","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":638942515,"node_id":"MDEyOklzc3VlQ29tbWVudDYzODk0MjUxNQ==","user":{"login":"consulthys","id":1280019,"node_id":"MDQ6VXNlcjEyODAwMTk=","avatar_url":"https://avatars2.githubusercontent.com/u/1280019?v=4","gravatar_id":"","url":"https://api.github.com/users/consulthys","html_url":"https://github.com/consulthys","followers_url":"https://api.github.com/users/consulthys/followers","following_url":"https://api.github.com/users/consulthys/following{/other_user}","gists_url":"https://api.github.com/users/consulthys/gists{/gist_id}","starred_url":"https://api.github.com/users/consulthys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/consulthys/subscriptions","organizations_url":"https://api.github.com/users/consulthys/orgs","repos_url":"https://api.github.com/users/consulthys/repos","events_url":"https://api.github.com/users/consulthys/events{/privacy}","received_events_url":"https://api.github.com/users/consulthys/received_events","type":"User","site_admin":false},"created_at":"2020-06-04T15:49:06Z","updated_at":"2020-06-04T15:49:06Z","author_association":"CONTRIBUTOR","body":"I've had the same issue and would love to see support for date histograms on open-ended date range fields.\r\n\r\nThe way I'm dealing with this right now is by using scripting (and #50190 will definitely help here, too). It's clunky, it's not flexible but it works for what I need to do.\r\n\r\nIn my documents, I have a `date_range` field (say `period`) but also the two date fields for the start and end of the date range and sometimes the latter can be null. So in my `date_histogram` aggregation I can deal with this open-ended situation by defaulting to `now` if the end date is missing, i.e. if the period is open-ended towards the future (but it would work the same towards the past):\r\n\r\n```\r\nPOST my-index/_search\r\n{\r\n  \"size\": 0,\r\n  \"query\": {\r\n    \"match_all\": {}\r\n  },\r\n  \"aggs\": {\r\n    \"histo\": {\r\n      \"date_histogram\": {\r\n        \"script\": \"\"\"\r\n        def start = doc['start_date'].value;\r\n        def end = null;\r\n        if (doc['end_date'].size() > 0) {\r\n          end = doc['end_date'].value;\r\n        } else {\r\n          // default to now\r\n          end = Instant.ofEpochMilli(new Date().getTime()).atZone(ZoneId.of(\"UTC\"));\r\n        }\r\n\r\n        // build buckets array\r\n        def buckets = [start];        \r\n        def months = ChronoUnit.MONTHS.between(start, end) + 1;\r\n        while (months > 0) {\r\n          start = start.plusMonths(1);\r\n          buckets.add(start);\r\n          months--;\r\n        }\r\n        \r\n        // return the date buckets\r\n        return buckets;\r\n        \"\"\",\r\n        \"interval\": \"month\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nAs I said, it's clunky, but it does return all the date buckets I'm interested in (either from start to end or from start to now if end is null). Of course, depending on the interval type, the code needs to adapted.\r\n\r\nJust thought I'd share if that can help anyone until a real fix is available.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/651935792","html_url":"https://github.com/elastic/elasticsearch/issues/50109#issuecomment-651935792","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50109","id":651935792,"node_id":"MDEyOklzc3VlQ29tbWVudDY1MTkzNTc5Mg==","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2020-06-30T17:27:25Z","updated_at":"2020-06-30T17:27:25Z","author_association":"MEMBER","body":"Here is an interesting question about hard bounds. With extended bounds the max bound is inclusive. So, if we have an hourly histogram and max is 10 hours, the 10:00:00-10:59:59 bucket will be included, which might make sense since we are extending the bounds. The hard bounds, on the other hands, are limiting. So, if max hard bound is 10:00:00 the bucket 10:00:00-10:59:59 is mostly outside of the bounds. ","performed_via_github_app":null}]