[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/29447959","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-29447959","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":29447959,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NDQ3OTU5","user":{"login":"spinscale","id":667544,"node_id":"MDQ6VXNlcjY2NzU0NA==","avatar_url":"https://avatars2.githubusercontent.com/u/667544?v=4","gravatar_id":"","url":"https://api.github.com/users/spinscale","html_url":"https://github.com/spinscale","followers_url":"https://api.github.com/users/spinscale/followers","following_url":"https://api.github.com/users/spinscale/following{/other_user}","gists_url":"https://api.github.com/users/spinscale/gists{/gist_id}","starred_url":"https://api.github.com/users/spinscale/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spinscale/subscriptions","organizations_url":"https://api.github.com/users/spinscale/orgs","repos_url":"https://api.github.com/users/spinscale/repos","events_url":"https://api.github.com/users/spinscale/events{/privacy}","received_events_url":"https://api.github.com/users/spinscale/received_events","type":"User","site_admin":false},"created_at":"2013-11-28T08:55:14Z","updated_at":"2013-11-28T08:55:14Z","author_association":"MEMBER","body":"Hey there,\n\nchecking out your hot threads output, the last third of the output is the important part, as you can clearly see, that a lot of queries are eating up your CPU.\n\nDo you run some complex queries including fuzzy support. Is your system acting different if you do not query it at all?\n\nAnd last question to get a better overview: What is your setup? number of nodes, etc? Looks like the cluster state might not be up to date on that node, where the client is trying to rejoin (due to load).. but this is just a wild assumption, which is not backed by facts.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/29448052","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-29448052","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":29448052,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NDQ4MDUy","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2013-11-28T08:57:12Z","updated_at":"2013-11-28T08:57:12Z","author_association":"CONTRIBUTOR","body":"I agree with @spinscale this seems very much like you shooting some queries against elasticsearch and they cause load? Can you identify if that is abnormal?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/29483626","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-29483626","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":29483626,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NDgzNjI2","user":{"login":"ydnitin","id":3760080,"node_id":"MDQ6VXNlcjM3NjAwODA=","avatar_url":"https://avatars0.githubusercontent.com/u/3760080?v=4","gravatar_id":"","url":"https://api.github.com/users/ydnitin","html_url":"https://github.com/ydnitin","followers_url":"https://api.github.com/users/ydnitin/followers","following_url":"https://api.github.com/users/ydnitin/following{/other_user}","gists_url":"https://api.github.com/users/ydnitin/gists{/gist_id}","starred_url":"https://api.github.com/users/ydnitin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ydnitin/subscriptions","organizations_url":"https://api.github.com/users/ydnitin/orgs","repos_url":"https://api.github.com/users/ydnitin/repos","events_url":"https://api.github.com/users/ydnitin/events{/privacy}","received_events_url":"https://api.github.com/users/ydnitin/received_events","type":"User","site_admin":false},"created_at":"2013-11-28T20:17:44Z","updated_at":"2013-11-29T01:59:01Z","author_association":"NONE","body":"I suspected the same so I dropped all traffic to ES and the load came down right away and then enabled the traffic again load jumped up. My users query ES via kibana interface. I am not familiar with fuzzy query concept. ES is totally new for me. \nLoad came down after I posted this last night. I guess user closed his kibana session? \nIs there a tool to look at the running queries, like pgmonitor?\nIs there a simplified query syntax documentation out there? http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-queries.html this one contains too much information for my users. \n\nI am running just one node, two shards.\nServer: Physical \nRAM: 24G\nHeapSize set: 8G\nCPU: 8\n\nI am to add another node to ES. Any suggestions as to how I should profile my ES setup to order new hardware?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/29501551","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-29501551","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":29501551,"node_id":"MDEyOklzc3VlQ29tbWVudDI5NTAxNTUx","user":{"login":"spinscale","id":667544,"node_id":"MDQ6VXNlcjY2NzU0NA==","avatar_url":"https://avatars2.githubusercontent.com/u/667544?v=4","gravatar_id":"","url":"https://api.github.com/users/spinscale","html_url":"https://github.com/spinscale","followers_url":"https://api.github.com/users/spinscale/followers","following_url":"https://api.github.com/users/spinscale/following{/other_user}","gists_url":"https://api.github.com/users/spinscale/gists{/gist_id}","starred_url":"https://api.github.com/users/spinscale/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spinscale/subscriptions","organizations_url":"https://api.github.com/users/spinscale/orgs","repos_url":"https://api.github.com/users/spinscale/repos","events_url":"https://api.github.com/users/spinscale/events{/privacy}","received_events_url":"https://api.github.com/users/spinscale/received_events","type":"User","site_admin":false},"created_at":"2013-11-29T08:13:38Z","updated_at":"2013-11-29T08:13:38Z","author_association":"MEMBER","body":"Hey,\n\nthere is a slow search log mechanism in elasticsearch. See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-slowlog.html#search-slow-log\n\nFurthermore you should talk to the kibana users and see what kind of queries they execute (almost every kibana widget can reveal the query being executed). Also make sure, that they are not reloading their data in kibana too often, maybe that already helps a bit (without getting to the cause of the problem, maybe it is just one query, you would have to try out the queries one by one).\n\nThere is no possibility to give any hardware recommendations without knowing data, configuration, mapping, documents, query load, index load etc... \n\nAdding more nodes and replicated the same data to them most likely will allow to at least spread the load.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/34865830","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-34865830","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":34865830,"node_id":"MDEyOklzc3VlQ29tbWVudDM0ODY1ODMw","user":{"login":"s3k","id":977800,"node_id":"MDQ6VXNlcjk3NzgwMA==","avatar_url":"https://avatars2.githubusercontent.com/u/977800?v=4","gravatar_id":"","url":"https://api.github.com/users/s3k","html_url":"https://github.com/s3k","followers_url":"https://api.github.com/users/s3k/followers","following_url":"https://api.github.com/users/s3k/following{/other_user}","gists_url":"https://api.github.com/users/s3k/gists{/gist_id}","starred_url":"https://api.github.com/users/s3k/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s3k/subscriptions","organizations_url":"https://api.github.com/users/s3k/orgs","repos_url":"https://api.github.com/users/s3k/repos","events_url":"https://api.github.com/users/s3k/events{/privacy}","received_events_url":"https://api.github.com/users/s3k/received_events","type":"User","site_admin":false},"created_at":"2014-02-12T12:55:04Z","updated_at":"2014-02-12T12:55:04Z","author_association":"NONE","body":"I have same problem with high cpu usage.\n(mb pro, osx, standard java 7, 2 core, 2.5Ghz, i5)\n\nHere some tips:\nOn my local machine i set in config/elasticsearch.yml\n\n   index.number_of_shards: 1\n   index.number_of_replicas: 0\n\nFor 1 index with 185k docs my cpu load is 2.5-5% for ES java process\n\nAlso plugins makes HUGE performance reduce. \nFor me that is Marvel and analysis-morphology plugins. When i turned it off, that has given me additional cpu resources for 2 times, maybe 4.\n\nI think that yml config is the solution.\n\nP.S Also i have server (Ubuntu 12.04, openjdk 7, 2 xeon cores for 2.4Ghz and 2Gb ram) with default config, with plugins (marvel and analysis-morphology) which works fine for 1 500 000 docs on index.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/48728631","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-48728631","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":48728631,"node_id":"MDEyOklzc3VlQ29tbWVudDQ4NzI4NjMx","user":{"login":"davidgraca","id":1884487,"node_id":"MDQ6VXNlcjE4ODQ0ODc=","avatar_url":"https://avatars3.githubusercontent.com/u/1884487?v=4","gravatar_id":"","url":"https://api.github.com/users/davidgraca","html_url":"https://github.com/davidgraca","followers_url":"https://api.github.com/users/davidgraca/followers","following_url":"https://api.github.com/users/davidgraca/following{/other_user}","gists_url":"https://api.github.com/users/davidgraca/gists{/gist_id}","starred_url":"https://api.github.com/users/davidgraca/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidgraca/subscriptions","organizations_url":"https://api.github.com/users/davidgraca/orgs","repos_url":"https://api.github.com/users/davidgraca/repos","events_url":"https://api.github.com/users/davidgraca/events{/privacy}","received_events_url":"https://api.github.com/users/davidgraca/received_events","type":"User","site_admin":false},"created_at":"2014-07-11T13:17:44Z","updated_at":"2014-07-11T13:17:44Z","author_association":"NONE","body":"Don't forget that if you installed Marvel to delete all indexes after you removed, since it doesn't do that.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/66324793","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-66324793","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":66324793,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzI0Nzkz","user":{"login":"gentunian","id":1090113,"node_id":"MDQ6VXNlcjEwOTAxMTM=","avatar_url":"https://avatars1.githubusercontent.com/u/1090113?v=4","gravatar_id":"","url":"https://api.github.com/users/gentunian","html_url":"https://github.com/gentunian","followers_url":"https://api.github.com/users/gentunian/followers","following_url":"https://api.github.com/users/gentunian/following{/other_user}","gists_url":"https://api.github.com/users/gentunian/gists{/gist_id}","starred_url":"https://api.github.com/users/gentunian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gentunian/subscriptions","organizations_url":"https://api.github.com/users/gentunian/orgs","repos_url":"https://api.github.com/users/gentunian/repos","events_url":"https://api.github.com/users/gentunian/events{/privacy}","received_events_url":"https://api.github.com/users/gentunian/received_events","type":"User","site_admin":false},"created_at":"2014-12-09T17:46:06Z","updated_at":"2014-12-09T17:46:06Z","author_association":"NONE","body":"Same issue with marvel plugin. I kept getting out of heap space at a minute of starting (1GB heap). I removed the plugin and all indices and now cpu is fine, here a graph showing the improved cpu USER time:\n![screenshot - 12092014 - 02 41 11 pm](https://cloud.githubusercontent.com/assets/1090113/5362471/06426490-7fb2-11e4-9647-19947aa2f99f.png)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/66386503","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-66386503","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":66386503,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mzg2NTAz","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-12-10T00:44:54Z","updated_at":"2014-12-10T00:44:54Z","author_association":"MEMBER","body":"@gentunian did you have the errors while the marvel UI was open in a browser, or also without. Would be great to get the out put of http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html#cluster-nodes-hot-threads to see what the node is doing. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/66388984","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-66388984","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":66388984,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mzg4OTg0","user":{"login":"gentunian","id":1090113,"node_id":"MDQ6VXNlcjEwOTAxMTM=","avatar_url":"https://avatars1.githubusercontent.com/u/1090113?v=4","gravatar_id":"","url":"https://api.github.com/users/gentunian","html_url":"https://github.com/gentunian","followers_url":"https://api.github.com/users/gentunian/followers","following_url":"https://api.github.com/users/gentunian/following{/other_user}","gists_url":"https://api.github.com/users/gentunian/gists{/gist_id}","starred_url":"https://api.github.com/users/gentunian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gentunian/subscriptions","organizations_url":"https://api.github.com/users/gentunian/orgs","repos_url":"https://api.github.com/users/gentunian/repos","events_url":"https://api.github.com/users/gentunian/events{/privacy}","received_events_url":"https://api.github.com/users/gentunian/received_events","type":"User","site_admin":false},"created_at":"2014-12-10T01:10:41Z","updated_at":"2014-12-10T01:11:24Z","author_association":"NONE","body":"@bleskes on both cases I ran out of heap space. I could see in `elasticsearch.log` every 2 or 3 seconds what I assume was GC calls (I can revise the log tomorrow at work if you want to). \n\n`curl -s -XGET 'localhost:9200/_cat/thread_pool?v'` was all 0s for every column.\n\nI had a lot of shards, maybe more than 500 (logstash defaults to 5 shards for every index and I have more than 100).\n\nIn the marvel stats, the marvel indices were on top of each column (index rate, search query, etc. Sorry if the names are wrong, I'm typing this from memory).\n\nAll this in QA. Tomorrow after finding out the CPU usage, I'm planning a production deployment.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/66624784","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-66624784","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":66624784,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NjI0Nzg0","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-12-11T14:21:24Z","updated_at":"2014-12-11T14:21:24Z","author_association":"MEMBER","body":"@gentunian I can think of a couple of things that _may_ be going on. Maybe open a new issue with your details, as your primary concern is memory and not CPU?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/103621012","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-103621012","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":103621012,"node_id":"MDEyOklzc3VlQ29tbWVudDEwMzYyMTAxMg==","user":{"login":"ACV2","id":6947506,"node_id":"MDQ6VXNlcjY5NDc1MDY=","avatar_url":"https://avatars0.githubusercontent.com/u/6947506?v=4","gravatar_id":"","url":"https://api.github.com/users/ACV2","html_url":"https://github.com/ACV2","followers_url":"https://api.github.com/users/ACV2/followers","following_url":"https://api.github.com/users/ACV2/following{/other_user}","gists_url":"https://api.github.com/users/ACV2/gists{/gist_id}","starred_url":"https://api.github.com/users/ACV2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ACV2/subscriptions","organizations_url":"https://api.github.com/users/ACV2/orgs","repos_url":"https://api.github.com/users/ACV2/repos","events_url":"https://api.github.com/users/ACV2/events{/privacy}","received_events_url":"https://api.github.com/users/ACV2/received_events","type":"User","site_admin":false},"created_at":"2015-05-19T18:15:28Z","updated_at":"2015-05-19T18:15:28Z","author_association":"NONE","body":"I'm using elasticsearch 1.5 \n\nand it is working perfectly the most part of the time, but everyday at the same time it becomes crazy, CPU % goes to ~70% when the average is around 3-5% there are SUPER servers with 32GB reserved for lucene, swap it is lock and clearing the cache doesn't solve the problem (it doesn't take down the heap mem) \n\nSettings: \n\n3 servers (nodes) 32 cores and 128GB RAM each \n2 buckets (indices) one with ~18 million documents (this one doesn't receive updates pretty often just indexing new docs) the other one have around 7-8 million documents but we are constantly bombarding it with updates search delete and indexing as well \n\nThe best distribution for our structure, was to have only 1 shard per node with not replicas, we can afford to have a % of the data off for few seconds, that will be back as soon as the server get online again, and this process is fast enough since it doesn't need to relocate anything. previously we used to have 3 shards with 1 replica, but the issue mentioned above occurs as well, so is easy to figure it out that the problem is not related with the distribution. \n\nThings that I already tried, \n\nMerging, i try to use the Optimize API trying to give less load to the schedule merge, but actually the merging process takes a lot of R/W of the disk but it doesn't affect substantially the mem or the CPU load. \n\nFlushing, I tried to flush with long and shot intervals, and the results were the same nothing \n![pic1-1](https://cloud.githubusercontent.com/assets/6947506/7710304/c5b74bc6-fe2c-11e4-819f-fc5c46bcd742.PNG)\n![pic2-1](https://cloud.githubusercontent.com/assets/6947506/7710313/d4861d12-fe2c-11e4-8577-06628fb7966a.PNG)\n![pic2-2](https://cloud.githubusercontent.com/assets/6947506/7710314/d489d43e-fe2c-11e4-9de4-649a9ac93a56.PNG)\n![pic1-2](https://cloud.githubusercontent.com/assets/6947506/7710315/d48cffce-fe2c-11e4-81ac-f199eb91405e.PNG)\n\nchanged, since flushing affects directly the merging process and as mentioned above, merging process doesn't takes that much of the CPU or mem usage. \n\nmanaging the cache, clearing it manually but it doesn't seems to take the cpu load to normal state not even for a moment. \n\nHere is the most of the elasticsearch.yml configs \n\n# Force all memory to be locked, forcing the JVM to never swap\n\nbootstrap.mlockall: true\n\n## Threadpool Settings\n\n# Search pool\n\nthreadpool.search.type: fixed\nthreadpool.search.size: 20\nthreadpool.search.queue_size: 200\n\n# Bulk pool\n\nthreadpool.bulk.type: fixed\nthreadpool.bulk.size: 60\nthreadpool.bulk.queue_size: 3000\n\n# Index pool\n\nthreadpool.index.type: fixed\nthreadpool.index.size: 20\nthreadpool.index.queue_size: 1000\n\n# Indices settings\n\nindices.memory.index_buffer_size: 30%\nindices.memory.min_shard_index_buffer_size: 12mb\nindices.memory.min_index_buffer_size: 96mb\n\n# Cache Sizes\n\nindices.fielddata.cache.size: 30%\n#indices.fielddata.cache.expire: 6h #will be depreciated & Dev recomend not to use it\nindices.cache.filter.size: 30%\n#indices.cache.filter.expire: 6h #will be depreciated & Dev recomend not to use it\n\n# Indexing Settings for Writes\n\nindex.refresh_interval: 30s\n#index.translog.flush_threshold_ops: 50000\n#index.translog.flush_threshold_size: 1024mb\nindex.translog.flush_threshold_period: 5m\nindex.merge.scheduler.max_thread_count: 1\n\nhere is the stats when the server is in a normal state: \nnode_stats_normal.txt\n\nNode stats during the problem. \nnode_stats.txt\n\nI will appreciate any help or discussion that can point me in the right direction to get rid of this behavior \n\nthanks in advance.. \n\nRegards, \n\nDaniel\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/105225543","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-105225543","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":105225543,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNTIyNTU0Mw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-05-25T12:34:02Z","updated_at":"2015-05-25T12:34:02Z","author_association":"CONTRIBUTOR","body":"Hi @ACV2 \n\nThis issue is closed.  I suggest you follow the advice given above (ie check the hot threads output, enable the slow query log), and see what happens at the time of day when your cluster goes wild.  If the answer isn't immediately obvious, I'd suggest asking about it in the forum: discuss.elastic.co\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/109799144","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-109799144","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":109799144,"node_id":"MDEyOklzc3VlQ29tbWVudDEwOTc5OTE0NA==","user":{"login":"travisbell","id":24766,"node_id":"MDQ6VXNlcjI0NzY2","avatar_url":"https://avatars1.githubusercontent.com/u/24766?v=4","gravatar_id":"","url":"https://api.github.com/users/travisbell","html_url":"https://github.com/travisbell","followers_url":"https://api.github.com/users/travisbell/followers","following_url":"https://api.github.com/users/travisbell/following{/other_user}","gists_url":"https://api.github.com/users/travisbell/gists{/gist_id}","starred_url":"https://api.github.com/users/travisbell/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/travisbell/subscriptions","organizations_url":"https://api.github.com/users/travisbell/orgs","repos_url":"https://api.github.com/users/travisbell/repos","events_url":"https://api.github.com/users/travisbell/events{/privacy}","received_events_url":"https://api.github.com/users/travisbell/received_events","type":"User","site_admin":false},"created_at":"2015-06-07T20:56:50Z","updated_at":"2015-06-07T20:58:51Z","author_association":"NONE","body":"@AVC2 Did you learn anything since your post about this? We're seeing more or less, the same problem. Sometimes daily, sometimes only one a week (it's that random) one of our ES hosts decides to go crazy. We usually restart the instance when we see this because it affects our end users response times but it's starting to be a problem.\n\nIn our situation, these hosts aren't idle, they're trucking along with 25-30% CPU and then BAM, 70-100%. _Sometimes_ they correct themselves after 24-48 hours, other times they don't.\n\nI was just curious is you learned anything that could help point us in the right direction. I'll see about checking the hot threads output and slowlogs.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/111453868","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-111453868","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":111453868,"node_id":"MDEyOklzc3VlQ29tbWVudDExMTQ1Mzg2OA==","user":{"login":"ACV2","id":6947506,"node_id":"MDQ6VXNlcjY5NDc1MDY=","avatar_url":"https://avatars0.githubusercontent.com/u/6947506?v=4","gravatar_id":"","url":"https://api.github.com/users/ACV2","html_url":"https://github.com/ACV2","followers_url":"https://api.github.com/users/ACV2/followers","following_url":"https://api.github.com/users/ACV2/following{/other_user}","gists_url":"https://api.github.com/users/ACV2/gists{/gist_id}","starred_url":"https://api.github.com/users/ACV2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ACV2/subscriptions","organizations_url":"https://api.github.com/users/ACV2/orgs","repos_url":"https://api.github.com/users/ACV2/repos","events_url":"https://api.github.com/users/ACV2/events{/privacy}","received_events_url":"https://api.github.com/users/ACV2/received_events","type":"User","site_admin":false},"created_at":"2015-06-12T11:08:53Z","updated_at":"2015-06-12T11:08:53Z","author_association":"NONE","body":"Hey there, @Travis. Actually it was a journey... Elastic Search is everything but human friendly, after this post where I actually checked every single configuration trying to optimize the performance to avoid this behavior, with not success... I reduce the query time from 12 sec to 2-3 in a period of 10-15 minutes still huge and unacceptable, however we are working to optimize our back end to stabilize the performance, that being said.. My findings:\n\n-merge process\n-flush process\n-background process (script) bombarding the server with queries slowing it down \n\nCan you post your server configurations so I can help you a little bit more :)\n\nRegards\nDaniel \n\n> On 7/6/2015, at 16:27, Travis Bell notifications@github.com wrote:\n> \n> @AVC2 Did you learn anything since your post about this? We're seeing more or less, the same problem. Sometimes daily, sometimes only one a week (it's that random) one of our ES hosts decides to go crazy. We usually restart the instance when we see this because it affects our end users response times but it's starting to be a problem.\n> \n> I was just curious is you learned anything that could help point us in the right direction. I'll see about checking the hot threads output and slowlogs.\n> \n> —\n> Reply to this email directly or view it on GitHub.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/114912502","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-114912502","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":114912502,"node_id":"MDEyOklzc3VlQ29tbWVudDExNDkxMjUwMg==","user":{"login":"travisbell","id":24766,"node_id":"MDQ6VXNlcjI0NzY2","avatar_url":"https://avatars1.githubusercontent.com/u/24766?v=4","gravatar_id":"","url":"https://api.github.com/users/travisbell","html_url":"https://github.com/travisbell","followers_url":"https://api.github.com/users/travisbell/followers","following_url":"https://api.github.com/users/travisbell/following{/other_user}","gists_url":"https://api.github.com/users/travisbell/gists{/gist_id}","starred_url":"https://api.github.com/users/travisbell/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/travisbell/subscriptions","organizations_url":"https://api.github.com/users/travisbell/orgs","repos_url":"https://api.github.com/users/travisbell/repos","events_url":"https://api.github.com/users/travisbell/events{/privacy}","received_events_url":"https://api.github.com/users/travisbell/received_events","type":"User","site_admin":false},"created_at":"2015-06-24T15:32:22Z","updated_at":"2015-06-24T15:32:22Z","author_association":"NONE","body":"Hi @ACV2 \n\nThanks for the reply. We noticed this was _usually_ tied to when we ran some big re-indexes, when the indexes were merging lots. During the re-indexes, we still have a full request load.\n\nAt the end of the re-index I added a task to run an optimize the index and low and behold, we haven't seen this problem crop up in the past ~12 days. Forcing an optimize to fix the runaway CPU problem doesn't feel like an actual fix but it does alleviate the issue for us. ES seems to get stuck in some kind of a weird state after these re-indexes and the optimize seems to kick it out of that state.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/138436550","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-138436550","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":138436550,"node_id":"MDEyOklzc3VlQ29tbWVudDEzODQzNjU1MA==","user":{"login":"suminda123","id":2336473,"node_id":"MDQ6VXNlcjIzMzY0NzM=","avatar_url":"https://avatars0.githubusercontent.com/u/2336473?v=4","gravatar_id":"","url":"https://api.github.com/users/suminda123","html_url":"https://github.com/suminda123","followers_url":"https://api.github.com/users/suminda123/followers","following_url":"https://api.github.com/users/suminda123/following{/other_user}","gists_url":"https://api.github.com/users/suminda123/gists{/gist_id}","starred_url":"https://api.github.com/users/suminda123/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/suminda123/subscriptions","organizations_url":"https://api.github.com/users/suminda123/orgs","repos_url":"https://api.github.com/users/suminda123/repos","events_url":"https://api.github.com/users/suminda123/events{/privacy}","received_events_url":"https://api.github.com/users/suminda123/received_events","type":"User","site_admin":false},"created_at":"2015-09-08T05:16:54Z","updated_at":"2015-09-08T05:16:54Z","author_association":"NONE","body":"I had a same issue, high cpu and high memory, for me it was marvel creating a new big index everyday.\nso I deleted all marvel indices and ran _flush, it cleared my memory and cpu came down fro 90% to 4%.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/138632515","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-138632515","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":138632515,"node_id":"MDEyOklzc3VlQ29tbWVudDEzODYzMjUxNQ==","user":{"login":"travisbell","id":24766,"node_id":"MDQ6VXNlcjI0NzY2","avatar_url":"https://avatars1.githubusercontent.com/u/24766?v=4","gravatar_id":"","url":"https://api.github.com/users/travisbell","html_url":"https://github.com/travisbell","followers_url":"https://api.github.com/users/travisbell/followers","following_url":"https://api.github.com/users/travisbell/following{/other_user}","gists_url":"https://api.github.com/users/travisbell/gists{/gist_id}","starred_url":"https://api.github.com/users/travisbell/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/travisbell/subscriptions","organizations_url":"https://api.github.com/users/travisbell/orgs","repos_url":"https://api.github.com/users/travisbell/repos","events_url":"https://api.github.com/users/travisbell/events{/privacy}","received_events_url":"https://api.github.com/users/travisbell/received_events","type":"User","site_admin":false},"created_at":"2015-09-08T16:54:44Z","updated_at":"2015-09-08T16:54:44Z","author_association":"NONE","body":"Yes, the daily optimize reduced our CPU in a similar amount (~60% ➞ ~10%). We still run our daily optimize task and haven't seen this issue since enabling it at the beginning of June.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/223238333","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-223238333","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":223238333,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMzIzODMzMw==","user":{"login":"Alino","id":763194,"node_id":"MDQ6VXNlcjc2MzE5NA==","avatar_url":"https://avatars3.githubusercontent.com/u/763194?v=4","gravatar_id":"","url":"https://api.github.com/users/Alino","html_url":"https://github.com/Alino","followers_url":"https://api.github.com/users/Alino/followers","following_url":"https://api.github.com/users/Alino/following{/other_user}","gists_url":"https://api.github.com/users/Alino/gists{/gist_id}","starred_url":"https://api.github.com/users/Alino/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Alino/subscriptions","organizations_url":"https://api.github.com/users/Alino/orgs","repos_url":"https://api.github.com/users/Alino/repos","events_url":"https://api.github.com/users/Alino/events{/privacy}","received_events_url":"https://api.github.com/users/Alino/received_events","type":"User","site_admin":false},"created_at":"2016-06-02T09:14:59Z","updated_at":"2016-06-02T09:14:59Z","author_association":"NONE","body":"have someone tried to upgrade to ES 2.+ ? does it fix this cpu issue?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/223309013","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-223309013","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":223309013,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMzMwOTAxMw==","user":{"login":"travisbell","id":24766,"node_id":"MDQ6VXNlcjI0NzY2","avatar_url":"https://avatars1.githubusercontent.com/u/24766?v=4","gravatar_id":"","url":"https://api.github.com/users/travisbell","html_url":"https://github.com/travisbell","followers_url":"https://api.github.com/users/travisbell/followers","following_url":"https://api.github.com/users/travisbell/following{/other_user}","gists_url":"https://api.github.com/users/travisbell/gists{/gist_id}","starred_url":"https://api.github.com/users/travisbell/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/travisbell/subscriptions","organizations_url":"https://api.github.com/users/travisbell/orgs","repos_url":"https://api.github.com/users/travisbell/repos","events_url":"https://api.github.com/users/travisbell/events{/privacy}","received_events_url":"https://api.github.com/users/travisbell/received_events","type":"User","site_admin":false},"created_at":"2016-06-02T14:28:49Z","updated_at":"2016-06-02T14:29:50Z","author_association":"NONE","body":"Hi @Alino, for us the problem was actually the fact we were using G1GC. As soon as we switched to CMS the problem went away (our ES cluster has been up for 51 days now!) What I mentioned in my last post ^ was still true, it greatly _reduced_ the times this happened but the runaway CPU problems continued to happen every ~7 days or so.\n\nThere's a long and interesting thread of some other users and I discussing this on the Elastic discussion board here: https://discuss.elastic.co/t/indexing-performance-degrading-over-time/40229\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/228134389","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-228134389","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":228134389,"node_id":"MDEyOklzc3VlQ29tbWVudDIyODEzNDM4OQ==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2016-06-23T18:09:28Z","updated_at":"2016-06-23T18:09:28Z","author_association":"CONTRIBUTOR","body":"@travisbell Hi Travis, I've been seeing very similar patters. Everything runs perfectly for a full week, then all of a sudden we get really high CPU usage, and elsaticsearch is unresponsive. Most operations fail. Can you expand a bit on what you did to fix this issue? How do you switch from G1GC to CMS?\n\nI have a feeling that this is caused by a node running out of java heap space. But I would imagine that would lead to a crash, not 100% CPU and I don't see OutOfMemory errors in the logs, so I'm not sure.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/228135357","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-228135357","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":228135357,"node_id":"MDEyOklzc3VlQ29tbWVudDIyODEzNTM1Nw==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2016-06-23T18:12:24Z","updated_at":"2016-06-23T18:13:24Z","author_association":"CONTRIBUTOR","body":"I see the following in the logs, right before I get 100% CPU, which is why I suspect it's an out of memory issue.\n\n```\n[2016-06-23 17:44:23,621][WARN ][monitor.jvm] \n[elasticsearch1-elasticsearch-5-vm] [gc][old][218473][270] \nduration [1.2m], collections [1]/[1.2m], total [1.2m]/[17.7m], \nmemory [25.2gb]->[25.2gb]/[25.9gb], \nall_pools {[young] [196.5mb]->[141.6mb]/[532.5mb]}{[survivor] \n[66.5mb]->[0b]/[66.5mb]}{[old] [25gb]->[25gb]/[25.3gb]}\n```\n\n(formatted for easier reading)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/228140237","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-228140237","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":228140237,"node_id":"MDEyOklzc3VlQ29tbWVudDIyODE0MDIzNw==","user":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"created_at":"2016-06-23T18:25:52Z","updated_at":"2016-06-23T18:26:31Z","author_association":"MEMBER","body":"Not quite, but very close.\n\n```\n[...] duration [1.2m] [...] memory [25.2gb]->[25.2gb]/[25.9gb] [...]\n```\n\nThis indicates a completely ineffective garbage collection cycle lasting for over a minute. The JVM is consuming CPU trying to collect when collecting is ineffective. You'll likely hit GC overhead limits soon at which point you will see `OutOfMemoryError`s.\n\nIt's best to open a new post on the [Elastic Discourse forum](https://discuss.elastic.co).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/228154184","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-228154184","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":228154184,"node_id":"MDEyOklzc3VlQ29tbWVudDIyODE1NDE4NA==","user":{"login":"speedplane","id":671052,"node_id":"MDQ6VXNlcjY3MTA1Mg==","avatar_url":"https://avatars0.githubusercontent.com/u/671052?v=4","gravatar_id":"","url":"https://api.github.com/users/speedplane","html_url":"https://github.com/speedplane","followers_url":"https://api.github.com/users/speedplane/followers","following_url":"https://api.github.com/users/speedplane/following{/other_user}","gists_url":"https://api.github.com/users/speedplane/gists{/gist_id}","starred_url":"https://api.github.com/users/speedplane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/speedplane/subscriptions","organizations_url":"https://api.github.com/users/speedplane/orgs","repos_url":"https://api.github.com/users/speedplane/repos","events_url":"https://api.github.com/users/speedplane/events{/privacy}","received_events_url":"https://api.github.com/users/speedplane/received_events","type":"User","site_admin":false},"created_at":"2016-06-23T19:12:12Z","updated_at":"2016-06-23T19:12:12Z","author_association":"CONTRIBUTOR","body":"@jasontedor I've started [a discussion here](https://discuss.elastic.co/t/elasticsearch-high-cpu-usage-gc-not-working/53816?u=michael_sander). Any help on the matter would be appreciated.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/258171259","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-258171259","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":258171259,"node_id":"MDEyOklzc3VlQ29tbWVudDI1ODE3MTI1OQ==","user":{"login":"magicalbanana","id":7953984,"node_id":"MDQ6VXNlcjc5NTM5ODQ=","avatar_url":"https://avatars0.githubusercontent.com/u/7953984?v=4","gravatar_id":"","url":"https://api.github.com/users/magicalbanana","html_url":"https://github.com/magicalbanana","followers_url":"https://api.github.com/users/magicalbanana/followers","following_url":"https://api.github.com/users/magicalbanana/following{/other_user}","gists_url":"https://api.github.com/users/magicalbanana/gists{/gist_id}","starred_url":"https://api.github.com/users/magicalbanana/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/magicalbanana/subscriptions","organizations_url":"https://api.github.com/users/magicalbanana/orgs","repos_url":"https://api.github.com/users/magicalbanana/repos","events_url":"https://api.github.com/users/magicalbanana/events{/privacy}","received_events_url":"https://api.github.com/users/magicalbanana/received_events","type":"User","site_admin":false},"created_at":"2016-11-03T15:12:02Z","updated_at":"2016-11-03T15:12:02Z","author_association":"NONE","body":"@acv2 what tool are you using for the visual monitoring data?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/385935238","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-385935238","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":385935238,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NTkzNTIzOA==","user":{"login":"ianchan0817","id":2121067,"node_id":"MDQ6VXNlcjIxMjEwNjc=","avatar_url":"https://avatars1.githubusercontent.com/u/2121067?v=4","gravatar_id":"","url":"https://api.github.com/users/ianchan0817","html_url":"https://github.com/ianchan0817","followers_url":"https://api.github.com/users/ianchan0817/followers","following_url":"https://api.github.com/users/ianchan0817/following{/other_user}","gists_url":"https://api.github.com/users/ianchan0817/gists{/gist_id}","starred_url":"https://api.github.com/users/ianchan0817/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ianchan0817/subscriptions","organizations_url":"https://api.github.com/users/ianchan0817/orgs","repos_url":"https://api.github.com/users/ianchan0817/repos","events_url":"https://api.github.com/users/ianchan0817/events{/privacy}","received_events_url":"https://api.github.com/users/ianchan0817/received_events","type":"User","site_admin":false},"created_at":"2018-05-02T10:36:17Z","updated_at":"2018-05-02T10:41:23Z","author_association":"NONE","body":"@ACV2 this issue also happen to me when search `*../../../../../../../../../../../../../../../../.* `\r\nit never timeout \r\n\r\n```indices:data/read/search               -                                transport 1525253037206 02:23:57  13.9m        x.x.x.x U5w91H7 indices[products], types[product], search_type[QUERY_THEN_FETCH], source[{\"from\":0,\"size\":24,\"query\":{\"bool\":{\"must\":[{\"match\":{\"owner_id\":{\"query\":\"\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":1.0}}},{\"match\":{\"status\":{\"query\":\"active\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":1.0}}}],\"must_not\":[{\"match\":{\"status\":{\"query\":\"removed\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":1.0}}},{\"match\":{\"type\":{\"query\":\"addon\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":1.0}}},{\"match\":{\"type\":{\"query\":\"gift\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":1.0}}}],\"should\":[{\"regexp\":{\"title_translations.zh-hant\":{\"value\":\".*../../../../../../../../../../../../../../../../.*\",\"flags_value\":65535,\"max_determinized_states\":10000,\"boost\":10.0}}},{\"regexp\":{\"title_translations.en\":{\"value\":\".*../../../../../../../../../../../../../../../../.*\",\"flags_value\":65535,\"max_determinized_states\":10000,\"boost\":10.0}}},{\"match\":{\"sku\":{\"query\":\"../../../../../../../../../../../../../../../../\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":100.0}}},{\"match\":{\"title_translations.zh-hant\":{\"query\":\"../../../../../../../../../../../../../../../../\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":50.0}}},{\"match\":{\"title_translations.en\":{\"query\":\"../../../../../../../../../../../../../../../../\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":50.0}}},{\"match\":{\"categories.name_translations.en\":{\"query\":\"../../../../../../../../../../../../../../../../\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":20.0}}},{\"match\":{\"categories.name_translations.zh-hant\":{\"query\":\"../../../../../../../../../../../../../../../../\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":20.0}}},{\"match\":{\"variations.sku\":{\"query\":\"../../../../../../../../../../../../../../../../\",\"operator\":\"OR\",\"prefix_length\":0,\"max_expansions\":50,\"fuzzy_transpositions\":true,\"lenient\":false,\"zero_terms_query\":\"NONE\",\"boost\":100.0}}}],\"disable_coord\":false,\"adjust_pure_negative\":true,\"minimum_should_match\":\"1\",\"boost\":1.0}},\"sort\":[{\"_score\":{\"order\":\"desc\"}},{\"created_at\":{\"order\":\"desc\"}}],\"ext\":{}}]```\r\n\r\n\r\nBtw, I am using ElasticSearch Service in AWS\r\nNo configuration there","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/391974935","html_url":"https://github.com/elastic/elasticsearch/issues/4288#issuecomment-391974935","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/4288","id":391974935,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MTk3NDkzNQ==","user":{"login":"flat010","id":2839949,"node_id":"MDQ6VXNlcjI4Mzk5NDk=","avatar_url":"https://avatars0.githubusercontent.com/u/2839949?v=4","gravatar_id":"","url":"https://api.github.com/users/flat010","html_url":"https://github.com/flat010","followers_url":"https://api.github.com/users/flat010/followers","following_url":"https://api.github.com/users/flat010/following{/other_user}","gists_url":"https://api.github.com/users/flat010/gists{/gist_id}","starred_url":"https://api.github.com/users/flat010/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/flat010/subscriptions","organizations_url":"https://api.github.com/users/flat010/orgs","repos_url":"https://api.github.com/users/flat010/repos","events_url":"https://api.github.com/users/flat010/events{/privacy}","received_events_url":"https://api.github.com/users/flat010/received_events","type":"User","site_admin":false},"created_at":"2018-05-25T08:03:29Z","updated_at":"2018-05-25T08:03:29Z","author_association":"NONE","body":"i also meet the problem,and the _nodes/hot_threads command return as flowing:\r\n::: {dmsnode-192}{q8IITWyORxO8a0zZ5rTIXA}{192.168.90.192}{192.168.90.192:9300}{rack=r1}\r\n   Hot threads at 2018-05-25T07:54:39.738Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:\r\n   \r\n   62.4% (312ms out of 500ms) cpu usage by thread 'elasticsearch[dmsnode-192][transport_client_worker][T#15]{New I/O worker #15}'\r\n     2/10 snapshots sharing following 10 elements\r\n       org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\r\n       org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)\r\n       org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\r\n       org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n       org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n       org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n       java.lang.Thread.run(Thread.java:748)\r\n","performed_via_github_app":null}]