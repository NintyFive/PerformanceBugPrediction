{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/31880","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31880/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31880/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/31880/events","html_url":"https://github.com/elastic/elasticsearch/issues/31880","id":339123410,"node_id":"MDU6SXNzdWUzMzkxMjM0MTA=","number":31880,"title":"support for standalone tokenizer","user":{"login":"judaschrist","id":5763903,"node_id":"MDQ6VXNlcjU3NjM5MDM=","avatar_url":"https://avatars2.githubusercontent.com/u/5763903?v=4","gravatar_id":"","url":"https://api.github.com/users/judaschrist","html_url":"https://github.com/judaschrist","followers_url":"https://api.github.com/users/judaschrist/followers","following_url":"https://api.github.com/users/judaschrist/following{/other_user}","gists_url":"https://api.github.com/users/judaschrist/gists{/gist_id}","starred_url":"https://api.github.com/users/judaschrist/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/judaschrist/subscriptions","organizations_url":"https://api.github.com/users/judaschrist/orgs","repos_url":"https://api.github.com/users/judaschrist/repos","events_url":"https://api.github.com/users/judaschrist/events{/privacy}","received_events_url":"https://api.github.com/users/judaschrist/received_events","type":"User","site_admin":false},"labels":[{"id":142001965,"node_id":"MDU6TGFiZWwxNDIwMDE5NjU=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Analysis","name":":Search/Analysis","color":"0e8a16","default":false,"description":"How text is split into tokens"},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2018-07-07T06:58:15Z","updated_at":"2018-07-16T15:14:25Z","closed_at":"2018-07-16T15:14:25Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\r\n\r\n** Please read the guidelines below. **\r\n\r\nIssues that do not follow these guidelines are likely to be closed.\r\n\r\n1.  GitHub is reserved for bug reports and feature requests. The best place to\r\n    ask a general question is at the Elastic [forums](https://discuss.elastic.co).\r\n    GitHub is not the place for general questions.\r\n\r\n2.  Is this bug report or feature request for a supported OS? If not, it\r\n    is likely to be closed.  See https://www.elastic.co/support/matrix#show_os\r\n\r\n3.  Please fill out EITHER the feature request block or the bug report block\r\n    below, and delete the other block.\r\n\r\n-->\r\n\r\n<!-- Feature request -->\r\n\r\n**Describe the feature**:\r\nShort version:\r\nDoes ES support standalone tokenizers? That is, i want to tokenize my corpus outside ES using my own tool (i.e., python), and then feed the segmented tokens into ES so that i can still use ES's full text search feature, including BM25 based scoring, etc.\r\n\r\nLong story:\r\nI am building a text search system where i first use a tokenizer to segment my text corpus, followed by a pipleline of NLP tasks like topic modeling, on independent computing platforms other than ES. I also want to use ES's full text search feature to index my corpus. As a result, the corpus will be tokenized twice, one before the NLP tasks by a python tokenizer, one by my ES tokenizer plugin, which is duplicated and a waste of resource because tokenizing is quite CPU intensive.\r\n\r\nBasically I want to be able to only tokenize my corpus just once, and use the result both for NLP tasks and full text search in ES, is this currently supported?","closed_by":{"login":"cbuescher","id":10398885,"node_id":"MDQ6VXNlcjEwMzk4ODg1","avatar_url":"https://avatars0.githubusercontent.com/u/10398885?v=4","gravatar_id":"","url":"https://api.github.com/users/cbuescher","html_url":"https://github.com/cbuescher","followers_url":"https://api.github.com/users/cbuescher/followers","following_url":"https://api.github.com/users/cbuescher/following{/other_user}","gists_url":"https://api.github.com/users/cbuescher/gists{/gist_id}","starred_url":"https://api.github.com/users/cbuescher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbuescher/subscriptions","organizations_url":"https://api.github.com/users/cbuescher/orgs","repos_url":"https://api.github.com/users/cbuescher/repos","events_url":"https://api.github.com/users/cbuescher/events{/privacy}","received_events_url":"https://api.github.com/users/cbuescher/received_events","type":"User","site_admin":false},"performed_via_github_app":null}