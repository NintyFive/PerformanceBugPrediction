{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/6831","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6831/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6831/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6831/events","html_url":"https://github.com/elastic/elasticsearch/issues/6831","id":37651394,"node_id":"MDU6SXNzdWUzNzY1MTM5NA==","number":6831,"title":"Elasticsearch Heap Memory analysis","user":{"login":"Aldian-fr","id":2630481,"node_id":"MDQ6VXNlcjI2MzA0ODE=","avatar_url":"https://avatars2.githubusercontent.com/u/2630481?v=4","gravatar_id":"","url":"https://api.github.com/users/Aldian-fr","html_url":"https://github.com/Aldian-fr","followers_url":"https://api.github.com/users/Aldian-fr/followers","following_url":"https://api.github.com/users/Aldian-fr/following{/other_user}","gists_url":"https://api.github.com/users/Aldian-fr/gists{/gist_id}","starred_url":"https://api.github.com/users/Aldian-fr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Aldian-fr/subscriptions","organizations_url":"https://api.github.com/users/Aldian-fr/orgs","repos_url":"https://api.github.com/users/Aldian-fr/repos","events_url":"https://api.github.com/users/Aldian-fr/events{/privacy}","received_events_url":"https://api.github.com/users/Aldian-fr/received_events","type":"User","site_admin":false},"labels":[{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2014-07-11T11:10:16Z","updated_at":"2014-08-25T08:15:49Z","closed_at":"2014-08-25T08:15:49Z","author_association":"NONE","active_lock_reason":null,"body":"Hi.\n\nI am struggling with Elasticsearch (v1.2.1) consuming too much heap memory, forcing me to restart it every two weeks. To better understand the problem, I went as far as analyzing a memory dump. This analysis was not performed after a crash because I cannot afford waiting for a crash if I can avoid it, but I waited for the heap reaching 1.7GB (on 1.9GB allowed) to perform a heap dump and make an analysis. Once clean of every free object the final heap dump size is about 1.5GB. Here is the overview I got from MAT.\n\n![00_overview](https://cloud.githubusercontent.com/assets/2630481/3551512/3e2f1da8-08e8-11e4-88d9-f388989c43a2.png)\n\nNext I opened the dominator tree and grouped by class name. Here is the result:\n\n![01_dominator_tree_group_by_class](https://cloud.githubusercontent.com/assets/2630481/3551547/9836bb1c-08e8-11e4-945f-66617d865942.png)\n\nAs you can see there are 63% (1GB) of HashMap objects. In some situations where HashMaps are used as a cache this can be normal. But still it seems worth investigating. Lets see what is nested in:\n\n![02_hashmap_global](https://cloud.githubusercontent.com/assets/2630481/3551576/edfbe856-08e8-11e4-8e4c-c6af44501ca2.png)\n\nWow! 755MB of strings. That is quite a lot. Now lets take a look at what can be inside it. First we will stop grouping by class in the dominator tree, and rather filter on HashMaps objects. Here it is:\n\n![03_hashmap_list](https://cloud.githubusercontent.com/assets/2630481/3551600/54a5ad9e-08e9-11e4-927d-1ef26e9310b9.png)\n\nOk so there are 9000 of them. Next lets open one of them to inspect what is inside it:\n\n![04_hashmap_detail](https://cloud.githubusercontent.com/assets/2630481/3551630/c162ab58-08e9-11e4-9ea8-bf3897aeedfa.png)\n\nSeems kinda like a log entry if you ask me. Should have mentioned it earlier, but I am using the usual stack Logstash/Elasticsearch/Kibana to monitor a java application. We have stacktraces in the logs and some of them are quite long and we use a multiline filter in logstash to gather them in a single message. No doubt that it is what happened here.\n\nPlease note that I am not saying that all 9000 Hashmaps contain log entries, actually even in the toped ranked 300ko Hashmaps I found some that do not contain that kind of stuff. But as far as I have seen, it happens quite often. The tokenizer seem to have a fixed 4096 char array size, and  since there was around 750M char[] in the heap dump, I would assume that there may be 92000 of them.\n\nNow I know that it is normal that the heap would keep growing since I am continuously indexing new stuff and such, but every day at midnight when it compress the index, it should as well free all those tokenizer stuff. Since I get the memory back to 1.1GB every time I restart ElasticSearch, this is the proof that the memory from 1.1GB to 1.7GB is just leaking. Or maybe there is something wrong with the configuration of ElasticSearch? I am using the default, I just changed allowed memory from 1GB to 2GB, was there something else important to do? \n\nWhat do you think?\n","closed_by":{"login":"Aldian-fr","id":2630481,"node_id":"MDQ6VXNlcjI2MzA0ODE=","avatar_url":"https://avatars2.githubusercontent.com/u/2630481?v=4","gravatar_id":"","url":"https://api.github.com/users/Aldian-fr","html_url":"https://github.com/Aldian-fr","followers_url":"https://api.github.com/users/Aldian-fr/followers","following_url":"https://api.github.com/users/Aldian-fr/following{/other_user}","gists_url":"https://api.github.com/users/Aldian-fr/gists{/gist_id}","starred_url":"https://api.github.com/users/Aldian-fr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Aldian-fr/subscriptions","organizations_url":"https://api.github.com/users/Aldian-fr/orgs","repos_url":"https://api.github.com/users/Aldian-fr/repos","events_url":"https://api.github.com/users/Aldian-fr/events{/privacy}","received_events_url":"https://api.github.com/users/Aldian-fr/received_events","type":"User","site_admin":false},"performed_via_github_app":null}