[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/153174319","html_url":"https://github.com/elastic/elasticsearch/issues/14425#issuecomment-153174319","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14425","id":153174319,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MzE3NDMxOQ==","user":{"login":"talevy","id":388837,"node_id":"MDQ6VXNlcjM4ODgzNw==","avatar_url":"https://avatars0.githubusercontent.com/u/388837?v=4","gravatar_id":"","url":"https://api.github.com/users/talevy","html_url":"https://github.com/talevy","followers_url":"https://api.github.com/users/talevy/followers","following_url":"https://api.github.com/users/talevy/following{/other_user}","gists_url":"https://api.github.com/users/talevy/gists{/gist_id}","starred_url":"https://api.github.com/users/talevy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/talevy/subscriptions","organizations_url":"https://api.github.com/users/talevy/orgs","repos_url":"https://api.github.com/users/talevy/repos","events_url":"https://api.github.com/users/talevy/events{/privacy}","received_events_url":"https://api.github.com/users/talevy/received_events","type":"User","site_admin":false},"created_at":"2015-11-02T22:13:49Z","updated_at":"2015-11-02T22:17:51Z","author_association":"CONTRIBUTOR","body":"@martijnvg, where would these benchmarks run?\n\nAlso, I was thinking of introducing `JMH` micro-benchmarks to help with improvements to the grok processor. Specifically, I was exploring different regex engines to see if it makes sense to stick with Joni. Do you feel this type of micro-benchmarking belongs in the source, or should I keep this outside in a separate project?\n\nWhich Data?\n\nwould we want to use synthetically generated log lines using randomly generated strings, \nor find a sample dataset of logs that we can use?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/153216894","html_url":"https://github.com/elastic/elasticsearch/issues/14425#issuecomment-153216894","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14425","id":153216894,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MzIxNjg5NA==","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2015-11-03T02:04:01Z","updated_at":"2015-11-03T02:04:01Z","author_association":"MEMBER","body":"> where would these benchmarks run?\n\nInitially adhoc from our dev machines and maybe later somewhere else but automated.\n\nThe idea I had was to start a full cluster (at least one data/master node and an ingest node) and run the benchmark against that. I haven't thought about micro benchmarks, but we add them maybe to a separate repository?\n\n> Which Data?\n\nNo idea yet. Would be great if just have a real sample set (~10GB or something like that) of logs.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/192330591","html_url":"https://github.com/elastic/elasticsearch/issues/14425#issuecomment-192330591","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14425","id":192330591,"node_id":"MDEyOklzc3VlQ29tbWVudDE5MjMzMDU5MQ==","user":{"login":"martijnvg","id":580421,"node_id":"MDQ6VXNlcjU4MDQyMQ==","avatar_url":"https://avatars3.githubusercontent.com/u/580421?v=4","gravatar_id":"","url":"https://api.github.com/users/martijnvg","html_url":"https://github.com/martijnvg","followers_url":"https://api.github.com/users/martijnvg/followers","following_url":"https://api.github.com/users/martijnvg/following{/other_user}","gists_url":"https://api.github.com/users/martijnvg/gists{/gist_id}","starred_url":"https://api.github.com/users/martijnvg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martijnvg/subscriptions","organizations_url":"https://api.github.com/users/martijnvg/orgs","repos_url":"https://api.github.com/users/martijnvg/repos","events_url":"https://api.github.com/users/martijnvg/events{/privacy}","received_events_url":"https://api.github.com/users/martijnvg/received_events","type":"User","site_admin":false},"created_at":"2016-03-04T15:51:26Z","updated_at":"2016-03-04T15:51:26Z","author_association":"MEMBER","body":"I ran a benchmark that indexes apache logs into a single shard index. The ingest overhead is based on comparing the recently added `ingest_took` in the bulk response with the time it took for the bulk API to answer.\n\nI ran several iterators. Indexing 300k, 600k and 1,2M document. For each volume I ran with different pipeles:\n- `empty` - A pipeline with no processors.\n- `set` - A pipeline that adds the current ingest time as a field to the document being processed.\n- `grok` - A pipeline that split up the apache log into several fields. \n- `grok_and_geoip` - Same as `grok`, but also adds geo information based on the client ip.\n\nSo in total I ran 12 iterations:\n\n| # docs | pipeline | ingest time overhead |\n| --- | --- | --- |\n| 300k | empty | 12% |\n| 300k | set | 21% |\n| 300k | grok | 27% |\n| 300k | grok_and_geoip | 37% |\n| 600k | empty | 13% |\n| 600k | set | 21% |\n| 600k | grok | 27% |\n| 600k | grok_and_geoip | 37% |\n| 1.2M | empty | 13% |\n| 1.2M | set | 21% |\n| 1.2M | grok | 26% |\n| 1.2M | grok_and_geoip | 37% |\n\nEach pipeline has a static tax on top of the indexing time and the time spent increases roughly linearly as more documents are being ingested. While ingesting these apache log with ingest, the used heap space overhead wasn't really noticeable.\n\nThe heap memory overhead of ingest is more tied to the size of the documents being pre-processed than to the amount of document being processed in a single bulk request. This is because ingest turns each document into a map of maps and because per bulk request, ingest processes a single document at the time.  So the ingest heap overhead becomes noticeable when preprocessing larger documents. This is why the used heap overhead wasn't noticeable when benchmarking with logs. However when testing with documents that have ~400 fields, I have seen the used heap overhead being 20% to 30% higher then when indexing the same data without ingest enabled.\n","performed_via_github_app":null}]