[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/517792726","html_url":"https://github.com/elastic/elasticsearch/issues/45153#issuecomment-517792726","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45153","id":517792726,"node_id":"MDEyOklzc3VlQ29tbWVudDUxNzc5MjcyNg==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2019-08-02T17:58:57Z","updated_at":"2019-08-02T17:58:57Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/518129578","html_url":"https://github.com/elastic/elasticsearch/issues/45153#issuecomment-518129578","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45153","id":518129578,"node_id":"MDEyOklzc3VlQ29tbWVudDUxODEyOTU3OA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2019-08-05T08:01:26Z","updated_at":"2019-08-05T08:01:26Z","author_association":"CONTRIBUTOR","body":"> Since we are never retrying an upload on failure\r\n\r\nThe SDK is using retries, no? There's a difference in retrying a 5GB upload to a 100MB upload.\r\n\r\n> Moreover, the maximum blob size in S3 is 5TB. There is no reason to split up files less than that into smaller blobs either\r\n\r\nI wonder if the same holds for minio, which can be backed by any file system, having other limitations.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/518146213","html_url":"https://github.com/elastic/elasticsearch/issues/45153#issuecomment-518146213","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45153","id":518146213,"node_id":"MDEyOklzc3VlQ29tbWVudDUxODE0NjIxMw==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-08-05T08:52:14Z","updated_at":"2019-08-05T09:19:38Z","author_association":"MEMBER","body":"@ywelsch \r\n\r\n> The SDK is using retries, no? There's a difference in retrying a 5GB upload to a 100MB upload.\r\n\r\nThe SDK takes an `InputStream` in `com.amazonaws.services.s3.model.PutObjectRequest#PutObjectRequest(java.lang.String, java.lang.String, java.io.InputStream, com.amazonaws.services.s3.model.ObjectMetadata)` and only buffers the whole content of that stream if you don't specify a size for the upload. Since we do in fact specify a size for the upload, I don't see how it could retry the upload on a broken-pipe during uploading (also, doesn't look like it does from the code)?\r\n\r\nUpdate: Seems with a mark and resettable input stream you might actually get retries out of the SDK. Still, on AWS with IO inbound being free it's really questionable whether these retries are worth anything.\r\nIn the end they increase the cost of each large file upload by a factor `>= 3`.\r\n\r\n> I wonder if the same holds for minio, which can be backed by any file system, having other limitations.\r\n\r\nYea that's a different story I think. But if you think about it, we're uploading files that are on disk pretty much exactly as they would be uploaded here on the ES node as well. I think this is a pretty unlikely thing to be an issue. But just in case, maybe we should simply set the defaults to `5TB` and `5GB` so we get the least number of requests but keep the settings as \"expert\" settings just in case?\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/537484434","html_url":"https://github.com/elastic/elasticsearch/issues/45153#issuecomment-537484434","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/45153","id":537484434,"node_id":"MDEyOklzc3VlQ29tbWVudDUzNzQ4NDQzNA==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2019-10-02T13:09:27Z","updated_at":"2019-10-02T13:09:27Z","author_association":"MEMBER","body":"Closing this (we discussed it a while back) as the practical impact of this is much smaller than expected due to the usual segment sizing and having smaller chunks here improves upload resiliency.","performed_via_github_app":null}]