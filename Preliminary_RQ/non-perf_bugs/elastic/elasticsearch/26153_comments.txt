[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/321752326","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-321752326","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":321752326,"node_id":"MDEyOklzc3VlQ29tbWVudDMyMTc1MjMyNg==","user":{"login":"markharwood","id":170925,"node_id":"MDQ6VXNlcjE3MDkyNQ==","avatar_url":"https://avatars0.githubusercontent.com/u/170925?v=4","gravatar_id":"","url":"https://api.github.com/users/markharwood","html_url":"https://github.com/markharwood","followers_url":"https://api.github.com/users/markharwood/followers","following_url":"https://api.github.com/users/markharwood/following{/other_user}","gists_url":"https://api.github.com/users/markharwood/gists{/gist_id}","starred_url":"https://api.github.com/users/markharwood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markharwood/subscriptions","organizations_url":"https://api.github.com/users/markharwood/orgs","repos_url":"https://api.github.com/users/markharwood/repos","events_url":"https://api.github.com/users/markharwood/events{/privacy}","received_events_url":"https://api.github.com/users/markharwood/received_events","type":"User","site_admin":false},"created_at":"2017-08-11T08:01:14Z","updated_at":"2017-08-11T08:01:14Z","author_association":"CONTRIBUTOR","body":">I believe it should be the job of the Scroll client (in this case the Reindex API) to identify the search queue has been exceeded and continue to retry.\r\n\r\nThat's a potentially dangerous assumption for us to make. Elasticsearch is not in a position to assume which functions are the most important to your business (servicing public-facing searches vs running a background reindex task).\r\n\r\nI'm confused by your exhaustion of search queues. Unless you are using [slicing](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html#docs-reindex-manual-slice) (which it appears you are not) I would assume there shouldn't be any parallelisation of searches and hence no exhaustion of search queues caused directly by reindex. Presumably it is other search loads that are contributing to the search thread pool exhaustion. \r\n\r\n>Supplementary Problem In addition to this...\r\n\r\nIs it possible the long delay observed here is tied to the same problem of thread-pool exhaustion - you have a large number of other concurrent search operations ongoing?\r\n\r\n>What I experienced actually was that setting requests_per_second to 0.5 resulted in a wait time of ~15500 seconds\r\n\r\nDo you have more than one data point for your test using other settings?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/321828792","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-321828792","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":321828792,"node_id":"MDEyOklzc3VlQ29tbWVudDMyMTgyODc5Mg==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-08-11T14:29:00Z","updated_at":"2017-08-11T14:29:00Z","author_association":"CONTRIBUTOR","body":"> In addition to this, I have a problem with the description of the requests_per_second URI parameter in the documentation: Reindex API: URL Parameters.\r\n\r\nThis is probably worth another issue. Indeed, I had intended it to be the number of write operations per second (deletes for delete_by_query, updates for update_by_query, and indexes for reindex). And it writes the whole batch at once rather than attempting to smooth out the writes. So `.5` would write all 10000 documents and then sleep 20000 seconds - the amount of time that the write took. The delay you see is expected. I think that problem is one of documentation. The docs should say \"index requests\" or \"delete requests\" or \"update requests\".\r\n\r\n> My suggestion is that the Reindex API should retry on this soft error.\r\n\r\nReindex has had code to do that for a very long time but it seems to not be working. You can see we even count the number of retries:\r\n```\r\n      \"retries\" : {\r\n        \"bulk\" : 0,\r\n        \"search\" : 0\r\n      },\r\n```\r\n\r\nWe must not be picking up the rejection that you are seeing somehow. We test for this on every build by gumming up the thread pools and starting a reindex, ungumming them, and asserting that the reindex succeeded and counted some retries. We're obviously doing something wrong though.\r\n\r\n@andy-elastic, are you interested in looking at this or should I have a look later on?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/321948947","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-321948947","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":321948947,"node_id":"MDEyOklzc3VlQ29tbWVudDMyMTk0ODk0Nw==","user":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"created_at":"2017-08-12T01:30:06Z","updated_at":"2017-08-12T01:30:06Z","author_association":"CONTRIBUTOR","body":"@nik9000 yeah I'll take a look and see if I can find out why this isn't being retried. Setting `thread_pool.search.queue_size` very low would be enough to cause the search queue to fill up right? It looks like that's what we do in [RetryTests](https://github.com/elastic/elasticsearch/blob/master/modules/reindex/src/test/java/org/elasticsearch/index/reindex/RetryTests.java#L105)\r\n\r\n@berglh to help me reproduce this, when you say\r\n\r\n>Increasing the Scroll size of the Reindex improves the ability of the Reindex API to make it most of the way through the process. However, on a large enough index, I continue to hit this problem.\r\n\r\ndo you mean that the higher you set `size` in the reindex request, the more likely it is to finish without this failure?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/321950695","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-321950695","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":321950695,"node_id":"MDEyOklzc3VlQ29tbWVudDMyMTk1MDY5NQ==","user":{"login":"berglh","id":12455338,"node_id":"MDQ6VXNlcjEyNDU1MzM4","avatar_url":"https://avatars1.githubusercontent.com/u/12455338?v=4","gravatar_id":"","url":"https://api.github.com/users/berglh","html_url":"https://github.com/berglh","followers_url":"https://api.github.com/users/berglh/followers","following_url":"https://api.github.com/users/berglh/following{/other_user}","gists_url":"https://api.github.com/users/berglh/gists{/gist_id}","starred_url":"https://api.github.com/users/berglh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/berglh/subscriptions","organizations_url":"https://api.github.com/users/berglh/orgs","repos_url":"https://api.github.com/users/berglh/repos","events_url":"https://api.github.com/users/berglh/events{/privacy}","received_events_url":"https://api.github.com/users/berglh/received_events","type":"User","site_admin":false},"created_at":"2017-08-12T02:05:08Z","updated_at":"2017-08-12T02:06:22Z","author_association":"CONTRIBUTOR","body":"@nik9000 \r\n>This is probably worth another issue. Indeed, I had intended it to be the number of write operations per second (deletes for delete_by_query, updates for update_by_query, and indexes for reindex). And it writes the whole batch at once rather than attempting to smooth out the writes. So .5 would write all 10000 documents and then sleep 20000 seconds - the amount of time that the write took. The delay you see is expected. I think that problem is one of documentation. The docs should say \"index requests\" or \"delete requests\" or \"update requests\".\r\n\r\nNo worries, I'll write this up soon.\r\n\r\n>We must not be picking up the rejection that you are seeing somehow. \r\n\r\nI figured there must be some mechanism for this already, thanks for explaining it. Another thought I had with the `-1` default `requests_per_second` setting was that I wonder if the Reindex API just keeps queuing up the Scroll searches until it fills the queue, but with a `-1` search size effecting the depth calculation. I wonder if it was then blowing it out to try to queue `1001` scroll requests, thus resulting in the error. My intuition would be that if this thought had any substance it would result in a max scroll request of `999`, and not `1001`. I have no evidence that this is the case, just a passing thought, I don't see how it should effect the queue depth anyway if that is being managed separately - just coming up with ideas here.\r\n\r\n@andy-elastic There are two size directives in the Reindex API. My goal is to Reindex a relatively large index, so I'm referring to the size of the Scroll as specified in the `source` object:\r\n\r\n>By default _reindex uses scroll batches of 1000. You can change the batch size with the size field in the source element.\r\n\r\nThis setting will be limited by the `index.max_result_window` which is `10000` by default if i remember correctly; I'm running the stock value for this setting. \r\n\r\nI found that I would hit this error relatively quickly at the `1000` default setting, although never at the exact same point. I could never managed to Reindex a whole index with the default settings. When I increased it to the maximum `10000` Scroll size, it would make it through a lot more documents before hitting the error, allowing to Reindex many of my smaller indices.\r\n\r\n@markharwood All very good points. We have 10 unique Kibana instances running with multiple users. The actual search usage is pretty low and ad-hoc mostly when investigating something. We do have some dashboards that are periodically captured and displayed as images on some TV around our various IT departments, I don't believe these update anymore than once every 5 minutes. I also noticed this behaviour in relatively low ES utilisation, unfortunately I don't have any metrics regarding average search requests per second, but I'm pretty confident that the large majority (+90%) would be coming from the Reindex API. \r\n\r\nIn terms of data points, I've been spending the past week attempting to Reindex after hitting the bug in [Kibana 5.5.0](https://github.com/elastic/kibana/issues/12728) to weed out my field conflicts. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/322083916","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-322083916","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":322083916,"node_id":"MDEyOklzc3VlQ29tbWVudDMyMjA4MzkxNg==","user":{"login":"berglh","id":12455338,"node_id":"MDQ6VXNlcjEyNDU1MzM4","avatar_url":"https://avatars1.githubusercontent.com/u/12455338?v=4","gravatar_id":"","url":"https://api.github.com/users/berglh","html_url":"https://github.com/berglh","followers_url":"https://api.github.com/users/berglh/followers","following_url":"https://api.github.com/users/berglh/following{/other_user}","gists_url":"https://api.github.com/users/berglh/gists{/gist_id}","starred_url":"https://api.github.com/users/berglh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/berglh/subscriptions","organizations_url":"https://api.github.com/users/berglh/orgs","repos_url":"https://api.github.com/users/berglh/repos","events_url":"https://api.github.com/users/berglh/events{/privacy}","received_events_url":"https://api.github.com/users/berglh/received_events","type":"User","site_admin":false},"created_at":"2017-08-14T01:58:27Z","updated_at":"2017-08-14T01:58:27Z","author_association":"CONTRIBUTOR","body":"When I set the `requests_per_second` to `10000`, I still get the `es_rejected_execution_exception` error. \r\n When I set it to `5000`, it did complete, however, took a much longer time.\r\n\r\n**10000 Results**\r\n\r\nItem | Result\r\n:---:|:---\r\nTotal Docs | 26690000\r\nTotal Batches | 2669\r\nTask Completion Time | 6195 s\r\nTime per Batch | 2.32s\r\nOverall EPS | 4308\r\nTime Throttled | 2668 s\r\nTime Throttled per Batch | 1s\r\nTime Working | 3527 s\r\nTime Working per Scroll | 1.32 s\r\nWorking EPS | 7567.3\r\nTime Throttle to Work Ratio | 0.75\r\n\r\n```javascript\r\n{\r\n  \"completed\": true,\r\n  \"task\": {\r\n    \"node\": \"fmVI6xlZQCmhqZqVPIjfXA\",\r\n    \"id\": 84157930,\r\n    \"type\": \"transport\",\r\n    \"action\": \"indices:data/write/reindex\",\r\n    \"status\": {\r\n      \"total\": 279063633,\r\n      \"updated\": 0,\r\n      \"created\": 26690000,\r\n      \"deleted\": 0,\r\n      \"batches\": 2669,\r\n      \"version_conflicts\": 0,\r\n      \"noops\": 0,\r\n      \"retries\": {\r\n        \"bulk\": 0,\r\n        \"search\": 0\r\n      },\r\n      \"throttled_millis\": 2667995,\r\n      \"requests_per_second\": 10000,\r\n      \"throttled_until_millis\": 0\r\n    },\r\n    \"description\": \"reindex from [anotherlargeindex] to [anotherlargeindex.es5]\",\r\n    \"start_time_in_millis\": 1502431979655,\r\n    \"running_time_in_nanos\": 6195367033709,\r\n    \"cancellable\": true\r\n  },\r\n  \"response\": {\r\n    \"took\": 6195366,\r\n    \"timed_out\": false,\r\n    \"total\": 279063633,\r\n    \"updated\": 0,\r\n    \"created\": 26690000,\r\n    \"deleted\": 0,\r\n    \"batches\": 2669,\r\n    \"version_conflicts\": 0,\r\n    \"noops\": 0,\r\n    \"retries\": {\r\n      \"bulk\": 0,\r\n      \"search\": 0\r\n    },\r\n    \"throttled_millis\": 2667995,\r\n    \"requests_per_second\": 10000,\r\n    \"throttled_until_millis\": 0,\r\n    \"failures\": [\r\n      {\r\n        \"shard\": -1,\r\n        \"reason\": {\r\n          \"type\": \"es_rejected_execution_exception\",\r\n          \"reason\": \"rejected execution of org.elasticsearch.transport.TransportService$7@48752bce on EsThreadPoolExecutor[search, queue capacity = 1000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@53fe6f13[Running, pool size = 49, active threads = 49, queued tasks = 999, completed tasks = 7727372]]\"\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n**5000 Results**\r\n\r\nItem | Result\r\n---:|:---\r\nTotal Docs | 133317017\r\nTotal Batches | 13332\r\nTask Completion Time | 44458.6 s\r\nTime per Batch | 3.33s\r\nOverall EPS | 2998.7\r\nTime Throttled | 26663 s\r\nTime Throttled per Batch | 1.99s\r\nTime Working | 17795 s\r\nTime Working per Batch | 1.33 s\r\nWorking EPS | 7491.6\r\nTime Throttle to Work Ratio | 1.49\r\n\r\n```javascript\r\n{\r\n  \"_index\": \".tasks\",\r\n  \"_type\": \"task\",\r\n  \"_id\": \"fmVI6xlZQCmhqZqVPIjfXA:81294668\",\r\n  \"_score\": 1,\r\n  \"_source\": {\r\n    \"completed\": true,\r\n    \"task\": {\r\n      \"node\": \"fmVI6xlZQCmhqZqVPIjfXA\",\r\n      \"id\": 81294668,\r\n      \"type\": \"transport\",\r\n      \"action\": \"indices:data/write/reindex\",\r\n      \"status\": {\r\n        \"total\": 133317017,\r\n        \"updated\": 0,\r\n        \"created\": 133317017,\r\n        \"deleted\": 0,\r\n        \"batches\": 13332,\r\n        \"version_conflicts\": 0,\r\n        \"noops\": 0,\r\n        \"retries\": {\r\n          \"bulk\": 0,\r\n          \"search\": 0\r\n        },\r\n        \"throttled_millis\": 26663379,\r\n        \"requests_per_second\": 5000,\r\n        \"throttled_until_millis\": 0\r\n      },\r\n      \"description\": \"reindex from [largeindex] to [largeindex.es5]\",\r\n      \"start_time_in_millis\": 1502414174752,\r\n      \"running_time_in_nanos\": 44458656303656,\r\n      \"cancellable\": true\r\n    },\r\n    \"response\": {\r\n      \"took\": 44458656,\r\n      \"timed_out\": false,\r\n      \"total\": 133317017,\r\n      \"updated\": 0,\r\n      \"created\": 133317017,\r\n      \"deleted\": 0,\r\n      \"batches\": 13332,\r\n      \"version_conflicts\": 0,\r\n      \"noops\": 0,\r\n      \"retries\": {\r\n        \"bulk\": 0,\r\n        \"search\": 0\r\n      },\r\n      \"throttled_millis\": 26663379,\r\n      \"requests_per_second\": 5000,\r\n      \"throttled_until_millis\": 0,\r\n      \"failures\": []\r\n    }\r\n  }\r\n}\r\n```","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/322509213","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-322509213","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":322509213,"node_id":"MDEyOklzc3VlQ29tbWVudDMyMjUwOTIxMw==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-08-15T15:57:04Z","updated_at":"2017-08-15T15:57:04Z","author_association":"CONTRIBUTOR","body":"> No worries, I'll write this up soon.\r\n\r\nThanks! I have #26185 on my list of things to review again today.\r\n\r\n> There are two size directives in the Reindex API.\r\n\r\nAnd my past mistakes continue to haunt me. One of them really should be called `scroll_size`. It'd be so much less confusing.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/325754783","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-325754783","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":325754783,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNTc1NDc4Mw==","user":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"created_at":"2017-08-29T18:33:31Z","updated_at":"2017-08-29T18:33:31Z","author_association":"CONTRIBUTOR","body":"I was able to reliably reproduce this by\r\n\r\n* Creating a ~40gb index with 5 shards, 1 replica on a 2.4.5 cluster with 3 nodes\r\n* Upgrading all nodes to 5.5.1\r\n* Setting `thread_pool.search.queue_size` to 1\r\n* Reindexing to a new index with\r\n    ```\r\n    POST localhost:9200/_reindex?wait_for_completion=false&wait_for_active_shards=all\r\n    {\r\n      \"source\": {\r\n        \"index\": \"nyc_taxis\",\r\n        \"size\":1000\r\n      },\r\n      \"dest\": {\r\n        \"index\": \"nyc_taxis_dest_1000\"\r\n      }\r\n    }\r\n    ```\r\n\r\nGetting a task status similar to the original posted - failure with search queue rejected, no retries marked\r\n\r\n```\r\n{\r\n  \"completed\" : true,\r\n  \"task\" : {\r\n    \"node\" : \"M_P0k50JTU6bfF2oVrvCyw\",\r\n    \"id\" : 1040,\r\n    \"type\" : \"transport\",\r\n    \"action\" : \"indices:data/write/reindex\",\r\n    \"status\" : {\r\n      \"total\" : 63438115,\r\n      \"updated\" : 0,\r\n      \"created\" : 2000,\r\n      \"deleted\" : 0,\r\n      \"batches\" : 2,\r\n      \"version_conflicts\" : 0,\r\n      \"noops\" : 0,\r\n      \"retries\" : {\r\n        \"bulk\" : 0,\r\n        \"search\" : 0\r\n      },\r\n      \"throttled_millis\" : 0,\r\n      \"requests_per_second\" : -1.0,\r\n      \"throttled_until_millis\" : 0\r\n    },\r\n    \"description\" : \"reindex from [nyc_taxis] to [nyc_taxis_dest_1000]\",\r\n    \"start_time_in_millis\" : 1504029858572,\r\n    \"running_time_in_nanos\" : 2412376075,\r\n    \"cancellable\" : true\r\n  },\r\n  \"response\" : {\r\n    \"took\" : 2404,\r\n    \"timed_out\" : false,\r\n    \"total\" : 63438115,\r\n    \"updated\" : 0,\r\n    \"created\" : 2000,\r\n    \"deleted\" : 0,\r\n    \"batches\" : 2,\r\n    \"version_conflicts\" : 0,\r\n    \"noops\" : 0,\r\n    \"retries\" : {\r\n      \"bulk\" : 0,\r\n      \"search\" : 0\r\n    },\r\n    \"throttled_millis\" : 0,\r\n    \"requests_per_second\" : -1.0,\r\n    \"throttled_until_millis\" : 0,\r\n    \"failures\" : [\r\n      {\r\n        \"index\" : \"nyc_taxis\",\r\n        \"shard\" : 3,\r\n        \"node\" : \"fiLlxgSaRN6VEezWoAvfnA\",\r\n        \"reason\" : {\r\n          \"type\" : \"es_rejected_execution_exception\",\r\n          \"reason\" : \"rejected execution of org.elasticsearch.transport.TcpTransport$RequestHandler@51feb839 on EsThreadPoolExecutor[search, queue capacity = 1, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@7ca35db8[Running, pool size = 13, active threads = 1, queued tasks = 0, completed tasks = 15]]\"\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n\r\n\r\nIt seems like the index being created in 2.x is the determining factor here, I was not able to reproduce this with only indices created in 5.x. I'll see if I can make the reproduction steps a little simpler and rule out some other factors","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/325854041","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-325854041","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":325854041,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNTg1NDA0MQ==","user":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"created_at":"2017-08-30T01:47:38Z","updated_at":"2017-08-30T01:47:38Z","author_association":"CONTRIBUTOR","body":"I was wrong about it only being indices created in 2.x, I'm able to reproduce it with indices created in 5.x now. Not sure what I was doing differently before, I must not have set the queue size low enough.\r\n\r\nIt looks like this is reproducible in 5.5.1 on a single node cluster with indices of any size. It does not reproduce on indices with only a single shard.\r\n\r\nI'll see if I can reproduce it in the test environment and find a cause","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/326171215","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-326171215","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":326171215,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNjE3MTIxNQ==","user":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"created_at":"2017-08-31T02:25:46Z","updated_at":"2017-08-31T02:25:46Z","author_association":"CONTRIBUTOR","body":"I think I found why search requests aren't being retried here. When it gets the response back from a scroll, it [retries if the request generates an exception](https://github.com/elastic/elasticsearch/blob/19c13d032a6a6b89ab6e5440cf1b3b07b948afbe/core/src/main/java/org/elasticsearch/index/reindex/ClientScrollableHitSource.java#L149-L173). However, if the request completes with failures, it doesn't retry but [still terminates the reindex](https://github.com/elastic/elasticsearch/blob/432f162981bfeb725f5a73bc462f4950d23ac7ad/modules/reindex/src/main/java/org/elasticsearch/index/reindex/AbstractAsyncBulkByScrollAction.java#L270-L277) when handling the scroll response. So I think what's happening here is a scroll request is completing with failures.\r\n\r\nThat said, I'm not sure if the cause of the failures @berglh is seeing are the same as what I've been reproducing here, because they look a little different. Mine consistently have a shard and node id associated, and I haven't seen any with the default `shard: -1` like the ones @berglh posted.\r\n\r\nIn my case, it's that some shards (but not all) are failing in the search request (because the queue is full). This doesn't get caught in our tests because [it only uses one shard](https://github.com/elastic/elasticsearch/blob/432f162981bfeb725f5a73bc462f4950d23ac7ad/modules/reindex/src/test/java/org/elasticsearch/index/reindex/RetryTests.java#L57), so all the searches that fail have all shards failed.\r\n\r\n@nik9000 any ideas about what could cause a search failure with `shard: -1`?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/327186335","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-327186335","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":327186335,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNzE4NjMzNQ==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-09-05T14:07:27Z","updated_at":"2017-09-05T14:07:27Z","author_association":"CONTRIBUTOR","body":"> @nik9000 any ideas about what could cause a search failure with shard: -1?\r\n\r\nNot really. `-1` means to me that it was a failure in the scroll request itself and not any of the shards and we should catch that and retry it.....\r\n\r\n\r\nSo about terminating the scroll when a shard fails, I don't know that you *can* (#26433) retry on the shard level. I'd forgotten about this when the issues came up in the first place.... I think, though, that the `_jim` API ought to allow for retries (#26472) when it is built....","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/339969486","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-339969486","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":339969486,"node_id":"MDEyOklzc3VlQ29tbWVudDMzOTk2OTQ4Ng==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2017-10-27T13:21:26Z","updated_at":"2017-10-27T13:21:26Z","author_association":"CONTRIBUTOR","body":"@colings86 pinged me about this issue, wanting to make sure that it is still appropriate to have this in `feedback_needed`. The retryable shard failures doesn't need feedback - we're just waiting on #26472 which @jimczi is going to have a look at before too long.\r\n\r\n@andyb-elastic's `shard: -1` could use some more investigation because we're not sure how that one comes about, but for now I don't think of that as part of this issue.\r\n\r\nSo this is officially blocked waiting on #26472.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/340007526","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-340007526","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":340007526,"node_id":"MDEyOklzc3VlQ29tbWVudDM0MDAwNzUyNg==","user":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"created_at":"2017-10-27T15:40:50Z","updated_at":"2017-10-27T15:56:38Z","author_association":"CONTRIBUTOR","body":"@nik9000 right, I don't think we need any more feedback here, and we're waiting for #26472 \r\n\r\nFor more background, the reason we can't fix this while it uses scrolls is because it will lose some documents if we retry and continue. When a scroll fails on some shards and returns partial results, there's no current way to rewind so that the reader makes sure to get those missing documents. In the context of the reindex API, losing documents is clearly very incorrect behavior, so the right thing to do is fail when this happens, even though it's unfortunately very inconvenient.\r\n\r\nWhen we replace reindex's use of scroll with #26472 we'll be able to retry this failure condition without losing documents.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374627647","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-374627647","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":374627647,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDYyNzY0Nw==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T14:54:30Z","updated_at":"2018-03-20T14:54:30Z","author_association":"MEMBER","body":"@andyb-elastic is this something you are still working on?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374658670","html_url":"https://github.com/elastic/elasticsearch/issues/26153#issuecomment-374658670","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26153","id":374658670,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDY1ODY3MA==","user":{"login":"andyb-elastic","id":29205940,"node_id":"MDQ6VXNlcjI5MjA1OTQw","avatar_url":"https://avatars2.githubusercontent.com/u/29205940?v=4","gravatar_id":"","url":"https://api.github.com/users/andyb-elastic","html_url":"https://github.com/andyb-elastic","followers_url":"https://api.github.com/users/andyb-elastic/followers","following_url":"https://api.github.com/users/andyb-elastic/following{/other_user}","gists_url":"https://api.github.com/users/andyb-elastic/gists{/gist_id}","starred_url":"https://api.github.com/users/andyb-elastic/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andyb-elastic/subscriptions","organizations_url":"https://api.github.com/users/andyb-elastic/orgs","repos_url":"https://api.github.com/users/andyb-elastic/repos","events_url":"https://api.github.com/users/andyb-elastic/events{/privacy}","received_events_url":"https://api.github.com/users/andyb-elastic/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T16:14:49Z","updated_at":"2018-03-20T16:14:49Z","author_association":"CONTRIBUTOR","body":"@dakrone not actively, we're waiting on a new API from #26472. I think we can close this as that feature isn't on the roadmap yet. Additional feedback is always welcome.\r\n\r\nIn the meantime, users encountering this problem with scrolls should use `search_after` if they can accept not having a point-in-time-view like scrolls provide.","performed_via_github_app":null}]