[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/712741314","html_url":"https://github.com/elastic/elasticsearch/issues/63918#issuecomment-712741314","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/63918","id":712741314,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMjc0MTMxNA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2020-10-20T10:03:49Z","updated_at":"2020-10-20T10:03:49Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed (:Distributed/Cluster Coordination)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/712797591","html_url":"https://github.com/elastic/elasticsearch/issues/63918#issuecomment-712797591","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/63918","id":712797591,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMjc5NzU5MQ==","user":{"login":"original-brownbear","id":6490959,"node_id":"MDQ6VXNlcjY0OTA5NTk=","avatar_url":"https://avatars0.githubusercontent.com/u/6490959?v=4","gravatar_id":"","url":"https://api.github.com/users/original-brownbear","html_url":"https://github.com/original-brownbear","followers_url":"https://api.github.com/users/original-brownbear/followers","following_url":"https://api.github.com/users/original-brownbear/following{/other_user}","gists_url":"https://api.github.com/users/original-brownbear/gists{/gist_id}","starred_url":"https://api.github.com/users/original-brownbear/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/original-brownbear/subscriptions","organizations_url":"https://api.github.com/users/original-brownbear/orgs","repos_url":"https://api.github.com/users/original-brownbear/repos","events_url":"https://api.github.com/users/original-brownbear/events{/privacy}","received_events_url":"https://api.github.com/users/original-brownbear/received_events","type":"User","site_admin":false},"created_at":"2020-10-20T11:56:58Z","updated_at":"2020-10-20T11:56:58Z","author_association":"MEMBER","body":"@DaveCTurner or @fcofdez I think this needs a look from one of you. I must admit I don't fully understand how https://github.com/elastic/elasticsearch/pull/62954 actually drains all the disruptions to begin with (all the `rarely()` restarting and disconnecting confuses me) so I'd just be guessing here :)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/712815490","html_url":"https://github.com/elastic/elasticsearch/issues/63918#issuecomment-712815490","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/63918","id":712815490,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMjgxNTQ5MA==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2020-10-20T12:31:44Z","updated_at":"2020-10-20T12:31:44Z","author_association":"CONTRIBUTOR","body":"Thanks @fcofdez; FWIW I don't think this is to do with #62954. It stabilises ok, then calls `originalLeader.blackhole()`, then does another `stabilise()` (without a `runRandomly()`) and it's this second stabilisation that's failing apparently because the new leader is still waiting for a response from the original leader before applying its cluster state.\r\n\r\nThe problem seems to be that the election at the start of the second stabilisation phase is very messy: `node0` and `node3` both trigger elections concurrently, twice, each of which fails before finally settling down and electing `node1`. I think this is just incredibly unlucky timing that isn't accounted for in the test, made slightly more likely by having 5 nodes in this cluster. It looks to be enough to extend the timeout by a couple of minutes to allow for this kind of mess.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/720360926","html_url":"https://github.com/elastic/elasticsearch/issues/63918#issuecomment-720360926","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/63918","id":720360926,"node_id":"MDEyOklzc3VlQ29tbWVudDcyMDM2MDkyNg==","user":{"login":"fcofdez","id":1033457,"node_id":"MDQ6VXNlcjEwMzM0NTc=","avatar_url":"https://avatars1.githubusercontent.com/u/1033457?v=4","gravatar_id":"","url":"https://api.github.com/users/fcofdez","html_url":"https://github.com/fcofdez","followers_url":"https://api.github.com/users/fcofdez/followers","following_url":"https://api.github.com/users/fcofdez/following{/other_user}","gists_url":"https://api.github.com/users/fcofdez/gists{/gist_id}","starred_url":"https://api.github.com/users/fcofdez/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fcofdez/subscriptions","organizations_url":"https://api.github.com/users/fcofdez/orgs","repos_url":"https://api.github.com/users/fcofdez/repos","events_url":"https://api.github.com/users/fcofdez/events{/privacy}","received_events_url":"https://api.github.com/users/fcofdez/received_events","type":"User","site_admin":false},"created_at":"2020-11-02T09:46:48Z","updated_at":"2020-11-02T09:46:48Z","author_association":"CONTRIBUTOR","body":"Sorry for the delay, It took me longer than expected to fully understand what was going on on this scenario.\r\n\r\nAs @DaveCTurner pointed out, this is a messy scenario:\r\n1. `node0` and `node3` starts an election concurrently\r\n2. `node0` becomes leader on term 4\r\n3. `node3` starts a new election with proposed term 5\r\n4. `node0` becomes candidate as a newer term is proposed\r\n5. `node0` starts a new election with proposed term 6\r\n6. `node3` becomes leader on term 5\r\n7. `node3` steps down as candidate as a newer term is proposed \r\n8. `node3` starts a new election with proposed term 7 after the cluster state publication\r\n9. `node3` election scheduler triggers a new election with term 8 as it learns about the term 7 proposed by `node3` during the pre voting round\r\n10. `node3` becomes leader on term 8\r\n11.  `node2` starts a new election on term 9\r\n12. `node3` steps down as candidate\r\n13. `node1` and `node3` start a new concurrent election proposed term 10\r\n14. `node1` wins the election on term 10, but since it missed the Join vote from `node3` it bumps its term to 11\r\n15. `node1` becomes candidate again and triggers a new election with term 12\r\n16. `node1` election scheduler triggers a new election on term 13\r\n17. `node1` is elected as leader on term 13, and things stabilize at this point\r\n18. The stabilization phase fails before `node1` has noticed that the old leader is unreachable, so there's still cluster state publications in flight.\r\n\r\nI've opened https://github.com/elastic/elasticsearch/pull/64462 to tackle this. Adding the time for an additional election seems to fix the issue, but maybe that can hide problems for smaller clusters? ","performed_via_github_app":null}]