{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/19879","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19879/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19879/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19879/events","html_url":"https://github.com/elastic/elasticsearch/issues/19879","id":170019800,"node_id":"MDU6SXNzdWUxNzAwMTk4MDA=","number":19879,"title":"Memory leak when shard fails to allocate on missing synonym dictionary","user":{"login":"adrianocrestani","id":7266694,"node_id":"MDQ6VXNlcjcyNjY2OTQ=","avatar_url":"https://avatars2.githubusercontent.com/u/7266694?v=4","gravatar_id":"","url":"https://api.github.com/users/adrianocrestani","html_url":"https://github.com/adrianocrestani","followers_url":"https://api.github.com/users/adrianocrestani/followers","following_url":"https://api.github.com/users/adrianocrestani/following{/other_user}","gists_url":"https://api.github.com/users/adrianocrestani/gists{/gist_id}","starred_url":"https://api.github.com/users/adrianocrestani/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/adrianocrestani/subscriptions","organizations_url":"https://api.github.com/users/adrianocrestani/orgs","repos_url":"https://api.github.com/users/adrianocrestani/repos","events_url":"https://api.github.com/users/adrianocrestani/events{/privacy}","received_events_url":"https://api.github.com/users/adrianocrestani/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":12,"created_at":"2016-08-08T20:39:21Z","updated_at":"2016-10-06T19:54:49Z","closed_at":"2016-08-08T20:45:12Z","author_association":"NONE","active_lock_reason":null,"body":"**Elasticsearch version**: 2.1.1 and 2.3.5\n\n**Plugins installed**: []\n\n**JVM version**: Oracle JDK 1.8.0_60\n\n**OS version**: Ubuntu 4.8.2 (Trusty 14.04)\n\n**Description of the problem including expected versus actual behavior**:\n\nWe ran into a memory issue in our ES cluster when some shards tried to be allocated on certain nodes that were missing a synonym dictionary. The heap usage went from 15GB to 30GB and the behaviour was 100% reproducible.\n\nThose nodes did not OOM, but were spending a lot of time doing GCs. The whole cluster got very unresponsive where stats and search requests would take a long time to respond and often time out. The dictionary was missing on 4 out of 16 data nodes, only those 4 nodes started running out of memory, the other 12 nodes had the synonym dictionary in place and were able to allocate shards.\n\nCluster configuration: ES 2.1.1, 16 data nodes, 2 masters, 6 proxy nodes.\n\nWe were able to reproduce the memory leak problem with the steps below.\n\n**Steps to reproduce**:\n\n1) Install ES 2.3.5 on two different nodes\n\n2) Create a synonym dictionary using the command below on each node:\n`mkdir $ES_HOME/config/analysis/ && cat /usr/share/dict/words | sed -r 's/(.*)/\\1,\\1_synonym/' > $ES_HOME/config/analysis/synonym.txt`\n3) Start both nodes with 256MB of heap.\n`export ES_HEAP_SIZE=256m && $ES_HOME/bin/elasticsearch`\n4) Create an index using the following schema:\n\n```\ncurl -XPUT localhost:9200/test_index -d '{\n  \"settings\": {\n    \"similarity\": {\n      \"ta_bm25\": {\n        \"type\": \"BM25\",\n        \"b\": 0.1,\n        \"k1\": 20\n      }\n    },\n    \"index\": {\n      \"number_of_shards\": 3,\n      \"analysis\": {\n        \"filter\": {\n          \"my_synonym\": {\n            \"type\": \"synonym\",\n            \"synonyms_path\": \"analysis/synonym.txt\"\n          },\n          \"word_delimiter_filter\": {\n            \"type\": \"word_delimiter\",\n            \"catenate_words\": true,\n            \"catenate_all\": true,\n            \"preserve_original\": true,\n            \"stem_english_possessive\": false\n          },\n          \"abbreviation_filter\": {\n            \"type\": \"pattern_replace\",\n            \"pattern\": \"([a-zA-Z0-9]{1})[.]\",\n            \"replacement\": \"$1\"\n          },\n          \"ngram\": {\n            \"type\": \"edgeNGram\",\n            \"min_gram\": 1,\n            \"max_gram\": 12\n          },\n          \"stem_en\": {\n            \"type\": \"stemmer\",\n            \"name\": \"minimal_english\"\n          }\n        },\n        \"analyzer\": {\n          \"search_analyzer\": {\n            \"filter\": [\n              \"asciifolding\",\n              \"lowercase\",\n              \"abbreviation_filter\",\n              \"word_delimiter_filter\",\n              \"my_synonym\",\n              \"stem_en\"\n            ],\n            \"tokenizer\": \"whitespace\"\n          },\n          \"index_analyzer\": {\n            \"tokenizer\": \"whitespace\",\n            \"filter\": [\n              \"asciifolding\",\n              \"lowercase\",\n              \"abbreviation_filter\",\n              \"word_delimiter_filter\",\n              \"stem_en\"\n            ]\n          }\n        }\n      },\n      \"number_of_replicas\": 1\n    }\n  },\n  \"mappings\": {\n    \"my_type\": {\n      \"_source\": {\n        \"enabled\": false\n      },\n      \"_all\": {\n        \"enabled\": false\n      },\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\",\n          \"store\": \"no\",\n          \"index\": \"analyzed\",\n          \"analyzer\": \"index_analyzer\",\n          \"search_analyzer\": \"search_analyzer\",\n          \"similarity\": \"BM25\"\n        }\n      }\n    }\n  }\n}'\n```\n\n5) Create a test index:\n`for k in {1..10000} ; do for i in {1..100} ; do printf \"{\\\"index\\\": {\\\"_index\\\":\\\"test_index\\\", \\\"_type\\\":\\\"my_type\\\",\\\"_id\\\":\\\"${k}-${i}\\\"}}\\n{\\\"name\\\":\\\"$(shuf -n 5 /usr/share/dict/words | tr '\\n' ' ')\\\"}\\n\" ; done | curl -XPUT 'localhost:9200/_bulk' --data-binary @- ; done`\n\n6) Once the indexing is done, there should be one index called test_index, with one million documents, 3 shards and one replica. That means that both nodes should have 3 shards, one replica each. The command below should say that heap is around 40-50%.\n`curl -s 'localhost:9200/_cat/nodes?v&h=name,heap.current,heap.max,heap.percent'`\n\n7) Shutdown one of the nodes.\n\n8) Remove the dictionary file from that node and start it again.\n\n```\nrm -rf $ES_HOME/config/analysis/synonym.txt\nexport ES_HEAP_SIZE=256m && $ES_HOME/bin/elasticsearch\n```\n\n9) At this point, the node without the dictionary file will attempt to allocate the index shards, but will fail with the exception pasted log section below. ES attempts to do that over and over. Use the command below to keep track of the allocated heap.\n`curl -s 'localhost:9200/_cat/nodes?v&h=name,heap.current,heap.max,heap.percent'`\n\n10) After about 15-20 minutes, the heap will go above 95% and GC doesn't seem to help.\n\n11) To make things even worse, recreate the synonym dictionary (recreate, copy from a backed up file or from the other node).\n\n12) At this point, the node that is running out of memory will finally be able to allocate the those shards. Unfortunately, it will OOM.\n\n**Provide logs (if relevant)**:\n\n```\n[2016-08-08 15:15:11,172][WARN ][indices.cluster ] [Xi'an Chi Xan] [[test_index][0]] marking and sending shard failed due to [failed to create index] [16/1885]\n[test_index] IndexCreationException[failed to create index]; nested: IllegalArgumentException[IOException while reading synonyms_path_path: /home/acrestani/Downloads/elasticsearch-2.3.5/config/analysis\n/synonym.txt (No such file or directory)];\nat org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:360)\nat org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:294)\nat org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:163)\nat org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)\nat org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)\nat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\nat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalArgumentException: IOException while reading synonyms_path_path: /home/acrestani/Downloads/elasticsearch-2.3.5/config/analysis/synonym.txt (No such file or directory)\nat org.elasticsearch.index.analysis.Analysis.getReaderFromFile(Analysis.java:288)\nat org.elasticsearch.index.analysis.SynonymTokenFilterFactory.<init>(SynonymTokenFilterFactory.java:64)\nat sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\nat org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)\nat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\nat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)\nat org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)\nat org.elasticsearch.common.inject.InjectorImpl$4$1.call(InjectorImpl.java:823)\nat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:886)\nat org.elasticsearch.common.inject.InjectorImpl$4.get(InjectorImpl.java:818)\nat org.elasticsearch.common.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:236)\nat com.sun.proxy.$Proxy15.create(Unknown Source)\nat org.elasticsearch.index.analysis.AnalysisService.<init>(AnalysisService.java:151)\nat org.elasticsearch.index.analysis.AnalysisService.<init>(AnalysisService.java:70)\nat sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\nat org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)\nat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\nat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)\nat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)\nat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:886)\nat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)\nat org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)\nat org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)\nat org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)\nat org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)\nat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)\nat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)\nat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)\nat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:886)\nat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)\nat org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)\nat org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)\nat org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)\nat org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)\nat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)\nat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)\n```\n\nAfter deploying the synonym dictionary it prints this:\n\n```\njava.lang.OutOfMemoryError: Java heap space\nDumping heap to java_pid5205.hprof ...\nHeap dump file created [317848006 bytes in 1.196 secs]\n[2016-08-08 15:35:54,090][WARN ][threadpool ] [Xi'an Chi Xan] failed to run org.elasticsearch.discovery.zen.fd.MasterFaultDetection$MasterPinger@3eb528ce\njava.lang.OutOfMemoryError: Java heap space\nException in thread \"elasticsearch[Xi'an Chi Xan][http_server_boss][T#1]\" Exception in thread \"elasticsearch[Xi'an Chi Xan][generic][T#5]\" Exception in thread \"elasticsearch[Xi'an Chi Xan][transport_client_boss][T#1]\" java.lang.OutOfMemoryError: Java heap space\nat java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)\nat java.lang.StringBuilder.<init>(StringBuilder.java:101)\nat org.elasticsearch.common.logging.support.LoggerMessageFormat.format(LoggerMessageFormat.java:52)\nat org.elasticsearch.common.logging.support.AbstractESLogger.warn(AbstractESLogger.java:109)\nat org.elasticsearch.transport.netty.NettyInternalESLogger.warn(NettyInternalESLogger.java:83)\nat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:340)\nat org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)\nat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\nat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\njava.lang.OutOfMemoryError: Java heap space\njava.lang.OutOfMemoryError: Java heap space\n[2016-08-08 15:35:54,095][WARN ][transport.netty ] [Xi'an Chi Xan] exception caught on transport layer [[id: 0x20cb9904, /10.33.154.216:60026 => /10.96.160.99:9300]], closing connection\njava.lang.IllegalStateException: Message not fully read (request) for requestId [69030], action [internal:discovery/zen/fd/ping], readerIndex [75] vs expected [181]; resetting\nat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:121)\nat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\nat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\nat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\nat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\nat org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\nat org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\nat org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\nat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\nat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\nat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\nat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)\nat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\nat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\nat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\nat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\nat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\nat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\nat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\nat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\nat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\nat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\nat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n```\n","closed_by":{"login":"abeyad","id":1631297,"node_id":"MDQ6VXNlcjE2MzEyOTc=","avatar_url":"https://avatars2.githubusercontent.com/u/1631297?v=4","gravatar_id":"","url":"https://api.github.com/users/abeyad","html_url":"https://github.com/abeyad","followers_url":"https://api.github.com/users/abeyad/followers","following_url":"https://api.github.com/users/abeyad/following{/other_user}","gists_url":"https://api.github.com/users/abeyad/gists{/gist_id}","starred_url":"https://api.github.com/users/abeyad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abeyad/subscriptions","organizations_url":"https://api.github.com/users/abeyad/orgs","repos_url":"https://api.github.com/users/abeyad/repos","events_url":"https://api.github.com/users/abeyad/events{/privacy}","received_events_url":"https://api.github.com/users/abeyad/received_events","type":"User","site_admin":false},"performed_via_github_app":null}