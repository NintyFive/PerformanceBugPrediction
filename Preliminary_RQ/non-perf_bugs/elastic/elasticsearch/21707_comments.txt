[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262254497","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262254497","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262254497,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjI1NDQ5Nw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T14:28:40Z","updated_at":"2016-11-22T14:28:40Z","author_association":"CONTRIBUTOR","body":"@nik9000 are you passing (or allowing to be passed) `?search_type=scan` to the 1.x cluster (which doesn't understand `?sort=_doc`.  If not, then the 1.x cluster is doing deep pagination which is why it is timing out.  This PR was only added in v2 https://github.com/elastic/elasticsearch/pull/4968","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262265535","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262265535","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262265535,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjI2NTUzNQ==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T15:09:10Z","updated_at":"2016-11-22T15:09:10Z","author_association":"CONTRIBUTOR","body":"> @nik9000 are you passing (or allowing to be passed) ?search_type=scan to the 1.x cluster (which doesn't understand ?sort=_doc.\r\n\r\nYes. I switch to `?search_type=scan` when the target cluster is older than `?sort=doc` and handle the empty response that it gives differently. I admit we don't have automated integration testing for this though so we might be doing something wrong.\r\n\r\n> we start seeing OutOfMemoryErrors, nodes get down etc\r\n\r\n@hariso, this seems bad. There are, sadly, quite a few things that can cause an OOM on a 1.7 node. I don't want reindex to invoke those things though! Are you doing anything other than reindex? If not then we have some problem with reindex to look at.\r\n\r\nLike you say it could be that your retry tool is flooding the cluster with these requests. You are right that configuring the socket timeout is probably a thing I should expose. Or bump the default to some crazy long time. Or both. Probably both. And that might help here. Because, you are right to assume that we don't have a mechanism to cancel the search requests. Actually that is added in 5.1 but isn't wired up to cancel the requests when the socket is closed....\r\n\r\nAnyway, I'll experiment a bit with 1.7 today and see if I can reproduce. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262271887","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262271887","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262271887,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjI3MTg4Nw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T15:30:14Z","updated_at":"2016-11-22T15:30:14Z","author_association":"CONTRIBUTOR","body":"@hariso could you paste the reindex request that you run in here please?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262274103","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262274103","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262274103,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjI3NDEwMw==","user":{"login":"hariso","id":2166300,"node_id":"MDQ6VXNlcjIxNjYzMDA=","avatar_url":"https://avatars0.githubusercontent.com/u/2166300?v=4","gravatar_id":"","url":"https://api.github.com/users/hariso","html_url":"https://github.com/hariso","followers_url":"https://api.github.com/users/hariso/followers","following_url":"https://api.github.com/users/hariso/following{/other_user}","gists_url":"https://api.github.com/users/hariso/gists{/gist_id}","starred_url":"https://api.github.com/users/hariso/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hariso/subscriptions","organizations_url":"https://api.github.com/users/hariso/orgs","repos_url":"https://api.github.com/users/hariso/repos","events_url":"https://api.github.com/users/hariso/events{/privacy}","received_events_url":"https://api.github.com/users/hariso/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T15:37:33Z","updated_at":"2016-11-22T15:37:33Z","author_association":"CONTRIBUTOR","body":"Thanks all for prompt responses!\r\n\r\n@clintongormley Here is a single request:\r\n\r\n```\r\n{\r\n    \"dest\": {\r\n        \"index\": \"reindex-01\"\r\n    },\r\n    \"script\": {\r\n        \"inline\": \"ctx._source.remove('_type')\",\r\n        \"lang\": \"painless\"\r\n    },\r\n    \"source\": {\r\n        \"index\": \"my-index\",\r\n        \"query\": {\r\n            \"range\": {\r\n                \"myDateField\": {\r\n                    \"gte\": \"2013-11-05T00:00:00.000Z\",\r\n                    \"lt\": \"2013-11-08T00:00:00.000Z\"\r\n                }\r\n            }\r\n        },\r\n        \"remote\": {\r\n            \"host\": \"http://remote-host-one-dot-seven:9200\"\r\n        },\r\n        \"size\": 5000\r\n    }\r\n}\r\n```\r\n\r\nBut, as I mentioned, a number of those is run in parallel.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262279600","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262279600","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262279600,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjI3OTYwMA==","user":{"login":"hariso","id":2166300,"node_id":"MDQ6VXNlcjIxNjYzMDA=","avatar_url":"https://avatars0.githubusercontent.com/u/2166300?v=4","gravatar_id":"","url":"https://api.github.com/users/hariso","html_url":"https://github.com/hariso","followers_url":"https://api.github.com/users/hariso/followers","following_url":"https://api.github.com/users/hariso/following{/other_user}","gists_url":"https://api.github.com/users/hariso/gists{/gist_id}","starred_url":"https://api.github.com/users/hariso/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hariso/subscriptions","organizations_url":"https://api.github.com/users/hariso/orgs","repos_url":"https://api.github.com/users/hariso/repos","events_url":"https://api.github.com/users/hariso/events{/privacy}","received_events_url":"https://api.github.com/users/hariso/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T15:55:36Z","updated_at":"2016-11-22T15:55:36Z","author_association":"CONTRIBUTOR","body":"> @hariso, this seems bad. There are, sadly, quite a few things that can cause an OOM on a 1.7 node. I don't want reindex to invoke those things though! Are you doing anything other than reindex? If not then we have some problem with reindex to look at.\r\n\r\n@nik9000 Yes, so I'll need to I'll need to find my way through that forest subsequently. It's a production cluster, and it's being used frequently, but I'm not sure how much is it for this index. And, of course, the nodes shards of `my-index`(121M docs, 2.6 TB) live, have shards from other indexes as well, so I'll need to see if some other index is heavily used (I doubt).\r\n\r\nWhen I start reindexing (again: with a few reindex requests at a time), we start seeing the same nodes having problems (the nodes which host shards for `my-index`).\r\n\r\n> Like you say it could be that your retry tool is flooding the cluster with these requests.\r\n\r\nYes, however, when it failed last time it was running 5-7 reindex tasks at the same time (reindex task example is above). That doesn't sound like something, but I might be wrong.\r\n\r\n> Because, you are right to assume that we don't have a mechanism to cancel the search requests.\r\n\r\nI'm not an Elasticsearch expert, and I'll be free to ask if there is something which could help me \"clean up\" a node after a failed search? E.g. [clearing the cache](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/indices-clearcache.html)? ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262284930","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262284930","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262284930,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjI4NDkzMA==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T16:12:49Z","updated_at":"2016-11-22T16:12:49Z","author_association":"CONTRIBUTOR","body":"> I'm not an Elasticsearch expert, and I'll be free to ask if there is something which could help me \"clean up\" a node after a failed search?\r\n\r\nSo the trouble is that the search doesn't really terminate when you close the socket. In later versions of 2.x you can use the tasks API to see that it is still running. In 1.7 you are a bit blind. You can jstack and hunt for the thread running your query but that is kind of hard. You can tell if you have a stack dump and look at the objects and squint just right. Like, if you took one on OOM.\r\n\r\nSo retrying after a socket timeout is dangerous because the old query could still be there and you pile up queries. We make this less likely to blow up on you by having a limited queue depth and size for the search thread pool. So I'd expect you'd see thread pool rejections too.\r\n\r\nI think the safest thing you can do for now is to refuse to retry on socket timeouts.\r\n\r\nYou might also want to make that size smaller - `?search_type=scan` returns `size` documents per shard rather than `size` documents total so 5000 is quite high. Maybe fine, maybe not. I dunno. Hard to tell without a heap dump of the oom. Which, btw, if you have one and you are willing to dump somewhere I'd be happy to look at it. That'd be the best way to figure out where the OOMs are coming from. But I'd see all the data in memory which can be sensitive.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262291892","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262291892","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262291892,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjI5MTg5Mg==","user":{"login":"hariso","id":2166300,"node_id":"MDQ6VXNlcjIxNjYzMDA=","avatar_url":"https://avatars0.githubusercontent.com/u/2166300?v=4","gravatar_id":"","url":"https://api.github.com/users/hariso","html_url":"https://github.com/hariso","followers_url":"https://api.github.com/users/hariso/followers","following_url":"https://api.github.com/users/hariso/following{/other_user}","gists_url":"https://api.github.com/users/hariso/gists{/gist_id}","starred_url":"https://api.github.com/users/hariso/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hariso/subscriptions","organizations_url":"https://api.github.com/users/hariso/orgs","repos_url":"https://api.github.com/users/hariso/repos","events_url":"https://api.github.com/users/hariso/events{/privacy}","received_events_url":"https://api.github.com/users/hariso/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T16:36:10Z","updated_at":"2016-11-22T16:36:10Z","author_association":"CONTRIBUTOR","body":"> So the trouble is that the search doesn't really terminate when you close the socket. In later versions of 2.x you can use the tasks API to see that it is still running. In 1.7 you are a bit blind. You can jstack and hunt for the thread running your query but that is kind of hard. \r\n\r\nGot it, thanks! \r\n\r\n5000 is what I used initially, plus retrying with half the size when it fails (2500, 1250, etc.) Today I used 500, with 7 requests at a time at most, with no retries. It went further, but still made the cluster blush. I am not aware of thread pool rejections right now (I was looking at the tool, which registers errors it got from the Reindex API and Tasks API in the target cluster). Given that I was running a single reindex job, with 5-7 reindex tasks at a time, I would doubt that happened. But it's still good to know we have a layer of protection there!\r\n\r\n> You might also want to make that size smaller - ?search_type=scan returns size documents per shard rather than size documents total so 5000 is quite high.\r\n\r\nThat's something I knew, but completely forgot.:S I did lower it down as mentioned above, but not because I was conscious of this. In any case, that explains some things. `my-index` has 10 shards: with 7 parallel reindex tasks and 500 docs per shard, that becomes 35 000 docs. And with the initial setting: 15 parallel reindex tasks and 5000 docs per shard, that is, wow, 750 000 docs.\r\n\r\n> Which, btw, if you have one and you are willing to dump somewhere I'd be happy to look at it. That'd be the best way to figure out where the OOMs are coming from. But I'd see all the data in memory which can be sensitive.\r\n\r\nI would be happy as well, but there is some customer data in the indexes, so it is quite sensitive. But I will try to get to one when it fails next time.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262294039","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262294039","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262294039,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjI5NDAzOQ==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T16:43:17Z","updated_at":"2016-11-22T16:43:17Z","author_association":"CONTRIBUTOR","body":"> That's something I knew, but completely forgot.:S\r\n\r\nI mean, reindex from remote *could* have done the division for you. I just didn't because I figured it was magic enough as is.\r\n\r\n> 750 000 docs.\r\n\r\nHow big are the docs? Are they fairly uniform. That is a lot of docs to be floating in memory at once. Usually it won't be all of them.\r\n\r\n> I would be happy as well, but there is some customer data in the indexes, so it is quite sensitive. But I will try to get to one when it fails next time.\r\n\r\nWe can talk over email about how to exchange it if you like.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262340430","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262340430","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262340430,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjM0MDQzMA==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T19:29:04Z","updated_at":"2016-11-22T19:29:04Z","author_association":"CONTRIBUTOR","body":"OK! I've played with reindex-from-remote from 1.7 and it works, sending `search_type=scan` just like when I first built it. I loaded a couple million docs without trouble on my laptop. I'll open a PR that makes timeout customizable and defaults it to 60 seconds, up from the 10 seconds that it is now.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262353924","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262353924","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262353924,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjM1MzkyNA==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T20:18:41Z","updated_at":"2016-11-22T20:18:41Z","author_association":"CONTRIBUTOR","body":"So I opened a PR to add the ability to set the timeout and to bump the default timeout to one minute. I'm not sure that it is going to help you if the cluster you are copying from is genuinely running out of memory though.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262354583","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262354583","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262354583,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjM1NDU4Mw==","user":{"login":"hariso","id":2166300,"node_id":"MDQ6VXNlcjIxNjYzMDA=","avatar_url":"https://avatars0.githubusercontent.com/u/2166300?v=4","gravatar_id":"","url":"https://api.github.com/users/hariso","html_url":"https://github.com/hariso","followers_url":"https://api.github.com/users/hariso/followers","following_url":"https://api.github.com/users/hariso/following{/other_user}","gists_url":"https://api.github.com/users/hariso/gists{/gist_id}","starred_url":"https://api.github.com/users/hariso/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hariso/subscriptions","organizations_url":"https://api.github.com/users/hariso/orgs","repos_url":"https://api.github.com/users/hariso/repos","events_url":"https://api.github.com/users/hariso/events{/privacy}","received_events_url":"https://api.github.com/users/hariso/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T20:21:26Z","updated_at":"2016-11-22T20:21:26Z","author_association":"CONTRIBUTOR","body":"> How big are the docs? Are they fairly uniform. That is a lot of docs to be floating in memory at once. Usually it won't be all of them.\r\n\r\nNope, they are very different in sizes, even within a single type in the index. But if some kind of statistics would be valuable, I would pull it out (some overview of size ranges within the index). And definitely, quite a lot. Even with the reduced size I had, 7 threads and size = 500, and 10 shards, it's still a lot.\r\n\r\n> We can talk over email about how to exchange it if you like.\r\n\r\nBecause of HIPAA and some other privacy regulations, it would be great if you would let me know, what you are looking for, and I can provide you the data. Something besides, would be harder to arrange, and I would to ask around in the company how we would do that.\r\n\r\n> OK! I've played with reindex-from-remote from 1.7 and it works, sending search_type=scan just like when I first built it. I loaded a couple million docs without trouble on my laptop. I'll open a PR that makes timeout customizable and defaults it to 60 seconds, up from the 10 seconds that it is now.\r\n\r\nYou rock, man.:) I can try the #21741 PR out hopefully tomorrow. `search_type=scan` is not in the PR, as I see. Is that a part of the solution as well, or did I misunderstand?\r\n\r\n> So I opened a PR to add the ability to set the timeout and to bump the default timeout to one minute. I'm not sure that it is going to help you if the cluster you are copying from is genuinely running out of memory though.\r\n\r\nThanks again! I'm aware of that, and we are investigating other possible issues of OOMs. But I'm hoping that this will at least alleviate some of the pains, since it won't leave hanging searches in the source cluster.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262358249","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262358249","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262358249,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjM1ODI0OQ==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-11-22T20:36:13Z","updated_at":"2016-11-22T20:36:13Z","author_association":"CONTRIBUTOR","body":"> HIPAA\r\n\r\nYeah, lets not send that heap then. I'd look for large object arrays inside an `org.apache.lucene.util.PriorityQueue` to check for deep pagination. That is the min-heap implementation of choice in Lucene and Elasticsearch and is used for stuff like collecting all the search hits on a node.\r\n\r\n> You rock, man.:) I can try the #21741 PR out hopefully tomorrow. search_type=scan is not in the PR, as I see. Is that a part of the solution as well, or did I misunderstand?\r\n\r\n`?search_type=scan` was never broken. It was a good guess though. It was certainly the first place to look for the failure.\r\n\r\nYou may prefer to cherry-pick the contents of the PR onto the 5.x branch for testing. The PR is against master which is what will eventually be 6.0. I'll backport it once it is reviewed.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/262464358","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-262464358","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":262464358,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MjQ2NDM1OA==","user":{"login":"hariso","id":2166300,"node_id":"MDQ6VXNlcjIxNjYzMDA=","avatar_url":"https://avatars0.githubusercontent.com/u/2166300?v=4","gravatar_id":"","url":"https://api.github.com/users/hariso","html_url":"https://github.com/hariso","followers_url":"https://api.github.com/users/hariso/followers","following_url":"https://api.github.com/users/hariso/following{/other_user}","gists_url":"https://api.github.com/users/hariso/gists{/gist_id}","starred_url":"https://api.github.com/users/hariso/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hariso/subscriptions","organizations_url":"https://api.github.com/users/hariso/orgs","repos_url":"https://api.github.com/users/hariso/repos","events_url":"https://api.github.com/users/hariso/events{/privacy}","received_events_url":"https://api.github.com/users/hariso/received_events","type":"User","site_admin":false},"created_at":"2016-11-23T09:16:00Z","updated_at":"2016-11-23T09:16:00Z","author_association":"CONTRIBUTOR","body":"> Yeah, lets not send that heap then. I'd look for large object arrays inside an org.apache.lucene.util.PriorityQueue to check for deep pagination. That is the min-heap implementation of choice in Lucene and Elasticsearch and is used for stuff like collecting all the search hits on a node.\r\n\r\nGracias! Will do as soon as I have the opportunity.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/264697657","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-264697657","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":264697657,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NDY5NzY1Nw==","user":{"login":"hariso","id":2166300,"node_id":"MDQ6VXNlcjIxNjYzMDA=","avatar_url":"https://avatars0.githubusercontent.com/u/2166300?v=4","gravatar_id":"","url":"https://api.github.com/users/hariso","html_url":"https://github.com/hariso","followers_url":"https://api.github.com/users/hariso/followers","following_url":"https://api.github.com/users/hariso/following{/other_user}","gists_url":"https://api.github.com/users/hariso/gists{/gist_id}","starred_url":"https://api.github.com/users/hariso/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hariso/subscriptions","organizations_url":"https://api.github.com/users/hariso/orgs","repos_url":"https://api.github.com/users/hariso/repos","events_url":"https://api.github.com/users/hariso/events{/privacy}","received_events_url":"https://api.github.com/users/hariso/received_events","type":"User","site_admin":false},"created_at":"2016-12-04T11:12:54Z","updated_at":"2016-12-04T11:12:54Z","author_association":"CONTRIBUTOR","body":"I just realized the \"feedback_needed\" label and that I haven't gotten back to you.\r\n\r\n@nik9000 I still haven't had the chance to examine the heap. However, I've backported the changes to 5.0.1 and it works pretty good; the problems are much less common (even though, we have documents which are up to 900 MB, so I'll do more \"hacking\" here, both with configuring the timeout and the buffer).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/264876272","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-264876272","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":264876272,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NDg3NjI3Mg==","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2016-12-05T15:02:20Z","updated_at":"2016-12-05T15:02:20Z","author_association":"CONTRIBUTOR","body":"> even though, we have documents which are up to 900 MB, so I'll do more \"hacking\" here, both with configuring the timeout and the buffer\r\n\r\nYeah, those kinds of documents don't make Elasticsearch happy! \r\n\r\nI'm glad to the see the patch worked. I was going to merge it this morning anyway because it looked \"done\" and I figured you'd reopen if it didn't work. To be honest all the time between opening the PR and merging the patch mostly came from my schedule (holidays and training) and me not being super noisy about getting a review and then not noticing when I had one.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/264917983","html_url":"https://github.com/elastic/elasticsearch/issues/21707#issuecomment-264917983","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/21707","id":264917983,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NDkxNzk4Mw==","user":{"login":"hariso","id":2166300,"node_id":"MDQ6VXNlcjIxNjYzMDA=","avatar_url":"https://avatars0.githubusercontent.com/u/2166300?v=4","gravatar_id":"","url":"https://api.github.com/users/hariso","html_url":"https://github.com/hariso","followers_url":"https://api.github.com/users/hariso/followers","following_url":"https://api.github.com/users/hariso/following{/other_user}","gists_url":"https://api.github.com/users/hariso/gists{/gist_id}","starred_url":"https://api.github.com/users/hariso/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hariso/subscriptions","organizations_url":"https://api.github.com/users/hariso/orgs","repos_url":"https://api.github.com/users/hariso/repos","events_url":"https://api.github.com/users/hariso/events{/privacy}","received_events_url":"https://api.github.com/users/hariso/received_events","type":"User","site_admin":false},"created_at":"2016-12-05T17:26:13Z","updated_at":"2016-12-05T17:26:13Z","author_association":"CONTRIBUTOR","body":"@nik9000 Thank you very much!","performed_via_github_app":null}]