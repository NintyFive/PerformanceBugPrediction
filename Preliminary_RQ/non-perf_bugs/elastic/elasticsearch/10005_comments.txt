[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/77470437","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-77470437","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":77470437,"node_id":"MDEyOklzc3VlQ29tbWVudDc3NDcwNDM3","user":{"login":"jordansissel","id":131818,"node_id":"MDQ6VXNlcjEzMTgxOA==","avatar_url":"https://avatars1.githubusercontent.com/u/131818?v=4","gravatar_id":"","url":"https://api.github.com/users/jordansissel","html_url":"https://github.com/jordansissel","followers_url":"https://api.github.com/users/jordansissel/followers","following_url":"https://api.github.com/users/jordansissel/following{/other_user}","gists_url":"https://api.github.com/users/jordansissel/gists{/gist_id}","starred_url":"https://api.github.com/users/jordansissel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jordansissel/subscriptions","organizations_url":"https://api.github.com/users/jordansissel/orgs","repos_url":"https://api.github.com/users/jordansissel/repos","events_url":"https://api.github.com/users/jordansissel/events{/privacy}","received_events_url":"https://api.github.com/users/jordansissel/received_events","type":"User","site_admin":false},"created_at":"2015-03-05T22:54:59Z","updated_at":"2015-03-05T22:54:59Z","author_association":"CONTRIBUTOR","body":"I know Joda's got a precision limit (the Instant class is millisecond precision) and a year limit (\"year must be in the range [-292275054,292278993]\"). I'm open to helping explore solutions in this area.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/77479544","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-77479544","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":77479544,"node_id":"MDEyOklzc3VlQ29tbWVudDc3NDc5NTQ0","user":{"login":"synhershko","id":212252,"node_id":"MDQ6VXNlcjIxMjI1Mg==","avatar_url":"https://avatars2.githubusercontent.com/u/212252?v=4","gravatar_id":"","url":"https://api.github.com/users/synhershko","html_url":"https://github.com/synhershko","followers_url":"https://api.github.com/users/synhershko/followers","following_url":"https://api.github.com/users/synhershko/following{/other_user}","gists_url":"https://api.github.com/users/synhershko/gists{/gist_id}","starred_url":"https://api.github.com/users/synhershko/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/synhershko/subscriptions","organizations_url":"https://api.github.com/users/synhershko/orgs","repos_url":"https://api.github.com/users/synhershko/repos","events_url":"https://api.github.com/users/synhershko/events{/privacy}","received_events_url":"https://api.github.com/users/synhershko/received_events","type":"User","site_admin":false},"created_at":"2015-03-06T00:00:00Z","updated_at":"2015-03-06T00:00:00Z","author_association":"CONTRIBUTOR","body":"What about consequences to field_date size? even with docvalues in place, cardinality will be ridiculously high. Even for those scenarios which need this, this could be an overkill, no?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/81557655","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-81557655","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":81557655,"node_id":"MDEyOklzc3VlQ29tbWVudDgxNTU3NjU1","user":{"login":"nikonyrh","id":1690501,"node_id":"MDQ6VXNlcjE2OTA1MDE=","avatar_url":"https://avatars0.githubusercontent.com/u/1690501?v=4","gravatar_id":"","url":"https://api.github.com/users/nikonyrh","html_url":"https://github.com/nikonyrh","followers_url":"https://api.github.com/users/nikonyrh/followers","following_url":"https://api.github.com/users/nikonyrh/following{/other_user}","gists_url":"https://api.github.com/users/nikonyrh/gists{/gist_id}","starred_url":"https://api.github.com/users/nikonyrh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikonyrh/subscriptions","organizations_url":"https://api.github.com/users/nikonyrh/orgs","repos_url":"https://api.github.com/users/nikonyrh/repos","events_url":"https://api.github.com/users/nikonyrh/events{/privacy}","received_events_url":"https://api.github.com/users/nikonyrh/received_events","type":"User","site_admin":false},"created_at":"2015-03-16T10:11:18Z","updated_at":"2015-03-16T10:11:18Z","author_association":"NONE","body":"Couldn't you just store the decimal part of the second in a secondary field (as a float or long) and sort by these two fields when needed? You could still aggregate based on the standard date field but not at a microsecond resolution.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/93635444","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-93635444","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":93635444,"node_id":"MDEyOklzc3VlQ29tbWVudDkzNjM1NDQ0","user":{"login":"markwalkom","id":3184718,"node_id":"MDQ6VXNlcjMxODQ3MTg=","avatar_url":"https://avatars0.githubusercontent.com/u/3184718?v=4","gravatar_id":"","url":"https://api.github.com/users/markwalkom","html_url":"https://github.com/markwalkom","followers_url":"https://api.github.com/users/markwalkom/followers","following_url":"https://api.github.com/users/markwalkom/following{/other_user}","gists_url":"https://api.github.com/users/markwalkom/gists{/gist_id}","starred_url":"https://api.github.com/users/markwalkom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markwalkom/subscriptions","organizations_url":"https://api.github.com/users/markwalkom/orgs","repos_url":"https://api.github.com/users/markwalkom/repos","events_url":"https://api.github.com/users/markwalkom/events{/privacy}","received_events_url":"https://api.github.com/users/markwalkom/received_events","type":"User","site_admin":false},"created_at":"2015-04-16T04:16:37Z","updated_at":"2015-04-16T04:16:37Z","author_association":"MEMBER","body":"I've been speaking to a few networking firms lately and it's dawned on me that microsecond level is going to be critical for IDS/network analytics.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/114729084","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-114729084","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":114729084,"node_id":"MDEyOklzc3VlQ29tbWVudDExNDcyOTA4NA==","user":{"login":"markwalkom","id":3184718,"node_id":"MDQ6VXNlcjMxODQ3MTg=","avatar_url":"https://avatars0.githubusercontent.com/u/3184718?v=4","gravatar_id":"","url":"https://api.github.com/users/markwalkom","html_url":"https://github.com/markwalkom","followers_url":"https://api.github.com/users/markwalkom/followers","following_url":"https://api.github.com/users/markwalkom/following{/other_user}","gists_url":"https://api.github.com/users/markwalkom/gists{/gist_id}","starred_url":"https://api.github.com/users/markwalkom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/markwalkom/subscriptions","organizations_url":"https://api.github.com/users/markwalkom/orgs","repos_url":"https://api.github.com/users/markwalkom/repos","events_url":"https://api.github.com/users/markwalkom/events{/privacy}","received_events_url":"https://api.github.com/users/markwalkom/received_events","type":"User","site_admin":false},"created_at":"2015-06-24T05:10:13Z","updated_at":"2015-06-24T05:10:13Z","author_association":"MEMBER","body":"Another request from the community - https://discuss.elastic.co/t/increase-the-resolution-of-elastic-timestamps-to-nanoseconds/24227\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/114729503","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-114729503","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":114729503,"node_id":"MDEyOklzc3VlQ29tbWVudDExNDcyOTUwMw==","user":{"login":"anoinoz","id":7310034,"node_id":"MDQ6VXNlcjczMTAwMzQ=","avatar_url":"https://avatars2.githubusercontent.com/u/7310034?v=4","gravatar_id":"","url":"https://api.github.com/users/anoinoz","html_url":"https://github.com/anoinoz","followers_url":"https://api.github.com/users/anoinoz/followers","following_url":"https://api.github.com/users/anoinoz/following{/other_user}","gists_url":"https://api.github.com/users/anoinoz/gists{/gist_id}","starred_url":"https://api.github.com/users/anoinoz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/anoinoz/subscriptions","organizations_url":"https://api.github.com/users/anoinoz/orgs","repos_url":"https://api.github.com/users/anoinoz/repos","events_url":"https://api.github.com/users/anoinoz/events{/privacy}","received_events_url":"https://api.github.com/users/anoinoz/received_events","type":"User","site_admin":false},"created_at":"2015-06-24T05:14:44Z","updated_at":"2015-06-24T05:14:44Z","author_association":"NONE","body":"that last request is mine I believe. I would add that to monitor networking (and other) activities in our field, nanosecond support is paramount. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/137782107","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-137782107","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":137782107,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNzc4MjEwNw==","user":{"login":"abrisse","id":263838,"node_id":"MDQ6VXNlcjI2MzgzOA==","avatar_url":"https://avatars3.githubusercontent.com/u/263838?v=4","gravatar_id":"","url":"https://api.github.com/users/abrisse","html_url":"https://github.com/abrisse","followers_url":"https://api.github.com/users/abrisse/followers","following_url":"https://api.github.com/users/abrisse/following{/other_user}","gists_url":"https://api.github.com/users/abrisse/gists{/gist_id}","starred_url":"https://api.github.com/users/abrisse/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abrisse/subscriptions","organizations_url":"https://api.github.com/users/abrisse/orgs","repos_url":"https://api.github.com/users/abrisse/repos","events_url":"https://api.github.com/users/abrisse/events{/privacy}","received_events_url":"https://api.github.com/users/abrisse/received_events","type":"User","site_admin":false},"created_at":"2015-09-04T16:25:17Z","updated_at":"2015-09-04T16:25:17Z","author_association":"NONE","body":":+1: for this feature\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/139899024","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-139899024","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":139899024,"node_id":"MDEyOklzc3VlQ29tbWVudDEzOTg5OTAyNA==","user":{"login":"jack-pappas","id":477287,"node_id":"MDQ6VXNlcjQ3NzI4Nw==","avatar_url":"https://avatars1.githubusercontent.com/u/477287?v=4","gravatar_id":"","url":"https://api.github.com/users/jack-pappas","html_url":"https://github.com/jack-pappas","followers_url":"https://api.github.com/users/jack-pappas/followers","following_url":"https://api.github.com/users/jack-pappas/following{/other_user}","gists_url":"https://api.github.com/users/jack-pappas/gists{/gist_id}","starred_url":"https://api.github.com/users/jack-pappas/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jack-pappas/subscriptions","organizations_url":"https://api.github.com/users/jack-pappas/orgs","repos_url":"https://api.github.com/users/jack-pappas/repos","events_url":"https://api.github.com/users/jack-pappas/events{/privacy}","received_events_url":"https://api.github.com/users/jack-pappas/received_events","type":"User","site_admin":false},"created_at":"2015-09-13T17:43:28Z","updated_at":"2015-09-13T17:43:28Z","author_association":"NONE","body":"What about switching from Joda Time to [date4j](http://www.date4j.net/)? It supports higher-precision timestamps compared to Joda and supposedly the performance is better as well.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/139899487","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-139899487","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":139899487,"node_id":"MDEyOklzc3VlQ29tbWVudDEzOTg5OTQ4Nw==","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2015-09-13T17:54:08Z","updated_at":"2015-09-13T17:54:08Z","author_association":"MEMBER","body":"Before looking on the technical side, is BSD License compatible with Apache2 license?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/139900286","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-139900286","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":139900286,"node_id":"MDEyOklzc3VlQ29tbWVudDEzOTkwMDI4Ng==","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2015-09-13T18:04:38Z","updated_at":"2015-09-13T18:04:38Z","author_association":"MEMBER","body":"So BSD is compatible with Apache2.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/141657179","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-141657179","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":141657179,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MTY1NzE3OQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-09-19T11:59:14Z","updated_at":"2015-09-19T11:59:14Z","author_association":"CONTRIBUTOR","body":"I'd like to hear @jpountz's thoughts on this comment https://github.com/elastic/elasticsearch/issues/10005#issuecomment-77479544 about high cardinality with regards to index size and performance.\n\nI could imagine adding a `precision` parameter to date fields which defaults to `ms`, but also accepts `s`, `us`, `ns`.\n\nWe would need to move away from Joda, but I wouldn't be in favour of replacing Joda with a different dependency. Instead, we have this issue discussing replacing Joda with Java.time https://github.com/elastic/elasticsearch/issues/12829\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/142926039","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-142926039","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":142926039,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MjkyNjAzOQ==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2015-09-24T13:24:20Z","updated_at":"2015-09-24T13:24:20Z","author_association":"CONTRIBUTOR","body":"@clintongormley It's hard to predict because it depends so much on the data so I ran an experiment for an application that ingests 1M messages at a 2000 messages per second per shard rate.\n\n| Precision | Terms dict (kB) | Doc values (kB) |\n| --- | --- | --- |\n| milliseconds | 3348 | 2448 |\n| microseconds | 10424 | 3912 |\n\nMillisecond precision is much more space-efficient, in particular because with 2k docs per second, several messages are in the same millisecond, but even if we go with 1M messages at a rate of 200 messages per second so that sharing the same millisecond is much more unlikely, there are still significant differences between millisecond and microsecond precision.\n\n| Precision | Terms dict (kB) | Doc values (kB) |\n| --- | --- | --- |\n| milliseconds | 7604 | 2936 |\n| microseconds | 10680 | 4888 |\n\nThat said, these numbers are for a single field, the overall difference would be much lower if you include `_source` storage, indexes and doc values for other fields, etc.\n\nRegarding performance, it should be pretty similar.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/143191963","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-143191963","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":143191963,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MzE5MTk2Mw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-09-25T11:33:57Z","updated_at":"2015-09-25T11:33:57Z","author_association":"CONTRIBUTOR","body":"From what users have told me, by far the most important reason for storing microseconds is for the sorting of results.  it make no sense to aggregate on buckets smaller than a millisecond.\n\nThis can be achieved very efficiently with the two-field approach: one for the date (in milliseconds) and one for the microseconds.  The microseconds field would not need to be indexed (unless you really need to run a range query with finer precision than one millisecond), so all that would be required is doc_values.  Microseconds can have a maximum of 1,000 values, so doc_values for this field would require just 12 bits per document. \n\nFor the above example, that would be only an extra 11kB.\n\nA logstash filter could make adding the separate microsecond field easy.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/143192382","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-143192382","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":143192382,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MzE5MjM4Mg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-09-25T11:37:15Z","updated_at":"2015-09-25T11:37:15Z","author_association":"CONTRIBUTOR","body":"Meh - there aren't 1000 bits in a byte.  /me hangs his head in shame.\n\nIt would require 1,500kB\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/144446125","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-144446125","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":144446125,"node_id":"MDEyOklzc3VlQ29tbWVudDE0NDQ0NjEyNQ==","user":{"login":"pfennema","id":1378714,"node_id":"MDQ6VXNlcjEzNzg3MTQ=","avatar_url":"https://avatars3.githubusercontent.com/u/1378714?v=4","gravatar_id":"","url":"https://api.github.com/users/pfennema","html_url":"https://github.com/pfennema","followers_url":"https://api.github.com/users/pfennema/followers","following_url":"https://api.github.com/users/pfennema/following{/other_user}","gists_url":"https://api.github.com/users/pfennema/gists{/gist_id}","starred_url":"https://api.github.com/users/pfennema/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pfennema/subscriptions","organizations_url":"https://api.github.com/users/pfennema/orgs","repos_url":"https://api.github.com/users/pfennema/repos","events_url":"https://api.github.com/users/pfennema/events{/privacy}","received_events_url":"https://api.github.com/users/pfennema/received_events","type":"User","site_admin":false},"created_at":"2015-09-30T15:27:14Z","updated_at":"2015-09-30T15:27:14Z","author_association":"NONE","body":"If we want to use the ELK framework proper analyzing network latency we really need nanosecond resolution. Are there any firm plans/roadmap to change the timestamps?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/144618651","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-144618651","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":144618651,"node_id":"MDEyOklzc3VlQ29tbWVudDE0NDYxODY1MQ==","user":{"login":"portante","id":949097,"node_id":"MDQ6VXNlcjk0OTA5Nw==","avatar_url":"https://avatars2.githubusercontent.com/u/949097?v=4","gravatar_id":"","url":"https://api.github.com/users/portante","html_url":"https://github.com/portante","followers_url":"https://api.github.com/users/portante/followers","following_url":"https://api.github.com/users/portante/following{/other_user}","gists_url":"https://api.github.com/users/portante/gists{/gist_id}","starred_url":"https://api.github.com/users/portante/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/portante/subscriptions","organizations_url":"https://api.github.com/users/portante/orgs","repos_url":"https://api.github.com/users/portante/repos","events_url":"https://api.github.com/users/portante/events{/privacy}","received_events_url":"https://api.github.com/users/portante/received_events","type":"User","site_admin":false},"created_at":"2015-10-01T04:43:17Z","updated_at":"2015-10-01T04:43:17Z","author_association":"NONE","body":"Let's say I index the following JSON document with nanosecond precision timestamps:\n\n```\n{ \"@timestamp\": \"2015-09-30T12:30:42.123456789-07:00\", \"message\": \"time is running out\" }\n```\n\nSo the internal date representation will be, `2015-09-30T19:30:42.123` UTC, right?\n\nBut if I issue a query matching that document, and ask for either the `_source` document or the `@timestamp` field explicitly, won't I get back the original string?  If so, then in cases where the original time string lexicographically sorts the same as the converted time value, would that be sufficient for a client to further sort to get what they need?\n\nOr is there a requirement that internal date manipulations in ES need such nanosecond precision?  I am imagining that if one has records with nanosecond precision, only being able to query for a date range with millisecond precision could potentially result in more document matches than wanted.  Is that the major concern?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/145029479","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-145029479","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":145029479,"node_id":"MDEyOklzc3VlQ29tbWVudDE0NTAyOTQ3OQ==","user":{"login":"pfennema","id":1378714,"node_id":"MDQ6VXNlcjEzNzg3MTQ=","avatar_url":"https://avatars3.githubusercontent.com/u/1378714?v=4","gravatar_id":"","url":"https://api.github.com/users/pfennema","html_url":"https://github.com/pfennema","followers_url":"https://api.github.com/users/pfennema/followers","following_url":"https://api.github.com/users/pfennema/following{/other_user}","gists_url":"https://api.github.com/users/pfennema/gists{/gist_id}","starred_url":"https://api.github.com/users/pfennema/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pfennema/subscriptions","organizations_url":"https://api.github.com/users/pfennema/orgs","repos_url":"https://api.github.com/users/pfennema/repos","events_url":"https://api.github.com/users/pfennema/events{/privacy}","received_events_url":"https://api.github.com/users/pfennema/received_events","type":"User","site_admin":false},"created_at":"2015-10-02T13:59:28Z","updated_at":"2015-10-02T13:59:28Z","author_association":"NONE","body":"I think the latter, internal date manipulations need probably nanosecond precision. Reason is that when  monitoring latency on 10Gb networks we get pcap records (or packets directly from the switch via UDP) which include multiple fields with nanosecond timestamps in the record. We like to find out the difference between the different timestamps in order to optimize our network/software and find correlations. In order to do this we like to zoom in on every single record and not aggregate records.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/150979596","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-150979596","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":150979596,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MDk3OTU5Ng==","user":{"login":"abierbaum","id":397889,"node_id":"MDQ6VXNlcjM5Nzg4OQ==","avatar_url":"https://avatars1.githubusercontent.com/u/397889?v=4","gravatar_id":"","url":"https://api.github.com/users/abierbaum","html_url":"https://github.com/abierbaum","followers_url":"https://api.github.com/users/abierbaum/followers","following_url":"https://api.github.com/users/abierbaum/following{/other_user}","gists_url":"https://api.github.com/users/abierbaum/gists{/gist_id}","starred_url":"https://api.github.com/users/abierbaum/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abierbaum/subscriptions","organizations_url":"https://api.github.com/users/abierbaum/orgs","repos_url":"https://api.github.com/users/abierbaum/repos","events_url":"https://api.github.com/users/abierbaum/events{/privacy}","received_events_url":"https://api.github.com/users/abierbaum/received_events","type":"User","site_admin":false},"created_at":"2015-10-25T22:13:09Z","updated_at":"2015-10-25T22:13:09Z","author_association":"NONE","body":":+1: for solving this.  It is causing major issues for us now in our logging infrastructure.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/151096664","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-151096664","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":151096664,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MTA5NjY2NA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-10-26T10:42:12Z","updated_at":"2015-10-26T10:42:12Z","author_association":"CONTRIBUTOR","body":"@pfennema @abierbaum What problems are you having that can't be solved with the two-field solution?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/151109180","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-151109180","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":151109180,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MTEwOTE4MA==","user":{"login":"pfennema","id":1378714,"node_id":"MDQ6VXNlcjEzNzg3MTQ=","avatar_url":"https://avatars3.githubusercontent.com/u/1378714?v=4","gravatar_id":"","url":"https://api.github.com/users/pfennema","html_url":"https://github.com/pfennema","followers_url":"https://api.github.com/users/pfennema/followers","following_url":"https://api.github.com/users/pfennema/following{/other_user}","gists_url":"https://api.github.com/users/pfennema/gists{/gist_id}","starred_url":"https://api.github.com/users/pfennema/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pfennema/subscriptions","organizations_url":"https://api.github.com/users/pfennema/orgs","repos_url":"https://api.github.com/users/pfennema/repos","events_url":"https://api.github.com/users/pfennema/events{/privacy}","received_events_url":"https://api.github.com/users/pfennema/received_events","type":"User","site_admin":false},"created_at":"2015-10-26T11:46:17Z","updated_at":"2015-10-26T11:46:17Z","author_association":"NONE","body":"What we like to have is that we have a timescale in the display (Kibana) where we can zoom in on the individual measurements which have a timestamp with nanosecond resolution. A record in our case has multiple fields (NICTimestamp, TransactionTimestamp, etc) which we like to correlate with each other on an individual basis hence not aggregated. We need to see where spikes occur to optimize our environment. If we can have on the x-axis the time in micro/nanosecond resolution we should be able to zoom in on individual measurements.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/151117222","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-151117222","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":151117222,"node_id":"MDEyOklzc3VlQ29tbWVudDE1MTExNzIyMg==","user":{"login":"abierbaum","id":397889,"node_id":"MDQ6VXNlcjM5Nzg4OQ==","avatar_url":"https://avatars1.githubusercontent.com/u/397889?v=4","gravatar_id":"","url":"https://api.github.com/users/abierbaum","html_url":"https://github.com/abierbaum","followers_url":"https://api.github.com/users/abierbaum/followers","following_url":"https://api.github.com/users/abierbaum/following{/other_user}","gists_url":"https://api.github.com/users/abierbaum/gists{/gist_id}","starred_url":"https://api.github.com/users/abierbaum/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abierbaum/subscriptions","organizations_url":"https://api.github.com/users/abierbaum/orgs","repos_url":"https://api.github.com/users/abierbaum/repos","events_url":"https://api.github.com/users/abierbaum/events{/privacy}","received_events_url":"https://api.github.com/users/abierbaum/received_events","type":"User","site_admin":false},"created_at":"2015-10-26T12:27:55Z","updated_at":"2015-10-26T12:27:55Z","author_association":"NONE","body":"@clintongormley Our use case is using ELK to analyze logs from the backend processes in our application.  The place we noticed it was postgresql logs.  With the current ELK code base, even though the logs coming from the database server have the commands in order, once they end up elastic search and are visualized in kibana the order of items happening on the same millisecond are lost.  We can add a secondary sequence number field, but that doesn't work well in Kibana queries (since you can't sort on multiple fields) and causes quite a bit of confusion on the team because they just expect the data in Kibana to be sorted in the same order as it came in from postgresql and logstash.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/158333795","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-158333795","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":158333795,"node_id":"MDEyOklzc3VlQ29tbWVudDE1ODMzMzc5NQ==","user":{"login":"gigi81","id":881067,"node_id":"MDQ6VXNlcjg4MTA2Nw==","avatar_url":"https://avatars0.githubusercontent.com/u/881067?v=4","gravatar_id":"","url":"https://api.github.com/users/gigi81","html_url":"https://github.com/gigi81","followers_url":"https://api.github.com/users/gigi81/followers","following_url":"https://api.github.com/users/gigi81/following{/other_user}","gists_url":"https://api.github.com/users/gigi81/gists{/gist_id}","starred_url":"https://api.github.com/users/gigi81/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gigi81/subscriptions","organizations_url":"https://api.github.com/users/gigi81/orgs","repos_url":"https://api.github.com/users/gigi81/repos","events_url":"https://api.github.com/users/gigi81/events{/privacy}","received_events_url":"https://api.github.com/users/gigi81/received_events","type":"User","site_admin":false},"created_at":"2015-11-20T09:20:34Z","updated_at":"2015-11-20T09:20:34Z","author_association":"NONE","body":"We have the same problem as @abierbaum described. When events happen on the same millisecond the order of the messages is lost.\nAny workaround or suggestion on how to fix this would be really appreciated.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172347328","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-172347328","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":172347328,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjM0NzMyOA==","user":{"login":"dtr2","id":892265,"node_id":"MDQ6VXNlcjg5MjI2NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/892265?v=4","gravatar_id":"","url":"https://api.github.com/users/dtr2","html_url":"https://github.com/dtr2","followers_url":"https://api.github.com/users/dtr2/followers","following_url":"https://api.github.com/users/dtr2/following{/other_user}","gists_url":"https://api.github.com/users/dtr2/gists{/gist_id}","starred_url":"https://api.github.com/users/dtr2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dtr2/subscriptions","organizations_url":"https://api.github.com/users/dtr2/orgs","repos_url":"https://api.github.com/users/dtr2/repos","events_url":"https://api.github.com/users/dtr2/events{/privacy}","received_events_url":"https://api.github.com/users/dtr2/received_events","type":"User","site_admin":false},"created_at":"2016-01-17T16:33:03Z","updated_at":"2016-01-17T16:33:03Z","author_association":"NONE","body":"You don't need to increase the timestamp accuracy: instead, the time sorting should be based on both timestamp and ID: message IDs are monotonically increasing, and specifically, they are monotonically increasing for a set of messages with the same timestamp...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172348562","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-172348562","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":172348562,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjM0ODU2Mg==","user":{"login":"jcollie","id":740022,"node_id":"MDQ6VXNlcjc0MDAyMg==","avatar_url":"https://avatars1.githubusercontent.com/u/740022?v=4","gravatar_id":"","url":"https://api.github.com/users/jcollie","html_url":"https://github.com/jcollie","followers_url":"https://api.github.com/users/jcollie/followers","following_url":"https://api.github.com/users/jcollie/following{/other_user}","gists_url":"https://api.github.com/users/jcollie/gists{/gist_id}","starred_url":"https://api.github.com/users/jcollie/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jcollie/subscriptions","organizations_url":"https://api.github.com/users/jcollie/orgs","repos_url":"https://api.github.com/users/jcollie/repos","events_url":"https://api.github.com/users/jcollie/events{/privacy}","received_events_url":"https://api.github.com/users/jcollie/received_events","type":"User","site_admin":false},"created_at":"2016-01-17T16:46:53Z","updated_at":"2016-01-17T16:46:53Z","author_association":"NONE","body":"@dtr That may be true for IDs that are automatically assigned, but only if the messages are indexed in the correct order in the first place, and only if the application isn't supplying it's own IDs. There's definitely no way that I could depend on that behavior. Also is the monotonically increasing IDs guaranteed, or is it an implementation artifact, especially when considering clusters of more than one node? \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172453609","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-172453609","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":172453609,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjQ1MzYwOQ==","user":{"login":"dtr2","id":892265,"node_id":"MDQ6VXNlcjg5MjI2NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/892265?v=4","gravatar_id":"","url":"https://api.github.com/users/dtr2","html_url":"https://github.com/dtr2","followers_url":"https://api.github.com/users/dtr2/followers","following_url":"https://api.github.com/users/dtr2/following{/other_user}","gists_url":"https://api.github.com/users/dtr2/gists{/gist_id}","starred_url":"https://api.github.com/users/dtr2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dtr2/subscriptions","organizations_url":"https://api.github.com/users/dtr2/orgs","repos_url":"https://api.github.com/users/dtr2/repos","events_url":"https://api.github.com/users/dtr2/events{/privacy}","received_events_url":"https://api.github.com/users/dtr2/received_events","type":"User","site_admin":false},"created_at":"2016-01-18T07:47:45Z","updated_at":"2016-01-18T07:47:45Z","author_association":"NONE","body":"I believe the original intent was to see a bulk of log messages originated from the same source. If a cluster is involved, then probably the timestamp is the only clustering item (unless, of course, there is a special \"context\" or \"session\" field)\nFor that purpose, we can rely on the id (assuming, of course, its monotonically increasing at least per source)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172653000","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-172653000","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":172653000,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjY1MzAwMA==","user":{"login":"jcollie","id":740022,"node_id":"MDQ6VXNlcjc0MDAyMg==","avatar_url":"https://avatars1.githubusercontent.com/u/740022?v=4","gravatar_id":"","url":"https://api.github.com/users/jcollie","html_url":"https://github.com/jcollie","followers_url":"https://api.github.com/users/jcollie/followers","following_url":"https://api.github.com/users/jcollie/following{/other_user}","gists_url":"https://api.github.com/users/jcollie/gists{/gist_id}","starred_url":"https://api.github.com/users/jcollie/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jcollie/subscriptions","organizations_url":"https://api.github.com/users/jcollie/orgs","repos_url":"https://api.github.com/users/jcollie/repos","events_url":"https://api.github.com/users/jcollie/events{/privacy}","received_events_url":"https://api.github.com/users/jcollie/received_events","type":"User","site_admin":false},"created_at":"2016-01-18T21:02:01Z","updated_at":"2016-01-18T21:02:01Z","author_association":"NONE","body":"@dtr2 that's a lot of \"ifs\" to be relying on ElasticSearch's autogenerated IDs, nearly all of which are violated in my cluster:\n\n1) Some messages supply their own ID if the source has a unique ID already associated with it (systemd journal messages in my case).\n2) All of my data runs through a RabbitMQ server (sometimes passing through multiple queues depending on the amount of processing that needs to be done) with multiple consumers per queue so there's no way that I can expect documents to be indexed in any specific order, much less by the same ElasticSearch node.\n\nIn any case, ElasticSearch does not guarantee the behavior of autogenerated IDs.  The docs only guarantee that the IDs are unique:\n\nhttps://www.elastic.co/guide/en/elasticsearch/guide/current/index-doc.html\n\nSo I can hope that you see that trying to impose an order based upon the ID cannot be relied upon in the general case.  Yes, there may be certain specific instances where that would work, but you'd be relying on undocumented implementation details.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172669491","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-172669491","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":172669491,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjY2OTQ5MQ==","user":{"login":"dtr2","id":892265,"node_id":"MDQ6VXNlcjg5MjI2NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/892265?v=4","gravatar_id":"","url":"https://api.github.com/users/dtr2","html_url":"https://github.com/dtr2","followers_url":"https://api.github.com/users/dtr2/followers","following_url":"https://api.github.com/users/dtr2/following{/other_user}","gists_url":"https://api.github.com/users/dtr2/gists{/gist_id}","starred_url":"https://api.github.com/users/dtr2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dtr2/subscriptions","organizations_url":"https://api.github.com/users/dtr2/orgs","repos_url":"https://api.github.com/users/dtr2/repos","events_url":"https://api.github.com/users/dtr2/events{/privacy}","received_events_url":"https://api.github.com/users/dtr2/received_events","type":"User","site_admin":false},"created_at":"2016-01-18T22:21:07Z","updated_at":"2016-01-18T22:21:07Z","author_association":"NONE","body":"@jcollie , In that case, trying to find a \"context\" is impossible - unless your data source provides it. The idea was to find a context and filter \"related\" lines together.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172669677","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-172669677","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":172669677,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjY2OTY3Nw==","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"created_at":"2016-01-18T22:22:18Z","updated_at":"2016-01-18T22:22:18Z","author_association":"CONTRIBUTOR","body":"@jcollie \"IDs\" (in fact they are counters per log source) have to be generated before ingestion, outside of elasticsearch.\n\nInstead of sorting on time you sort on time, counter.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172691049","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-172691049","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":172691049,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjY5MTA0OQ==","user":{"login":"jcollie","id":740022,"node_id":"MDQ6VXNlcjc0MDAyMg==","avatar_url":"https://avatars1.githubusercontent.com/u/740022?v=4","gravatar_id":"","url":"https://api.github.com/users/jcollie","html_url":"https://github.com/jcollie","followers_url":"https://api.github.com/users/jcollie/followers","following_url":"https://api.github.com/users/jcollie/following{/other_user}","gists_url":"https://api.github.com/users/jcollie/gists{/gist_id}","starred_url":"https://api.github.com/users/jcollie/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jcollie/subscriptions","organizations_url":"https://api.github.com/users/jcollie/orgs","repos_url":"https://api.github.com/users/jcollie/repos","events_url":"https://api.github.com/users/jcollie/events{/privacy}","received_events_url":"https://api.github.com/users/jcollie/received_events","type":"User","site_admin":false},"created_at":"2016-01-19T00:23:13Z","updated_at":"2016-01-19T00:23:13Z","author_association":"NONE","body":"I don't get it - what is the resistance to extending timestamps to nanosecond accuracy?  I realize that would take a lot of work and would likely be a \"3.x\" feature, but anything else is just a workaround until nanosecond timestamps are available.\n\nHaving a counter per log source only really helps correlating messages from the same source, but is really not very useful in correlating messages across sources/systems.\n\nAs an example, let's say that I implement this counter in each of my log sources (for example as a logstash plugin).  Then let's say that I have one source that generates 1000 messages per millisecond and another source that generates 100000 messages per millisecond.  There's no way that I could reliably tell what order those messages should be in relative to each source.  That may be an extreme example but I think that it illustrates the point.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172790794","html_url":"https://github.com/elastic/elasticsearch/issues/10005#issuecomment-172790794","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/10005","id":172790794,"node_id":"MDEyOklzc3VlQ29tbWVudDE3Mjc5MDc5NA==","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"created_at":"2016-01-19T09:31:17Z","updated_at":"2016-01-19T09:31:17Z","author_association":"CONTRIBUTOR","body":"> Having a counter per log source only really helps correlating messages from the same source, but is really not very useful in correlating messages across sources/systems.\n\n@jcollie can you tell me how you keep clocks on your machines in perfect sync so nanosecond accuracy starts making sense? Even Google struggles to do so:\n- http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/spanner-osdi2012.pdf\n\n> Then let's say that I have one source that generates 1000 messages per millisecond and another source that generates 100000 messages per millisecond. There's no way that I could reliably tell what order those messages should be in relative to each source.\n\nYou are right here. There is no way. You can easily see jitter of a few ms between ntp sync between machines in the same rack:\n\n```\n19 Jan 09:27:40 ntpdate[11731]: adjust time server 10.36.14.18 offset -0.002199 sec\n19 Jan 09:27:50 ntpdate[11828]: adjust time server 10.36.14.18 offset 0.004238 sec\n```\n\nOn the other hand, you can reliably say in which order messages were processed by a single source:\n- Single thread of your program.\n- Single queue in logstash.\n- Some other strictly ordered sequence (ex: kafka partition).\n\nI'd be happy to be proven wrong, though.\n","performed_via_github_app":null}]