[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39282033","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39282033","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39282033,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MjgyMDMz","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2014-04-02T02:06:19Z","updated_at":"2014-04-02T02:06:19Z","author_association":"MEMBER","body":"When snapshot is finished, could you execute the following command to see if there are any shard failures there: `curl -XGET \"localhost:9200/_snapshot/repository_name/snapshot_name\"`? If you have replicas for these indices enabled it's also possible that all primary shards are located on node 1 and node 2, in this case the snapshot might be successful since only primary shards are getting snapshotted. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39282592","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39282592","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39282592,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MjgyNTky","user":{"login":"RobbieHer","id":7075611,"node_id":"MDQ6VXNlcjcwNzU2MTE=","avatar_url":"https://avatars2.githubusercontent.com/u/7075611?v=4","gravatar_id":"","url":"https://api.github.com/users/RobbieHer","html_url":"https://github.com/RobbieHer","followers_url":"https://api.github.com/users/RobbieHer/followers","following_url":"https://api.github.com/users/RobbieHer/following{/other_user}","gists_url":"https://api.github.com/users/RobbieHer/gists{/gist_id}","starred_url":"https://api.github.com/users/RobbieHer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RobbieHer/subscriptions","organizations_url":"https://api.github.com/users/RobbieHer/orgs","repos_url":"https://api.github.com/users/RobbieHer/repos","events_url":"https://api.github.com/users/RobbieHer/events{/privacy}","received_events_url":"https://api.github.com/users/RobbieHer/received_events","type":"User","site_admin":false},"created_at":"2014-04-02T02:17:57Z","updated_at":"2014-04-02T02:17:57Z","author_association":"NONE","body":"Thanks! I did verify the case where I had a node in a cluster with only one shard with 0 replicas. This node could not connect to the repo and spewed out errors in the elastic search logs. However, the curl command to take a snapshot did not return any errors and reported all shards to be snapshots successfully.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39282689","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39282689","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39282689,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MjgyNjg5","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2014-04-02T02:19:59Z","updated_at":"2014-04-02T02:19:59Z","author_association":"MEMBER","body":"Could you gist the errors from the log that you have seen?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39289111","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39289111","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39289111,"node_id":"MDEyOklzc3VlQ29tbWVudDM5Mjg5MTEx","user":{"login":"RobbieHer","id":7075611,"node_id":"MDQ6VXNlcjcwNzU2MTE=","avatar_url":"https://avatars2.githubusercontent.com/u/7075611?v=4","gravatar_id":"","url":"https://api.github.com/users/RobbieHer","html_url":"https://github.com/RobbieHer","followers_url":"https://api.github.com/users/RobbieHer/followers","following_url":"https://api.github.com/users/RobbieHer/following{/other_user}","gists_url":"https://api.github.com/users/RobbieHer/gists{/gist_id}","starred_url":"https://api.github.com/users/RobbieHer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RobbieHer/subscriptions","organizations_url":"https://api.github.com/users/RobbieHer/orgs","repos_url":"https://api.github.com/users/RobbieHer/repos","events_url":"https://api.github.com/users/RobbieHer/events{/privacy}","received_events_url":"https://api.github.com/users/RobbieHer/received_events","type":"User","site_admin":false},"created_at":"2014-04-02T05:02:20Z","updated_at":"2014-04-02T05:02:20Z","author_association":"NONE","body":"Sure. Here are the errors from the node which had the data , but could not write to the repository.\n\n[2014-03-31 17:33:37,684][WARN ][repositories             ] [Hermod] failed to create repository [fs][my_backup]\norg.elasticsearch.common.inject.CreationException: Guice creation errors:\n\n1) Error injecting constructor, org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [C:/CanAccessOnlyFromNode1]\n  at org.elasticsearch.repositories.fs.FsRepository.<init>(Unknown Source)\n  while locating org.elasticsearch.repositories.fs.FsRepository\n  while locating org.elasticsearch.repositories.Repository\n\n1 error\n        at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)\n        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)\n        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)\n        at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)\n        at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)\n        at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:384)\n        at org.elasticsearch.repositories.RepositoriesService.clusterChanged(RepositoriesService.java:280)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:427)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\nCaused by: org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [C:/CanAccessOnlyFromNode1]\n        at org.elasticsearch.common.blobstore.fs.FsBlobStore.<init>(FsBlobStore.java:52)\n        at org.elasticsearch.repositories.fs.FsRepository.<init>(FsRepository.java:83)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\n        at java.lang.reflect.Constructor.newInstance(Unknown Source)\n        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)\n        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\n        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\n        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\n        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\n        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\n        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)\n        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)\n        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)\n        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)\n        ... 10 more\n[2014-03-31 17:33:37,699][WARN ][repositories             ] [Hermod] failure updating cluster state\norg.elasticsearch.repositories.RepositoryException: [my_backup] failed to create repository\n        at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:394)\n        at org.elasticsearch.repositories.RepositoriesService.clusterChanged(RepositoriesService.java:280)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:427)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\nCaused by: org.elasticsearch.common.inject.CreationException: Guice creation errors:\n\n1) Error injecting constructor, org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [C:/CanAccessOnlyFromNode1]\n  at org.elasticsearch.repositories.fs.FsRepository.<init>(Unknown Source)\n  while locating org.elasticsearch.repositories.fs.FsRepository\n  while locating org.elasticsearch.repositories.Repository\n\n1 error\n        at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)\n        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)\n        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)\n        at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)\n        at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)\n        at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:384)\n        ... 6 more\nCaused by: org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [C:/CanAccessOnlyFromNode1]\n        at org.elasticsearch.common.blobstore.fs.FsBlobStore.<init>(FsBlobStore.java:52)\n        at org.elasticsearch.repositories.fs.FsRepository.<init>(FsRepository.java:83)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\n        at java.lang.reflect.Constructor.newInstance(Unknown Source)\n        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)\n        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\n        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\n        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\n        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\n        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\n        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)\n        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)\n        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)\n        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)\n        ... 10 more\n[2014-03-31 17:33:37,738][INFO ][gateway                  ] [Hermod] recovered [4] indices into cluster_state\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39322099","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39322099","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39322099,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzIyMDk5","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2014-04-02T12:05:12Z","updated_at":"2014-04-02T12:05:12Z","author_association":"MEMBER","body":"I tried to reproduce this issue, but wasn't able to. Could you paste here the output of GET snapshot command for one of the snapshot that wasn't complete. The GET snapshot command look like this: `curl -XGET \"localhost:9200/_snapshot/my_backup/snapshot_name\"`, please replace `snapshot_name` with the name of your test snapshot.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39357096","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39357096","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39357096,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzU3MDk2","user":{"login":"RobbieHer","id":7075611,"node_id":"MDQ6VXNlcjcwNzU2MTE=","avatar_url":"https://avatars2.githubusercontent.com/u/7075611?v=4","gravatar_id":"","url":"https://api.github.com/users/RobbieHer","html_url":"https://github.com/RobbieHer","followers_url":"https://api.github.com/users/RobbieHer/followers","following_url":"https://api.github.com/users/RobbieHer/following{/other_user}","gists_url":"https://api.github.com/users/RobbieHer/gists{/gist_id}","starred_url":"https://api.github.com/users/RobbieHer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RobbieHer/subscriptions","organizations_url":"https://api.github.com/users/RobbieHer/orgs","repos_url":"https://api.github.com/users/RobbieHer/repos","events_url":"https://api.github.com/users/RobbieHer/events{/privacy}","received_events_url":"https://api.github.com/users/RobbieHer/received_events","type":"User","site_admin":false},"created_at":"2014-04-02T17:13:00Z","updated_at":"2014-04-02T17:13:00Z","author_association":"NONE","body":"The Get command shows that there are no failures\n\n$ curl -XGET \"localhost:9200/_snapshot/my_backup/snapshot_13\"\n{\"snapshots\":[{\"snapshot\":\"snapshot_13\",\"indices\":[\"restore\",\"newstore\",\"store\",\"default-encryptor-963\"],\"state\":\"SUCCESS\",\"start_time\":\"2014-04-02T17:07:25.634Z\",\"start_time_in_millis\":1396458445634,\"end_time\":\"2014-04-02T17:07:26.069Z\",\"end_time_in_millis\":1396458446069,\"duration_in_millis\":435,\"failures\":[],\"shards\":{\"total\":16,\"failed\":0,\"successful\":16}}]}\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39357762","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39357762","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39357762,"node_id":"MDEyOklzc3VlQ29tbWVudDM5MzU3NzYy","user":{"login":"RobbieHer","id":7075611,"node_id":"MDQ6VXNlcjcwNzU2MTE=","avatar_url":"https://avatars2.githubusercontent.com/u/7075611?v=4","gravatar_id":"","url":"https://api.github.com/users/RobbieHer","html_url":"https://github.com/RobbieHer","followers_url":"https://api.github.com/users/RobbieHer/followers","following_url":"https://api.github.com/users/RobbieHer/following{/other_user}","gists_url":"https://api.github.com/users/RobbieHer/gists{/gist_id}","starred_url":"https://api.github.com/users/RobbieHer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RobbieHer/subscriptions","organizations_url":"https://api.github.com/users/RobbieHer/orgs","repos_url":"https://api.github.com/users/RobbieHer/repos","events_url":"https://api.github.com/users/RobbieHer/events{/privacy}","received_events_url":"https://api.github.com/users/RobbieHer/received_events","type":"User","site_admin":false},"created_at":"2014-04-02T17:18:52Z","updated_at":"2014-04-02T17:18:52Z","author_association":"NONE","body":"Could you also confirm if it is absolutely necessary for the same shared repository to be 'registered' from each of the ES nodes in a cluster? Currently, I registered the shared repository only from the node [Node 1] where I ran the backup command. When I run the 'register' command on Node 1, I see that the ES logs on Node 3 ( which cannot access the shared repo) show an error. However, the register command on Node 1 shows success.\n\n$ curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d '{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"X:/CanAccessOnlyFromNode1\",\n        \"compress\": true\n    }\n}'\n{\"acknowledged\":true}\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39412288","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39412288","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39412288,"node_id":"MDEyOklzc3VlQ29tbWVudDM5NDEyMjg4","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2014-04-03T04:44:05Z","updated_at":"2014-04-03T04:44:05Z","author_association":"MEMBER","body":"As I mentioned before as long as you have this directory available on all nodes where primary shards are located, the snapshot will succeed. It looks like this is what happened to the snapshot that you provided. However, since you cannot control primary shard allocation, it's a good practice to have this directory available on all nodes.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39421577","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39421577","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39421577,"node_id":"MDEyOklzc3VlQ29tbWVudDM5NDIxNTc3","user":{"login":"geekpete","id":2070843,"node_id":"MDQ6VXNlcjIwNzA4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/2070843?v=4","gravatar_id":"","url":"https://api.github.com/users/geekpete","html_url":"https://github.com/geekpete","followers_url":"https://api.github.com/users/geekpete/followers","following_url":"https://api.github.com/users/geekpete/following{/other_user}","gists_url":"https://api.github.com/users/geekpete/gists{/gist_id}","starred_url":"https://api.github.com/users/geekpete/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/geekpete/subscriptions","organizations_url":"https://api.github.com/users/geekpete/orgs","repos_url":"https://api.github.com/users/geekpete/repos","events_url":"https://api.github.com/users/geekpete/events{/privacy}","received_events_url":"https://api.github.com/users/geekpete/received_events","type":"User","site_admin":false},"created_at":"2014-04-03T07:45:36Z","updated_at":"2014-04-03T07:45:36Z","author_association":"MEMBER","body":"Hi Team,\n\nI've also seen the same symptom of snapshot being marked as successful when some nodes cannot write to the repository.\n\nI'm using 1.0.1 with the aws-cloud plugin to store snapshots to S3.\n\nWhen performing a snapshot, the snapshot is stored in S3 and marked as Successful. It also shows as an available \"Successful\" snapshot on my other cluster that I'm testing restore from.\n\nWhen attempting a restore, elasticsearch performs some sort of consistency check on the snapshot and determines it is incomplete and unable to be restored, returning:\n\n```\n{\n  \"readyState\": 4,\n  \"responseText\": \"{\\\"error\\\":\\\"SnapshotRestoreException[[test_snapshots_repo:test_snapshot_2014-04-03.1046] index [my_test_index] wasn't fully snapshotted - cannot restore]\\\",\\\"status\\\":500}\",\n  \"responseJSON\": {\n    \"error\": \"SnapshotRestoreException[[test_snapshots_repo:test_snapshot_2014-04-03.1046] index [my_test_index] wasn't fully snapshotted - cannot restore]\",\n    \"status\": 500\n  },\n  \"status\": 500,\n  \"statusText\": \"Internal Server Error\"\n}\n```\n\nThis is for the same reason explained above, where some nodes cannot access the storage location, so these nodes do not get to write their lucene segments to storage, but the other nodes do complete their pieces of the snapshot successfully. In my case, this was due to one or more nodes in my cluster being too far out of time sync that S3 would reject their connections. The cloud-aws plugin did throw a nice error in the logs for this from the nodes with the issue:\n\n```\n[2014-04-02 20:29:26,251][WARN ][snapshots                ] [elastic_node_1] [[my_test_index][8]] [my_test_repo:test_snapshot_2014-04-03.1046] failed to create snapshot\norg.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [my_test_index][8] The difference between the request time and the current time is too large.\n```\n\nWhich is great, except that though this error is detected by the plugin and notified in the logs, Elasticsearch still marks the snapshot as successful.\n\nI fixed the time sync (ntp) issues on my nodes that were too far out of sync with S3 and tried again.\n\nOn trying again, one other node had a problem with the snapshot repository, some sort of problem where cluster state didn't update this one node with the repository config:\n\n```\nfailed to load class with value [s3]; tried [s3, org.elasticsearch.repositories.S3RepositoryModule, org.elasticsearch.repositories.s3.S3RepositoryModule, org.elasticsearch.repositories.s3.S3RepositoryModule]\n```\n\nAfter restarting this node, it was then able to write to the repository fine and I got a successful backup that was able to be restored on my other cluster.\n\nSo a really nice feature/fix would be to identify whatever logic elasticsearch runs at restore time to verify the consistency of the snapshot, use this logic after a snapshot is taken to verify the snapshot worked ok and mark it as bad if it has failed. As well as that, return a message stating which nodes had issues performing their snapshot task if this is able to be fed back to the user, eg:\n\n```\n\"Snapshot failed: nodes that were unable to write to the snapshot repository: elastic_node_4, elastic_node_7.\"\n```\n\nThe user will then have the clues to go and investigate specific nodes to troubleshoot.\n\nCheers.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39421737","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39421737","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39421737,"node_id":"MDEyOklzc3VlQ29tbWVudDM5NDIxNzM3","user":{"login":"geekpete","id":2070843,"node_id":"MDQ6VXNlcjIwNzA4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/2070843?v=4","gravatar_id":"","url":"https://api.github.com/users/geekpete","html_url":"https://github.com/geekpete","followers_url":"https://api.github.com/users/geekpete/followers","following_url":"https://api.github.com/users/geekpete/following{/other_user}","gists_url":"https://api.github.com/users/geekpete/gists{/gist_id}","starred_url":"https://api.github.com/users/geekpete/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/geekpete/subscriptions","organizations_url":"https://api.github.com/users/geekpete/orgs","repos_url":"https://api.github.com/users/geekpete/repos","events_url":"https://api.github.com/users/geekpete/events{/privacy}","received_events_url":"https://api.github.com/users/geekpete/received_events","type":"User","site_admin":false},"created_at":"2014-04-03T07:47:55Z","updated_at":"2014-04-03T07:47:55Z","author_association":"MEMBER","body":"Adding the consistency check of snapshots to the API would be a great feature as well, so users can run checks on snapshots in a repository manually and check to see if there are broken snapshots in there.\n\n:)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/39421813","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-39421813","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":39421813,"node_id":"MDEyOklzc3VlQ29tbWVudDM5NDIxODEz","user":{"login":"geekpete","id":2070843,"node_id":"MDQ6VXNlcjIwNzA4NDM=","avatar_url":"https://avatars2.githubusercontent.com/u/2070843?v=4","gravatar_id":"","url":"https://api.github.com/users/geekpete","html_url":"https://github.com/geekpete","followers_url":"https://api.github.com/users/geekpete/followers","following_url":"https://api.github.com/users/geekpete/following{/other_user}","gists_url":"https://api.github.com/users/geekpete/gists{/gist_id}","starred_url":"https://api.github.com/users/geekpete/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/geekpete/subscriptions","organizations_url":"https://api.github.com/users/geekpete/orgs","repos_url":"https://api.github.com/users/geekpete/repos","events_url":"https://api.github.com/users/geekpete/events{/privacy}","received_events_url":"https://api.github.com/users/geekpete/received_events","type":"User","site_admin":false},"created_at":"2014-04-03T07:48:49Z","updated_at":"2014-04-03T07:48:49Z","author_association":"MEMBER","body":"Also, if you wanted to reproduce my particular scenario, you could get a cluster of nodes, manually set one node to more than say 30 minutes out of sync with the rest and then try to snapshot to S3 and it should be rejected.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/44715958","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-44715958","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":44715958,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NzE1OTU4","user":{"login":"imotov","id":655851,"node_id":"MDQ6VXNlcjY1NTg1MQ==","avatar_url":"https://avatars3.githubusercontent.com/u/655851?v=4","gravatar_id":"","url":"https://api.github.com/users/imotov","html_url":"https://github.com/imotov","followers_url":"https://api.github.com/users/imotov/followers","following_url":"https://api.github.com/users/imotov/following{/other_user}","gists_url":"https://api.github.com/users/imotov/gists{/gist_id}","starred_url":"https://api.github.com/users/imotov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imotov/subscriptions","organizations_url":"https://api.github.com/users/imotov/orgs","repos_url":"https://api.github.com/users/imotov/repos","events_url":"https://api.github.com/users/imotov/events{/privacy}","received_events_url":"https://api.github.com/users/imotov/received_events","type":"User","site_admin":false},"created_at":"2014-05-31T02:38:12Z","updated_at":"2014-05-31T02:38:12Z","author_association":"MEMBER","body":"The new \"PARTIAL\" state that was added by #5792 should help to distinguish between snapshots that were completely successful and snapshots that contained some shards that failed to snapshot. Closing.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/68353629","html_url":"https://github.com/elastic/elasticsearch/issues/5657#issuecomment-68353629","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5657","id":68353629,"node_id":"MDEyOklzc3VlQ29tbWVudDY4MzUzNjI5","user":{"login":"Rams20","id":10349094,"node_id":"MDQ6VXNlcjEwMzQ5MDk0","avatar_url":"https://avatars3.githubusercontent.com/u/10349094?v=4","gravatar_id":"","url":"https://api.github.com/users/Rams20","html_url":"https://github.com/Rams20","followers_url":"https://api.github.com/users/Rams20/followers","following_url":"https://api.github.com/users/Rams20/following{/other_user}","gists_url":"https://api.github.com/users/Rams20/gists{/gist_id}","starred_url":"https://api.github.com/users/Rams20/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Rams20/subscriptions","organizations_url":"https://api.github.com/users/Rams20/orgs","repos_url":"https://api.github.com/users/Rams20/repos","events_url":"https://api.github.com/users/Rams20/events{/privacy}","received_events_url":"https://api.github.com/users/Rams20/received_events","type":"User","site_admin":false},"created_at":"2014-12-30T12:46:06Z","updated_at":"2014-12-31T05:59:04Z","author_association":"NONE","body":"why primary shards are require for Elasticsearch backup snapshot...?\n","performed_via_github_app":null}]