[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/345633613","html_url":"https://github.com/elastic/elasticsearch/issues/27447#issuecomment-345633613","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27447","id":345633613,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NTYzMzYxMw==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-11-20T09:08:43Z","updated_at":"2017-11-20T09:08:43Z","author_association":"CONTRIBUTOR","body":"you're asking for 5 years worth of buckets at second-level interval granularity and also force creation of empty buckets. This means creating 150 million buckets. Could take a while ;-) I guess we need another safeguard that prevents people to shoot themselves in the foot. \r\n@cbuescher WDYT?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/345633971","html_url":"https://github.com/elastic/elasticsearch/issues/27447#issuecomment-345633971","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27447","id":345633971,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NTYzMzk3MQ==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2017-11-20T09:10:08Z","updated_at":"2017-11-20T09:22:30Z","author_association":"MEMBER","body":"You are asking Elasticsearch to produce 157,766,400 1-second buckets for each of your many requests here and because you are using extended bounds the range of the data is irrelevant as the coordinating node has to generated the empty buckets around the data. So even just taking a single request into account, the coordinating node has to retrieve the buckets from the shards and hold that in memory and then also hold the result of the reduce in memory while also building empty buckets to reach a total of 157,766,400 buckets and execute two painless scripts. This is not a trivial amount of work and I'm not surprised its using a lot of memory, time and causing GCs. Even if each bucket is using only 220 bytes you would run out of a 32GB heap and thats not taking into account all the other concurrent requests you issued and other memory users on the node.\r\n\r\nI think the solution here is to add a soft limit that restricts the number of buckets that can be created by using extended bounds so we protect against this case.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/345636084","html_url":"https://github.com/elastic/elasticsearch/issues/27447#issuecomment-345636084","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27447","id":345636084,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NTYzNjA4NA==","user":{"login":"colings86","id":236731,"node_id":"MDQ6VXNlcjIzNjczMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/236731?v=4","gravatar_id":"","url":"https://api.github.com/users/colings86","html_url":"https://github.com/colings86","followers_url":"https://api.github.com/users/colings86/followers","following_url":"https://api.github.com/users/colings86/following{/other_user}","gists_url":"https://api.github.com/users/colings86/gists{/gist_id}","starred_url":"https://api.github.com/users/colings86/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/colings86/subscriptions","organizations_url":"https://api.github.com/users/colings86/orgs","repos_url":"https://api.github.com/users/colings86/repos","events_url":"https://api.github.com/users/colings86/events{/privacy}","received_events_url":"https://api.github.com/users/colings86/received_events","type":"User","site_admin":false},"created_at":"2017-11-20T09:18:33Z","updated_at":"2017-11-20T09:18:33Z","author_association":"MEMBER","body":"I created https://github.com/elastic/elasticsearch/issues/27452 to address adding a soft limit","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/346245473","html_url":"https://github.com/elastic/elasticsearch/issues/27447#issuecomment-346245473","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27447","id":346245473,"node_id":"MDEyOklzc3VlQ29tbWVudDM0NjI0NTQ3Mw==","user":{"login":"cwurm","id":694597,"node_id":"MDQ6VXNlcjY5NDU5Nw==","avatar_url":"https://avatars2.githubusercontent.com/u/694597?v=4","gravatar_id":"","url":"https://api.github.com/users/cwurm","html_url":"https://github.com/cwurm","followers_url":"https://api.github.com/users/cwurm/followers","following_url":"https://api.github.com/users/cwurm/following{/other_user}","gists_url":"https://api.github.com/users/cwurm/gists{/gist_id}","starred_url":"https://api.github.com/users/cwurm/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cwurm/subscriptions","organizations_url":"https://api.github.com/users/cwurm/orgs","repos_url":"https://api.github.com/users/cwurm/repos","events_url":"https://api.github.com/users/cwurm/events{/privacy}","received_events_url":"https://api.github.com/users/cwurm/received_events","type":"User","site_admin":false},"created_at":"2017-11-22T05:11:36Z","updated_at":"2017-11-22T05:11:36Z","author_association":"CONTRIBUTOR","body":"Thanks, @colings86. I agree it's a rather unusual query.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/455431922","html_url":"https://github.com/elastic/elasticsearch/issues/27447#issuecomment-455431922","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/27447","id":455431922,"node_id":"MDEyOklzc3VlQ29tbWVudDQ1NTQzMTkyMg==","user":{"login":"jehutywong","id":13827736,"node_id":"MDQ6VXNlcjEzODI3NzM2","avatar_url":"https://avatars1.githubusercontent.com/u/13827736?v=4","gravatar_id":"","url":"https://api.github.com/users/jehutywong","html_url":"https://github.com/jehutywong","followers_url":"https://api.github.com/users/jehutywong/followers","following_url":"https://api.github.com/users/jehutywong/following{/other_user}","gists_url":"https://api.github.com/users/jehutywong/gists{/gist_id}","starred_url":"https://api.github.com/users/jehutywong/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jehutywong/subscriptions","organizations_url":"https://api.github.com/users/jehutywong/orgs","repos_url":"https://api.github.com/users/jehutywong/repos","events_url":"https://api.github.com/users/jehutywong/events{/privacy}","received_events_url":"https://api.github.com/users/jehutywong/received_events","type":"User","site_admin":false},"created_at":"2019-01-18T05:26:50Z","updated_at":"2019-01-18T05:26:50Z","author_association":"NONE","body":"i am also facing this issue, a limit on buckets number not seem to be an acceptable solution. Is there a way to achieve an unlimited bucket size aggregation, if i can accept a very long time running time?","performed_via_github_app":null}]