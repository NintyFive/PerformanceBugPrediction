{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/20390","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20390/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20390/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/20390/events","html_url":"https://github.com/elastic/elasticsearch/issues/20390","id":175807497,"node_id":"MDU6SXNzdWUxNzU4MDc0OTc=","number":20390,"title":"ingest-attachment pipeline is rejecting documents where chars count > default 100k","user":{"login":"Analect","id":4063815,"node_id":"MDQ6VXNlcjQwNjM4MTU=","avatar_url":"https://avatars2.githubusercontent.com/u/4063815?v=4","gravatar_id":"","url":"https://api.github.com/users/Analect","html_url":"https://github.com/Analect","followers_url":"https://api.github.com/users/Analect/followers","following_url":"https://api.github.com/users/Analect/following{/other_user}","gists_url":"https://api.github.com/users/Analect/gists{/gist_id}","starred_url":"https://api.github.com/users/Analect/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Analect/subscriptions","organizations_url":"https://api.github.com/users/Analect/orgs","repos_url":"https://api.github.com/users/Analect/repos","events_url":"https://api.github.com/users/Analect/events{/privacy}","received_events_url":"https://api.github.com/users/Analect/received_events","type":"User","site_admin":false},"labels":[{"id":268963484,"node_id":"MDU6TGFiZWwyNjg5NjM0ODQ=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Features/Ingest","name":":Core/Features/Ingest","color":"0e8a16","default":false,"description":"Execution or management of Ingest Pipelines"},{"id":23173,"node_id":"MDU6TGFiZWwyMzE3Mw==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Ebug","name":">bug","color":"b60205","default":false,"description":null}],"state":"closed","locked":false,"assignee":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"assignees":[{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false}],"milestone":null,"comments":7,"created_at":"2016-09-08T17:01:47Z","updated_at":"2017-05-09T08:14:51Z","closed_at":"2016-11-18T11:49:23Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\nGitHub is reserved for bug reports and feature requests. The best place\nto ask a general question is at the Elastic Discourse forums at\nhttps://discuss.elastic.co. If you are in fact posting a bug report or\na feature request, please include one and only one of the below blocks\nin your new issue. Note that whether you're filing a bug report or a\nfeature request, ensure that your submission is for an\n[OS that we support](https://www.elastic.co/support/matrix#show_os).\nBug reports on an OS that we do not support or feature requests\nspecific to an OS that we do not support will be closed.\n-->\n\n<!--\nIf you are filing a bug report, please remove the below feature\nrequest block and provide responses for all of the below items.\n-->\n\n**Elasticsearch version**: 5.0 alpha5\n\n**Plugins installed**: [aggs-matrix-stats, ingest-common, lang-expression, lang-groovy, lang-mustache, lang-painless, percolator, reindex, transport-netty3, transport-netty4, discovery-ec2, ingest-attachment, lang-javascript, lang-python]\n\n**JVM version**: JVM[Oracle Corporation/OpenJDK 64-Bit Server VM/1.8.0_102/25.102-b14]\n\n**OS version**:  Linux/4.2.0-18-generic/amd64\n\n**Description of the problem including expected versus actual behavior**:\nI have been testing the ingest-attachment functionality against 5.0alpha5. I previously used mapper-attachment on earlier versions of ES.  It seems that for larger files (I tested with PDFs), where there are greater than the 100k character default limit, instead of the ingest-attachment processing up to 100k characters (as it suggests), it rejects the document outright. It only works if I re-apply the pipeline to use indexed_chars = 1000000 (ie. 10x the limit).\n\nThe expected behaviour should be to just process up to the default lower limit, but this doesn't appear to be happening.  The document only ends up getting processed if it is within the new higher limit for indexed_chars that I have set.\n\n**Steps to reproduce**:\nstep1. Create a new ingest pipeline for attachment types, explicitly setting the index_chars to 100k.\n\n```\ncurl -H 'Expect:' -XPUT \"http://xxx:9200/_ingest/pipeline/files_ingest_attachment\" -d '\n{\n  \"description\" : \"An ingest-attachment pipeline into the files type\",\n  \"processors\" : [ {\n    \"attachment\" : {\n      \"field\": \"blob\",\n      \"indexed_chars\": 100000\n    }\n  } ]\n}'\n```\n\nstep2. Push the document in base64 form to ES ... I'm using the _bulk API via elasticsearch-py to send my document ... which includes the base64 encoded field blob ... this works well.\n\nstep3. With 100k limit, the documented is rejected by ES with logs as per below.\n\nstep4. Apply a new indexed_chars limit of 1,000,000 characters.\n\n```\ncurl -H 'Expect:' -XPUT \"http://xxx:9200/_ingest/pipeline/files_ingest_attachment\" -d '\n{\n  \"description\" : \"An ingest-attachment pipeline into the files type\",\n  \"processors\" : [ {\n    \"attachment\" : {\n      \"field\": \"blob\",\n      \"indexed_chars\": 1000000\n    }\n  } ]\n}'\n```\n\nstep5. Document processes into ES fine\n\n**Provide logs (if relevant)**:\n\n```\nmaster_1        | [2016-09-08 16:20:16,688][DEBUG][action.ingest            ] [kGGmfo7] failed to execute pipeline [files_ingest_attachment] for document [my-index/files/506c4eb31a91c1711039d6d1]\nmaster_1        | ElasticsearchException[java.lang.IllegalArgumentException: ElasticsearchParseException[Error parsing document in field [blob]]; nested: TikaException[Unable to extract all PDF content]; nested: IOExceptionWithCause[Unable to write a string: short extract of text from document ]; nested: TaggedSAXException[Your document contained more than 100000 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).]; nested: WriteLimitReachedException[Your document contained more than 100000 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).];]; nested: IllegalArgumentException[ElasticsearchParseException[Error parsing document in field [blob]]; nested: TikaException[Unable to extract all PDF content]; nested: IOExceptionWithCause[Unable to write a string: short extract of text from the document ]; nested: TaggedSAXException[Your document contained more than 100000 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).]; nested: WriteLimitReachedException[Your document contained more than 100000 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).];]; nested: ElasticsearchParseException[Error parsing document in field [blob]]; nested: TikaException[Unable to extract all PDF content]; nested: IOExceptionWithCause[Unable to write a string: short extract of text ]; nested: TaggedSAXException[Your document contained more than 100000 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).]; nested: WriteLimitReachedException[Your document contained more than 100000 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).];\nmaster_1        |   at org.elasticsearch.ingest.CompoundProcessor.newCompoundProcessorException(CompoundProcessor.java:156)\nmaster_1        |   at org.elasticsearch.ingest.CompoundProcessor.execute(CompoundProcessor.java:107)\nmaster_1        |   at org.elasticsearch.ingest.Pipeline.execute(Pipeline.java:52)\nmaster_1        |   at org.elasticsearch.ingest.PipelineExecutionService.innerExecute(PipelineExecutionService.java:166)\nmaster_1        |   at org.elasticsearch.ingest.PipelineExecutionService.access$000(PipelineExecutionService.java:41)\nmaster_1        |   at org.elasticsearch.ingest.PipelineExecutionService$2.doRun(PipelineExecutionService.java:88)\nmaster_1        |   at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:510)\nmaster_1        |   at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\nmaster_1        |   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nmaster_1        |   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nmaster_1        |   at java.lang.Thread.run(Thread.java:745)\nmaster_1        | Caused by: java.lang.IllegalArgumentException: ElasticsearchParseException[Error parsing document in field [blob]]; nested: TikaException[Unable to extract all PDF content]; nested: IOExceptionWithCause[Unable to write a string: short extract of text from document]; nested: TaggedSAXException[Your document contained more than 100000 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).]; nested: WriteLimitReachedException[Your document contained more than 100000 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).];\nmaster_1        |   ... 11 more\nmaster_1        | Caused by: ElasticsearchParseException[Error parsing document in field [blob]]; nested: TikaException[Unable to extract all PDF content]; nested: IOExceptionWithCause[short extract of text from the document ]; nested: TaggedSAXException[Your document contained more than 100000 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).]; nested: WriteLimitReachedException[Your document contained more than 100000 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).];\nmaster_1        |   at org.elasticsearch.ingest.attachment.AttachmentProcessor.execute(AttachmentProcessor.java:126)\nmaster_1        |   at org.elasticsearch.ingest.CompoundProcessor.execute(CompoundProcessor.java:100)\nmaster_1        |   ... 9 more\nmaster_1        | Caused by: org.apache.tika.exception.TikaException: Unable to extract all PDF content\nmaster_1        |   at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:184)\nmaster_1        |   at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:144)\nmaster_1        |   at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)\nmaster_1        |   at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)\nmaster_1        |   at org.apache.tika.Tika.parseToString(Tika.java:568)\nmaster_1        |   at org.elasticsearch.ingest.attachment.TikaImpl$1.run(TikaImpl.java:94)\nmaster_1        |   at org.elasticsearch.ingest.attachment.TikaImpl$1.run(TikaImpl.java:91)\nmaster_1        |   at java.security.AccessController.doPrivileged(Native Method)\nmaster_1        |   at org.elasticsearch.ingest.attachment.TikaImpl.parse(TikaImpl.java:91)\nmaster_1        |   at org.elasticsearch.ingest.attachment.AttachmentProcessor.execute(AttachmentProcessor.java:72)\nmaster_1        |   ... 10 more\n\n```\n","closed_by":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"performed_via_github_app":null}