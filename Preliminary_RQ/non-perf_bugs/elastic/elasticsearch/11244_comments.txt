[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/103997387","html_url":"https://github.com/elastic/elasticsearch/issues/11244#issuecomment-103997387","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11244","id":103997387,"node_id":"MDEyOklzc3VlQ29tbWVudDEwMzk5NzM4Nw==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2015-05-20T19:03:32Z","updated_at":"2015-05-20T19:03:32Z","author_association":"MEMBER","body":"Looks like we do this on purpose for anything over `Integer.MAX_VALUE`, see: https://github.com/elastic/elasticsearch/blob/a40ba3be5aecc00e9b0b7ef79a0220d09159e327/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java#L191-L195\n\nAre you doing bulk inserts over 2gb in size? That is definitely higher than recommended.\n\nI'll add a commit to mention this in the documentation.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/375366710","html_url":"https://github.com/elastic/elasticsearch/issues/11244#issuecomment-375366710","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11244","id":375366710,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NTM2NjcxMA==","user":{"login":"kun-zhou","id":19158151,"node_id":"MDQ6VXNlcjE5MTU4MTUx","avatar_url":"https://avatars1.githubusercontent.com/u/19158151?v=4","gravatar_id":"","url":"https://api.github.com/users/kun-zhou","html_url":"https://github.com/kun-zhou","followers_url":"https://api.github.com/users/kun-zhou/followers","following_url":"https://api.github.com/users/kun-zhou/following{/other_user}","gists_url":"https://api.github.com/users/kun-zhou/gists{/gist_id}","starred_url":"https://api.github.com/users/kun-zhou/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kun-zhou/subscriptions","organizations_url":"https://api.github.com/users/kun-zhou/orgs","repos_url":"https://api.github.com/users/kun-zhou/repos","events_url":"https://api.github.com/users/kun-zhou/events{/privacy}","received_events_url":"https://api.github.com/users/kun-zhou/received_events","type":"User","site_admin":false},"created_at":"2018-03-22T16:19:40Z","updated_at":"2018-03-22T16:19:40Z","author_association":"NONE","body":"But it is very common to have hundreds of gigabytes of data that needs to be migrated to elastic search during the migration period, how should we approach such task? This is clearly a problem every developer needs to deal with.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/375415940","html_url":"https://github.com/elastic/elasticsearch/issues/11244#issuecomment-375415940","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11244","id":375415940,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NTQxNTk0MA==","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2018-03-22T18:42:14Z","updated_at":"2018-03-22T18:42:14Z","author_association":"MEMBER","body":"Reduce the size of the bulk. Like send 1000 documents per bulk request for example.","performed_via_github_app":null}]