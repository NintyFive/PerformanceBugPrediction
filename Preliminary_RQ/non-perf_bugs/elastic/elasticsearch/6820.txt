{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/6820","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6820/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6820/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/6820/events","html_url":"https://github.com/elastic/elasticsearch/issues/6820","id":37598320,"node_id":"MDU6SXNzdWUzNzU5ODMyMA==","number":6820,"title":"Long GC pause","user":{"login":"shaoweite","id":5125384,"node_id":"MDQ6VXNlcjUxMjUzODQ=","avatar_url":"https://avatars1.githubusercontent.com/u/5125384?v=4","gravatar_id":"","url":"https://api.github.com/users/shaoweite","html_url":"https://github.com/shaoweite","followers_url":"https://api.github.com/users/shaoweite/followers","following_url":"https://api.github.com/users/shaoweite/following{/other_user}","gists_url":"https://api.github.com/users/shaoweite/gists{/gist_id}","starred_url":"https://api.github.com/users/shaoweite/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shaoweite/subscriptions","organizations_url":"https://api.github.com/users/shaoweite/orgs","repos_url":"https://api.github.com/users/shaoweite/repos","events_url":"https://api.github.com/users/shaoweite/events{/privacy}","received_events_url":"https://api.github.com/users/shaoweite/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2014-07-10T19:50:18Z","updated_at":"2014-07-11T11:47:13Z","closed_at":"2014-07-11T11:47:13Z","author_association":"NONE","active_lock_reason":null,"body":"Hi,\n\nWe are seeing split-brain problem in our Elasticsearch (v1.1.2) cluster where two nodes host hundred of indices. The bigger index has about 30 million documents while others are quite small. It seems that at some point in time Java GC was hogging the CPUs so that both nodes failed to ping each other. The logs are attached below.\n\nWe had never had such issue before when we had just a handful of indexes (each of them had more documents). At this point, we have a few theories:\n- We can tune node filter cache and field data cache. But our total documents remains about the same as before when we had smaller set of indexes and did not have such problem. So we are not sure if this will help. Is it that each index will incur additional memory usage in addition to cache? If so, how to estimate the usage? We also flush index and clear cache periodically but it does not seem to help.\n- Is it that some of our aggregation queries that take up too much memory. Is there way to estimate memory usage of the queries?\n- Is it advisable to tune Java GC to be less aggressive?\n- Finally, we know this has been asked before. Is it advisable to have hundreds or even thousands of indexes in a cluster?\n\nThanks for the help.\n- Wei Shao\n\n[2014-07-10 08:21:54,020][WARN ][monitor.jvm              ] [Zero-G] [gc][young][221250][80781] duration [25.2s], collections [1]/[2\n8.8s], total [25.2s]/[1.2h], memory [3.6gb]->[3.7gb]/[4.9gb], all_pools {[young] [18.8mb]->[3.1mb]/[133.1mb]}{[survivor] [16.6mb]->[\n16.6mb]/[16.6mb]}{[old] [3.6gb]->[3.7gb]/[4.8gb]}\n[2014-07-10 08:23:38,768][WARN ][monitor.jvm              ] [Zero-G] [gc][young][221253][80782] duration [1s], collections [1]/[1.3s\n], total [1s]/[1.2h], memory [3.8gb]->[3.7gb]/[4.9gb], all_pools {[young] [124.2mb]->[265.1kb]/[133.1mb]}{[survivor] [16.6mb]->[16.6\nmb]/[16.6mb]}{[old] [3.7gb]->[3.7gb]/[4.8gb]}\n[2014-07-10 08:23:42,667][WARN ][monitor.jvm              ] [Zero-G] [gc][young][221254][80783] duration [1.9s], collections [1]/[3.\n8s], total [1.9s]/[1.2h], memory [3.7gb]->[3.7gb]/[4.9gb], all_pools {[young] [265.1kb]->[7.2mb]/[133.1mb]}{[survivor] [16.6mb]->[16\n.6mb]/[16.6mb]}{[old] [3.7gb]->[3.7gb]/[4.8gb]}\n[2014-07-10 08:23:44,052][INFO ][discovery.zen            ] [Zero-G] master_left [[Mary Jane Parker][F4QVviuGQ_Sg6h9NBlVVsQ][ha-elas\ntic2][inet[/192.168.0.45:9300]]], reason [do not exists on master, act as master failure]\n[2014-07-10 08:23:45,702][INFO ][cluster.service          ] [Zero-G] master {new [Zero-G][ECw8eS5zTMKXSf1bIFOCKA][ha-elastic1][inet[/192.168.0.46:9300]], previous [Mary Jane Parker][F4QVviuGQ_Sg6h9NBlVVsQ][ha-elastic2][inet[/192.168.0.45:9300]]}, removed {[Mary Jane Parker][F4QVviuGQ_Sg6h9NBlVVsQ][ha-elastic2][inet[/192.168.0.45:9300]],}, reason: zen-disco-master_failed ([Mary Jane Parker][F4QVviuGQ_Sg6h9NBlVVsQ][ha-elastic2][inet[/192.168.0.45:9300]])\n[2014-07-10 08:23:47,587][WARN ][monitor.jvm              ] [Zero-G] [gc][young][221258][80786] duration [1s], collections [1]/[1.6s], total [1s]/[1.2h], memory [3.7gb]->[3.6gb]/[4.9gb], all_pools {[young] [7.2mb]->[7.1mb]/[133.1mb]}{[survivor] [7.9mb]->[8mb]/[16.6mb]}{[old] [3.7gb]->[3.6gb]/[4.8gb]}\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}