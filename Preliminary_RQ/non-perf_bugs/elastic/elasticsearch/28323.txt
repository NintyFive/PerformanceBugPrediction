{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/28323","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28323/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28323/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28323/events","html_url":"https://github.com/elastic/elasticsearch/issues/28323","id":290355362,"node_id":"MDU6SXNzdWUyOTAzNTUzNjI=","number":28323,"title":"Hot and Warm architecture to be extended for Disk Allocation within Nodes","user":{"login":"antonpious","id":15054021,"node_id":"MDQ6VXNlcjE1MDU0MDIx","avatar_url":"https://avatars3.githubusercontent.com/u/15054021?v=4","gravatar_id":"","url":"https://api.github.com/users/antonpious","html_url":"https://github.com/antonpious","followers_url":"https://api.github.com/users/antonpious/followers","following_url":"https://api.github.com/users/antonpious/following{/other_user}","gists_url":"https://api.github.com/users/antonpious/gists{/gist_id}","starred_url":"https://api.github.com/users/antonpious/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/antonpious/subscriptions","organizations_url":"https://api.github.com/users/antonpious/orgs","repos_url":"https://api.github.com/users/antonpious/repos","events_url":"https://api.github.com/users/antonpious/events{/privacy}","received_events_url":"https://api.github.com/users/antonpious/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2018-01-22T05:34:24Z","updated_at":"2018-04-06T14:35:47Z","closed_at":"2018-04-06T14:35:47Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\r\n\r\n** Please read the guidelines below. **\r\n\r\nIssues that do not follow these guidelines are likely to be closed.\r\n\r\n1.  GitHub is reserved for bug reports and feature requests. The best place to\r\n    ask a general question is at the Elastic [forums](https://discuss.elastic.co).\r\n    GitHub is not the place for general questions.\r\n\r\n2.  Is this bug report or feature request for a supported OS? If not, it\r\n    is likely to be closed.  See https://www.elastic.co/support/matrix#show_os\r\n\r\n3.  Please fill out EITHER the feature request block or the bug report block\r\n    below, and delete the other block.\r\n\r\n-->\r\n\r\n<!-- Feature request -->\r\nIn Elastic search one can store data, the data can be divided into multiple types, to name a few important data, data that is regular etc.\r\nThe current hot /warm feature splits the nodes which handle the hot and warm indices, which cause nodes to be reserved for hot and warm indices.  What this causes is having to have reservation on nodes for hot and warm leading to wastage of computing resource when the load on both hot and warm nodes are not uniform.\r\nThis feature needs to be extended to include this at data storage.\r\n\r\nAt present most cloud provider have different types of storage namely IO1, GP2 in case of AWS. Due to the high IOPS requirement one could mount a high IO disk to the cluster designate this as hot and a normal IO disk namely GP2 to the same cluster and designate this as warm and so forth. Based on the cost and customer preference we could place the data between these storage devices.\r\n\r\nWe can then use the same nodes but when indices are created we need to specify which disk type these needs to be stored and place those in these storage devices. When the use of the data is complete we could move these to warm or have both hot and warm data storage devices seamless to the cluster. The client can decide where they need to be place the data depending on the need causing effective shared utilization on all nodes with each node's CPU and memory utilized and the difference is present only on the data store.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","closed_by":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"performed_via_github_app":null}