{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/32340","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32340/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32340/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/32340/events","html_url":"https://github.com/elastic/elasticsearch/issues/32340","id":344144869,"node_id":"MDU6SXNzdWUzNDQxNDQ4Njk=","number":32340,"title":"Curious case of draining a data node in the presence of X-Pack Security","user":{"login":"jordansissel","id":131818,"node_id":"MDQ6VXNlcjEzMTgxOA==","avatar_url":"https://avatars1.githubusercontent.com/u/131818?v=4","gravatar_id":"","url":"https://api.github.com/users/jordansissel","html_url":"https://github.com/jordansissel","followers_url":"https://api.github.com/users/jordansissel/followers","following_url":"https://api.github.com/users/jordansissel/following{/other_user}","gists_url":"https://api.github.com/users/jordansissel/gists{/gist_id}","starred_url":"https://api.github.com/users/jordansissel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jordansissel/subscriptions","organizations_url":"https://api.github.com/users/jordansissel/orgs","repos_url":"https://api.github.com/users/jordansissel/repos","events_url":"https://api.github.com/users/jordansissel/events{/privacy}","received_events_url":"https://api.github.com/users/jordansissel/received_events","type":"User","site_admin":false},"labels":[{"id":837246479,"node_id":"MDU6TGFiZWw4MzcyNDY0Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Allocation","name":":Distributed/Allocation","color":"0e8a16","default":false,"description":"All issues relating to the decision making around placing a shard (both master logic & on the nodes)"},{"id":912838879,"node_id":"MDU6TGFiZWw5MTI4Mzg4Nzk=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Security/Security","name":":Security/Security","color":"0e8a16","default":false,"description":"Security issues without another label"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2018-07-24T18:06:03Z","updated_at":"2018-07-25T07:48:25Z","closed_at":"2018-07-25T07:48:25Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the feature**:\r\n\r\nWhen x-pack security is enabled and at least one data node is excluded with `cluster.routing.allocation.exclude._ip`, the cluster state will never show `green` because a single .security shard remains unallocated.\r\n\r\nIdeally, cluster state `green` would be usable in a draining scenario even when X-Pack Security is active. \r\n\r\n**Elasticsearch version** (`bin/elasticsearch --version`): docker.elastic.co/elasticsearch/elasticsearch-platinum:6.2.3\r\n\r\n**Plugins installed**: x-pack and whatever else comes on the docker image.\r\n\r\n**JVM version** (`java -version`): docker.elastic.co/elasticsearch/elasticsearch-platinum:6.2.3\r\n\r\n**OS version** (`uname -a` if on a Unix-like system): docker.elastic.co/elasticsearch/elasticsearch-platinum:6.2.3\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\nA few weeks ago, I migrated my Elasticisearch data nodes to bigger machines. To do this, I roughly followed these steps:\r\n\r\n1) Bring new nodes online and join the cluster\r\n2) Drain one old machine at a time with [shard allocation filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-filtering.html)\r\n3) Proceed with additional drains, one at a time, only when the cluster finishes moving shards. I intended to observe this with cluster state \"green\".\r\n\r\nMy plan was to drain a single node and wait for the cluster to be green. However, it seems the way X-Pack Security configures the `.security` index prevents this. If there are 6 data nodes, the number of replicas for `.security` will be 5. When draining a node with shard allocation filtering, this means the cluster will complete draining and still be yellow!\r\n\r\nI understand solving this may be a challenge given the complexities (replica count based on data nodes, shard allocation filtering to exclude whole nodes, etc), but I wanted to raise it for discussion.\r\n\r\nMy current workaround is to use `/_cat/shards`, looking for UNASSIGNED, and concluding \"draining complete\" only when exactly 1 UNASSIGNED shard exists and that shard must belong to the `.security` index.\r\n\r\nExample:\r\n\r\n```\r\nindex                              shard prirep state          docs   store ip          node\r\n.security-6                        0     r      UNASSIGNED                              \r\n.security-6                        0     r      UNASSIGNED                              \r\n.security-6                        0     r      UNASSIGNED                              \r\n.security-6                        0     r      UNASSIGNED                              \r\n```\r\n\r\nFor the above, I expect one UNASSIGNED per drained data node. In the case above, I have 4 drained data nodes, so 4 .security replica shards are unassigned.\r\n\r\n**Steps to reproduce**:\r\n\r\n\r\n 1. Enable X-Pack Security\r\n 2. Use shard allocation filtering to exclude a single node\r\n 3. Observe the cluster state will never go green because a single .security shard will remain unassigned.","closed_by":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"performed_via_github_app":null}