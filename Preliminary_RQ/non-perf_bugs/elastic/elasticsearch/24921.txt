{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/24921","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24921/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24921/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24921/events","html_url":"https://github.com/elastic/elasticsearch/issues/24921","id":231784796,"node_id":"MDU6SXNzdWUyMzE3ODQ3OTY=","number":24921,"title":"delayed_unassigned_shards not picked from returning node","user":{"login":"ywilkof","id":4441877,"node_id":"MDQ6VXNlcjQ0NDE4Nzc=","avatar_url":"https://avatars2.githubusercontent.com/u/4441877?v=4","gravatar_id":"","url":"https://api.github.com/users/ywilkof","html_url":"https://github.com/ywilkof","followers_url":"https://api.github.com/users/ywilkof/followers","following_url":"https://api.github.com/users/ywilkof/following{/other_user}","gists_url":"https://api.github.com/users/ywilkof/gists{/gist_id}","starred_url":"https://api.github.com/users/ywilkof/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywilkof/subscriptions","organizations_url":"https://api.github.com/users/ywilkof/orgs","repos_url":"https://api.github.com/users/ywilkof/repos","events_url":"https://api.github.com/users/ywilkof/events{/privacy}","received_events_url":"https://api.github.com/users/ywilkof/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2017-05-27T08:01:08Z","updated_at":"2017-05-29T16:08:27Z","closed_at":"2017-05-27T16:21:55Z","author_association":"NONE","active_lock_reason":null,"body":"Hello,\r\n\r\nWe have a 6 node cluster, on top of EC2 instances. All are data nodes and master-eligible nodes. (It's a full Spot machine cluster where all machines can go up and down) \r\nSometimes we replace node instances expecting a new instance to return (with the same EBS, hence has all the necessary data) within a few minutes, and for that we are using the index.unassigned.node_left.delayed_timeout  setting. \r\nSometimes this feature works correctly, and when the node returns the delayed unassigned shards are picked up immediately and cluster recovery is immediate with minimum data  transfer from other nodes. However, at times, although the cluster is delaying the assignment of the shards, and the node returns within the timeout limit, it does not assign them back to the node, which results in the delayed_shards becoming unassigned shards - leading to data transfer, and post relocation of shards. I verified that the EBS is mounted correctly before Elasticsearch nodestarts.\r\nLooking at my logs, I can see the node is lost and the shards are  delayed.\r\nIt is also clear in the logs that leaving instance is back within 2 mins, which should result in immediate allocation of the delayed shards.  I highlighted in the logs what I believe is relevant.\r\n\r\nIs this the expected behavior when the node goes down? Thoughts on this would be appreciated. Thanks in advance! \r\n\r\nMy cluster state before:\r\n\r\n{\r\n   \"cluster_name\": \"elaclu\",\r\n   \"status\": \"yellow\",\r\n   \"timed_out\": false,\r\n   \"number_of_nodes\": 6,\r\n   \"number_of_data_nodes\": 6,\r\n   \"active_primary_shards\": 52,\r\n   \"active_shards\": 91,\r\n   \"relocating_shards\": 1,\r\n   \"initializing_shards\": 2,\r\n   \"unassigned_shards\": 15,\r\n   \"delayed_unassigned_shards\": 15,\r\n   \"number_of_pending_tasks\": 0,\r\n   \"number_of_in_flight_fetch\": 0,\r\n   \"task_max_waiting_in_queue_millis\": 0,\r\n   \"active_shards_percent_as_number\": 84.25925925925925\r\n}\r\n\r\nmy cluster  state  after a few minutes:\r\n{\r\n   \"cluster_name\": \"elaclu\",\r\n   \"status\": \"yellow\",\r\n   \"timed_out\": false,\r\n   \"number_of_nodes\": 6,\r\n   \"number_of_data_nodes\": 6,\r\n   \"active_primary_shards\": 52,\r\n   \"active_shards\": 92,\r\n   \"relocating_shards\": 1,\r\n   \"initializing_shards\": 2,\r\n   \"unassigned_shards\": 14,\r\n   \"delayed_unassigned_shards\": 0,\r\n   \"number_of_pending_tasks\": 0,\r\n   \"number_of_in_flight_fetch\": 0,\r\n   \"task_max_waiting_in_queue_millis\": 0,\r\n   \"active_shards_percent_as_number\": 85.18518518518519\r\n}\r\n\r\n And it stays  like  that for about 20 mins, until all shards are assigned eventually. \r\n\r\ncluster settings:\r\n\r\n{\r\n   \"persistent\": {},\r\n   \"transient\": {\r\n      \"cluster\": {\r\n         \"routing\": {\r\n            \"allocation\": {\r\n               \"enable\": \"all\"\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n\r\n**Elasticsearch version**: 2.4.4\r\n\r\n**Plugins installed**: [cloud-aws]\r\n\r\n**JVM version** 1.8.0_131\r\n\r\n**OS version**: 4.4.0-77\r\n\r\n**Logs:**\r\n\r\n[2017-05-27 07:20:17,196][WARN ][transport.netty          ] [Helmut Zemo] exception caught on transport layer [[id: 0x4b6dac95, /10.1.2.82:37064 => /10.1.7.29:9300]], closing connection\r\njava.io.IOException: No route to host\r\n\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\r\n\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\r\n\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\r\n\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\r\n\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n[2017-05-27 07:20:35,372][WARN ][transport.netty          ] [Helmut Zemo] exception caught on transport layer [[id: 0x48cccb19, /10.1.2.82:37060 => /10.1.7.29:9300]], closing connection\r\njava.io.IOException: No route to host\r\n\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\r\n\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\r\n\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\r\n\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\r\n\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n[2017-05-27 07:20:54,060][WARN ][transport.netty          ] [Helmut Zemo] exception caught on transport layer [[id: 0x41813568, /10.1.2.82:37062 => /10.1.7.29:9300]], closing connection\r\njava.io.IOException: No route to host\r\n\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\r\n\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\r\n\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\r\n\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\r\n\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n[2017-05-27 07:37:47,283][INFO ][cluster.routing.allocation] [Helmut Zemo] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[profiles-second-party][10]] ...]).\r\n[2017-05-27 07:45:15,102][DEBUG][action.admin.cluster.node.stats] [Helmut Zemo] failed to execute on node [PbUh6zhnT8-wJVjzkKpzew]\r\nReceiveTimeoutTransportException[[All-American][10.1.8.96:9300][cluster:monitor/nodes/stats[n]] request_id [1293628] timed out after [15000ms]]\r\n\tat org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:698)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n**[2017-05-27 07:46:03,692][INFO ][cluster.routing.allocation] [Helmut Zemo] Cluster health status changed from [GREEN] to [YELLOW] (reason: [{All-American}{PbUh6zhnT8-wJVjzkKpzew}{10.1.8.96}{10.1.8.96:9300}{max_local_storage_nodes=1, master=true} failed to ping, tried [3] times, each with maximum [30s] timeout]).\r\n[2017-05-27 07:46:03,692][INFO ][cluster.service          ] [Helmut Zemo] removed {{All-American}{PbUh6zhnT8-wJVjzkKpzew}{10.1.8.96}{10.1.8.96:9300}{max_local_storage_nodes=1, master=true},}, reason: zen-disco-node-failed({All-American}{PbUh6zhnT8-wJVjzkKpzew}{10.1.8.96}{10.1.8.96:9300}{max_local_storage_nodes=1, master=true}), reason(failed to ping, tried [3] times, each with maximum [30s] timeout)**\r\n[2017-05-27 07:46:03,751][WARN ][cluster.action.shard     ] [Helmut Zemo] [profiles-second-party][3] received shard failed for target shard [[profiles-second-party][3], node[PbUh6zhnT8-wJVjzkKpzew], [R], v[138], s[STARTED], a[id=11Dhlxc_SL-TGyrQ3MIa4g]], indexUUID [7wwgfPsiRPy0xNn_jBpFyw], message [failed to perform indices:data/write/bulk[s] on replica on node {All-American}{PbUh6zhnT8-wJVjzkKpzew}{10.1.8.96}{10.1.8.96:9300}{max_local_storage_nodes=1, master=true}], failure [NodeDisconnectedException[[All-American][10.1.8.96:9300][indices:data/write/bulk[s][r]] disconnected]]\r\nNodeDisconnectedException[[All-American][10.1.8.96:9300][indices:data/write/bulk[s][r]] disconnected]\r\n**[2017-05-27 07:46:03,756][INFO ][cluster.routing          ] [Helmut Zemo] delaying allocation for [18] unassigned shards, next check in [5m]**\r\n[2017-05-27 07:46:03,757][WARN ][cluster.action.shard     ] [Helmut Zemo] [profiles-second-party][3] received shard failed for target shard [[profiles-second-party][3], node[PbUh6zhnT8-wJVjzkKpzew], [R], v[138], s[STARTED], a[id=11Dhlxc_SL-TGyrQ3MIa4g]], indexUUID [7wwgfPsiRPy0xNn_jBpFyw], message [failed to perform indices:data/write/bulk[s] on replica on node {All-American}{PbUh6zhnT8-wJVjzkKpzew}{10.1.8.96}{10.1.8.96:9300}{max_local_storage_nodes=1, master=true}], failure [NodeDisconnectedException[[All-American][10.1.8.96:9300][indices:data/write/bulk[s][r]] disconnected]]\r\nNodeDisconnectedException[[All-American][10.1.8.96:9300][indices:data/write/bulk[s][r]] disconnected]\r\n[2017-05-27 07:46:03,834][DEBUG][action.admin.cluster.node.info] [Helmut Zemo] failed to execute on node [PbUh6zhnT8-wJVjzkKpzew]\r\nNodeDisconnectedException[[All-American][10.1.8.96:9300][cluster:monitor/nodes/info[n]] disconnected]\r\n[2017-05-27 07:46:03,835][DEBUG][action.admin.indices.stats] [Helmut Zemo] failed to execute [indices:monitor/stats] on node [PbUh6zhnT8-wJVjzkKpzew]\r\nNodeDisconnectedException[[All-American][10.1.8.96:9300][indices:monitor/stats[n]] disconnected]\r\n[2017-05-27 07:46:03,834][DEBUG][action.admin.cluster.node.info] [Helmut Zemo] failed to execute on node [PbUh6zhnT8-wJVjzkKpzew]\r\nNodeDisconnectedException[[All-American][10.1.8.96:9300][cluster:monitor/nodes/info[n]] disconnected]\r\n[2017-05-27 07:46:03,836][DEBUG][action.admin.indices.stats] [Helmut Zemo] failed to execute [indices:monitor/stats] on node [PbUh6zhnT8-wJVjzkKpzew]\r\nNodeDisconnectedException[[All-American][10.1.8.96:9300][indices:monitor/stats[n]] disconnected]\r\n[2017-05-27 07:46:03,842][DEBUG][action.admin.cluster.node.stats] [Helmut Zemo] failed to execute on node [PbUh6zhnT8-wJVjzkKpzew]\r\nNodeDisconnectedException[[All-American][10.1.8.96:9300][cluster:monitor/nodes/stats[n]] disconnected]\r\n**[2017-05-27 07:47:13,380][INFO ][cluster.service          ] [Helmut Zemo] added {{Madame Menace}{AaT8D-D8QK6m4Vbvr542Bw}{10.1.4.99}{10.1.4.99:9300}{max_local_storage_nodes=1, master=true},}, reason: zen-disco-join(join from node[{Madame Menace}{AaT8D-D8QK6m4Vbvr542Bw}{10.1.4.99}{10.1.4.99:9300}{max_local_storage_nodes=1, master=true}])**\r\n","closed_by":{"login":"abeyad","id":1631297,"node_id":"MDQ6VXNlcjE2MzEyOTc=","avatar_url":"https://avatars2.githubusercontent.com/u/1631297?v=4","gravatar_id":"","url":"https://api.github.com/users/abeyad","html_url":"https://github.com/abeyad","followers_url":"https://api.github.com/users/abeyad/followers","following_url":"https://api.github.com/users/abeyad/following{/other_user}","gists_url":"https://api.github.com/users/abeyad/gists{/gist_id}","starred_url":"https://api.github.com/users/abeyad/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/abeyad/subscriptions","organizations_url":"https://api.github.com/users/abeyad/orgs","repos_url":"https://api.github.com/users/abeyad/repos","events_url":"https://api.github.com/users/abeyad/events{/privacy}","received_events_url":"https://api.github.com/users/abeyad/received_events","type":"User","site_admin":false},"performed_via_github_app":null}