{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/26152","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26152/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26152/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26152/events","html_url":"https://github.com/elastic/elasticsearch/issues/26152","id":249515638,"node_id":"MDU6SXNzdWUyNDk1MTU2Mzg=","number":26152,"title":"problem with large-ish ES clusters on 5.4.1 that we are not able to reproduce on 2.x","user":{"login":"buildlackey","id":1577925,"node_id":"MDQ6VXNlcjE1Nzc5MjU=","avatar_url":"https://avatars3.githubusercontent.com/u/1577925?v=4","gravatar_id":"","url":"https://api.github.com/users/buildlackey","html_url":"https://github.com/buildlackey","followers_url":"https://api.github.com/users/buildlackey/followers","following_url":"https://api.github.com/users/buildlackey/following{/other_user}","gists_url":"https://api.github.com/users/buildlackey/gists{/gist_id}","starred_url":"https://api.github.com/users/buildlackey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/buildlackey/subscriptions","organizations_url":"https://api.github.com/users/buildlackey/orgs","repos_url":"https://api.github.com/users/buildlackey/repos","events_url":"https://api.github.com/users/buildlackey/events{/privacy}","received_events_url":"https://api.github.com/users/buildlackey/received_events","type":"User","site_admin":false},"labels":[{"id":836504707,"node_id":"MDU6TGFiZWw4MzY1MDQ3MDc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Distributed","name":":Distributed/Distributed","color":"0e8a16","default":false,"description":"A catch all label for anything in the Distributed Area. If you aren't sure, use this one."},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2017-08-11T00:58:21Z","updated_at":"2018-02-13T19:26:53Z","closed_at":"2017-08-18T19:36:09Z","author_association":"NONE","active_lock_reason":null,"body":"<!-- Bug report -->\r\n\r\nPLUGINS\r\n-------\r\n/apps/elasticsearch $ sudo bin/elasticsearch-plugin list\r\nanalysis-icu\r\nanalysis-kuromoji\r\nmapper-murmur3\r\nraigad-discovery\r\nrepository-s3\r\n\r\n\r\n\r\nJAVA VERSION\r\n-------------\r\n  \"1.8.0_131\"\r\n\r\n\r\nOS\r\n--\r\n\r\nuname -a\r\nLinux es-mcelogs2-useast1-data-useast1e-i-08eda1e5c789e6467 4.4.0-81-generic #104-Ubuntu SMP Wed Jun 14 08:17:06 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n\r\n\r\nDESCRIPTION\r\n-----------\r\n\r\nWe are seeing a problem with large-ish ES clusters on 5.4.1 that we are not able to reproduce \r\non ES 2.4.1. This is triggered by:\r\n\r\n    creating an index template with 120 shards / 2 replicas\r\n\r\n    indexing one new document that conforms to the pattern specified in the above template.\r\n\r\nAfter we perform the indexing operation we get stuck 'initializing shards'.. \r\ntypically three shards stuck in this state for a very long time (hours.)\r\n\r\n\r\nTo reproduce the issue I am uploading the script we use to:\r\n\r\n    - delete the target index we will create (if it is there)\r\n    - create a template that will match against the to-be-created index \r\n\r\n\r\n\r\n\r\nREPRO STEPS\r\n------------\r\n\r\nThe steps to trigger the problem are as follows:\r\n\r\n\r\n1)  save attached script to ~/bugReport.sh\r\n\r\n2)  Invoke script to create new template for 'ndbench_index'\r\n    #\r\n    host=foo.com        # or whatever is appropriate\r\n    port=9200           # or whatever is appropriate\r\n    shards=120\r\n\r\n    bash ~/bugReport.sh $host   $shards  text  default  $port\r\n\r\n\r\n3)  Write simple document  to 'ndbench_index'\r\n    #\r\n    curl -X PUT http://$host:$port/ndbench_index/junktype/someId    \\\r\n            -H 'Content-Type: application/json' -d '{ \"foo\": \"bar\" }'\r\n\r\n\r\nat this point you should see your cluster state go yellow, and you will see some shards \r\nstuck in the 'initializing' state.\r\n\r\nIf you delete the index the cluster state will go back to 'green'\r\n\r\nWhile cluster is yellow, If you use /_cat/shards to find the nodes that host the shards that are initializing, and then go to one of the nodes, \r\nyou should see something like this in the log:\r\n\r\n\r\n\r\n    [2017-08-03T01:41:10,002][WARN ][o.e.c.NodeConnectionsService] [us-east-1c.i-dummy1] failed to connect to node {us-east-1d.i-dummy2}{LXjLhNreS76NnPmgMwfedQ}{yrmdDOgzSzSZ_oQ5B-ETNw}{xxx.xx.81.194}{xxx.xx.81.194:7102}{rack_id=us-east-1d} (tried [1] times)\r\n    org.elasticsearch.transport.ConnectTransportException: [us-east-1d.i-dummy2][xxx.xx.81.194:7102] general node connection failure\r\n            at org.elasticsearch.transport.TcpTransport.openConnection(TcpTransport.java:559) ~[elasticsearch-5.4.1.jar:5.4.1]\r\n            at org.elasticsearch.cluster.NodeConnectionsService.validateAndConnectIfNeeded(NodeConnectionsService.java:154) [elasticsearch-5.4.1.jar:5.4.1]\r\n            at org.elasticsearch.cluster.NodeConnectionsService$1.doRun(NodeConnectionsService.java:107) [elasticsearch-5.4.1.jar:5.4.1]\r\n                <snip>\r\n            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]\r\n            at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]\r\n    Caused by: java.lang.IllegalStateException: handshake failed, channel already closed\r\n            at org.elasticsearch.transport.TcpTransport.executeHandshake(TcpTransport.java:1589) ~[elasticsearch-5.4.1.jar:5.4.1]\r\n            at org.elasticsearch.transport.TcpTransport.openConnection(TcpTransport.java:541) ~[elasticsearch-5.4.1.jar:5.4.1]\r\n        ... 10 more\r\n\r\n\r\n\r\nFinal notes: \r\n\r\n    - we have not been able to reproduce this problem when we shrink the cluster size to something \r\n      smaller (we tried 45 nodes.. no issue occured -- even with 360 shards)\r\n\r\n    - using the script you can vary the number of shards in the index template \r\n        -we found that with a 120 node cluster when we specified 36 shards, \r\n          the index was created with no issue. Once we moved past that (e.g., 37 shards)\r\n         the problem started surfacing.\r\n\r\n    - we also found no issues when we followed the same procedure on ES 2.4.1\r\n\r\n        when using ES 2.x you would follow the same steps, but you invoke the script like this:\r\n\r\n            bash ~/bugReport.sh $hostPort $shards  string  default \r\n\r\n[bugReport.sh.txt](https://github.com/elastic/elasticsearch/files/1216718/bugReport.sh.txt)\r\n","closed_by":{"login":"buildlackey","id":1577925,"node_id":"MDQ6VXNlcjE1Nzc5MjU=","avatar_url":"https://avatars3.githubusercontent.com/u/1577925?v=4","gravatar_id":"","url":"https://api.github.com/users/buildlackey","html_url":"https://github.com/buildlackey","followers_url":"https://api.github.com/users/buildlackey/followers","following_url":"https://api.github.com/users/buildlackey/following{/other_user}","gists_url":"https://api.github.com/users/buildlackey/gists{/gist_id}","starred_url":"https://api.github.com/users/buildlackey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/buildlackey/subscriptions","organizations_url":"https://api.github.com/users/buildlackey/orgs","repos_url":"https://api.github.com/users/buildlackey/repos","events_url":"https://api.github.com/users/buildlackey/events{/privacy}","received_events_url":"https://api.github.com/users/buildlackey/received_events","type":"User","site_admin":false},"performed_via_github_app":null}