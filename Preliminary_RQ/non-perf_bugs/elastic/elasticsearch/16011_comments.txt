[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/171917912","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-171917912","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":171917912,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MTkxNzkxMg==","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2016-01-15T09:58:27Z","updated_at":"2016-01-15T09:58:27Z","author_association":"MEMBER","body":"Just a thought: I wonder if the default limit should be something like the default `http.max_content_length` divided by two or something along those lines?\nMay be we should (also?) check that this max size is actually not bigger than `http.max_content_length` and reject the settings in such a case?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/171948369","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-171948369","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":171948369,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MTk0ODM2OQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-01-15T12:28:41Z","updated_at":"2016-01-15T12:28:41Z","author_association":"MEMBER","body":"Thanks for your input @dadoonet! I am not sure about your suggestion on the check of `http.max_content_length` though as this relies on a protocol level setting whereas I'd try to enforce bulk request size in a protocol-independent fashion. I mean this should also work if somebody uses only the transport protocol and then I wouldn't do a validation of this setting based on a http related one. But tbh I did not look into the code yet. wdyt?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/171950213","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-171950213","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":171950213,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MTk1MDIxMw==","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2016-01-15T12:38:27Z","updated_at":"2016-01-15T12:38:27Z","author_association":"MEMBER","body":"ha! That is correct. I thought `http.max_content_length` was applied to transport layer as well... \nIgnore me then! :D \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/171950410","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-171950410","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":171950410,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MTk1MDQxMA==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-01-15T12:39:54Z","updated_at":"2016-01-15T12:39:54Z","author_association":"MEMBER","body":"Ok, thanks for the clarification.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172583454","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-172583454","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":172583454,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjU4MzQ1NA==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-01-18T16:47:11Z","updated_at":"2016-01-18T16:47:11Z","author_association":"MEMBER","body":"### Design Parameters\n\nAfter looking at the source, I have identified a few options which I'd like to discuss before implementing this feature. Below are some design parameters which are worth considering from my point of view.\n\n#### Type of endpoint\n\nA (bulk) request can hit the cluster via the following endpoints:\n1. HTTP: hint: the REST handler reissues the HTTP request as a transport request via the client interface.\n2. Transport (or more precise: remote transport)\n3. Local transport: An optimization to avoid the overhead of a remote call when just forwarding a request within the same process.\n\n#### Applicability of the limit\n\nThis means whether we want to apply the limit for all types of requests or just for a limited number of explicitly defined request types (like bulk requests).\n\n#### When to check the limit\n- Apply during deserialization: This would allow us to abort request handling as soon as we know the limit is hit but at the expense of implementing this for all protocols separately. For the transport protocol, the right place looks like `MessageChannelHandler#handleRequest()`.\n- Apply before a (transport) request is handled (i.e. in the appropriate subclass of `TransportAction`): This would simplify the validation as we would just check once for all endpoint types (as we know that HTTP requests are reissued as transport requests). However, at this point in time the request is already deserialized thus consuming memory and all we can do is preserve any additional system resources by aborting request processing.\n\n#### Request size calculation\n\nTo know whether a request has (b)reached the limit, we need to calculate its size. Considering `BulkRequest` for now, we have two options on how to determine the request size:\n- `BulkRequest#estimatedSize()`: This works but has shortcomings in corner cases (see e.g. #15589)\n- If we enforce the limit already during deserialization we can calculate request size exactly, e.g. by writing a custom `StreamInput` implementation that counts the bytes deserialized.\n\n### Proposal\n\nBased on the options above, I want to sketch a simple solution proposal:\n\nConsidering that it is likely we want limit checks not only for bulk requests but also similar ones, like multi-gets, we should not tie this too specifically to bulk requests.\n\nHence, each for each request type that should be size-limited, the corresponding `TransportRequest` class implements a new interface `RequestSizeEstimator` (draft name):\n\n``` java\npublic interface RequestSizeEstimator {\n  int getRequestSizeInBytes();\n}\n```\n\nWe define one configurable request size limit (default e.g. 50MB) for all request types and implement limit breach detection in a high-level fashion as an `ActionFilter`. This `ActionFilter` checks whether the `TransportRequest` implements `RequestSizeEstimator` and checks against this limit. If the limit is breached, we throw an appropriate exception indicating a client error (4xx in HTTP speak).\n\nIn summary, the pros and cons of this solution are:\n\nPros:\n- The limit is only checked in a single place in the code base\n- Easily extensible: We can add checks for new request types easily by having the corresponding transport request class implement `RequestSizeEstimator`.\n\nCons:\n- We deserialize the full request before we check the limit. This means we still hit a potential memory spike but we avoid using any other resources (CPU, I/O) after that point.\n- The size calculation may not be entirely accurate (see my comments above).\n\nFeedback is very much appreciated.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172844033","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-172844033","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":172844033,"node_id":"MDEyOklzc3VlQ29tbWVudDE3Mjg0NDAzMw==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-01-19T12:51:56Z","updated_at":"2016-01-19T13:04:42Z","author_association":"MEMBER","body":"Based on feedback I got by @bleskes, here's a revised approach:\n\nWe limit the size of requests on protocol level during deserialization. We consider two cases:\n- Receiving a request: We will apply one global limit for all requests that are in flight\n- Sending a request (internally in the cluster): We will apply a limit for each individual request\n\nLimiting on protocol level has a couple of advantages:\n- We cancel request processing as soon as we hit the configured memory limit\n- It allows us to introduce a limit per action (not in the scope of this ticket though)\n\nIn the first step, we will implement an action-independent limit which applies to all actions not just bulk actions.\n\nTransports that need to be considered are Netty transport and local transport (for testing purposes). HTTP is unaffected because we just stream data from the HTTP layer to the transport layer (no up-front allocation).\n\nWe need to check whether circuit breakers are a feasible option to detect limit breaches.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/173505136","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-173505136","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":173505136,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MzUwNTEzNg==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-01-21T09:01:53Z","updated_at":"2016-01-21T09:01:53Z","author_association":"MEMBER","body":"@dakrone: Could you share your thoughts whether using `CircuitBreaker` is feasible for limiting memory usage in the following two scenarios?\n1. Limiting the memory usage of all in-flight requests on a single node level.\n2. Limiting the memory usage of each individual outgoing request of a node.\n\nIn both cases the memory usage will be determined by the size of the deserialized request payload.\n\n(more details above; that's just the summary for you)\n\n### Scenario 1\n\nI have seen that there is already a \"request\" circuit breaker but its `BreakerSettings` are (a) cluster-wide and (b) relative to the amount of available heap memory. I'd tackle this by introducing new `BreakerSettings` for all requests in flight at node level (which would have to be a new value in `org.elasticsearch.common.settings.Settings.Scope`)) and with an absolute but user-configurable limit (e.g. 50mb).\n\n### Scenario 2\n\nWe also want to limit memory usage on a single request basis (not across all requests that are in flight) and tbh I think that circuit breakers are not a good fit because we would need a new circuit breaker instance for each request as far as I understand it. However, it would make the implementation more uniform.\n\nIt would be great to hear your input on this.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/178093416","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-178093416","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":178093416,"node_id":"MDEyOklzc3VlQ29tbWVudDE3ODA5MzQxNg==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2016-02-01T17:50:43Z","updated_at":"2016-02-01T17:50:43Z","author_association":"MEMBER","body":"@danielmitterdorfer \n\n> Could you share your thoughts whether using CircuitBreaker is feasible for\n> limiting memory usage in the following two scenarios?\n> \n>    Limiting the memory usage of all in-flight requests on a single node level.\n>    Limiting the memory usage of each individual outgoing request of a node.\n> \n> I have seen that there is already a \"request\" circuit breaker but its\n> BreakerSettings are (a) cluster-wide and (b) relative to the amount of\n> available heap memory. I'd tackle this by introducing new BreakerSettings for\n> all requests in flight at node level (which would have to be a new value in\n> org.elasticsearch.common.settings.Settings.Scope)) and with an absolute but\n> user-configurable limit (e.g. 50mb).\n\nThis is exactly what the circuit breaker is for, you can easily define a new\nbreaker (and breaker type) in `HierarchyCircuitBreakerService` to do just that.\nI would recommend this heartily :) (and I'm happy to help)\n\n> We also want to limit memory usage on a single request basis (not across all\n> requests that are in flight) and tbh I think that circuit breakers are not a\n> good fit because we would need a new circuit breaker instance for each request\n> as far as I understand it. However, it would make the implementation more\n> uniform.\n\nTo me, this doesn't sound like a good fit for the circuit breaker (since it is\ndesigned to be per-node, not per-request). Instead, this sounds like a better\nfit for the validation framework, since that works on the per-request level.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/178486776","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-178486776","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":178486776,"node_id":"MDEyOklzc3VlQ29tbWVudDE3ODQ4Njc3Ng==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-02-02T09:59:01Z","updated_at":"2016-02-02T09:59:01Z","author_association":"MEMBER","body":"Thanks for your feedback! I'll implement it based on your pointers. Would be great if you could take a look at it then.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/178633361","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-178633361","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":178633361,"node_id":"MDEyOklzc3VlQ29tbWVudDE3ODYzMzM2MQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-02-02T15:26:50Z","updated_at":"2016-02-02T15:26:50Z","author_association":"MEMBER","body":"@dakrone,\n\nI have started implementing request size limiting based on your pointers (see danielmitterdorfer/elasticsearch@802f6ed5f6eeb0ee8d46c294ddf53058aec3157b). I have added a new circuit breaker to `HierarchyCircuitBreakerService` but then fell over `CircuitBreakerServiceIT#testParentChecking()`. The test spuriously hangs and the reason is that the parent circuit breaker tripped and we cannot get any request through at the transport layer afterwards. I am not sure whether we want the dependency among circuit breakers at all in this case. wdyt?\n\nOther things I have noted:\n- I am setting an absolute limit for my new circuit breaker but it looks to me that the intention is that all leaf circuit breaker limits add up to 100%, so I think I should reduce the other values accordingly and create a relative setting instead of an absolute one, right?\n- I have introduced an adapter interface `Limiter` in my implementation and either use one that delegates to a circuit breaker (for scenario 1; in-flight requests) and one were I manually check the bytes used (scenario 2, per request limiting). I throw a `CircuitBreakingException` in both implementations but for the second case I think it is a bit too implementation specific and does not fit well. I can now either introduce a new exception or rename / move `CircuitBreakingException` to something like `QuotaExceededException` which would fit in both cases.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/179979142","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-179979142","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":179979142,"node_id":"MDEyOklzc3VlQ29tbWVudDE3OTk3OTE0Mg==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2016-02-04T18:18:55Z","updated_at":"2016-02-04T18:18:55Z","author_association":"MEMBER","body":"> The test spuriously hangs and the reason is that the parent circuit breaker\n> tripped and we cannot get any request through at the transport layer\n> afterwards. I am not sure whether we want the dependency among circuit\n> breakers at all in this case. wdyt?\n\nThis came up when I added the request breaker on `BigArrays`, because we always\nwant to be able to allocate a buffer to send an error response! The way that I\nworked around it before was splitting it into a circuit-breaking and\nnon-circuit-breaking category, where the non-circuit-breaking did the\n`addWithoutBreaking(numBytes)` so that the usage was still _tracked_, it just\ndidn't _prevent_ requests from being sent out.\n\nSo in your case, I think we may want to limit which requests fall into the\ncircuit breaking category (index requests, bulk requests, etc), and which don't\n(errors, cluster health, cluster state, etc).\n\nWhat do you think?\n\n> I am setting an absolute limit for my new circuit breaker but it looks to me\n> that the intention is that all leaf circuit breaker limits add up to 100%, so\n> I think I should reduce the other values accordingly and create a relative\n> setting instead of an absolute one, right?\n\nNo, they definitely don't have to add up to 100%, that's why the parent is\nthere. The idea is that you can have something like this:\n\nfielddata: 40%\nbig_arrays: 30%\nrequests (your new one): 50mb\n\nparent: 60%\n\nWhich allows us to limit individual parts to certain amounts (like absolute\namounts), while still limiting circuit breaking memory across the node on the\nwhole (the parent breaker).\n\nI definitely think this new breaker should be absolute instead of relative.\nYou're on the right track with it.\n\n> I have introduced an adapter interface Limiter in my implementation and either\n> use one that delegates to a circuit breaker (for scenario 1; in-flight\n> requests) and one were I manually check the bytes used (scenario 2, per\n> request limiting). I throw a CircuitBreakingException in both implementations\n> but for the second case I think it is a bit too implementation specific and\n> does not fit well. I can now either introduce a new exception or rename / move\n> CircuitBreakingException to something like QuotaExceededException which would\n> fit in both cases.\n\nWhat makes you think it is too implementation specific? You could always\nsub-class `CircuitBreakingException` if you wanted it to be a separate type, but\nI don't think it's necessarily bad to keep it as such. It is a circuit breaker,\njust on the per-request side instead of the node side, so with a good message I\nthink that can be made clear to the user.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/180017155","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-180017155","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":180017155,"node_id":"MDEyOklzc3VlQ29tbWVudDE4MDAxNzE1NQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-02-04T19:37:01Z","updated_at":"2016-02-04T19:37:01Z","author_association":"MEMBER","body":">  because we always\n> want to be able to allocate a buffer to send an error response!\n\nI think it makes sense to never circuit break a response (both when sending and receiving). \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/181871168","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-181871168","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":181871168,"node_id":"MDEyOklzc3VlQ29tbWVudDE4MTg3MTE2OA==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-02-09T13:49:30Z","updated_at":"2016-02-09T13:49:30Z","author_association":"MEMBER","body":"@dakrone: First of all, thanks for your thoughts.\n\n> So in your case, I think we may want to limit which requests fall into the circuit breaking category (index requests, bulk requests, etc), and which don't (errors, cluster health, cluster state, etc).\n\nWith the current implementation this is not as easy as I've added a `LimitingInputStream` and an `LimitingOutputStream` which wraps the actual streams. Therefore, size limiting is transparent to the user. We could make the client aware though and expose an additional method for disabling limit tracking.\n\nAnother approach I am thinking of is to decide at creation time of stream based on the action whether it needs to break or not. If yes, we wrap the original one in a limiting stream, otherwise we just leave the stream as is. For me, the most appropriate place to add this support seems to be `TransportRequestOptions`.\n\nFor now I have stabilized the test in question by increasing the limit so it still breaks as intended but does not hit the request size limit. I think this is ok given that we need some minimum amount to handle the request at all.\n\n> [The child circuit breaker limits] definitely don't have to add up to 100%\n> [...]\n> I definitely think this new breaker should be absolute instead of relative.\n> You're on the right track with it.\n\nThat's great, then I'll leave that part as is. Thanks for the clarification. :)\n\n> > I think it is a bit too implementation specific [to throw a `CircuitBreakingException` in all cases]\n> > What makes you think it is too implementation specific?\n\nMy thought was that we should throw `CircuitBreakingException` only when a \"proper\" circuit breaker is involved (as the name of the exception indicates) but I am fine with using `CircuitBreakingException` in the other case as well.\n\n> I think it makes sense to never circuit break a response (both when sending and receiving). \n\n@bleskes: Thanks for the hint. I have reduced the scope of size limiting to requests only (i.e. `TransportStatus.isRequest(status) == true`).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/182836460","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-182836460","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":182836460,"node_id":"MDEyOklzc3VlQ29tbWVudDE4MjgzNjQ2MA==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-02-11T12:28:33Z","updated_at":"2016-02-11T12:28:33Z","author_association":"MEMBER","body":"I have pushed another commit on the [feature branch](https://github.com/danielmitterdorfer/elasticsearch/commits/bulk-size-limit). We exhibit the following behaviour when a (bulk) request hits the limit:\n- Transport client: `CircuitBreakingException` is thrown\n- Node client: We get errors on the individual items of a bulk request (i.e. `BulkItemResponse.isFailure() == true`). This is due to the fact that the node client does not hit the transport layer but starts executing `TransportBulkAction` directly (i.e. in the same process).\n- REST API: This bypasses the transport layer so the recommended practice is to use `http.max_content_length`. While I think we could add support in `NettyHttpServerTransport` these two mechanisms are quite similar so I'd avoid reimplementing it there.\n\nI am not too happy that we behave differently depending on the protocol but as this is implemented on (transport) protocol level, it is not much of a surprise. Wdyt @bleskes?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/192610116","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-192610116","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":192610116,"node_id":"MDEyOklzc3VlQ29tbWVudDE5MjYxMDExNg==","user":{"login":"Dieken","id":2365,"node_id":"MDQ6VXNlcjIzNjU=","avatar_url":"https://avatars0.githubusercontent.com/u/2365?v=4","gravatar_id":"","url":"https://api.github.com/users/Dieken","html_url":"https://github.com/Dieken","followers_url":"https://api.github.com/users/Dieken/followers","following_url":"https://api.github.com/users/Dieken/following{/other_user}","gists_url":"https://api.github.com/users/Dieken/gists{/gist_id}","starred_url":"https://api.github.com/users/Dieken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Dieken/subscriptions","organizations_url":"https://api.github.com/users/Dieken/orgs","repos_url":"https://api.github.com/users/Dieken/repos","events_url":"https://api.github.com/users/Dieken/events{/privacy}","received_events_url":"https://api.github.com/users/Dieken/received_events","type":"User","site_admin":false},"created_at":"2016-03-05T09:11:52Z","updated_at":"2016-03-05T09:12:30Z","author_association":"NONE","body":"@danielmitterdorfer  I think the http.max_content_length is to limit content length for **any single** http request,  not the total size of in-flight http requests.\n\nI made a patch https://github.com/Dieken/elasticsearch/commit/f2d487eaf1213daa6ad25f13fbccdd1d6b5930f4 against v2.2.0 to limit total size of in-flight HTTP bulk requests as a temporary solution,  because it seems your patch hasn't finished and it may be not merged to 2.x.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/193134632","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-193134632","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":193134632,"node_id":"MDEyOklzc3VlQ29tbWVudDE5MzEzNDYzMg==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2016-03-07T07:20:51Z","updated_at":"2016-03-07T07:20:51Z","author_association":"MEMBER","body":"@Dieken You're right: `http.max_content_length` limits only individual requests so we need to tackle this differently.\n\nRegarding my changes: Support on transport level is finished but we need to iron out some details (property names, default values).\n\nYour patch looks fine for the bulk use case but I just want to raise your awareness of one (minor) thing though: You use `LongAdder` which is a JDK 8 class but Elasticsearch 2.x should be built with JDK 7.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/193147432","html_url":"https://github.com/elastic/elasticsearch/issues/16011#issuecomment-193147432","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/16011","id":193147432,"node_id":"MDEyOklzc3VlQ29tbWVudDE5MzE0NzQzMg==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-03-07T08:11:28Z","updated_at":"2016-03-07T08:11:28Z","author_association":"MEMBER","body":"for people following - @danielmitterdorfer and I went through the code together and Daniel is working on another iteration.\n","performed_via_github_app":null}]