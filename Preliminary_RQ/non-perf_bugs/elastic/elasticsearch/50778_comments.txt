[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/572360274","html_url":"https://github.com/elastic/elasticsearch/issues/50778#issuecomment-572360274","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50778","id":572360274,"node_id":"MDEyOklzc3VlQ29tbWVudDU3MjM2MDI3NA==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2020-01-09T02:59:58Z","updated_at":"2020-01-09T02:59:58Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-core-features (:Core/Features/ILM+SLM)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/572625336","html_url":"https://github.com/elastic/elasticsearch/issues/50778#issuecomment-572625336","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50778","id":572625336,"node_id":"MDEyOklzc3VlQ29tbWVudDU3MjYyNTMzNg==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2020-01-09T15:57:47Z","updated_at":"2020-01-09T16:18:03Z","author_association":"CONTRIBUTOR","body":"I do not think this should be an `URGENT` task. There's something very wrong with the cluster if the master does not get around to the `NORMAL`-priority tasks in a reasonably short amount of time, and the solution to that is definitely not to promote things to `URGENT`.\r\n\r\nNote that we split up the historically-expensive `shard-started` task into a cheap `URGENT` part and a more expensive `NORMAL` part in https://github.com/elastic/elasticsearch/pull/44433, and batched the more expensive parts together to avoid duplicate work. If we are still seeing `URGENT`-level tasks running unduly slowly then I would like more details so we can continue in this direction.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/572678936","html_url":"https://github.com/elastic/elasticsearch/issues/50778#issuecomment-572678936","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50778","id":572678936,"node_id":"MDEyOklzc3VlQ29tbWVudDU3MjY3ODkzNg==","user":{"login":"gwbrown","id":1522844,"node_id":"MDQ6VXNlcjE1MjI4NDQ=","avatar_url":"https://avatars1.githubusercontent.com/u/1522844?v=4","gravatar_id":"","url":"https://api.github.com/users/gwbrown","html_url":"https://github.com/gwbrown","followers_url":"https://api.github.com/users/gwbrown/followers","following_url":"https://api.github.com/users/gwbrown/following{/other_user}","gists_url":"https://api.github.com/users/gwbrown/gists{/gist_id}","starred_url":"https://api.github.com/users/gwbrown/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gwbrown/subscriptions","organizations_url":"https://api.github.com/users/gwbrown/orgs","repos_url":"https://api.github.com/users/gwbrown/repos","events_url":"https://api.github.com/users/gwbrown/events{/privacy}","received_events_url":"https://api.github.com/users/gwbrown/received_events","type":"User","site_admin":false},"created_at":"2020-01-09T17:57:50Z","updated_at":"2020-01-09T17:57:50Z","author_association":"CONTRIBUTOR","body":"We have seen an issue in larger clusters where it often takes longer than the 30s default to process a rollover, which in currently-released versions of ES will cause ILM to stop for an index until a user intervenes.  This is definitely a problem as you say, because it can lead to indices growing very large.\r\n\r\nHowever, we're already taking steps to address this problem in another way. In https://github.com/elastic/elasticsearch/pull/50388 we've made Rollover a single cluster state update (rather than several in sequence), which enables us to implement automatic retries (see https://github.com/elastic/elasticsearch/issues/48183) for rollover, and should help alleviate this problem without having to adjust the priority of the task.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/572917364","html_url":"https://github.com/elastic/elasticsearch/issues/50778#issuecomment-572917364","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50778","id":572917364,"node_id":"MDEyOklzc3VlQ29tbWVudDU3MjkxNzM2NA==","user":{"login":"shwetathareja","id":2943091,"node_id":"MDQ6VXNlcjI5NDMwOTE=","avatar_url":"https://avatars3.githubusercontent.com/u/2943091?v=4","gravatar_id":"","url":"https://api.github.com/users/shwetathareja","html_url":"https://github.com/shwetathareja","followers_url":"https://api.github.com/users/shwetathareja/followers","following_url":"https://api.github.com/users/shwetathareja/following{/other_user}","gists_url":"https://api.github.com/users/shwetathareja/gists{/gist_id}","starred_url":"https://api.github.com/users/shwetathareja/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shwetathareja/subscriptions","organizations_url":"https://api.github.com/users/shwetathareja/orgs","repos_url":"https://api.github.com/users/shwetathareja/repos","events_url":"https://api.github.com/users/shwetathareja/events{/privacy}","received_events_url":"https://api.github.com/users/shwetathareja/received_events","type":"User","site_admin":false},"created_at":"2020-01-10T07:57:53Z","updated_at":"2020-01-10T07:59:25Z","author_association":"NONE","body":"Thanks @DaveCTurner  and @gwbrown  for your response.\r\n\r\n> Note that we split up the historically-expensive `shard-started` task into a cheap `URGENT` part and a more expensive `NORMAL` part in #44433, and batched the more expensive parts together to avoid duplicate work. If we are still seeing `URGENT`-level tasks running unduly slowly then I would like more details so we can continue in this direction.\r\n\r\nLike you mentioned reroute is an \"expensive\" NORMAL task, based on insertion order it can still delay the rollover task which will timeout after default 30secs waiting in the queue. With \"NORMAL\" priority it is competing not only with URGENT/ HIGH but with \"NORMAL\" priority tasks as well. And, rollover operation not performed in time could cause single index to grow for high ingestion rate and can have more side effects in the cluster. individual create-index/ alias-switch tasks run with \"URGENT\" priority but why rollover should have lower priority than that (considering all are customer initiated actions). I would like to understand the criteria based on which priority is decided for various tasks.\r\n\r\n> However, we're already taking steps to address this problem in another way. In #50388 we've made Rollover a single cluster state update (rather than several in sequence), which enables us to implement automatic retries (see #48183) for rollover, and should help alleviate this problem without having to adjust the priority of the task.\r\n\r\nyes, thanks for the fix of rollover in a single cluster state update. This is really helpful.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/577734240","html_url":"https://github.com/elastic/elasticsearch/issues/50778#issuecomment-577734240","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/50778","id":577734240,"node_id":"MDEyOklzc3VlQ29tbWVudDU3NzczNDI0MA==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2020-01-23T15:34:21Z","updated_at":"2020-01-23T15:34:21Z","author_association":"MEMBER","body":"We discussed this today and decided that we'd rather not increase the priority of this task, instead, we have other ways to address it:\r\n\r\n- The rollover is now a single cluster state update instead of multiple, so it adds fewer things to the cluster state update queue\r\n- The rollover action is now retryable, so even if it does time out, it can be retried at the next stage (instead of staying in the ERROR step)\r\n- We are making the master node timeout configurable in the event it needs to be increased\r\n\r\nAs David said, we should solve the underlying issue rather than increasing the priority, as once more things move to `URGENT`, fewer things actually end up \"being\" urgently executed.\r\n\r\nWith that I'm going to close this issue. Thanks!","performed_via_github_app":null}]