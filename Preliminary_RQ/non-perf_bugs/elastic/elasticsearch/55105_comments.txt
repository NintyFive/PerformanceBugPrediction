[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/612826795","html_url":"https://github.com/elastic/elasticsearch/issues/55105#issuecomment-612826795","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/55105","id":612826795,"node_id":"MDEyOklzc3VlQ29tbWVudDYxMjgyNjc5NQ==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2020-04-13T09:29:00Z","updated_at":"2020-04-13T09:29:00Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed (:Distributed/Recovery)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/613338655","html_url":"https://github.com/elastic/elasticsearch/issues/55105#issuecomment-613338655","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/55105","id":613338655,"node_id":"MDEyOklzc3VlQ29tbWVudDYxMzMzODY1NQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2020-04-14T09:45:00Z","updated_at":"2020-04-14T09:45:00Z","author_association":"CONTRIBUTOR","body":"I think that this hiccup is caused by the cluster coordination layer, and a messy master election. We have a three node cluster, with node_t1 being shut down. InternalTestCluster adds a voting configuration exclusion for node_t1, which was the master at the time. That node now abdicates to node_t0 (with term 2), but node_t2 seems to be trying to elect itself as master first (in term 3), which it looks to win, with node_t1 running itself an election in term 2 (unsuccessful, then tries term 4). Ultimately node_t2 ends up winning in term 5. There was an intermediate state where node_t2 had won in term 3 and where it processed the cluster health request which confirmed that there were two nodes in the cluster.\r\n\r\n```\r\n1> [2020-04-13T03:46:29,618][INFO ][o.e.t.InternalTestCluster] [testDanglingIndicesAreNotRecoveredWhenNotWritten] Restarting node [node_t1] \r\n  1> [2020-04-13T03:46:29,619][INFO ][o.e.t.InternalTestCluster] [testDanglingIndicesAreNotRecoveredWhenNotWritten] adding voting config exclusions [node_t1] prior to restart/shutdown\r\n  1> [2020-04-13T03:46:29,708][INFO ][o.e.c.c.Coordinator      ] [node_t1] abdicating to {node_t0}{VjxLuFUcSKKbDZf76xazPA}{95I0Y3BKTy21QrOVOt5WTQ}{127.0.0.1}{127.0.0.1:36331}{dim} with term 2\r\n  1> [2020-04-13T03:46:29,712][INFO ][o.e.c.s.ClusterApplierService] [node_t1] master node changed {previous [{node_t1}{Rk5ZWJbhRHig0jdNn7iMFw}{Bzk-zMv3Teaqi6cAji6TiQ}{127.0.0.1}{127.0.0.1:40465}{dim}], current []}, term: 1, version: 10, reason: becoming candidate: after abdicating to {node_t0}{VjxLuFUcSKKbDZf76xazPA}{95I0Y3BKTy21QrOVOt5WTQ}{127.0.0.1}{127.0.0.1:36331}{dim}\r\n  1> [2020-04-13T03:46:29,712][INFO ][o.e.c.s.ClusterApplierService] [node_t0] master node changed {previous [{node_t1}{Rk5ZWJbhRHig0jdNn7iMFw}{Bzk-zMv3Teaqi6cAji6TiQ}{127.0.0.1}{127.0.0.1:40465}{dim}], current []}, term: 1, version: 10, reason: becoming candidate: joinLeaderInTerm\r\n  1> [2020-04-13T03:46:29,713][INFO ][o.e.c.s.ClusterApplierService] [node_t2] master node changed {previous [{node_t1}{Rk5ZWJbhRHig0jdNn7iMFw}{Bzk-zMv3Teaqi6cAji6TiQ}{127.0.0.1}{127.0.0.1:40465}{dim}], current []}, term: 1, version: 10, reason: becoming candidate: joinLeaderInTerm\r\n  1> [2020-04-13T03:46:29,818][INFO ][o.e.c.s.MasterService    ] [node_t2] elected-as-master ([1] nodes joined)[{node_t2}{7PGOypgYQK-RyF6uZl9ioA}{FlGOmbwrTFaJu0zgTwJSqw}{127.0.0.1}{127.0.0.1:38241}{dim} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 3, version: 11, delta: master node changed {previous [], current [{node_t2}{7PGOypgYQK-RyF6uZl9ioA}{FlGOmbwrTFaJu0zgTwJSqw}{127.0.0.1}{127.0.0.1:38241}{dim}]}\r\n  1> [2020-04-13T03:46:29,849][INFO ][o.e.c.s.ClusterApplierService] [node_t1] master node changed {previous [], current [{node_t2}{7PGOypgYQK-RyF6uZl9ioA}{FlGOmbwrTFaJu0zgTwJSqw}{127.0.0.1}{127.0.0.1:38241}{dim}]}, term: 3, version: 11, reason: ApplyCommitRequest{term=3, version=11, sourceNode={node_t2}{7PGOypgYQK-RyF6uZl9ioA}{FlGOmbwrTFaJu0zgTwJSqw}{127.0.0.1}{127.0.0.1:38241}{dim}}\r\n  1> [2020-04-13T03:46:29,875][INFO ][o.e.c.s.MasterService    ] [node_t0] elected-as-master ([2] nodes joined)[{node_t0}{VjxLuFUcSKKbDZf76xazPA}{95I0Y3BKTy21QrOVOt5WTQ}{127.0.0.1}{127.0.0.1:36331}{dim} elect leader, {node_t2}{7PGOypgYQK-RyF6uZl9ioA}{FlGOmbwrTFaJu0zgTwJSqw}{127.0.0.1}{127.0.0.1:38241}{dim} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 4, version: 11, delta: master node changed {previous [], current [{node_t0}{VjxLuFUcSKKbDZf76xazPA}{95I0Y3BKTy21QrOVOt5WTQ}{127.0.0.1}{127.0.0.1:36331}{dim}]}\r\n  1> [2020-04-13T03:46:29,876][WARN ][o.e.c.s.MasterService    ] [node_t0] failing [elected-as-master ([2] nodes joined)[{node_t0}{VjxLuFUcSKKbDZf76xazPA}{95I0Y3BKTy21QrOVOt5WTQ}{127.0.0.1}{127.0.0.1:36331}{dim} elect leader, {node_t2}{7PGOypgYQK-RyF6uZl9ioA}{FlGOmbwrTFaJu0zgTwJSqw}{127.0.0.1}{127.0.0.1:38241}{dim} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_]]: failed to commit cluster state version [11]\r\n  1> org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException: node is no longer master for term 4 while handling publication\r\n  1> \tat org.elasticsearch.cluster.coordination.Coordinator.publish(Coordinator.java:1050) ~[main/:?]\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.publish(MasterService.java:268) [main/:?]\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:250) [main/:?]\r\n  1> \tat org.elasticsearch.cluster.service.MasterService.access$000(MasterService.java:73) [main/:?]\r\n  1> \tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:151) [main/:?]\r\n  1> \tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) [main/:?]\r\n  1> \tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188) [main/:?]\r\n  1> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:633) [main/:?]\r\n  1> \tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252) [main/:?]\r\n  1> \tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215) [main/:?]\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]\r\n  1> \tat java.lang.Thread.run(Thread.java:832) [?:?]\r\n```\r\n\r\nI'm not sure what we should do about this one. @DaveCTurner thoughts?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/613365551","html_url":"https://github.com/elastic/elasticsearch/issues/55105#issuecomment-613365551","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/55105","id":613365551,"node_id":"MDEyOklzc3VlQ29tbWVudDYxMzM2NTU1MQ==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2020-04-14T10:45:00Z","updated_at":"2020-04-14T10:45:00Z","author_association":"CONTRIBUTOR","body":"I think we've discussed previously that acking of a specific cluster state update seems like the wrong thing to check since they can fail like this but can still be applied successfully by a subsequent update.\r\n\r\nReally only a problem when the master isn't completely established; maybe we should call `validateClusterFormed()`?","performed_via_github_app":null}]