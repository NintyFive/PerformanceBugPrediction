[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38098168","html_url":"https://github.com/elastic/elasticsearch/issues/5466#issuecomment-38098168","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5466","id":38098168,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MDk4MTY4","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2014-03-19T19:47:20Z","updated_at":"2014-03-19T19:47:20Z","author_association":"MEMBER","body":"To extract data from elasticsearch you should use [scan&scroll API](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-scroll.html).\n\nUsing `size` is not the way to go.\n\nClosing. Feel free to reopen if I misunderstood your use case. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38104150","html_url":"https://github.com/elastic/elasticsearch/issues/5466#issuecomment-38104150","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5466","id":38104150,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MTA0MTUw","user":{"login":"bobbyhubbard","id":5067336,"node_id":"MDQ6VXNlcjUwNjczMzY=","avatar_url":"https://avatars1.githubusercontent.com/u/5067336?v=4","gravatar_id":"","url":"https://api.github.com/users/bobbyhubbard","html_url":"https://github.com/bobbyhubbard","followers_url":"https://api.github.com/users/bobbyhubbard/followers","following_url":"https://api.github.com/users/bobbyhubbard/following{/other_user}","gists_url":"https://api.github.com/users/bobbyhubbard/gists{/gist_id}","starred_url":"https://api.github.com/users/bobbyhubbard/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobbyhubbard/subscriptions","organizations_url":"https://api.github.com/users/bobbyhubbard/orgs","repos_url":"https://api.github.com/users/bobbyhubbard/repos","events_url":"https://api.github.com/users/bobbyhubbard/events{/privacy}","received_events_url":"https://api.github.com/users/bobbyhubbard/received_events","type":"User","site_admin":false},"created_at":"2014-03-19T20:42:52Z","updated_at":"2014-03-19T20:48:57Z","author_association":"NONE","body":"Its a performance defect that someone could exploit.\n\nMy point is that some unsuspecting user typing a simple search query with a size >999999 could have significant performance impact on a cluster as response time seems to increase exponentially until MAX_INT at which point you get an index out of bounds from the json parser. Of course specifying a size that large is not optimal... but I'm not always the one writing the query. \n\nI'm unable to reopen but if I could, i would. :)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38105454","html_url":"https://github.com/elastic/elasticsearch/issues/5466#issuecomment-38105454","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5466","id":38105454,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MTA1NDU0","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2014-03-19T20:54:16Z","updated_at":"2014-03-19T20:54:16Z","author_association":"CONTRIBUTOR","body":"It might be worth having a cluster wide max size that could be configured\nto reject requests like this.  It opens up a can of worms, too.  Like, you\nshould probably excuse scan type queries.\n\nOn Wed, Mar 19, 2014 at 4:42 PM, bobbyhubbard notifications@github.comwrote:\n\n> My point is that some unsuspecting user typing a simple search query with\n> a size >999999 could have significant performance impact on a cluster as\n> response time seems to increase exponentially until MAX_INT when you an\n> index out of bounds from the json parser. Of course specifying a size that\n> large is not optimal... but I'm not always the one writing the query. Its a\n> performance defect that someone could exploit.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5466#issuecomment-38104150\n> .\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38106381","html_url":"https://github.com/elastic/elasticsearch/issues/5466#issuecomment-38106381","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5466","id":38106381,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MTA2Mzgx","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2014-03-19T21:02:39Z","updated_at":"2014-03-19T21:02:39Z","author_association":"MEMBER","body":"I agreed that we should perhaps come with reasonable defaults (500?) which can be set.\nWhat does others think?\n\nReopening.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/38143776","html_url":"https://github.com/elastic/elasticsearch/issues/5466#issuecomment-38143776","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5466","id":38143776,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MTQzNzc2","user":{"login":"uboness","id":211019,"node_id":"MDQ6VXNlcjIxMTAxOQ==","avatar_url":"https://avatars3.githubusercontent.com/u/211019?v=4","gravatar_id":"","url":"https://api.github.com/users/uboness","html_url":"https://github.com/uboness","followers_url":"https://api.github.com/users/uboness/followers","following_url":"https://api.github.com/users/uboness/following{/other_user}","gists_url":"https://api.github.com/users/uboness/gists{/gist_id}","starred_url":"https://api.github.com/users/uboness/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/uboness/subscriptions","organizations_url":"https://api.github.com/users/uboness/orgs","repos_url":"https://api.github.com/users/uboness/repos","events_url":"https://api.github.com/users/uboness/events{/privacy}","received_events_url":"https://api.github.com/users/uboness/received_events","type":"User","site_admin":false},"created_at":"2014-03-20T08:23:47Z","updated_at":"2014-03-20T08:23:47Z","author_association":"CONTRIBUTOR","body":"It's not just about the json... There are many factors that play here (the priority queues that are responsible for the sorting and the size of the docs to name a couple). I agree that this should ideally be handled gracefully, probably by introducing another circuit breaker - and the response should be handled like all other CBs we have. It's a bit tricky though to figure out the proper thresholds for this CB... Will require some thought.\n\nBtw, I'm not sure that the error belongs to the 500 range as this should not be perceived as a system error... \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/56552835","html_url":"https://github.com/elastic/elasticsearch/issues/5466#issuecomment-56552835","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5466","id":56552835,"node_id":"MDEyOklzc3VlQ29tbWVudDU2NTUyODM1","user":{"login":"skade","id":47542,"node_id":"MDQ6VXNlcjQ3NTQy","avatar_url":"https://avatars2.githubusercontent.com/u/47542?v=4","gravatar_id":"","url":"https://api.github.com/users/skade","html_url":"https://github.com/skade","followers_url":"https://api.github.com/users/skade/followers","following_url":"https://api.github.com/users/skade/following{/other_user}","gists_url":"https://api.github.com/users/skade/gists{/gist_id}","starred_url":"https://api.github.com/users/skade/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/skade/subscriptions","organizations_url":"https://api.github.com/users/skade/orgs","repos_url":"https://api.github.com/users/skade/repos","events_url":"https://api.github.com/users/skade/events{/privacy}","received_events_url":"https://api.github.com/users/skade/received_events","type":"User","site_admin":false},"created_at":"2014-09-23T16:57:10Z","updated_at":"2014-09-23T17:39:39Z","author_association":"CONTRIBUTOR","body":"I had a run-in with this bug today, where the maximum size was set to an \"arbitrary high value\" (9999999) because the number of potential responses was small (<100) and it was meant to retrieve \"all\". That query degraded to a point where it brought down the whole cluster node by node. That's a naive approach I see from time to time. In the end, the query retrieved only < 100kb payload.\nThe query was sent to the type-specific end-point `/*index*/*type*`.\n\nThe response time grew with the number of documents in the whole index, not showing itself on small datasets (dev), less on slightly larger (stage) and disastrous (largest time was 18m) on the live system. In Prod, it lead to sudden allocations of huge chunks of heap that would immediately be collected by a stop the world collection of multiple seconds, leading to nodes dropping out of the cluster and the search queue exploding. It seems like there is something leading to such a query to visit more documents then necessary. I'll try to build a testcase.\n\nWhile this is misuse, I would expect Elasticsearch to handle such cases more gracefully. Also, the behaviour of \"size\" should be better documented.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/56553635","html_url":"https://github.com/elastic/elasticsearch/issues/5466#issuecomment-56553635","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5466","id":56553635,"node_id":"MDEyOklzc3VlQ29tbWVudDU2NTUzNjM1","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2014-09-23T17:02:21Z","updated_at":"2014-09-23T17:02:21Z","author_association":"CONTRIBUTOR","body":"Closing in favour of #4026\n","performed_via_github_app":null}]