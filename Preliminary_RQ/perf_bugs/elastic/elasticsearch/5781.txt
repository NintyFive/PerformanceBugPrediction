{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781/events","html_url":"https://github.com/elastic/elasticsearch/issues/5781","id":31341456,"node_id":"MDU6SXNzdWUzMTM0MTQ1Ng==","number":5781,"title":"Suboptimal performance when trying to get ids","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"assignees":[{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false}],"milestone":null,"comments":11,"created_at":"2014-04-11T16:27:38Z","updated_at":"2015-01-15T10:25:31Z","closed_at":"2015-01-15T10:25:31Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I have dense set of documents with ids from 1 to roughly 80 000 000. The whole db takes 70gb on disk.\n\nI'm trying to iterate items in database with the following query:\n\n``` json\n{\"size\":20000,\"fields\":[],\"query\":{\"filtered\":{\"filter\":{\"bool\":{\"must\":[{\"term\":{\"is_mobile\":true}},{\"range\":{\"user_id\":{\"gt\":0,\"lte\":20000},\"_cache\":false}}]}}}}}\n```\n\nHere range changes every requests by 20 000 forward. There are around 1 000 mobile users per 20 000 users if that matters.\n\nWhen I set size to 0 iteration finishes very quickly and each iteration takes 1-10ms,\nbut when I set size to 20 000 every step takes up to 200-300ms. The root cause of it\nis fetch stage: with size=0 disk io is ~10mbps, with size=20000 disk io is ~220mbps.\n\nHere are some screenshots from bigdesk during my tests:\n\n![one](http://puu.sh/84Br6.png)\n![two](http://puu.sh/84BrU.png)\n![three](http://puu.sh/84Bte.png)\n\nFirst test with size=0 has spike in query time, second test with size=20000 has spike in fetch time.\n\nI'm using elasticsearch 1.1.0 and my first idea was that sorting was to blame like in #5573, but restored node behaved the same. This node has 10g of heap, 2g of field data cache and 3g of filter cache, total ram is 16g.\n\nIs there a reason to fetch 220 megabytes from disk when the only thing I need from a document is an id? Looks like this instance was much faster a week ago, configuration and search/indexing didn't change.\n\n4.5 minutes \\* 60 seconds \\* 220mbps = 60 gigabytes read from disk during second test, but only 19 200 000 docs were fetched. How can I figure out what causes such load?\n\nLet me know if I could provide more info.\n","closed_by":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"performed_via_github_app":null}