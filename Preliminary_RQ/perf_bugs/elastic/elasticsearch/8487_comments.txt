[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/63299645","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-63299645","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":63299645,"node_id":"MDEyOklzc3VlQ29tbWVudDYzMjk5NjQ1","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-11-17T12:47:07Z","updated_at":"2014-11-17T12:47:07Z","author_association":"MEMBER","body":"> The 53 node cluster is our oldest cluster (with 144 indices & 3330 shards) and has under gone several upgrades using the offline method and in each scenario, the cluster returned to yellow within 15 minutes and green within 40 minutes. Monday night we upgraded this cluster and it took 3 hours to get to yellow and 6 hours to get to green.\n\nWhen you bring a node back up and it has replicas on disk, ES will sync the replicas with the current primaries. That can't take varying amount of time, depending on how different the local segments file are. We are working on making it faster.\n\n> It was taking > 10 minutes for the node to rejoin the cluster\n\nWith 1.4.0 we  considerably improved the joining process by using batching- it should be much faster. See #7493 \n\n> When I query _cat/recovery?pretty=true&v&active_only=true, no shards are listed however _cat/shards would show 1 or 2 shards as INITIALIZING. \n\nThis is worrying. Can you reproduce the issue? Are there any errors in the logs? I'm looking for something like ConcurrentModificationException (but it may be something else!).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65712958","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-65712958","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":65712958,"node_id":"MDEyOklzc3VlQ29tbWVudDY1NzEyOTU4","user":{"login":"portante","id":949097,"node_id":"MDQ6VXNlcjk0OTA5Nw==","avatar_url":"https://avatars2.githubusercontent.com/u/949097?v=4","gravatar_id":"","url":"https://api.github.com/users/portante","html_url":"https://github.com/portante","followers_url":"https://api.github.com/users/portante/followers","following_url":"https://api.github.com/users/portante/following{/other_user}","gists_url":"https://api.github.com/users/portante/gists{/gist_id}","starred_url":"https://api.github.com/users/portante/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/portante/subscriptions","organizations_url":"https://api.github.com/users/portante/orgs","repos_url":"https://api.github.com/users/portante/repos","events_url":"https://api.github.com/users/portante/events{/privacy}","received_events_url":"https://api.github.com/users/portante/received_events","type":"User","site_admin":false},"created_at":"2014-12-04T22:10:22Z","updated_at":"2014-12-05T02:53:19Z","author_association":"NONE","body":"I have a small one node ES instance (1.4.1, RHEL 7, openjdk 1.7.0), with\n256GB of memory, 4 sockets, 10 cores per socket (no hyper-threads), and\nwith 623 indexes (no replication, one shard per index), the cluster becomes\nunresponsive with Java running out of memory (spends all its time in\ngarbage collection) on ES startup.  We just upgraded from 1.3.2-1.\n\nI modified my startup sequence to immediately close all indexes after\nrestarting elasticsearch and then open them in sets of 10, waiting for the\ncluster health to go green.  That process ends up taking >10 min per set\nto open those indexes, but eventually dies mid-way through the indexes.\n\nWe then opened them one at a time, from the most recent to oldest, for\nonly about 60 indexes, to see if we can limp along until we find a fix for\nthis problem.  The indexes are all around 11-13 GB in size, about 150M\ndocuments.\n\nDoes this sound familiar to anyone?\n\nOn Mon, Nov 17, 2014 at 7:47 AM, Boaz Leskes notifications@github.com\nwrote:\n\n> The 53 node cluster is our oldest cluster (with 144 indices & 3330 shards)\n> and has under gone several upgrades using the offline method and in each\n> scenario, the cluster returned to yellow within 15 minutes and green within\n> 40 minutes. Monday night we upgraded this cluster and it took 3 hours to\n> get to yellow and 6 hours to get to green.\n> \n> When you bring a node back up and it has replicas on disk, ES will sync\n> the replicas with the current primaries. That can't take varying amount of\n> time, depending on how different the local segments file are. We are\n> working on making it faster.\n> \n> It was taking > 10 minutes for the node to rejoin the cluster\n> \n> With 1.4.0 we considerably improved the joining process by using batching-\n> it should be much faster. See #7493\n> https://github.com/elasticsearch/elasticsearch/pull/7493\n> \n> When I query _cat/recovery?pretty=true&v&active_only=true, no shards are\n> listed however _cat/shards would show 1 or 2 shards as INITIALIZING.\n> \n> This is worrying. Can you reproduce the issue? Are there any errors in the\n> logs? I'm looking for something like ConcurrentModificationException (but\n> it may be something else!).\n> \n> â€”\n> Reply to this email directly or view it on GitHub\n> https://github.com/elasticsearch/elasticsearch/issues/8487#issuecomment-63299645\n> .\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65859130","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-65859130","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":65859130,"node_id":"MDEyOklzc3VlQ29tbWVudDY1ODU5MTMw","user":{"login":"TrentStewart","id":9732342,"node_id":"MDQ6VXNlcjk3MzIzNDI=","avatar_url":"https://avatars0.githubusercontent.com/u/9732342?v=4","gravatar_id":"","url":"https://api.github.com/users/TrentStewart","html_url":"https://github.com/TrentStewart","followers_url":"https://api.github.com/users/TrentStewart/followers","following_url":"https://api.github.com/users/TrentStewart/following{/other_user}","gists_url":"https://api.github.com/users/TrentStewart/gists{/gist_id}","starred_url":"https://api.github.com/users/TrentStewart/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/TrentStewart/subscriptions","organizations_url":"https://api.github.com/users/TrentStewart/orgs","repos_url":"https://api.github.com/users/TrentStewart/repos","events_url":"https://api.github.com/users/TrentStewart/events{/privacy}","received_events_url":"https://api.github.com/users/TrentStewart/received_events","type":"User","site_admin":false},"created_at":"2014-12-05T21:42:29Z","updated_at":"2014-12-05T21:42:29Z","author_association":"NONE","body":"> > The 53 node cluster is our oldest cluster (with 144 indices & 3330 shards) and has under gone several upgrades using the offline method and in each scenario, the cluster returned to yellow within 15 minutes and green within 40 minutes. Monday night we upgraded this cluster and it took 3 hours to get to yellow and 6 hours to get to green.\n> \n> When you bring a node back up and it has replicas on disk, ES will sync the replicas with the current primaries. That can't take varying amount of time, depending on how different the local segments file are. We are working on making it faster.\n\nTrue, there appears to be a greater percentage of the recovery time is spent in the translog stage.\n\n> > It was taking > 10 minutes for the node to rejoin the cluster\n> \n> With 1.4.0 we considerably improved the joining process by using batching- it should be much faster. See #7493 \n> \n> > When I query _cat/recovery?pretty=true&v&active_only=true, no shards are listed however _cat/shards would show 1 or 2 shards as INITIALIZING. \n> \n> This is worrying. Can you reproduce the issue? Are there any errors in the logs? I'm looking for something like ConcurrentModificationException (but it may be something else!).\n\nYes, it's not 100% reproducible, but it does happen frequently.  There are no exceptions in the logs that I have discovered. \n\nSince I posted this, I done several rolling reboots and node failure recoveries of our clusters.  With 1.3.4 vs 1.3.2, a full recovery has gone from ~40 minutes to over 4 hours.  Prior to 1.3.4 I never paid any attention to pending_tasks, it's part of my problem triage process now, so I'm not able to do an apples to apples comparison. \n\nWhen _cat/recovery does not show anything, pending_tasks will grow to over 2000 tasks while its stuck on a single task, usually its shard-started that wasn't able to allocate to a node.  Some times I can cancel the allocation, most of the time the cancel would timeout as well and the cluster is left in a limbo waiting and waiting.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/65884332","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-65884332","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":65884332,"node_id":"MDEyOklzc3VlQ29tbWVudDY1ODg0MzMy","user":{"login":"portante","id":949097,"node_id":"MDQ6VXNlcjk0OTA5Nw==","avatar_url":"https://avatars2.githubusercontent.com/u/949097?v=4","gravatar_id":"","url":"https://api.github.com/users/portante","html_url":"https://github.com/portante","followers_url":"https://api.github.com/users/portante/followers","following_url":"https://api.github.com/users/portante/following{/other_user}","gists_url":"https://api.github.com/users/portante/gists{/gist_id}","starred_url":"https://api.github.com/users/portante/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/portante/subscriptions","organizations_url":"https://api.github.com/users/portante/orgs","repos_url":"https://api.github.com/users/portante/repos","events_url":"https://api.github.com/users/portante/events{/privacy}","received_events_url":"https://api.github.com/users/portante/received_events","type":"User","site_admin":false},"created_at":"2014-12-06T03:54:06Z","updated_at":"2014-12-06T03:54:06Z","author_association":"NONE","body":"https://github.com/elasticsearch/elasticsearch/issues/8394#issuecomment-65871792 addressed the problem I saw above.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/67304131","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-67304131","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":67304131,"node_id":"MDEyOklzc3VlQ29tbWVudDY3MzA0MTMx","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2014-12-17T10:32:06Z","updated_at":"2014-12-17T10:32:06Z","author_association":"MEMBER","body":"@TrentStewart sorry for not returning to you earlier\n\n> When _cat/recovery does not show anything, pending_tasks will grow to over 2000 tasks while its stuck on a single task, usually its shard-started that wasn't able to allocate to a node. \n\nWhen this happen again (master stuck on a single task), can you run the hot threads api on it? that will tell us what it's busy doing.\n\n> Some times I can cancel the allocation, most of the time the cancel would timeout as well and the cluster is left in a limbo waiting and waiting.\n\nCancel is in itself a task, albeit with highest priority. It needs to wait until the current task is done before having effect.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75573480","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-75573480","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":75573480,"node_id":"MDEyOklzc3VlQ29tbWVudDc1NTczNDgw","user":{"login":"julien51","id":17735,"node_id":"MDQ6VXNlcjE3NzM1","avatar_url":"https://avatars0.githubusercontent.com/u/17735?v=4","gravatar_id":"","url":"https://api.github.com/users/julien51","html_url":"https://github.com/julien51","followers_url":"https://api.github.com/users/julien51/followers","following_url":"https://api.github.com/users/julien51/following{/other_user}","gists_url":"https://api.github.com/users/julien51/gists{/gist_id}","starred_url":"https://api.github.com/users/julien51/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/julien51/subscriptions","organizations_url":"https://api.github.com/users/julien51/orgs","repos_url":"https://api.github.com/users/julien51/repos","events_url":"https://api.github.com/users/julien51/events{/privacy}","received_events_url":"https://api.github.com/users/julien51/received_events","type":"User","site_admin":false},"created_at":"2015-02-23T16:13:09Z","updated_at":"2015-02-23T16:15:42Z","author_association":"NONE","body":"We are seeing a similar issue where recovery seems to be stuck in the \"translog\" state.\nWhen checking `/<idx>/_recovery?pretty=true&active_only=true` we do find that translog recovery is in progress:\n\n```\n{\n      \"id\" : 2,\n      \"type\" : \"GATEWAY\",\n      \"stage\" : \"TRANSLOG\",\n      \"primary\" : true,\n      \"start_time_in_millis\" : 1424704351606,\n      \"stop_time_in_millis\" : 0,\n      \"total_time_in_millis\" : 3657431,\n      \"source\" : {\n        \"id\" : \"lv6TBuiTSoS0nz1qQ9a1Bg\",\n        \"host\" : \"elastic5.<host>.com.\",\n        \"transport_address\" : \"inet[/192.168.162.33:9300]\",\n        \"ip\" : \"104.237.137.221\",\n        \"name\" : \"elastic5.<host>.com\"\n      },\n      \"target\" : {\n        \"id\" : \"lv6TBuiTSoS0nz1qQ9a1Bg\",\n        \"host\" : \"elastic5.<host>.com.\",\n        \"transport_address\" : \"inet[/192.168.162.33:9300]\",\n        \"ip\" : \"104.237.137.221\",\n        \"name\" : \"elastic5.<host>.com\"\n      },\n      \"index\" : {\n        \"files\" : {\n          \"total\" : 647,\n          \"reused\" : 647,\n          \"recovered\" : 647,\n          \"percent\" : \"100.0%\"\n        },\n        \"bytes\" : {\n          \"total\" : 8834720289,\n          \"reused\" : 8834720289,\n          \"recovered\" : 8834720289,\n          \"percent\" : \"100.0%\"\n        },\n        \"total_time_in_millis\" : 48\n      },\n      \"translog\" : {\n        \"recovered\" : 36274,\n        \"total_time_in_millis\" : 0\n      },\n      \"start\" : {\n        \"check_index_time_in_millis\" : 0,\n        \"total_time_in_millis\" : 1021\n      }\n    }\n```\n\nBut it is extremely slow (the 0 value in translog.total_time_in_millis is a lie!)... and we have no ETA idea. What does `recovered` mean in the context of translog? Are these bytes?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75573974","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-75573974","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":75573974,"node_id":"MDEyOklzc3VlQ29tbWVudDc1NTczOTc0","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-23T16:15:42Z","updated_at":"2015-02-23T16:15:42Z","author_association":"MEMBER","body":"@julien51 those are translog operations . To find out how many there are in total and what the size of the translog is, you can run `GET /<idx>/_stats?level=shards&human` and check the primary shard. I'm working on a change that will supply those as part of the recovery output.  Which version are you on?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75574709","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-75574709","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":75574709,"node_id":"MDEyOklzc3VlQ29tbWVudDc1NTc0NzA5","user":{"login":"julien51","id":17735,"node_id":"MDQ6VXNlcjE3NzM1","avatar_url":"https://avatars0.githubusercontent.com/u/17735?v=4","gravatar_id":"","url":"https://api.github.com/users/julien51","html_url":"https://github.com/julien51","followers_url":"https://api.github.com/users/julien51/followers","following_url":"https://api.github.com/users/julien51/following{/other_user}","gists_url":"https://api.github.com/users/julien51/gists{/gist_id}","starred_url":"https://api.github.com/users/julien51/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/julien51/subscriptions","organizations_url":"https://api.github.com/users/julien51/orgs","repos_url":"https://api.github.com/users/julien51/repos","events_url":"https://api.github.com/users/julien51/events{/privacy}","received_events_url":"https://api.github.com/users/julien51/received_events","type":"User","site_admin":false},"created_at":"2015-02-23T16:19:20Z","updated_at":"2015-02-23T16:22:20Z","author_association":"NONE","body":"Version: 1.4.2\nThe problem: this is a primary shard initializing (after a recovery), so it looks like the number of operations is not available :(\nAlso, in about 5 minutes, we progressed by ~1500 operations... so slow!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75575911","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-75575911","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":75575911,"node_id":"MDEyOklzc3VlQ29tbWVudDc1NTc1OTEx","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-23T16:25:03Z","updated_at":"2015-02-23T16:25:03Z","author_association":"MEMBER","body":"@julien51 I see, this is your primary shard? In this case it's hard to tell indeed. You can check the file system for the size of a file called \"translog-????.recovering\" (again, working on improving this). \n\nWhen you shut down the cluster (did you?) did you have any relocations/recoveries going on? (if you know).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75576706","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-75576706","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":75576706,"node_id":"MDEyOklzc3VlQ29tbWVudDc1NTc2NzA2","user":{"login":"julien51","id":17735,"node_id":"MDQ6VXNlcjE3NzM1","avatar_url":"https://avatars0.githubusercontent.com/u/17735?v=4","gravatar_id":"","url":"https://api.github.com/users/julien51","html_url":"https://github.com/julien51","followers_url":"https://api.github.com/users/julien51/followers","following_url":"https://api.github.com/users/julien51/following{/other_user}","gists_url":"https://api.github.com/users/julien51/gists{/gist_id}","starred_url":"https://api.github.com/users/julien51/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/julien51/subscriptions","organizations_url":"https://api.github.com/users/julien51/orgs","repos_url":"https://api.github.com/users/julien51/repos","events_url":"https://api.github.com/users/julien51/events{/privacy}","received_events_url":"https://api.github.com/users/julien51/received_events","type":"User","site_admin":false},"created_at":"2015-02-23T16:29:00Z","updated_at":"2015-02-23T17:00:23Z","author_association":"NONE","body":"Unfortunaty, there was a bit of a disaster and the shutdown of the server was unexpected (still trying to understand what happend). We had 2 servers failing (out of 5) and of course they had the primary and secondary for one of our indices. But yes, based on the logs, I believe there was some relocation going on.\n\nThe file size of 1752285717 bytes. Any idea of how that roughly converts to number of operations? \nIs there one operation per line? The file has 511,014 lines... so if that's 1 line per operation, we're not even 10% of the way :( any idea how to make this _much_ faster? (because at this point it will take 16hours to recover a file of less than 2GB)\n\nI should add that it's similarly slow for the 2ndary shards which are also initializing and in the \"translog\" state.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75584877","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-75584877","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":75584877,"node_id":"MDEyOklzc3VlQ29tbWVudDc1NTg0ODc3","user":{"login":"julien51","id":17735,"node_id":"MDQ6VXNlcjE3NzM1","avatar_url":"https://avatars0.githubusercontent.com/u/17735?v=4","gravatar_id":"","url":"https://api.github.com/users/julien51","html_url":"https://github.com/julien51","followers_url":"https://api.github.com/users/julien51/followers","following_url":"https://api.github.com/users/julien51/following{/other_user}","gists_url":"https://api.github.com/users/julien51/gists{/gist_id}","starred_url":"https://api.github.com/users/julien51/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/julien51/subscriptions","organizations_url":"https://api.github.com/users/julien51/orgs","repos_url":"https://api.github.com/users/julien51/repos","events_url":"https://api.github.com/users/julien51/events{/privacy}","received_events_url":"https://api.github.com/users/julien51/received_events","type":"User","site_admin":false},"created_at":"2015-02-23T17:07:06Z","updated_at":"2015-02-23T17:07:06Z","author_association":"NONE","body":"Extra stupid question: this would in theory be an absolutely bad time to upgrade to 1.4.4, but I see 1.4.3 does bring improvements on the recovery front.  \nWhat if I prevent the cluster from doing any re-alloc and then upgrade the node on which this slow initilizating is going on?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75660646","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-75660646","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":75660646,"node_id":"MDEyOklzc3VlQ29tbWVudDc1NjYwNjQ2","user":{"login":"julien51","id":17735,"node_id":"MDQ6VXNlcjE3NzM1","avatar_url":"https://avatars0.githubusercontent.com/u/17735?v=4","gravatar_id":"","url":"https://api.github.com/users/julien51","html_url":"https://github.com/julien51","followers_url":"https://api.github.com/users/julien51/followers","following_url":"https://api.github.com/users/julien51/following{/other_user}","gists_url":"https://api.github.com/users/julien51/gists{/gist_id}","starred_url":"https://api.github.com/users/julien51/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/julien51/subscriptions","organizations_url":"https://api.github.com/users/julien51/orgs","repos_url":"https://api.github.com/users/julien51/repos","events_url":"https://api.github.com/users/julien51/events{/privacy}","received_events_url":"https://api.github.com/users/julien51/received_events","type":"User","site_admin":false},"created_at":"2015-02-23T23:25:05Z","updated_at":"2015-02-23T23:25:05Z","author_association":"NONE","body":"After 6 hours, it was still processing the translog, but at an excruciatingly slow pace... so I restarted the node, and well, it finished in a matter of seconds on another host. We seem to have the same number of documents. \nYet, we'll start re-indexing from our main datastore, just in case we miss anything. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75755052","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-75755052","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":75755052,"node_id":"MDEyOklzc3VlQ29tbWVudDc1NzU1MDUy","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-24T13:23:35Z","updated_at":"2015-02-24T13:23:35Z","author_association":"MEMBER","body":"> The file size of 1752285717 bytes. Any idea of how that roughly converts to number of operations? \n> Is there one operation per line? The file has 511,014 lines... so if that's 1 line per operation, \n\nThe translog is binary. New lines are just there by accident (if viewed as text)\n\n>  this would in theory be an absolutely bad time to upgrade to 1.4.4, but I see 1.4.3 does bring improvements on the recovery front.\n\n1.4.3 helps by being more aggressive in trimming the translog post recovery but we're still chasing this issue. It is very rare but do occur (as you sadly noticed).\n\n> I restarted the node, and well, it finished in a matter of seconds on another host. \n> We seem to have the same number of documents. \n\nYes. The translog on replica is being flushed during recovery. It's only the primary than can grow, because we need it a safety measure to catch all the documents indexed between starting to copy lucene files and starting the translog phase. Depending on what exactly happened, there might be none but it is not guaranteed. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/158682873","html_url":"https://github.com/elastic/elasticsearch/issues/8487#issuecomment-158682873","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/8487","id":158682873,"node_id":"MDEyOklzc3VlQ29tbWVudDE1ODY4Mjg3Mw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-11-21T21:18:52Z","updated_at":"2015-11-21T21:18:52Z","author_association":"CONTRIBUTOR","body":"The never-ending-translog bug was fixed several versions ago.  I'm going to close this issue.\n","performed_via_github_app":null}]