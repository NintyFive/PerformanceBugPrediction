{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/24339","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24339/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24339/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/24339/events","html_url":"https://github.com/elastic/elasticsearch/issues/24339","id":224548546,"node_id":"MDU6SXNzdWUyMjQ1NDg1NDY=","number":24339,"title":"Primary shards shouldn't be stuck in excluded allocation nodes during rolling patch upgrade","user":{"login":"f-w","id":12823994,"node_id":"MDQ6VXNlcjEyODIzOTk0","avatar_url":"https://avatars0.githubusercontent.com/u/12823994?v=4","gravatar_id":"","url":"https://api.github.com/users/f-w","html_url":"https://github.com/f-w","followers_url":"https://api.github.com/users/f-w/followers","following_url":"https://api.github.com/users/f-w/following{/other_user}","gists_url":"https://api.github.com/users/f-w/gists{/gist_id}","starred_url":"https://api.github.com/users/f-w/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/f-w/subscriptions","organizations_url":"https://api.github.com/users/f-w/orgs","repos_url":"https://api.github.com/users/f-w/repos","events_url":"https://api.github.com/users/f-w/events{/privacy}","received_events_url":"https://api.github.com/users/f-w/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2017-04-26T17:49:34Z","updated_at":"2017-04-26T18:06:01Z","closed_at":"2017-04-26T18:06:01Z","author_association":"NONE","active_lock_reason":null,"body":"I have ES (based on official docker image) running in OpenShift/Kubernetes that supports rolling deployement. During a deployment which also upgrades ES from 2.4.3 to 2.4.4, if I cancel in the middle and try to roll back gracefully by putting the new version nodes onto excluded shard allocation filter, I found to my surprise all primary shards are stuck in the new nodes, not honoring the filter. The cause is found to be node version difference using `_cluster/reroute` api.\r\n\r\nI understand data schema/structure depends on version. But according to semver, a patch difference should be backwards-compatible and contains only bug fixes. It's a common practice that incompatible data schema change calls for a major version bump up so I don't expect to see this happening, especial during a production deployment.\r\n\r\nAt a minimum this behavior should be documented on page [Shard Allocation Filtering](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/allocation-filtering.html).","closed_by":{"login":"jasontedor","id":4744941,"node_id":"MDQ6VXNlcjQ3NDQ5NDE=","avatar_url":"https://avatars3.githubusercontent.com/u/4744941?v=4","gravatar_id":"","url":"https://api.github.com/users/jasontedor","html_url":"https://github.com/jasontedor","followers_url":"https://api.github.com/users/jasontedor/followers","following_url":"https://api.github.com/users/jasontedor/following{/other_user}","gists_url":"https://api.github.com/users/jasontedor/gists{/gist_id}","starred_url":"https://api.github.com/users/jasontedor/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasontedor/subscriptions","organizations_url":"https://api.github.com/users/jasontedor/orgs","repos_url":"https://api.github.com/users/jasontedor/repos","events_url":"https://api.github.com/users/jasontedor/events{/privacy}","received_events_url":"https://api.github.com/users/jasontedor/received_events","type":"User","site_admin":false},"performed_via_github_app":null}