[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/380640656","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-380640656","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":380640656,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MDY0MDY1Ng==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2018-04-12T00:55:46Z","updated_at":"2018-04-12T00:55:46Z","author_association":"MEMBER","body":"@danopia Would you mind sharing the shard-level stats with us? This can be retrieved via `GET /_stats?level=shards`. Thank you!","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/380640673","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-380640673","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":380640673,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MDY0MDY3Mw==","user":{"login":"elasticmachine","id":15837671,"node_id":"MDQ6VXNlcjE1ODM3Njcx","avatar_url":"https://avatars3.githubusercontent.com/u/15837671?v=4","gravatar_id":"","url":"https://api.github.com/users/elasticmachine","html_url":"https://github.com/elasticmachine","followers_url":"https://api.github.com/users/elasticmachine/followers","following_url":"https://api.github.com/users/elasticmachine/following{/other_user}","gists_url":"https://api.github.com/users/elasticmachine/gists{/gist_id}","starred_url":"https://api.github.com/users/elasticmachine/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elasticmachine/subscriptions","organizations_url":"https://api.github.com/users/elasticmachine/orgs","repos_url":"https://api.github.com/users/elasticmachine/repos","events_url":"https://api.github.com/users/elasticmachine/events{/privacy}","received_events_url":"https://api.github.com/users/elasticmachine/received_events","type":"User","site_admin":false},"created_at":"2018-04-12T00:55:55Z","updated_at":"2018-04-12T00:55:55Z","author_association":"COLLABORATOR","body":"Pinging @elastic/es-distributed","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/380644255","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-380644255","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":380644255,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MDY0NDI1NQ==","user":{"login":"danopia","id":40628,"node_id":"MDQ6VXNlcjQwNjI4","avatar_url":"https://avatars2.githubusercontent.com/u/40628?v=4","gravatar_id":"","url":"https://api.github.com/users/danopia","html_url":"https://github.com/danopia","followers_url":"https://api.github.com/users/danopia/followers","following_url":"https://api.github.com/users/danopia/following{/other_user}","gists_url":"https://api.github.com/users/danopia/gists{/gist_id}","starred_url":"https://api.github.com/users/danopia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danopia/subscriptions","organizations_url":"https://api.github.com/users/danopia/orgs","repos_url":"https://api.github.com/users/danopia/repos","events_url":"https://api.github.com/users/danopia/events{/privacy}","received_events_url":"https://api.github.com/users/danopia/received_events","type":"User","site_admin":false},"created_at":"2018-04-12T01:19:25Z","updated_at":"2018-04-12T01:19:25Z","author_association":"NONE","body":"Sure. I have a hundred indexes so hopefully just stats from just one impacted index will be useful.\r\n\r\nIndex: `yux-2018.04.11d`\r\n\r\nIt's been an hour since I wrote into this index, and repeated synced-flush attempts say things like \"pending operations\" and \"commit has changed\" for many shards. Most recent synced-flush output: https://gist.github.com/danopia/79cf1394b9806cf374c6ea16c90f9623\r\n\r\nShard-level stats for that index: https://gist.github.com/danopia/af7b5df82b3d939e6e99df87cabb8b01\r\n\r\nThanks for the fast response :)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/380749648","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-380749648","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":380749648,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MDc0OTY0OA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-04-12T10:07:26Z","updated_at":"2018-04-12T10:07:26Z","author_association":"MEMBER","body":"Thanks @danopia \r\n\r\nI want to double check a few things:\r\n\r\n1. When you have an outage, I presume you continue indexing. Correct? How fast do you index? i.e., roughly how many bytes per second?\r\n2. Why did you set `flush_threshold_size` to 5gb? \r\n3. I noticed that the index has 14685147 documents but the maximum sequence number is 15253725. \r\n3.1 are you using auto generated ids or supplying your own?\r\n3.2 once indexed, do you update or delete documents? if so, using which API (delete/update by query?)\r\n3.3 do use versioned operations?\r\n\r\n> The worst part about the stuck translogs is even after I've moved on from indexing into those indexes, they're still in the cluster and take AGES to recover. I'm talking multiple hours per shard recovery because the entire translog must be read in order to recover from existing store, and then read AGAIN to recover a replica.\r\n\r\nCan you clarify a bit more about what you mean? Did you stop indexing while replicas were still recovering or were you trying to move the indices to another node?\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/380898500","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-380898500","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":380898500,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MDg5ODUwMA==","user":{"login":"danopia","id":40628,"node_id":"MDQ6VXNlcjQwNjI4","avatar_url":"https://avatars2.githubusercontent.com/u/40628?v=4","gravatar_id":"","url":"https://api.github.com/users/danopia","html_url":"https://github.com/danopia","followers_url":"https://api.github.com/users/danopia/followers","following_url":"https://api.github.com/users/danopia/following{/other_user}","gists_url":"https://api.github.com/users/danopia/gists{/gist_id}","starred_url":"https://api.github.com/users/danopia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danopia/subscriptions","organizations_url":"https://api.github.com/users/danopia/orgs","repos_url":"https://api.github.com/users/danopia/repos","events_url":"https://api.github.com/users/danopia/events{/privacy}","received_events_url":"https://api.github.com/users/danopia/received_events","type":"User","site_admin":false},"created_at":"2018-04-12T18:18:58Z","updated_at":"2018-04-12T18:18:58Z","author_association":"NONE","body":"1. The indexers don't automatically stop, no. I have replication over two racks with restricted allocation, so one node's failure effectively means some shards no longer have a replica, but everything keeps working. Otherwise the indexers would see indexing errors. The cluster I've provided stats from takes 1-2 million documents per minute, most of that going into one index at a time. One day of data is ~2TB in ES, including replicas.\r\n    1. I already knew that heavy indexing into a recovering shard is bad. My strategy is creating a new index with healthy allocations and switching indexing over to that. Because of the issue I'm reporting here, though, I don't have a way to clean up the nasty indexes later.\r\n2. I set `flush_threshold_size` on my previous ES 2.2 clusters and copied over the index templates ðŸ˜’  Since I've had these issues on the new ES version, I've been stepping down to 1gb, but these indexes were created using the 5gb figure. The goal was to let the translog go longer without being flushed, because I'm on spinning disks (in RAID 0).\r\n3. Good catch, I am supplying my own IDs. What you're seeing is unrelated data duplication (reprocessing, retries, etc) being deduplicated by the Elasticsearch indexing. this is actually a nice feature for us because even though data in ES is perfectly deduplicated, the _version attribute lets us check if a certain event _was_ duplicated originally.\r\n    1. The IDs are generated upstream. They are prefixed with the document's ISO-8601 timestamp, followed by a hash. Most timestamps are from the past 5 minutes but otherwise not very related.\r\n    2. Yea, but only by including the same document (or slightly different document) in another bulk-index API. I never call delete/update APIs.\r\n    3. Versioned operations don't sound familiar. My only regular load is _bulk calls, and Kibana usage + _search.\r\n\r\n---\r\n\r\nSure. When I see translogs not being cleared, I manually stop all indexing until I see fully-allocated hot indexes. Usually I just make new empty indexes, wait for them to allocate, then start indexing with the new indexes. If the new index translogs aren't stuck, this is all good.\r\n\r\n(One time, I made a new index, _then_ performed a full cluster restart to fix a systemd config, and none of the translogs on that new index worked once I started putting data in)\r\n\r\nAnyway, a pain point here is that any _future_ recoveries that involve a broken shard can take hours to complete. At worst, I had a full index of translogs with 67 million entries each, and each recovery took upwards of 10 hours. ES tried to rebalance and used up all the file descriptors, crashing multiple nodes. Then the primary shards had to read the entire translog in order to recover from existing store, so my cluster was red for that entire time. Replicas also took just as long to do a peer recovery. Here's a screenshot of those recoveries:\r\n\r\n<img width=\"984\" alt=\"screen shot 2018-03-24 at 1 31 19 am-crop\" src=\"https://user-images.githubusercontent.com/40628/38695906-7cd18d64-3e42-11e8-975c-84f1a904f691.png\">\r\n\r\nBasically, once I have these broken indexes, any sort of node restart (config changes) or node failure starts the whole recovery ordeal over again.\r\n\r\nGoing by https://github.com/elastic/elasticsearch/pull/28205 it sounds like Elasticsearch doesn't have the capability of seeking into the translog, so I assume it was reading all of those entries just looking for the end.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/380901665","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-380901665","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":380901665,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MDkwMTY2NQ==","user":{"login":"danopia","id":40628,"node_id":"MDQ6VXNlcjQwNjI4","avatar_url":"https://avatars2.githubusercontent.com/u/40628?v=4","gravatar_id":"","url":"https://api.github.com/users/danopia","html_url":"https://github.com/danopia","followers_url":"https://api.github.com/users/danopia/followers","following_url":"https://api.github.com/users/danopia/following{/other_user}","gists_url":"https://api.github.com/users/danopia/gists{/gist_id}","starred_url":"https://api.github.com/users/danopia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danopia/subscriptions","organizations_url":"https://api.github.com/users/danopia/orgs","repos_url":"https://api.github.com/users/danopia/repos","events_url":"https://api.github.com/users/danopia/events{/privacy}","received_events_url":"https://api.github.com/users/danopia/received_events","type":"User","site_admin":false},"created_at":"2018-04-12T18:29:25Z","updated_at":"2018-04-12T18:29:54Z","author_association":"NONE","body":"Maybe this is related? Doesn't read like the same trigger, but I'm seeing the same behavior with # of open files, around the same time. https://github.com/elastic/elasticsearch/issues/29097","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/381049857","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-381049857","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":381049857,"node_id":"MDEyOklzc3VlQ29tbWVudDM4MTA0OTg1Nw==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2018-04-13T07:31:41Z","updated_at":"2018-04-13T07:31:41Z","author_association":"MEMBER","body":"Thanks @danopia . Let's first try to reduce the number of moving parts.\r\n\r\n> The goal was to let the translog go longer without being flushed, because I'm on spinning disks (in RAID 0).\r\n\r\nI'm not sure whether this achieves what you want - every operation written to the translog is fsynced at the end of each bulk. The flush parameters controls how often a lucene commit point is made and not how much documents are held in memory before the translog is written to disk. \r\n\r\nSince recovery first copies over lucene files, it uses the commit point. If the commit is very old (i.e., 5gb behind) you need to re-index a lot of data and recovery will be slow.  Another side effect is that while a recovery is ongoing we can't trim the translog. So if takes a long time and you index really quickly, the translog can get very large.\r\n\r\nI suggest you go back to using defaults and see what effect it has.\r\n\r\n> It's been an hour since I wrote into this index, and repeated synced-flush attempts say things like \"pending operations\" and \"commit has changed\" for many shards.\r\n\r\nIt sounds like you were doing this while the indexers were running. If so it's expected. Synced flush only works when no indexing is going on.\r\n\r\n> Going by #28205 it sounds like Elasticsearch doesn't have the capability of seeking into the translog, so I assume it was reading all of those entries just looking for the end.\r\n\r\nWe can random access into the translog with a granularity of 64MB. That's good enough for recovery, not good enough for other stuff (hence the issue you linked to). \r\n\r\n> Maybe this is related? Doesn't read like the same trigger, but I'm seeing the same behavior with # of open files, around the same time. #29097\r\n\r\nYou symptoms are similar because you translog is huge. I think the cause is different. \r\n\r\nCan you please reduce the translog flush size to default (set it to null, it's a dynamic setting) and see how things go? I don't expect them to be solved but I think it will be easier to figure out.\r\n\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/384492772","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-384492772","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":384492772,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NDQ5Mjc3Mg==","user":{"login":"danopia","id":40628,"node_id":"MDQ6VXNlcjQwNjI4","avatar_url":"https://avatars2.githubusercontent.com/u/40628?v=4","gravatar_id":"","url":"https://api.github.com/users/danopia","html_url":"https://github.com/danopia","followers_url":"https://api.github.com/users/danopia/followers","following_url":"https://api.github.com/users/danopia/following{/other_user}","gists_url":"https://api.github.com/users/danopia/gists{/gist_id}","starred_url":"https://api.github.com/users/danopia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danopia/subscriptions","organizations_url":"https://api.github.com/users/danopia/orgs","repos_url":"https://api.github.com/users/danopia/repos","events_url":"https://api.github.com/users/danopia/events{/privacy}","received_events_url":"https://api.github.com/users/danopia/received_events","type":"User","site_admin":false},"created_at":"2018-04-26T02:27:15Z","updated_at":"2018-04-26T03:29:15Z","author_association":"NONE","body":"Sorry for the radio silence. I removed the translog size override from all my clusters, thanks for the heads-up. The metrics are smoother now.\r\n\r\nAfter my last comment I upgraded the aforementioned cluster to a 6.2.4-SNAPSHOT [(using this tree)](https://github.com/elastic/elasticsearch/tree/2091fc3b9ee6841c73b5ed28b48671dc5c6a8152) based on a belief that #29097 was at play. The upgrade didn't fix the translogs (as you expected), but does fix the out-of-files crashes. The number-of-files metric has been beautiful ever since. No.FDs is what was actually crashing nodes and destabilizing clusters. This means I can take my time with the translog investigation.\r\n\r\n(The fun part was losing visibility while 6.2.4 was fixing the number-of-files. The node's stats were completely unresponsive for the hour or so that ES needed to clean up all the files, for each cleanup that happened.)\r\n\r\n---\r\n\r\nAnyway, after the upgrade was completed: Four translogs (2 shards out of 9, +replicas) got stuck on the same day without a known cause. They just stuck in-sync. ~12 hours later at 17:00PT, indexes rolled over and the new indexes cleanly took the load.\r\n\r\n![screen shot 2018-04-25 at 6 16 38 pm](https://user-images.githubusercontent.com/40628/39280942-faf8fdca-48b6-11e8-8e57-682fda04fe83.png)\r\n\r\n* Index settings: https://gist.github.com/danopia/b7020dbd6832c89d9da21273ff43ec28\r\n* Shard-level index stats: https://gist.github.com/danopia/d35fd1b1e9f6c0acb77dd078021abf1b\r\n* This shard hasn't been written in at least 5 days.\r\n* Flush is no longer complaining (presumably due to the upgrade). It says every shard successfully flushed, but as before, the stuck translogs don't budge.\r\n\r\n```json\r\n\"POST /dice.prod-2018.04.17/_flush/synced\"\r\n{\r\n  \"_shards\": {\r\n    \"total\": 18,\r\n    \"successful\": 18,\r\n    \"failed\": 0\r\n  },\r\n  \"dice.prod-2018.04.17\": {\r\n    \"total\": 18,\r\n    \"successful\": 18,\r\n    \"failed\": 0\r\n  }\r\n}\r\n```\r\n\r\nA week later, I purposely caused other translogs to inflate, by starting recovery on a hot index. Once the index was rolled over, the recoveries completed and the translogs were properly trimmed in a reasonable timeframe. But the stuck translogs are still there. In the same cluster!\r\n\r\n![screen shot 2018-04-25 at 7 14 07 pm](https://user-images.githubusercontent.com/40628/39282113-dbebf634-48bc-11e8-981b-8b14d99ea6eb.png)\r\n\r\nHopefully running a SNAPSHOT doesn't invalidate all this information. But if it does, I have another cluster running real 6.2.4 and with multiple instances of the stuck-translog symptom. I'm waiting to see if the cluster going green helps that one.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/386012110","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-386012110","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":386012110,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NjAxMjExMA==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2018-05-02T15:10:26Z","updated_at":"2018-05-02T15:10:26Z","author_association":"MEMBER","body":"@danopia It would be helpful if you can provide this information.\r\n\r\n- Did you use dynamic mapping updates?\r\n- Did you use bulk index API? If so, was there any failure in bulk responses?\r\n\r\nThank you!","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387137856","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-387137856","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":387137856,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzEzNzg1Ng==","user":{"login":"danopia","id":40628,"node_id":"MDQ6VXNlcjQwNjI4","avatar_url":"https://avatars2.githubusercontent.com/u/40628?v=4","gravatar_id":"","url":"https://api.github.com/users/danopia","html_url":"https://github.com/danopia","followers_url":"https://api.github.com/users/danopia/followers","following_url":"https://api.github.com/users/danopia/following{/other_user}","gists_url":"https://api.github.com/users/danopia/gists{/gist_id}","starred_url":"https://api.github.com/users/danopia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danopia/subscriptions","organizations_url":"https://api.github.com/users/danopia/orgs","repos_url":"https://api.github.com/users/danopia/repos","events_url":"https://api.github.com/users/danopia/events{/privacy}","received_events_url":"https://api.github.com/users/danopia/received_events","type":"User","site_admin":false},"created_at":"2018-05-07T17:20:12Z","updated_at":"2018-05-07T17:20:12Z","author_association":"NONE","body":"* Yes, I have a common template across every index.\r\n  * https://gist.github.com/danopia/737731a488f9103df3fb25cd6c6f38cf\r\n  * We only add string-IDs, numbers, and bools dynamically so the mapping disables some auto-detection.\r\n* Yes, everything is bulked, currently set to a 20MB max size.\r\n* Looking at a few recent cases, there's definitely a couple hundred documents that were rejected for some reason or another right around the index rollover times. The # of rejects seems related to how many shards get stuck. I let indexes get created an hour early and misaligned data seeds them a little, but it must not be enough to have the mapping filled out.\r\n\r\n![screen shot 2018-05-07 at 10 18 58 am](https://user-images.githubusercontent.com/40628/39714823-18af09ae-51e0-11e8-95f8-7f498d17e9bd.png)\r\n\r\nDo you have a possible cause for bulk errors causing translog poison pills of some kind? It sounds like a lead..","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387232260","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-387232260","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":387232260,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzIzMjI2MA==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2018-05-07T22:59:36Z","updated_at":"2018-05-07T22:59:36Z","author_association":"MEMBER","body":"> Looking at a few recent cases, there's definitely a couple hundred documents that were rejected for some reason or another right around the index rollover times. The # of rejects seems related to how many shards get stuck.\r\n\r\n@danopia Thanks again. I think we have figured out the source of the issue and fixed it in https://github.com/elastic/elasticsearch/pull/30244. @bleskes WDYT?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387236261","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-387236261","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":387236261,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzIzNjI2MQ==","user":{"login":"danopia","id":40628,"node_id":"MDQ6VXNlcjQwNjI4","avatar_url":"https://avatars2.githubusercontent.com/u/40628?v=4","gravatar_id":"","url":"https://api.github.com/users/danopia","html_url":"https://github.com/danopia","followers_url":"https://api.github.com/users/danopia/followers","following_url":"https://api.github.com/users/danopia/following{/other_user}","gists_url":"https://api.github.com/users/danopia/gists{/gist_id}","starred_url":"https://api.github.com/users/danopia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danopia/subscriptions","organizations_url":"https://api.github.com/users/danopia/orgs","repos_url":"https://api.github.com/users/danopia/repos","events_url":"https://api.github.com/users/danopia/events{/privacy}","received_events_url":"https://api.github.com/users/danopia/received_events","type":"User","site_admin":false},"created_at":"2018-05-07T23:22:29Z","updated_at":"2018-05-07T23:26:17Z","author_association":"NONE","body":"Nice, good to see there's a known thing that correlates :)\r\n\r\nI've consulted the logging cluster that my clusters log to, and definitely see this from the time period of the above graphs:\r\n\r\n`org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException: failed to process cluster event (put-mapping) within 30s`\r\n\r\nOther unaffected days don't have that log.\r\n\r\nSounds like the fix in 6.3.0 won't fix old data, so I'm not looking forward to reprocessing all the tranlogs I've accumulated ðŸ˜› \r\n\r\n[edit] Just noticed a referenced issue mentioning an open/close fixing the indexes, so I'll see what that does for me","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387241200","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-387241200","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":387241200,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzI0MTIwMA==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2018-05-07T23:52:29Z","updated_at":"2018-05-07T23:52:29Z","author_association":"MEMBER","body":"> Sounds like the fix in 6.3.0 won't fix old data, so I'm not looking forward to reprocessing all the tranlogs I've accumulated\r\n\r\nYou can fix the affected indices by rebuilding its replicas. This can be done by changing the `number_of_replicas` to 0 then restore to the original value.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/387256470","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-387256470","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":387256470,"node_id":"MDEyOklzc3VlQ29tbWVudDM4NzI1NjQ3MA==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2018-05-08T01:31:42Z","updated_at":"2018-05-08T01:31:42Z","author_association":"MEMBER","body":"> failed to process cluster event (put-mapping) within 30s\r\n\r\n@danopia This is really fixed by https://github.com/elastic/elasticsearch/pull/30244. I hope you don't mind if I close this but feel free to reopen. We really appreciate your cooperation here.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/457471517","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-457471517","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":457471517,"node_id":"MDEyOklzc3VlQ29tbWVudDQ1NzQ3MTUxNw==","user":{"login":"malpani","id":368133,"node_id":"MDQ6VXNlcjM2ODEzMw==","avatar_url":"https://avatars1.githubusercontent.com/u/368133?v=4","gravatar_id":"","url":"https://api.github.com/users/malpani","html_url":"https://github.com/malpani","followers_url":"https://api.github.com/users/malpani/followers","following_url":"https://api.github.com/users/malpani/following{/other_user}","gists_url":"https://api.github.com/users/malpani/gists{/gist_id}","starred_url":"https://api.github.com/users/malpani/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/malpani/subscriptions","organizations_url":"https://api.github.com/users/malpani/orgs","repos_url":"https://api.github.com/users/malpani/repos","events_url":"https://api.github.com/users/malpani/events{/privacy}","received_events_url":"https://api.github.com/users/malpani/received_events","type":"User","site_admin":false},"created_at":"2019-01-25T06:36:24Z","updated_at":"2019-01-25T06:36:24Z","author_association":"CONTRIBUTOR","body":"@dnhatn @bleskes @danopia - I understand this is now fixed 6.3+ but had a quick question on the impact of diverging local checkpoints. \r\n\r\nWhen i hit this scenario on 6.2, translogs growing beyond `index.translog.retention.size` and not being truncated even beyond the retention periods set by `index.translog.retention.age` . \r\n\r\ntldr; Are the translog retention settings ignored if the local checkpoints diverge? Ideally it should not right? But maybe the primary needs to keep all sequence-ids around since last `global_checkpoint` which will never come into sync because of this bug ? \r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/457484661","html_url":"https://github.com/elastic/elasticsearch/issues/29488#issuecomment-457484661","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29488","id":457484661,"node_id":"MDEyOklzc3VlQ29tbWVudDQ1NzQ4NDY2MQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2019-01-25T07:46:07Z","updated_at":"2019-01-25T07:46:07Z","author_association":"MEMBER","body":"@malpani the translog is used for two things - the first is to reindexing all operations into lucene in the case of a restart. For this you need all operations above the global checkpoint (which is the minimum of the local checkpoints). The second things is to bring replicas that were offline up to speed using operation recoveries (they come from the translog in the 6.x polices). The first usage is required and we always keep the required portion of the translog for it (this is what you're seeing). The second usage is optional as we can always copy files. The retention setting controls how much of the translog should be kept around for the second usage alone. I hope this helps. If you have more questions please feel free to ask on [our discuss forums](http://discuss.elastic.co). We try to keep GitHub for feature requests and bugs.","performed_via_github_app":null}]