[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75824676","html_url":"https://github.com/elastic/elasticsearch/issues/9853#issuecomment-75824676","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9853","id":75824676,"node_id":"MDEyOklzc3VlQ29tbWVudDc1ODI0Njc2","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2015-02-24T19:15:28Z","updated_at":"2015-02-24T19:15:28Z","author_association":"MEMBER","body":"Okay, I spent some time looking into this, here is my theory - \n\nEvery time an index's settings are updated, Elasticsearch first applies the new\nsettings to the index, and attempts to re-route the cluster (in case settings\nlike `number_of_replicas` have changed). See this line in\n[MetaDataUpdateSettingsService](https://github.com/elasticsearch/elasticsearch/blob/1816951b6b0320e7a011436c7c7519ec2bfabc6e/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java#L313-L315)\n\nSo here's what is happening, the first index is having the\n`index.routing.allocation.require.class: ssd` setting added to it, so the\nsetting is added, and then the master nodes reroutes all of the shards. This\nmeans the `BalancedShardsAllocator` will run through shards checking if they can\nremain, in your case none of the nodes have the `node.class: ssd` setting, so\nthe shard for the index can not remain on the current node. The allocator\nattempts to move it to any other node, which it can't, because none of the nodes\nhave that tag yet. Eventually, it fails because there is nowhere to move the\nshards for this index and it gives up.\n\nThe cluster state in incremented and sent out to all nodes.\n\nNow, the second index has the `index.routing.allocation.require.class: ssd`\nsetting applied. Again, the master node adds the setting and attempts another\nreroute. This time the balanced shards allocator tries to move the second index\naway, and it also tries to move all the shards for the first index away. Again,\nthere are no nodes with the \"ssd\" tag, so it eventually gives up.\n\nThe cluster state is incremented and sent out to all nodes.\n\nThis repeats for every index that you have, so you end up with:\n\n`n * (n + 1) / 2` total moving-an-index-away attempts.\n\nFor 1000 indices, this means 500500 attempted rebalances for a particular index,\nassuming you are applying the setting incrementally.\n\nFortunately, there is a way to make this better -\n\nCurrently, Curator issues requests on an index-by-index basis after resolving\nthe indices that need to have settings changed, however, after every request, a\nnew reroute is performed and a new cluster state issued. To prevent this, we can\nchange:\n\n```\ncurl -XPUT 'localhost:9200/index1/_settings' -d'{...}'\ncurl -XPUT 'localhost:9200/index2/_settings' -d'{...}'\ncurl -XPUT 'localhost:9200/index3/_settings' -d'{...}'\ncurl -XPUT 'localhost:9200/index4/_settings' -d'{...}'\ncurl -XPUT 'localhost:9200/index5/_settings' -d'{...}'\n```\n\nInto a single call:\n\n```\ncurl -XPUT 'localhost:9200/index1,index2,index3,index4,index5/_settings' -d'{...}'\n\n... or ...\n\ncurl -XPUT 'localhost:9200/index*/_settings' -d'{...}'\n```\n\nBy doing it in a single request, only a single reroute is attempted after\napplying the settings to all the indices.\n\nI've spoken with @untergeek about making this change in Curator, and I think we\nboth agreed that it should be added. Until it is, as a workaround, you can\nmanually apply the setting to multiple indices at once, which should prevent so\nmuch churn in the cluster with regard to rerouting and new cluster states.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75843285","html_url":"https://github.com/elastic/elasticsearch/issues/9853#issuecomment-75843285","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9853","id":75843285,"node_id":"MDEyOklzc3VlQ29tbWVudDc1ODQzMjg1","user":{"login":"petecheslock","id":511733,"node_id":"MDQ6VXNlcjUxMTczMw==","avatar_url":"https://avatars2.githubusercontent.com/u/511733?v=4","gravatar_id":"","url":"https://api.github.com/users/petecheslock","html_url":"https://github.com/petecheslock","followers_url":"https://api.github.com/users/petecheslock/followers","following_url":"https://api.github.com/users/petecheslock/following{/other_user}","gists_url":"https://api.github.com/users/petecheslock/gists{/gist_id}","starred_url":"https://api.github.com/users/petecheslock/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/petecheslock/subscriptions","organizations_url":"https://api.github.com/users/petecheslock/orgs","repos_url":"https://api.github.com/users/petecheslock/repos","events_url":"https://api.github.com/users/petecheslock/events{/privacy}","received_events_url":"https://api.github.com/users/petecheslock/received_events","type":"User","site_admin":false},"created_at":"2015-02-24T20:52:39Z","updated_at":"2015-02-24T20:52:39Z","author_association":"NONE","body":"I think you're on the right track. For another datapoint - after restarting the cluster to pick up the new node class tags set.  We ran (running) curator again on indices older than x days and seeing a similar things happen even with there being nodes in the cluster with the proper class assigned.   Maybe just due to the sheer number of indices and nodes?  We don't see it on a dev cluster of 100 indices and like 6 systems (yet).  But the workaround can work for me in the meantime.   \n\nThanks!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/75845207","html_url":"https://github.com/elastic/elasticsearch/issues/9853#issuecomment-75845207","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9853","id":75845207,"node_id":"MDEyOklzc3VlQ29tbWVudDc1ODQ1MjA3","user":{"login":"petecheslock","id":511733,"node_id":"MDQ6VXNlcjUxMTczMw==","avatar_url":"https://avatars2.githubusercontent.com/u/511733?v=4","gravatar_id":"","url":"https://api.github.com/users/petecheslock","html_url":"https://github.com/petecheslock","followers_url":"https://api.github.com/users/petecheslock/followers","following_url":"https://api.github.com/users/petecheslock/following{/other_user}","gists_url":"https://api.github.com/users/petecheslock/gists{/gist_id}","starred_url":"https://api.github.com/users/petecheslock/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/petecheslock/subscriptions","organizations_url":"https://api.github.com/users/petecheslock/orgs","repos_url":"https://api.github.com/users/petecheslock/repos","events_url":"https://api.github.com/users/petecheslock/events{/privacy}","received_events_url":"https://api.github.com/users/petecheslock/received_events","type":"User","site_admin":false},"created_at":"2015-02-24T21:02:27Z","updated_at":"2015-02-24T21:02:27Z","author_association":"NONE","body":"Actually - i take that back - after restarting the cluster - with the tags set, it's cranking thru the indexes at a rate of one a second (which is far better than before) and does not show likes it's slowing down.  So i think mostly related to when you don't have the node applied with that tag yet that may exacerbate the problem.  \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/80739757","html_url":"https://github.com/elastic/elasticsearch/issues/9853#issuecomment-80739757","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9853","id":80739757,"node_id":"MDEyOklzc3VlQ29tbWVudDgwNzM5NzU3","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2015-03-14T22:00:58Z","updated_at":"2015-03-14T22:00:58Z","author_association":"MEMBER","body":"@petecheslock okay, curator [3.0.0](https://github.com/elastic/curator/releases) was released, which batches these requests instead of sending them one-at-a-time. This should fix this for the case when the tag is not set on a node and it keeps taking longer and longer, because it will only cause a single cluster state update.\n\nI'm going to close this for now, feel free to re-open or open a new issue!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/81103082","html_url":"https://github.com/elastic/elasticsearch/issues/9853#issuecomment-81103082","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9853","id":81103082,"node_id":"MDEyOklzc3VlQ29tbWVudDgxMTAzMDgy","user":{"login":"petecheslock","id":511733,"node_id":"MDQ6VXNlcjUxMTczMw==","avatar_url":"https://avatars2.githubusercontent.com/u/511733?v=4","gravatar_id":"","url":"https://api.github.com/users/petecheslock","html_url":"https://github.com/petecheslock","followers_url":"https://api.github.com/users/petecheslock/followers","following_url":"https://api.github.com/users/petecheslock/following{/other_user}","gists_url":"https://api.github.com/users/petecheslock/gists{/gist_id}","starred_url":"https://api.github.com/users/petecheslock/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/petecheslock/subscriptions","organizations_url":"https://api.github.com/users/petecheslock/orgs","repos_url":"https://api.github.com/users/petecheslock/repos","events_url":"https://api.github.com/users/petecheslock/events{/privacy}","received_events_url":"https://api.github.com/users/petecheslock/received_events","type":"User","site_admin":false},"created_at":"2015-03-15T15:34:48Z","updated_at":"2015-03-15T15:34:48Z","author_association":"NONE","body":"Thanks @dakrone 👍\n","performed_via_github_app":null}]