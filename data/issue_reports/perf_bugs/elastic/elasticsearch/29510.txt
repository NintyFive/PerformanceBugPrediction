{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/29510","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29510/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29510/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/29510/events","html_url":"https://github.com/elastic/elasticsearch/issues/29510","id":314050090,"node_id":"MDU6SXNzdWUzMTQwNTAwOTA=","number":29510,"title":"Termvector slow performance","user":{"login":"lahloug","id":22119606,"node_id":"MDQ6VXNlcjIyMTE5NjA2","avatar_url":"https://avatars2.githubusercontent.com/u/22119606?v=4","gravatar_id":"","url":"https://api.github.com/users/lahloug","html_url":"https://github.com/lahloug","followers_url":"https://api.github.com/users/lahloug/followers","following_url":"https://api.github.com/users/lahloug/following{/other_user}","gists_url":"https://api.github.com/users/lahloug/gists{/gist_id}","starred_url":"https://api.github.com/users/lahloug/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lahloug/subscriptions","organizations_url":"https://api.github.com/users/lahloug/orgs","repos_url":"https://api.github.com/users/lahloug/repos","events_url":"https://api.github.com/users/lahloug/events{/privacy}","received_events_url":"https://api.github.com/users/lahloug/received_events","type":"User","site_admin":false},"labels":[{"id":146832564,"node_id":"MDU6TGFiZWwxNDY4MzI1NjQ=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Search/Search","name":":Search/Search","color":"0e8a16","default":false,"description":"Search-related issues that do not fall into other categories"},{"id":23174,"node_id":"MDU6TGFiZWwyMzE3NA==","url":"https://api.github.com/repos/elastic/elasticsearch/labels/%3Eenhancement","name":">enhancement","color":"4a4ea8","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2018-04-13T10:15:46Z","updated_at":"2019-05-07T07:26:27Z","closed_at":"2019-05-07T07:26:27Z","author_association":"NONE","active_lock_reason":null,"body":"Hi !\r\n I am using elasticsearch to catch n-grams on text. Here is my mapping : \r\n```\r\n    mapp = {'settings': {\r\n                'number_of_shards': 1,\r\n                'analysis': {\r\n                    'analyzer': {\r\n                        'body_analyzer': {\r\n                            \"type\": \"custom\",\r\n                            \"char_filter\": [\"html_strip\"],\r\n                            \"tokenizer\": \"standard\",\r\n                            \"filter\": [\"standard\",\r\n                                       \"lowercase\",\r\n                                       \"filter_shingle\"]\r\n                        }\r\n                    },\r\n                    \"filter\": {\r\n                        \"filter_shingle\": {\r\n                            \"type\": \"shingle\",\r\n                            \"max_shingle_size\": 6,\r\n                            \"min_shingle_size\": 2,\r\n                            \"output_unigrams\": \"true\"\r\n                        }\r\n                    }\r\n                }\r\n            }, \r\n           'mappings': {\r\n\r\n     'article': {\r\n                    'properties': {\r\n                        'body': {'type': 'text',\r\n                                 'analyzer': 'body_analyzer',\r\n                                 'term_vector': 'with_positions_offsets',\r\n                                 'store': \"true\"}}}}}\r\n```\r\n\r\n\r\nI have so far several thousands of texts with average length of 200 words.\r\nThe thing is, using termvectors to retrieve documents count is unusually slow. Here is the query I do using python client : \r\n\r\n```\r\n    for art_id in [articles_ids[i: i+20] for i in range(0,\r\n                                                        len(articles_ids),\r\n                                                        20)]:\r\n        t3 = time()\r\n        res = cnx.mtermvectors(index=index_name, doc_type='article', ids=art_id,\r\n                               field_statistics=False,\r\n                               positions=False,\r\n                               offsets=False,\r\n                               payloads=False,\r\n                               fields='body')\r\n        t4 = time()\r\n        get_res += t4 - t3\r\n```\r\nIt takes ~3 seconds to execute on 2700 articles.\r\n\r\nSince our filter ask for 1 gram up to 6 grams, each document has ( 6  * 200 = 1200) 1200 keys. That may be the reason of its slow performance. But we only want to see the count frequencies of a small batch of words.\r\nIs it possible to store dynamically only those specific words? Or ask elasticsearch to filter the keys on specific words in the termvector request ? \r\n","closed_by":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"performed_via_github_app":null}