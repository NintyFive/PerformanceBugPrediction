[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/317436569","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-317436569","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":317436569,"node_id":"MDEyOklzc3VlQ29tbWVudDMxNzQzNjU2OQ==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-07-24T14:16:58Z","updated_at":"2017-07-24T14:16:58Z","author_association":"CONTRIBUTOR","body":"Can you provide \r\n- the full task list?\r\n- which plugins you have installed?\r\n\r\nHow did you upgrade to 5.4.2? Rolling restart or full cluster restart?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/317690139","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-317690139","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":317690139,"node_id":"MDEyOklzc3VlQ29tbWVudDMxNzY5MDEzOQ==","user":{"login":"bra-fsn","id":820331,"node_id":"MDQ6VXNlcjgyMDMzMQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820331?v=4","gravatar_id":"","url":"https://api.github.com/users/bra-fsn","html_url":"https://github.com/bra-fsn","followers_url":"https://api.github.com/users/bra-fsn/followers","following_url":"https://api.github.com/users/bra-fsn/following{/other_user}","gists_url":"https://api.github.com/users/bra-fsn/gists{/gist_id}","starred_url":"https://api.github.com/users/bra-fsn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bra-fsn/subscriptions","organizations_url":"https://api.github.com/users/bra-fsn/orgs","repos_url":"https://api.github.com/users/bra-fsn/repos","events_url":"https://api.github.com/users/bra-fsn/events{/privacy}","received_events_url":"https://api.github.com/users/bra-fsn/received_events","type":"User","site_admin":false},"created_at":"2017-07-25T09:59:50Z","updated_at":"2017-07-25T10:13:22Z","author_association":"NONE","body":"https://pastebin.com/ycZaxf4E\r\nI've removed sources, because they contain sensitive information.\r\nAs you can see, currently all stuck requests wait for a single shard. When we saw millions of slow/stuck requests, they were spreaded on more shards.\r\n```\r\n$ curl -s 'http://localhost:9200/_cat/shards' | grep messages_13\r\nmessages_13      0  p STARTED  26201377  13.2gb 10.6.145.206 ffe13.f.private\r\nmessages_13      0  r STARTED  26201375  13.2gb 10.6.145.204 ffe11.f.private\r\n$ curl -s 'http://localhost:9200/_cat/segments' | grep messages_13\r\nmessages_13     0  p 10.6.145.206 _4yusv  8344687  25894849 111017     13gb 22009475 true  true  6.5.1 false\r\nmessages_13     0  p 10.6.145.206 _5396a  8549938    278936   6793  150.8mb   414051 true  true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53all  8551785      4064    583    2.7mb    12924 true  true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53cqh  8554553      2406    127    1.4mb        0 true  false 6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53da8  8555264      3280    161    2.1mb    11234 true  true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53e8r  8556507      2433    120    1.5mb        0 true  false 6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53epr  8557119      2506    183    1.6mb     9481 true  true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53fa6  8557854      3767    166    2.4mb    11582 true  true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53fxk  8558696       652      2    444kb        0 true  false 6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53fye  8558726       705      1  447.7kb        0 true  false 6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53fyp  8558737       668      0    450kb        0 true  false 6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53fyq  8558738         2      0    6.9kb        0 true  false 6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53fyr  8558739         1      0      6kb        0 true  false 6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53fys  8558740         3      0    7.4kb        0 true  false 6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53fyt  8558741         1      0    5.8kb        0 true  false 6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53h9s  8560432      4327     54    2.6mb    12626 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53i5f  8561571      2265     23    1.4mb     9564 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53i5p  8561581      2306     23    1.4mb     9056 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53i5z  8561591      4520     21    2.8mb    13278 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53ids  8561872       583      1  402.1kb     5447 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53idt  8561873         1      1    7.3kb     3818 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53idu  8561874         1      0    6.7kb     3191 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53idv  8561875         1      0    5.9kb     2709 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53idw  8561876         2      0    7.9kb     3848 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53idx  8561877         1      0    5.8kb     2709 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53idy  8561878         6      0   10.9kb     4195 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53idz  8561879         1      0    5.7kb     2709 false true  6.5.1 true\r\nmessages_13     0  p 10.6.145.206 _53ie0  8561880         1      0    5.9kb     2709 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4ywaz  8346635  25895205 111086     13gb 22009942 true  true  6.5.1 false\r\nmessages_13     0  r 10.6.145.204 _4ywiw  8346920       107      3   83.8kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4ywl5  8347001       142      1  110.7kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4yws6  8347254       137      1  105.6kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4ywtu  8347314        96      2     70kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4ywv8  8347364       155     11  119.2kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4ywvi  8347374       343      0  240.3kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4ywyk  8347484        27      0   28.4kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4ywyu  8347494        29      0   29.4kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4ywze  8347514        84      0   68.9kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4yx0s  8347564        32      0   32.4kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4yx12  8347574        33      1   33.7kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4yx1c  8347584        41      0     37kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _4yx1m  8347594        40      0   36.6kb        0 true  false 6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _5370f  8547135    268078   7613  145.6mb   402299 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53foy  8558386      6586    200      4mb    16948 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53jb2  8563070     30000    212   17.3mb    52469 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53jeo  8563200       274      9  216.2kb     7848 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53jmr  8563491       223      2  199.8kb    10763 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53jq4  8563612       391      7    285kb     5111 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53jsn  8563703       186     11  142.3kb     9884 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53juc  8563764       161      0  124.7kb     9133 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53jun  8563775       153     10  131.3kb     8969 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53juy  8563786       158      0  124.1kb     9033 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53juz  8563787         1      0    5.7kb     2709 false true  6.5.1 true\r\nmessages_13     0  r 10.6.145.204 _53jv0  8563788         3      0    7.7kb     3149 false true  6.5.1 true\r\n```\r\nOnly analysis-icu is installed.\r\n\r\nWe've started with rolling restart (from 5.1.2) then we saw an increasing amount of slowness, so we decided to go with full cluster restart (removed all traffic, shut down all nodes and restarted them).\r\n\r\nThe cluster started its life at around 2.x, but a reindex was done later. A force merge was also done.\r\n\r\nThe bulks contain index operations into multiple indices and every failed operation is retried with some wait and a retry count in new bulks. (I don't think the latter is relevant)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/317754073","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-317754073","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":317754073,"node_id":"MDEyOklzc3VlQ29tbWVudDMxNzc1NDA3Mw==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-07-25T14:24:28Z","updated_at":"2017-07-25T14:24:28Z","author_association":"CONTRIBUTOR","body":"Thanks @bra-fsn.\r\n\r\nWhat's odd is that the indexing task that got stuck is in the \"starting\" phase:\r\n\r\n```\r\n\"x6u3Q2G7TzeZwsMi4q6P8g:17901896\" : {\r\n          \"node\" : \"x6u3Q2G7TzeZwsMi4q6P8g\",\r\n          \"id\" : 17901896,\r\n          \"type\" : \"direct\",\r\n          \"action\" : \"indices:data/write/bulk[s][p]\",\r\n          \"status\" : {\r\n            \"phase\" : \"starting\"\r\n          },\r\n          \"description\" : \"BulkShardRequest [[messages_13][0]] containing [index {[messages_13][message][9439467.1397985422], source[{}]}]\",\r\n          \"start_time_in_millis\" : 1499661826584,\r\n          \"running_time_in_nanos\" : 1311027017177258,\r\n          \"cancellable\" : false,\r\n          \"parent_task_id\" : \"x6u3Q2G7TzeZwsMi4q6P8g:17901895\"\r\n        },\r\n```\r\n\r\nIs it possible to get a heap dump from this node that you could share with us in private?\r\n\r\n@bleskes this could be an issue with the `IndexShardOperationsLock` / `IndexShardOperationPermits`?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/317996043","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-317996043","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":317996043,"node_id":"MDEyOklzc3VlQ29tbWVudDMxNzk5NjA0Mw==","user":{"login":"bra-fsn","id":820331,"node_id":"MDQ6VXNlcjgyMDMzMQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820331?v=4","gravatar_id":"","url":"https://api.github.com/users/bra-fsn","html_url":"https://github.com/bra-fsn","followers_url":"https://api.github.com/users/bra-fsn/followers","following_url":"https://api.github.com/users/bra-fsn/following{/other_user}","gists_url":"https://api.github.com/users/bra-fsn/gists{/gist_id}","starred_url":"https://api.github.com/users/bra-fsn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bra-fsn/subscriptions","organizations_url":"https://api.github.com/users/bra-fsn/orgs","repos_url":"https://api.github.com/users/bra-fsn/repos","events_url":"https://api.github.com/users/bra-fsn/events{/privacy}","received_events_url":"https://api.github.com/users/bra-fsn/received_events","type":"User","site_admin":false},"created_at":"2017-07-26T09:10:30Z","updated_at":"2017-07-26T09:10:30Z","author_association":"NONE","body":"I've sent the dumps via email.\r\nThanks.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/318114448","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-318114448","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":318114448,"node_id":"MDEyOklzc3VlQ29tbWVudDMxODExNDQ0OA==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-07-26T16:51:03Z","updated_at":"2017-07-26T16:51:03Z","author_association":"CONTRIBUTOR","body":"Just a small update, nothing conclusive yet:\r\n\r\nI've given the heap dump a quick look and found the object associated with the task seen above, but there are no other components besides the task manager holding a reference to it. This means that the request did not get stuck in the classical sense, but in presence of the async communication methods used by transport, there was never a response send to the channel for this request. The `TransportChannelWrapper` that should normally keep a reference to the task to remove it from the task manager upon sending a response has been garbage-collected, which means that a response was never sent, and that the channel object (i.e. response listener) must have gotten lost / ignored somewhere along the way. The tricky part is figuring out where this could have happened (needs more investigation).\r\n\r\n@bra-fsn can you check whether there was a warning of the form `failed to execute failure callback on` in the logs of the node `x6u3Q2G7TzeZwsMi4q6P8g` 2 weeks ago?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/318149055","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-318149055","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":318149055,"node_id":"MDEyOklzc3VlQ29tbWVudDMxODE0OTA1NQ==","user":{"login":"bra-fsn","id":820331,"node_id":"MDQ6VXNlcjgyMDMzMQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820331?v=4","gravatar_id":"","url":"https://api.github.com/users/bra-fsn","html_url":"https://github.com/bra-fsn","followers_url":"https://api.github.com/users/bra-fsn/followers","following_url":"https://api.github.com/users/bra-fsn/following{/other_user}","gists_url":"https://api.github.com/users/bra-fsn/gists{/gist_id}","starred_url":"https://api.github.com/users/bra-fsn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bra-fsn/subscriptions","organizations_url":"https://api.github.com/users/bra-fsn/orgs","repos_url":"https://api.github.com/users/bra-fsn/repos","events_url":"https://api.github.com/users/bra-fsn/events{/privacy}","received_events_url":"https://api.github.com/users/bra-fsn/received_events","type":"User","site_admin":false},"created_at":"2017-07-26T18:55:28Z","updated_at":"2017-07-26T18:55:28Z","author_association":"NONE","body":"Yes:\r\n```\r\n[2017-07-10T04:51:46,388][WARN ][o.e.i.s.IndexShard       ] [ffe14.f.private] [messages_13][0] failed to execute failure callback on [org.elasticsearch.action.support.ContextPreservingActionListener@40a1\r\n96a4]\r\norg.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution of org.elasticsearch.action.support.ThreadedActionListener$2@64ec773a on EsThreadPoolExecutor[bulk, queue capacity = 300,\r\n org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@1afaaa21[Running, pool size = 24, active threads = 24, queued tasks = 297, completed tasks = 2527871]]\r\n        at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:50) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) ~[?:1.8.0_121]\r\n        at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.doExecute(EsThreadPoolExecutor.java:94) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:89) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.support.ThreadedActionListener.onFailure(ThreadedActionListener.java:110) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.index.shard.IndexShardOperationsLock.acquire(IndexShardOperationsLock.java:120) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.index.shard.IndexShardOperationsLock.lambda$blockOperations$0(IndexShardOperationsLock.java:101) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:569) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]\r\n[2017-07-10T04:51:46,388][DEBUG][o.e.a.b.TransportShardBulkAction] [ffe14.f.private] [messages_13][0] failed to execute bulk item (update) BulkShardRequest [[messages_13][0]] containing [20] requests blocking until refresh\r\norg.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[CLOSED] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]\r\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:1106) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:614) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:165) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:83) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:78) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.executeUpdateRequest(TransportShardBulkAction.java:269) ~[elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.executeBulkItemRequest(TransportShardBulkAction.java:159) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:113) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:69) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryShardReference.perform(TransportReplicationAction.java:939) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryShardReference.perform(TransportReplicationAction.java:908) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.support.replication.ReplicationOperation.execute(ReplicationOperation.java:113) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.onResponse(TransportReplicationAction.java:322) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.onResponse(TransportReplicationAction.java:264) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.support.replication.TransportReplicationAction$1.onResponse(TransportReplicationAction.java:888) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.support.replication.TransportReplicationAction$1.onResponse(TransportReplicationAction.java:885) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.action.support.ThreadedActionListener$1.doRun(ThreadedActionListener.java:98) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:638) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.4.2.jar:5.4.2]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]\r\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]\r\n```\r\n\r\nBTW, we use wait_for requests, I'm not sure whether there are too many of them (filling up the queue), or just the stuck tasks block them.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/318156147","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-318156147","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":318156147,"node_id":"MDEyOklzc3VlQ29tbWVudDMxODE1NjE0Nw==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-07-26T19:23:38Z","updated_at":"2017-07-26T19:23:38Z","author_association":"CONTRIBUTOR","body":"Great. I think I have this figured out. Will look at options to fix this tomorrow. In the meanwhile, you can disable shard balancing for primaries (The issue can only occur during primary relocations AFAICS), i.e., set `cluster.routing.rebalance.enable=replicas`.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/318160995","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-318160995","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":318160995,"node_id":"MDEyOklzc3VlQ29tbWVudDMxODE2MDk5NQ==","user":{"login":"bra-fsn","id":820331,"node_id":"MDQ6VXNlcjgyMDMzMQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820331?v=4","gravatar_id":"","url":"https://api.github.com/users/bra-fsn","html_url":"https://github.com/bra-fsn","followers_url":"https://api.github.com/users/bra-fsn/followers","following_url":"https://api.github.com/users/bra-fsn/following{/other_user}","gists_url":"https://api.github.com/users/bra-fsn/gists{/gist_id}","starred_url":"https://api.github.com/users/bra-fsn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bra-fsn/subscriptions","organizations_url":"https://api.github.com/users/bra-fsn/orgs","repos_url":"https://api.github.com/users/bra-fsn/repos","events_url":"https://api.github.com/users/bra-fsn/events{/privacy}","received_events_url":"https://api.github.com/users/bra-fsn/received_events","type":"User","site_admin":false},"created_at":"2017-07-26T19:43:40Z","updated_at":"2017-07-26T19:43:40Z","author_association":"NONE","body":"You may be right, because I couldn't cure this by restarting the nodes on which there were stuck tasks. BTW, even restarting all of the data nodes didn't help. But if I restarted the master nodes too, it seems to have normalized.\r\nIs this in line with what you've found?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/318346360","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-318346360","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":318346360,"node_id":"MDEyOklzc3VlQ29tbWVudDMxODM0NjM2MA==","user":{"login":"bra-fsn","id":820331,"node_id":"MDQ6VXNlcjgyMDMzMQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820331?v=4","gravatar_id":"","url":"https://api.github.com/users/bra-fsn","html_url":"https://github.com/bra-fsn","followers_url":"https://api.github.com/users/bra-fsn/followers","following_url":"https://api.github.com/users/bra-fsn/following{/other_user}","gists_url":"https://api.github.com/users/bra-fsn/gists{/gist_id}","starred_url":"https://api.github.com/users/bra-fsn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bra-fsn/subscriptions","organizations_url":"https://api.github.com/users/bra-fsn/orgs","repos_url":"https://api.github.com/users/bra-fsn/repos","events_url":"https://api.github.com/users/bra-fsn/events{/privacy}","received_events_url":"https://api.github.com/users/bra-fsn/received_events","type":"User","site_admin":false},"created_at":"2017-07-27T12:22:25Z","updated_at":"2017-07-27T12:22:25Z","author_association":"NONE","body":"Thanks. Do you have any timeframe for 5.5.2 release?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/318352962","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-318352962","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":318352962,"node_id":"MDEyOklzc3VlQ29tbWVudDMxODM1Mjk2Mg==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-07-27T12:53:29Z","updated_at":"2017-07-27T12:53:29Z","author_association":"CONTRIBUTOR","body":"As you've seen, the bug is fixed and will be released as part of ES v5.5.2.\r\n\r\n> Do you have any timeframe for 5.5.2 release?\r\n\r\nWith v5.5.1 just released, it will probably take a few weeks for v5.5.2 to be released. By following the suggestion above, you should hopefully not be encountering these issues anymore in the mean-time.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/320124915","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-320124915","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":320124915,"node_id":"MDEyOklzc3VlQ29tbWVudDMyMDEyNDkxNQ==","user":{"login":"bra-fsn","id":820331,"node_id":"MDQ6VXNlcjgyMDMzMQ==","avatar_url":"https://avatars2.githubusercontent.com/u/820331?v=4","gravatar_id":"","url":"https://api.github.com/users/bra-fsn","html_url":"https://github.com/bra-fsn","followers_url":"https://api.github.com/users/bra-fsn/followers","following_url":"https://api.github.com/users/bra-fsn/following{/other_user}","gists_url":"https://api.github.com/users/bra-fsn/gists{/gist_id}","starred_url":"https://api.github.com/users/bra-fsn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bra-fsn/subscriptions","organizations_url":"https://api.github.com/users/bra-fsn/orgs","repos_url":"https://api.github.com/users/bra-fsn/repos","events_url":"https://api.github.com/users/bra-fsn/events{/privacy}","received_events_url":"https://api.github.com/users/bra-fsn/received_events","type":"User","site_admin":false},"created_at":"2017-08-04T00:30:12Z","updated_at":"2017-08-04T00:30:12Z","author_association":"NONE","body":"@ywelsch: I was impatient and built a release from commit fb747b83c68855558d10631bf5bf4e1d370caa21 and your patch applied to it, and deployed that version to our servers.\r\nThe effect is even worse than before. Now I can't start the cluster no matter what I'm doing without tasks getting accumulated on one or more nodes.\r\nAll of this is with a persistent \"cluster.routing.rebalance.enable\":\"none\" (or replicas).\r\nI've tried switching releases back and forth (the official 5.5.1 and the patched one) and without the patch the cluster always worked fine after all nodes came up (no long running, stuck tasks) and with the patch one or more nodes always started to grow old, never completing (get, search, index etc) tasks.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/336434632","html_url":"https://github.com/elastic/elasticsearch/issues/25863#issuecomment-336434632","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/25863","id":336434632,"node_id":"MDEyOklzc3VlQ29tbWVudDMzNjQzNDYzMg==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-10-13T12:08:25Z","updated_at":"2017-10-13T12:08:25Z","author_association":"CONTRIBUTOR","body":"As a continuation of this issue and #26293, we analyzed and discussed this issue further per e-mail with @bra-fsn, where he provided us with additional logs, heap dumps, and where he tested customized ES versions to provide more insight into the issue. We found two issues:\r\n\r\n- Our Netty plugin was not properly propagating failures back to its callers when writing to a broken or already closed channel. This in return would make it look to our application as if the messages had just disappeared, and the application or task would be stuck waiting on a response. Note that this affected both our Netty3 as well as our Netty4 plugin. We also found out that as part of a general code clean-up, this issue was [already fixed](https://github.com/elastic/elasticsearch/pull/23559) for the upcoming ES v6.0.0. That fix was now backported to ES v5.6.3.\r\n- With this fix in place, @bra-fsn was not experiencing the stuck tasks anymore, but would still see recoveries timing out. With the additional logging added as part of ES v5.6.3, @bra-fsn saw network connection failures during startup, for which he suspected the TCP backlog queue. @bra-fsn:\r\n> On FreeBSD (which we use), it's very low by default (128), which means if a lot of connections are opened in a sudden (during cluster startup), but they are slowly accept()-ed, it can fill up. After it reaches the maximum, the IP stack will refuse new connections, so this could cause our problems...\r\n\r\nElasticsearch opens 13 connections per target node, see https://www.elastic.co/blog/found-elasticsearch-networking#connections-and-channels which is an old blog post but still valid. We refer to these connections as channels. With a 40 node cluster, for example, this amounts to opening 39*13 connections on a node at the same time, which is well above the FreeBSD backlog queue size of 128. Linux-based systems have a higher backlog queue (2048 on my Ubuntu), but for large clusters, even that one might fill up. ES should still have worked correctly, even after encountering these initial connection issues. If a channel is closed (e.g. due to socket break, or due to the backlog queue being full), Elasticsearch should normally close all other channels to that same target node and reconnect them. When we looked at @bra-fsn's most recent logs, we noticed, however, that the same closed channel was still being used one hour later for communication, showing that no such clean-up and reconnect had happened. This led us to find a bug that would explain why, even after startup, the cluster was still behaving incorrectly and recoveries failing (explanation on the Github issue):\r\n\r\nhttps://github.com/elastic/elasticsearch/pull/26932\r\n\r\nThis fix will be released as part of v5.6.4. Increasing the backlog queue size should be a temporary fix.\r\n\r\nIn conclusion, the combination of the small backlog queue together with the failure to properly clean up / reconnect connections that were closed during initialization were causing channels to remain broken. The connections that remained broken in combination with a bug not to notify callers of failed channel writes were then causing tasks to be stuck.","performed_via_github_app":null}]