{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/18289","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18289/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18289/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18289/events","html_url":"https://github.com/elastic/elasticsearch/issues/18289","id":154401225,"node_id":"MDU6SXNzdWUxNTQ0MDEyMjU=","number":18289,"title":"failed to upgrade from 1.7.4 to 2.3.2, initializing primary shards stuck in TRANSLOG stage.","user":{"login":"sweetest","id":6868049,"node_id":"MDQ6VXNlcjY4NjgwNDk=","avatar_url":"https://avatars0.githubusercontent.com/u/6868049?v=4","gravatar_id":"","url":"https://api.github.com/users/sweetest","html_url":"https://github.com/sweetest","followers_url":"https://api.github.com/users/sweetest/followers","following_url":"https://api.github.com/users/sweetest/following{/other_user}","gists_url":"https://api.github.com/users/sweetest/gists{/gist_id}","starred_url":"https://api.github.com/users/sweetest/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sweetest/subscriptions","organizations_url":"https://api.github.com/users/sweetest/orgs","repos_url":"https://api.github.com/users/sweetest/repos","events_url":"https://api.github.com/users/sweetest/events{/privacy}","received_events_url":"https://api.github.com/users/sweetest/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2016-05-12T05:37:52Z","updated_at":"2016-05-12T07:16:16Z","closed_at":"2016-05-12T07:16:16Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"when I restarted cluster, some of the primaries are properly restored, but some recovery task tasks hang\n\nthey are in the 'TRANSLOG' stage and are staying there for more than an hour, which is not expected since I run _flush before restart meaning there should be no remaining translog.\n\nhot threads api shows below\n\n```\n0.0% (97.7micros out of 500ms) cpu usage by thread 'elasticsearch[IP][transport_client_timer][T#1]{Hashed wheel timer #1}'\n 10/10 snapshots sharing following 5 elements\n   java.lang.Thread.sleep(Native Method)\n   org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)\n   org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)\n   org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n```\n\nwhile /_cat/recovery shows a lot of lines just like below\n\n```\nINDEX_NAME 9 1688129 store translog 10.96.250.224 10.96.250.224 n/a n/a 0 100.0% 0 100.0% 111 5087612156 0 -1.0% -1\n```\n\nsome of the relevant error logs in data nodes are as below.\n\n``````\n[2016-05-12 11:54:30,666][WARN ][cluster.service          ] [IP] failed to connect to node [{IP}{P9z8Kl0SSV6d8VaZWViNLg}{IP}{IP:9300}{box_type=hdd, river=_none, master=false, node_id=IP}]\nConnectTransportException[[IP][IP:9300] connect_timeout[30s]]; nested: SocketException[연결이 �~A�~@편�~W~P �~X해 �~J어짐];\n        at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:940)\n        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:855)\n        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:828)\n        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:243)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:474)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:225)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:188)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.SocketException: 연결이 �~A�~@편�~W~P �~X해 �~J어짐\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n        at org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152)\n        at org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)\n        at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\n        at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        ... 3 more\n[2016-05-12 11:59:37,265][INFO ][indices.breaker          ] [IP] Updated breaker settings fielddata: [fielddata,type=MEMORY,limit=644245094/614.3mb,overhead=1.03]\n[2016-05-12 11:59:37,266][INFO ][cluster.routing.allocation.decider] [IP] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]\n[2016-05-12 12:00:01,034][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\njava.lang.OutOfMemoryError: Java heap space\n        at java.lang.Integer.valueOf(Integer.java:832)\n        at sun.nio.ch.EPollSelectorImpl.updateSelectedKeys(EPollSelectorImpl.java:106)\n        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:84)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)```\n``````\n\n```\n[2016-05-12 12:00:54,273][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\njava.lang.OutOfMemoryError: Java heap space\n```\n\n```\n2016-05-12 12:00:54,940][DEBUG][action.admin.indices.stats] [IP] [indices:monitor/stats] failed to execute operation for shard [[log-2016-04-13][3], node[kji2YtUgQ_O6MCh40Tb26A], [P], v[15], s[INITIALIZING], a[id=ZggyMnC7Sfm3ZHhYv2YxUg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-05-12T02:59:33.040Z]]]\n[log-2016-04-13][[log-2016-04-13][3]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:399)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:376)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:365)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [log-2016-04-13][[log-2016-04-13][3]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:957)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:791)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:612)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:395)\n        ... 7 more\n```\n\nI tried again to upgrade from 1.7.4 to 2.0.0, just out of curiosity, but similar problem occurred. Do you have any idea on why this is happening?\n","closed_by":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"performed_via_github_app":null}