[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/157087180","html_url":"https://github.com/elastic/elasticsearch/issues/14697#issuecomment-157087180","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14697","id":157087180,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NzA4NzE4MA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-11-16T16:24:20Z","updated_at":"2015-11-16T16:24:20Z","author_association":"MEMBER","body":"Answering in another order:\n\n> Nodes should never change IDs\n\nNode ids actually change all the time. You can think of a node ID as an equivalent of a process id - every time the node is restarted and we have a new process, the id changes and the node joins the cluster again. This allows us to make sure that network connections are re-opened to the node etc.\n\n> I think that passing the nodesIds instead of response.getNodes() is not a particularly good solution because it defends against something that should never happen, \n\nWe are making sure that we use the information based on where it come from, not where we thought we sent it to. I think this is safer.\n\n> and poison the cache in the process as the master will never get a response from node 1234.\n\nIt's true that we do not populate the cache with the response and we keep on waiting for an answer from 1234. However, the [node fault detection will discover that a new node was started on the same port](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java#L245) and will remove the node from the cluster. At the point the cache is [cleaned](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java#L234) and the master will nog longer wait for a response. \n\n> While trying to upgrade our clusters to 1.7.3 I came across a fun issue with the async shard fetch \n\nDo you have any more information about what you saw? Logs or similar will be great.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/157163031","html_url":"https://github.com/elastic/elasticsearch/issues/14697#issuecomment-157163031","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14697","id":157163031,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NzE2MzAzMQ==","user":{"login":"jolynch","id":1420460,"node_id":"MDQ6VXNlcjE0MjA0NjA=","avatar_url":"https://avatars1.githubusercontent.com/u/1420460?v=4","gravatar_id":"","url":"https://api.github.com/users/jolynch","html_url":"https://github.com/jolynch","followers_url":"https://api.github.com/users/jolynch/followers","following_url":"https://api.github.com/users/jolynch/following{/other_user}","gists_url":"https://api.github.com/users/jolynch/gists{/gist_id}","starred_url":"https://api.github.com/users/jolynch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jolynch/subscriptions","organizations_url":"https://api.github.com/users/jolynch/orgs","repos_url":"https://api.github.com/users/jolynch/repos","events_url":"https://api.github.com/users/jolynch/events{/privacy}","received_events_url":"https://api.github.com/users/jolynch/received_events","type":"User","site_admin":false},"created_at":"2015-11-16T20:41:09Z","updated_at":"2015-11-16T20:41:09Z","author_association":"NONE","body":"> Node ids actually change all the time. \n\nSorry you are right and I should be precise, node ids should have a 1:1 mapping with process ids. The problem I was observing was that this was not the case when our discovery plugin was setting the local node id to one value (similar to the linked method in `LocalDiscovery`) but the `InternalClusterService` sets it to something else. So in the same PID the node would change ids while joining the cluster between construction and when our discovery plugin's `doStart` method was called. This is clearly a bug in our discovery service and I have since defaulted to just using whatever id is provided by the injected `ClusterService` which has eliminated this bug. We didn't have this problem pre-1.7 because shard fetches were sync and did not depend on the node id.\n\n> It's true that we do not populate the cache with the response and we keep on waiting for an answer from 1234. However, the node fault detection will discover that a new node was started on the same port and will remove the node from the cluster. At the point the cache is cleaned and the master will nog longer wait for a response.\n\nThis is what I mean by I can't come up with a real world situation in which the node is still in the cluster but the ID has changed along the way. The only way I can see is if there was a similar issue with a `Discovery` provider (namely that the discovery provider set the node ID instead of letting the `ClusterService` do it for them) or if the separate invalidation doesn't work for some reason. That being said I can conceive of fun network partitions that could maybe make it possible, and it's pretty simple to make the async shard fetch mechanism as robust to such issues as the sync method was before so why not.\n\n> Do you have any more information about what you saw? Logs or similar will be great.\n\nI'm not sure how generally applicable this is, but an example of when we got stuck (note the \"ignoring already fetching\" lines):\n\n```\n[2015-11-10 14:41:59,988] [TRACE] [test_node_redacted] [test_index][11], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]: ignoring allocation, still fetching shard stores\n[2015-11-10 14:41:59,988] [TRACE] [test_node_redacted] Can not allocate [[test_index][2], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]] on node [i2w0iPhaSbe5Etw7Ean2ZA] due to [SameShardAllocationDecider]\n[2015-11-10 14:41:59,988] [TRACE] [test_node_redacted] Can not allocate [[test_index][2], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]] on node [MA-IP3CLRYWP26Y6m3fl0w] due to [SameShardAllocationDecider]\n[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] usage without relocations: [b8jBlkgMTWqVYnYU446AQg][10-40-26-45-uswest1cdevc.dev.yelpcorp.com] free: 304.9gb[99.9%]\n[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] usage with relocations: [0 bytes] [b8jBlkgMTWqVYnYU446AQg][10-40-26-45-uswest1cdevc.dev.yelpcorp.com] free: 304.9gb[99.9%]\n[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] Node [b8jBlkgMTWqVYnYU446AQg] has 0.010641291131321395% used disk\n[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] [test_index][2], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]: ignoring allocation, still fetching shard stores\n[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] Can not allocate [[test_index][1], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]] on node [i2w0iPhaSbe5Etw7Ean2ZA] due to [SameShardAllocationDecider]\n[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] usage without relocations: [MA-IP3CLRYWP26Y6m3fl0w][10-40-11-209-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]\n[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] usage with relocations: [0 bytes] [MA-IP3CLRYWP26Y6m3fl0w][10-40-11-209-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]\n[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] Node [MA-IP3CLRYWP26Y6m3fl0w] has 0.010612524124269385% used disk\n[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] [test_index][1], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]: ignoring allocation, still fetching shard stores\n[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] usage without relocations: [i2w0iPhaSbe5Etw7Ean2ZA][10-40-11-207-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]\n[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] usage with relocations: [0 bytes] [i2w0iPhaSbe5Etw7Ean2ZA][10-40-11-207-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]\n[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] Node [i2w0iPhaSbe5Etw7Ean2ZA] has 0.010672553476894109% used disk\n[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] [test_index][9], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]: ignoring allocation, still fetching shard stores\n[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] usage without relocations: [i2w0iPhaSbe5Etw7Ean2ZA][10-40-11-207-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]\n[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] usage with relocations: [0 bytes] [i2w0iPhaSbe5Etw7Ean2ZA][10-40-11-207-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]\n```\n\nI'm totally happy with saying \"this shouldn't happen and if it does there is a bug elsewhere\", but I'm also happy to submit a PR to make the async shard fetch robust to such inconsistencies if you like.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/157307250","html_url":"https://github.com/elastic/elasticsearch/issues/14697#issuecomment-157307250","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14697","id":157307250,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NzMwNzI1MA==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-11-17T08:26:27Z","updated_at":"2015-11-17T08:26:27Z","author_association":"MEMBER","body":"> That being said I can conceive of fun network partitions that could maybe make it possible, and it's pretty simple to make the async shard fetch mechanism as robust to such issues as the sync method was before so why not.\n\nLooking forward to a PR :)\n\n>  (note the \"ignoring already fetching\" lines):\n\nI think that this is just working as we discussed/designed? If so, can we close the ticket?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/157312682","html_url":"https://github.com/elastic/elasticsearch/issues/14697#issuecomment-157312682","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14697","id":157312682,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NzMxMjY4Mg==","user":{"login":"jolynch","id":1420460,"node_id":"MDQ6VXNlcjE0MjA0NjA=","avatar_url":"https://avatars1.githubusercontent.com/u/1420460?v=4","gravatar_id":"","url":"https://api.github.com/users/jolynch","html_url":"https://github.com/jolynch","followers_url":"https://api.github.com/users/jolynch/followers","following_url":"https://api.github.com/users/jolynch/following{/other_user}","gists_url":"https://api.github.com/users/jolynch/gists{/gist_id}","starred_url":"https://api.github.com/users/jolynch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jolynch/subscriptions","organizations_url":"https://api.github.com/users/jolynch/orgs","repos_url":"https://api.github.com/users/jolynch/repos","events_url":"https://api.github.com/users/jolynch/events{/privacy}","received_events_url":"https://api.github.com/users/jolynch/received_events","type":"User","site_admin":false},"created_at":"2015-11-17T08:59:35Z","updated_at":"2015-11-17T08:59:35Z","author_association":"NONE","body":"Sure thing, and I can work on a PR if you think it's worth having some kind of expiration mechanism. I'm also fine with saying \"this shouldn't happen and if it does it's a bug elsewhere\". Either way feel free to close this, thank you for the feedback!\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/157323742","html_url":"https://github.com/elastic/elasticsearch/issues/14697#issuecomment-157323742","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/14697","id":157323742,"node_id":"MDEyOklzc3VlQ29tbWVudDE1NzMyMzc0Mg==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-11-17T09:54:52Z","updated_at":"2015-11-17T09:54:52Z","author_association":"MEMBER","body":"yeah, I don't think an expiration mechanism is something to add here. We already have the node FD pinging from the same node. Closing for now then. Thanks!\n","performed_via_github_app":null}]