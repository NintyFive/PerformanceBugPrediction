{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/11616","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11616/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11616/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11616/events","html_url":"https://github.com/elastic/elasticsearch/issues/11616","id":87516330,"node_id":"MDU6SXNzdWU4NzUxNjMzMA==","number":11616,"title":"degradation performance bulk insert","user":{"login":"swood","id":131139,"node_id":"MDQ6VXNlcjEzMTEzOQ==","avatar_url":"https://avatars1.githubusercontent.com/u/131139?v=4","gravatar_id":"","url":"https://api.github.com/users/swood","html_url":"https://github.com/swood","followers_url":"https://api.github.com/users/swood/followers","following_url":"https://api.github.com/users/swood/following{/other_user}","gists_url":"https://api.github.com/users/swood/gists{/gist_id}","starred_url":"https://api.github.com/users/swood/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/swood/subscriptions","organizations_url":"https://api.github.com/users/swood/orgs","repos_url":"https://api.github.com/users/swood/repos","events_url":"https://api.github.com/users/swood/events{/privacy}","received_events_url":"https://api.github.com/users/swood/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2015-06-11T22:10:29Z","updated_at":"2015-06-12T16:55:01Z","closed_at":"2015-06-12T16:55:00Z","author_association":"NONE","active_lock_reason":null,"body":"Hello.\n\nI use ElasticSearch for collection applications log. But it's huge logs. I have 17 physical servers and 8 node on each servers. Each servers has 4 disks.\nMy config looks like this:\n\n``` json\ndiscovery.zen.minimum_master_nodes: 2\ndiscovery.zen.ping.multicast.enabled: false\ndiscovery.zen.ping.timeout: 5s\ndiscovery.zen.ping.unicast.hosts: [\"192.168.3.23:9300\"]\ngateway.expected_nodes: 2\ngateway.recover_after_nodes: 3\ngateway.recover_after_time: 5m\ngateway.type: local\nindex.indexing.slowlog.threshold.index.debug: 2s\nindex.indexing.slowlog.threshold.index.info: 5s\nindex.indexing.slowlog.threshold.index.trace: 500ms\nindex.indexing.slowlog.threshold.index.warn: 10s\nindex.number_of_replicas: 2\nindex.number_of_shards: 4\nindex.search.slowlog.threshold.fetch.debug: 500ms\nindex.search.slowlog.threshold.fetch.info: 800ms\nindex.search.slowlog.threshold.fetch.trace: 200ms\nindex.search.slowlog.threshold.fetch.warn: 1s\nindex.search.slowlog.threshold.query.debug: 2s\nindex.search.slowlog.threshold.query.info: 5s\nindex.search.slowlog.threshold.query.trace: 500ms\nindex.search.slowlog.threshold.query.warn: 10s\nmonitor.jvm.gc.young.debug: 400ms\nmonitor.jvm.gc.young.info: 700ms\nmonitor.jvm.gc.young.warn: 1000ms\nnetwork.bind_host: 192.168.3.7\nnetwork.publish_host: 192.168.3.7\nnode.data: true\nnode.master: true\nnode.name: \"server_1\"\npath.data: /var/www/elastic,/var/www/elastic,/var/www/elastic,/var/www/elastic\npath.logs: /var/log/elasticsearch\n#\ntransport.tcp.port: 9300\nhttp.port: 9200\ncluster.routing.allocation.disk.watermark.low: 1gb\ncluster.routing.allocation.disk.watermark.high: 500mb\ncluster.routing.allocation.node_concurrent_recoveries: 4\ncluster.routing.allocation.node_initial_primaries_recoveries: 8\nindices.recovery.concurrent_streams: 8\nindices.recovery.max_bytes_per_sec: 100mb\nthreadpool.bulk.queue_size: 5000\n```\n\nEach node has using own path on all disks.\nFor insert of data I used to Logstash. For transport data from my servers to Logstash I used to logstash_forwarder with spool-size=512.\n\nWhen cluster has not much nodes all works fine. But, when cluster is increased, Logstash begins to wait ES with next messages:\n\n\"Failed to flush outgoing items\", :outgoing_count=>4872, :exception=>java.lang.OutOfMemoryError: Java heap space, :backtrace=>[], :level=>:warn}\"\n\nBut, my attempts increases memory for Logstash have not been successful.\nMaybe I have a wrong configuration for ElasticSearch?\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}