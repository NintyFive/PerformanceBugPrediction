[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/265728911","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-265728911","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":265728911,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NTcyODkxMQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-12-08T12:29:01Z","updated_at":"2016-12-08T12:29:01Z","author_association":"MEMBER","body":"@blinken how much time is there between the beginning of log spamming and the relocation command to move shards from hot to cold?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/265770564","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-265770564","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":265770564,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NTc3MDU2NA==","user":{"login":"blinken","id":250444,"node_id":"MDQ6VXNlcjI1MDQ0NA==","avatar_url":"https://avatars0.githubusercontent.com/u/250444?v=4","gravatar_id":"","url":"https://api.github.com/users/blinken","html_url":"https://github.com/blinken","followers_url":"https://api.github.com/users/blinken/followers","following_url":"https://api.github.com/users/blinken/following{/other_user}","gists_url":"https://api.github.com/users/blinken/gists{/gist_id}","starred_url":"https://api.github.com/users/blinken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/blinken/subscriptions","organizations_url":"https://api.github.com/users/blinken/orgs","repos_url":"https://api.github.com/users/blinken/repos","events_url":"https://api.github.com/users/blinken/events{/privacy}","received_events_url":"https://api.github.com/users/blinken/received_events","type":"User","site_admin":false},"created_at":"2016-12-08T15:38:23Z","updated_at":"2016-12-08T15:38:23Z","author_association":"NONE","body":"The index logstash-na-runit-2016.12.07 would have been created at 2016-12-07 00:01 UTC. Our curator policy is to move indices to cold storage 24 hours after they are created, and the next curator run was at 2016-12-08 00:07:01 UTC - so I'd expect the index to be moved off at that time.\r\n\r\nThe master started spamming the logs at 2016-12-07 08:31:39 UTC, only eight hours after the index was created. Here's a list of curator runs, and I'm not sure I see much of a correlation with the issue starting at 2016-12-07 08:31:39.\r\n\r\n(last few runs)\r\n2016-12-07 02:07:02.20316\r\n2016-12-07 03:07:02.18692\r\n2016-12-07 10:07:02.35900\r\n2016-12-07 16:07:02.36152\r\n2016-12-07 18:07:02.48396\r\n2016-12-07 19:07:02.25172\r\n2016-12-07 22:07:02.39559\r\n2016-12-08 00:07:02.54826\r\n2016-12-08 01:07:01.98269\r\n2016-12-08 08:07:01.79631","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/266487984","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-266487984","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":266487984,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NjQ4Nzk4NA==","user":{"login":"blinken","id":250444,"node_id":"MDQ6VXNlcjI1MDQ0NA==","avatar_url":"https://avatars0.githubusercontent.com/u/250444?v=4","gravatar_id":"","url":"https://api.github.com/users/blinken","html_url":"https://github.com/blinken","followers_url":"https://api.github.com/users/blinken/followers","following_url":"https://api.github.com/users/blinken/following{/other_user}","gists_url":"https://api.github.com/users/blinken/gists{/gist_id}","starred_url":"https://api.github.com/users/blinken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/blinken/subscriptions","organizations_url":"https://api.github.com/users/blinken/orgs","repos_url":"https://api.github.com/users/blinken/repos","events_url":"https://api.github.com/users/blinken/events{/privacy}","received_events_url":"https://api.github.com/users/blinken/received_events","type":"User","site_admin":false},"created_at":"2016-12-12T17:04:41Z","updated_at":"2016-12-12T17:04:41Z","author_association":"NONE","body":"Thanks all, I notice this has a fix going in for 5.0.3 / 5.1.2. I'll test when those versions are released and advise if we run into this again. Appreciate the quick response!","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/266496122","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-266496122","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":266496122,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NjQ5NjEyMg==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-12-12T17:33:30Z","updated_at":"2016-12-12T17:33:30Z","author_association":"CONTRIBUTOR","body":"@blinken The fix covers the log file spamming. I don't have a good theory though for the three shards which seemed to be stuck in an endless loop trying to relocate. Please report back if anything like this occurs again.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/267080596","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-267080596","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":267080596,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NzA4MDU5Ng==","user":{"login":"blinken","id":250444,"node_id":"MDQ6VXNlcjI1MDQ0NA==","avatar_url":"https://avatars0.githubusercontent.com/u/250444?v=4","gravatar_id":"","url":"https://api.github.com/users/blinken","html_url":"https://github.com/blinken","followers_url":"https://api.github.com/users/blinken/followers","following_url":"https://api.github.com/users/blinken/following{/other_user}","gists_url":"https://api.github.com/users/blinken/gists{/gist_id}","starred_url":"https://api.github.com/users/blinken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/blinken/subscriptions","organizations_url":"https://api.github.com/users/blinken/orgs","repos_url":"https://api.github.com/users/blinken/repos","events_url":"https://api.github.com/users/blinken/events{/privacy}","received_events_url":"https://api.github.com/users/blinken/received_events","type":"User","site_admin":false},"created_at":"2016-12-14T16:26:54Z","updated_at":"2016-12-14T16:26:54Z","author_association":"NONE","body":"@ywelsch ok, here's a similar I just noticed. logstash-na-nginx-2016.12.12 shard 6 replica has been unassigned for at least 24 hours now, and on investigation I find that in Marvel it's looping between INIT and TRANSLOG (shown).\r\n\r\n![image](https://cloud.githubusercontent.com/assets/250444/21190340/54024f64-c219-11e6-85a7-c766f32dcb4a.png)\r\n\r\nI note that there's a _second_ entry in Marvel for the same shard, with the same translog data - which seems to be a UI bug because I suspect it's trying to show that the primary is also relocating:\r\n\r\n```\r\n          \"6\": [\r\n            {\r\n              \"state\": \"RELOCATING\",\r\n              \"primary\": true,\r\n              \"node\": \"GFhQ_bRuQgyTxSDih69efw\",\r\n              \"relocating_node\": \"yvqmYP7CR1yXUE-04lKkDw\",\r\n              \"shard\": 6,\r\n              \"index\": \"logstash-na-nginx-2016.12.12\",\r\n              \"expected_shard_size_in_bytes\": 97396189390,\r\n              \"allocation_id\": {\r\n                \"id\": \"PYlTDKUKSEyAwt90PDe--Q\",\r\n                \"relocation_id\": \"wS62LxLNSTiIG52ujUYWeA\"\r\n              }\r\n            },\r\n            {\r\n              \"state\": \"INITIALIZING\",\r\n              \"primary\": false,\r\n              \"node\": \"3hrYNKELRq-KY8_ju3O65A\",\r\n              \"relocating_node\": null,\r\n              \"shard\": 6,\r\n              \"index\": \"logstash-na-nginx-2016.12.12\",\r\n              \"recovery_source\": {\r\n                \"type\": \"PEER\"\r\n              },\r\n              \"allocation_id\": {\r\n                \"id\": \"RMrA_0sARNWkTrW1TvzwwA\"\r\n              },\r\n              \"unassigned_info\": {\r\n                \"reason\": \"NODE_LEFT\",\r\n                \"at\": \"2016-12-13T19:02:13.914Z\",\r\n                \"delayed\": true,\r\n                \"details\": \"node_left[3hrYNKELRq-KY8_ju3O65A]\",\r\n                \"allocation_status\": \"no_attempt\"\r\n              }\r\n            }\r\n          ],\r\n```\r\n\r\nCorrect me if I'm wrong, but my understanding is that a primary should not be relocating while the secondary shard is in the process of initializing?\r\n\r\nI don't mind leaving this in this state overnight (UTC) as we still seem to be ingesting data - let me know if you would like me to collect any specific logs etc.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/267091566","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-267091566","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":267091566,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NzA5MTU2Ng==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-12-14T17:03:24Z","updated_at":"2016-12-14T17:03:24Z","author_association":"CONTRIBUTOR","body":"> a primary should not be relocating while the secondary shard is in the process of initializing\r\n\r\nthat's actually ok. We allow this to happen concurrently. Once primary relocation completes, the replica fails over to the new primary to resume initialization.\r\n\r\n> let me know if you would like me to collect any specific logs etc.\r\n\r\nIt looks like an issue with primary relocation. At the end of primary relocation there is a (usually) short phase where replica recoveries are delayed (this would explain the switching back and forth of the replica from INIT to TRANSLOG). The primary relocation seems to be stuck. To investigate this further I need:\r\n- stack dumps from the nodes involved in primary relocation (source node: GFhQ_bRuQgyTxSDih69efw and target node: yvqmYP7CR1yXUE-04lKkDw). \r\n- the list of current tasks running on these nodes provided through `/_tasks?nodes= GFhQ_bRuQgyTxSDih69efw,yvqmYP7CR1yXUE-04lKkDw` , see also here: https://www.elastic.co/guide/en/elasticsearch/reference/5.0/tasks.html\r\n- the full logs from the master node and the two nodes above.\r\n\r\nIf you don't want to share this information publicly you can send it to my e-mail address (__first_name__@elastic.co).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/267098408","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-267098408","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":267098408,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NzA5ODQwOA==","user":{"login":"blinken","id":250444,"node_id":"MDQ6VXNlcjI1MDQ0NA==","avatar_url":"https://avatars0.githubusercontent.com/u/250444?v=4","gravatar_id":"","url":"https://api.github.com/users/blinken","html_url":"https://github.com/blinken","followers_url":"https://api.github.com/users/blinken/followers","following_url":"https://api.github.com/users/blinken/following{/other_user}","gists_url":"https://api.github.com/users/blinken/gists{/gist_id}","starred_url":"https://api.github.com/users/blinken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/blinken/subscriptions","organizations_url":"https://api.github.com/users/blinken/orgs","repos_url":"https://api.github.com/users/blinken/repos","events_url":"https://api.github.com/users/blinken/events{/privacy}","received_events_url":"https://api.github.com/users/blinken/received_events","type":"User","site_admin":false},"created_at":"2016-12-14T17:28:49Z","updated_at":"2016-12-14T17:28:49Z","author_association":"NONE","body":"No problem, I'll send via email. Unfortunately it seems dal111 (GFhQ_bRuQgyTxSDih69efw) has generated 208GB of logs since midnight UTC. I'll attach the last 100k lines unless there's specific data you would like.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/267099186","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-267099186","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":267099186,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NzA5OTE4Ng==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-12-14T17:31:54Z","updated_at":"2016-12-14T17:31:54Z","author_association":"CONTRIBUTOR","body":"> I'll attach the last 100k lines\r\n\r\nsounds good. The logs might not reveal much (the interesting bits in this case are only logged at TRACE level, but increasing logging to that level would probably bring the cluster down). The stack traces / task lists on the other hand could reveal some interesting stuff.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/267101622","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-267101622","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":267101622,"node_id":"MDEyOklzc3VlQ29tbWVudDI2NzEwMTYyMg==","user":{"login":"blinken","id":250444,"node_id":"MDQ6VXNlcjI1MDQ0NA==","avatar_url":"https://avatars0.githubusercontent.com/u/250444?v=4","gravatar_id":"","url":"https://api.github.com/users/blinken","html_url":"https://github.com/blinken","followers_url":"https://api.github.com/users/blinken/followers","following_url":"https://api.github.com/users/blinken/following{/other_user}","gists_url":"https://api.github.com/users/blinken/gists{/gist_id}","starred_url":"https://api.github.com/users/blinken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/blinken/subscriptions","organizations_url":"https://api.github.com/users/blinken/orgs","repos_url":"https://api.github.com/users/blinken/repos","events_url":"https://api.github.com/users/blinken/events{/privacy}","received_events_url":"https://api.github.com/users/blinken/received_events","type":"User","site_admin":false},"created_at":"2016-12-14T17:41:26Z","updated_at":"2016-12-14T17:41:26Z","author_association":"NONE","body":"I've sent the data through except the stack traces (which I'll follow up with tomorrow morning).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/269618906","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-269618906","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":269618906,"node_id":"MDEyOklzc3VlQ29tbWVudDI2OTYxODkwNg==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2016-12-29T11:43:11Z","updated_at":"2016-12-29T11:43:11Z","author_association":"CONTRIBUTOR","body":"As some of the conversation went by e-mail (sharing private logs etc.), I'm going to quickly summarize our findings here.\r\n\r\nThe endless recovery loop can happen when the initial connection between recovery target and source is prematurely closed. A fix has been made in #22325 that detects this situation and correctly initiates a second recovery attempt instead of looping endlessly. The fix will be released as part of ES v5.1.2.\r\n\r\nWe've also investigated why the connection could have been prematurely closed (and suggest a fix, see below). In this specific case we think that is was triggered by an inactivity timeout on the connection. The reason for this can be best explained by a visualization of how the recovery process proceeds at the connection level:\r\n\r\n```\r\nRecovery Source            Recovery Target\r\n\r\n\r\n            (channel 1) start recovery\r\n  <-------------------------------------\r\n\r\n     send file chunk X  (channel 2)\r\n  ------------------------------------->\r\n                    (channel 2)  ack\r\n  <-------------------------------------\r\n\r\n     send file chunk Y  (channel 2)\r\n  ------------------------------------->\r\n                    (channel 2)  ack\r\n  <-------------------------------------\r\n\r\n  ...\r\n\r\n    ack  (channel 1)\r\n  ------------------------------------->\r\n```\r\n\r\nThis should illustrate that while the files are being sent from the recovery source to the target there is an idling channel (channel 1) waiting for the recovery to finish. @blinken confirmed that recoveries / relocations take multiple hours (large shards over slow connections). We therefore suggest configuring / enabling of the following connection keep-alive options:\r\n- ES has support for TCP keepalive (enabled by default - configured at OS level).\r\n- ES also supports an application-level keep-alive setting (`transport.ping_schedule` - disabled by default), see https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html . As it is not a dynamic setting it requires a restart of the nodes to take effect.\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/270128516","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-270128516","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":270128516,"node_id":"MDEyOklzc3VlQ29tbWVudDI3MDEyODUxNg==","user":{"login":"blinken","id":250444,"node_id":"MDQ6VXNlcjI1MDQ0NA==","avatar_url":"https://avatars0.githubusercontent.com/u/250444?v=4","gravatar_id":"","url":"https://api.github.com/users/blinken","html_url":"https://github.com/blinken","followers_url":"https://api.github.com/users/blinken/followers","following_url":"https://api.github.com/users/blinken/following{/other_user}","gists_url":"https://api.github.com/users/blinken/gists{/gist_id}","starred_url":"https://api.github.com/users/blinken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/blinken/subscriptions","organizations_url":"https://api.github.com/users/blinken/orgs","repos_url":"https://api.github.com/users/blinken/repos","events_url":"https://api.github.com/users/blinken/events{/privacy}","received_events_url":"https://api.github.com/users/blinken/received_events","type":"User","site_admin":false},"created_at":"2017-01-03T14:41:49Z","updated_at":"2017-01-03T14:41:49Z","author_association":"NONE","body":"Just to close the circle on my end, the issue has not reoccurred in the last two weeks after setting the following sysctls -\r\n\r\nnet.ipv4.tcp_keepalive_time=300\r\nnet.ipv4.tcp_keepalive_intvl=10\r\nnet.ipv4.tcp_keepalive_probes=18\r\n\r\nI also changed cluster.routing.allocation.node_concurrent_recoveries from 8 to 4, which would reduce network load.\r\n\r\nI have not deployed transport.ping_schedule (but I plan to).\r\n\r\nThanks again to everyone for the assistance here.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/270136022","html_url":"https://github.com/elastic/elasticsearch/issues/22043#issuecomment-270136022","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/22043","id":270136022,"node_id":"MDEyOklzc3VlQ29tbWVudDI3MDEzNjAyMg==","user":{"login":"ywelsch","id":3718355,"node_id":"MDQ6VXNlcjM3MTgzNTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3718355?v=4","gravatar_id":"","url":"https://api.github.com/users/ywelsch","html_url":"https://github.com/ywelsch","followers_url":"https://api.github.com/users/ywelsch/followers","following_url":"https://api.github.com/users/ywelsch/following{/other_user}","gists_url":"https://api.github.com/users/ywelsch/gists{/gist_id}","starred_url":"https://api.github.com/users/ywelsch/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ywelsch/subscriptions","organizations_url":"https://api.github.com/users/ywelsch/orgs","repos_url":"https://api.github.com/users/ywelsch/repos","events_url":"https://api.github.com/users/ywelsch/events{/privacy}","received_events_url":"https://api.github.com/users/ywelsch/received_events","type":"User","site_admin":false},"created_at":"2017-01-03T15:13:35Z","updated_at":"2017-01-03T15:13:35Z","author_association":"CONTRIBUTOR","body":"@blinken Thanks for the update. I'm closing the issue now as we have a fix for v5.1.2 and a confirmed workaround.","performed_via_github_app":null}]