[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73322833","html_url":"https://github.com/elastic/elasticsearch/issues/9604#issuecomment-73322833","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9604","id":73322833,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzIyODMz","user":{"login":"nik9000","id":215970,"node_id":"MDQ6VXNlcjIxNTk3MA==","avatar_url":"https://avatars2.githubusercontent.com/u/215970?v=4","gravatar_id":"","url":"https://api.github.com/users/nik9000","html_url":"https://github.com/nik9000","followers_url":"https://api.github.com/users/nik9000/followers","following_url":"https://api.github.com/users/nik9000/following{/other_user}","gists_url":"https://api.github.com/users/nik9000/gists{/gist_id}","starred_url":"https://api.github.com/users/nik9000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nik9000/subscriptions","organizations_url":"https://api.github.com/users/nik9000/orgs","repos_url":"https://api.github.com/users/nik9000/repos","events_url":"https://api.github.com/users/nik9000/events{/privacy}","received_events_url":"https://api.github.com/users/nik9000/received_events","type":"User","site_admin":false},"created_at":"2015-02-06T22:15:19Z","updated_at":"2015-02-06T22:15:19Z","author_association":"CONTRIBUTOR","body":"Now that I think of it you might want to check  `total_shards_per_node`.  In [production](http://en.wikipedia.org/w/api.php?action=cirrus-settings-dump&srbackend=CirrusSearch&format=json) enwiki_general has that set to 2.  So what you might be seeing is that setting leaking into your environment.  In that case it'd _look_ like the shards were phantom - they would reappear as soon as you made them and Elasticsearch wouldn't be able to assign them.  When I check [beta](http://en.wikipedia.beta.wmflabs.org/w/api.php?action=cirrus-settings-dump&format=json) it looks like it has that set to -1 but I can't be sure something isn't fishy.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73506646","html_url":"https://github.com/elastic/elasticsearch/issues/9604#issuecomment-73506646","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9604","id":73506646,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTA2NjQ2","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-02-09T13:18:06Z","updated_at":"2015-02-09T13:18:06Z","author_association":"CONTRIBUTOR","body":"The `total_shards_per_node` setting is dangerous, because it is an absolute setting, while the other allocation settings provide preferences.  \n\nYou don't mention how many replicas were configured?  If you had any replicas then, when you shut down one node, the replicas should have been promoted to primaries.  Presumably you had replicas set to zero, which is why your cluster went red instead of yellow.\n\nThen you deleted the index (removing the remaining primary shards) and restarted the node, which caused the dangling primaries to be reimported.  But of course you were missing some primaries (because you deleted the index), so the index went red.\n\nYou haven't provided the cluster state, or the command you used to reroute shards, so it is difficult to figure out if you did something reasonable, or tried to allocate primary shards back to the nodes where they were already allocated (which is what the exception seems to indicate).\n\nCould you provide more info, eg the settings you're using, the output of `GET _cat/shards?v` at each step, the reroute command you used, the cluster health (`?level=shards`) etc\n","performed_via_github_app":null}]