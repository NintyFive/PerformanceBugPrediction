[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/69459737","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-69459737","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":69459737,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDU5NzM3","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-01-10T15:36:16Z","updated_at":"2015-01-10T15:36:16Z","author_association":"MEMBER","body":"When the shard got unstuck, did it become green on the same node or did it move to another node? Also , which version of Es was it?\n\nOn Sat, Jan 10, 2015 at 12:10 AM, Pius notifications@github.com wrote:\n\n> A primary shard was stuck in RELOCATION state.  Recovery API shows that all files have completed at 100%.  The stage shows that it is TRANSLOG.  But it has been sitting there for > 15 hours.\n> \n> ``` json\n>   {\n>         \"id\": 0,\n>         \"type\": \"RELOCATION\",\n>         \"stage\": \"TRANSLOG\",\n>         \"primary\": true,\n>         \"start_time\": \"16443.3d\",\n>         \"start_time_in_millis\": 1420704589802,\n>         \"stop_time\": \"0s\",\n>         \"stop_time_in_millis\": 0,\n>         \"total_time\": \"15.1h\",\n>         \"total_time_in_millis\": 54706836,\n>         \"source\": {\n>           \"id\": \"0eltfic0RdmC9v-Yb17Wzw\",\n>           \"host\": \"elasticsearch1.localdomain\",\n>           \"transport_address\": \"inet[/IP:9300]\",\n>           \"ip\": \"127.0.1.1\",\n>           \"name\": \"elasticsearch1.localdomain\"\n>         },\n>         \"target\": {\n>           \"id\": \"ctCRm_huQsSBoTobhmqJdg\",\n>           \"host\": \"elasticsearch8.localdomain\",\n>           \"transport_address\": \"inet[/IP:9300]\",\n>           \"ip\": \"127.0.1.1\",\n>           \"name\": \"elasticsearch8.localdomain\"\n>         },\n>         \"index\": {\n>           \"files\": {\n>             \"total\": 318,\n>             \"reused\": 0,\n>             \"recovered\": 318,\n>             \"percent\": \"100.0%\",\n>             \"details\": [\n>              ... list of files\n>             ]\n>           },\n>           \"bytes\": {\n>             \"total\": 58111147927,\n>             \"reused\": 0,\n>             \"recovered\": 58111147927,\n>             \"percent\": \"100.0%\"\n>           },\n>           \"total_time\": \"0s\",\n>           \"total_time_in_millis\": 0\n>         },\n>         \"translog\": {\n>           \"recovered\": 24685532,\n>           \"total_time\": \"0s\",\n>           \"total_time_in_millis\": 0\n>         },\n>         \"start\": {\n>           \"check_index_time\": \"0s\",\n>           \"check_index_time_in_millis\": 0,\n>           \"total_time\": \"0s\",\n>           \"total_time_in_millis\": 0\n>         }\n>       }\n> ```\n> \n> Running a reroute command with no post body got it to unstuck:\n> \n> ```\n> curl -XPOST 'localhost:9200/_cluster/reroute?pretty&explain=true'\n> ```\n> \n> ## Not sure how it got into this state (no log entries related to recovery of this shard in the data or master logs) and why we have to run an empty reroute request to get it unstuck.\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/elasticsearch/elasticsearch/issues/9226\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/69466115","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-69466115","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":69466115,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDY2MTE1","user":{"login":"ppf2","id":7216393,"node_id":"MDQ6VXNlcjcyMTYzOTM=","avatar_url":"https://avatars0.githubusercontent.com/u/7216393?v=4","gravatar_id":"","url":"https://api.github.com/users/ppf2","html_url":"https://github.com/ppf2","followers_url":"https://api.github.com/users/ppf2/followers","following_url":"https://api.github.com/users/ppf2/following{/other_user}","gists_url":"https://api.github.com/users/ppf2/gists{/gist_id}","starred_url":"https://api.github.com/users/ppf2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ppf2/subscriptions","organizations_url":"https://api.github.com/users/ppf2/orgs","repos_url":"https://api.github.com/users/ppf2/repos","events_url":"https://api.github.com/users/ppf2/events{/privacy}","received_events_url":"https://api.github.com/users/ppf2/received_events","type":"User","site_admin":false},"created_at":"2015-01-10T18:25:37Z","updated_at":"2015-01-10T18:26:33Z","author_association":"MEMBER","body":"Once it got unstuck, it became green on the expected target node, thx.  This is 1.4.2.  The following is from the output of running the empty reroute command (which got it unstuck):\n\n``` json\n          \"shards\" : {\n            \"0\" : [ {\n              \"state\" : \"STARTED\",\n              \"primary\" : true,\n              \"node\" : \"ctCRm_huQsSBoTobhmqJdg\",\n              \"relocating_node\" : null,\n              \"shard\" : 0,\n              \"index\" : \"index_name\"\n            } ],\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/69774218","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-69774218","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":69774218,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Nzc0MjE4","user":{"login":"kstaken","id":283724,"node_id":"MDQ6VXNlcjI4MzcyNA==","avatar_url":"https://avatars0.githubusercontent.com/u/283724?v=4","gravatar_id":"","url":"https://api.github.com/users/kstaken","html_url":"https://github.com/kstaken","followers_url":"https://api.github.com/users/kstaken/followers","following_url":"https://api.github.com/users/kstaken/following{/other_user}","gists_url":"https://api.github.com/users/kstaken/gists{/gist_id}","starred_url":"https://api.github.com/users/kstaken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kstaken/subscriptions","organizations_url":"https://api.github.com/users/kstaken/orgs","repos_url":"https://api.github.com/users/kstaken/repos","events_url":"https://api.github.com/users/kstaken/events{/privacy}","received_events_url":"https://api.github.com/users/kstaken/received_events","type":"User","site_admin":false},"created_at":"2015-01-13T16:37:25Z","updated_at":"2015-01-13T16:37:25Z","author_association":"NONE","body":"I've been seeing what I think is a similar issue. We upgraded from 1.3.5 to 1.4.2 last week but now from time to time will see a shard get stuck in RELO or INIT. Most recent example was overnight after a node was restarted late yesterday. It moved a shard on restart and this morning it was still sitting in translog state well beyond what would be normal recovery time. I canceled the allocation and the shard is now allocating on another node currently in the translog phase. It hasn't been running long enough to say if there is a problem with this allocation. There seems to be nothing in the logs that indicates there was a problem. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/69778954","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-69778954","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":69778954,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Nzc4OTU0","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-01-13T17:03:24Z","updated_at":"2015-01-13T17:03:24Z","author_association":"MEMBER","body":"@kstaken it's a bit hard to debug those things. It might an instance of #8720 . Did you see any networking related issues in the logs? potentially this message \"failed to send shard started\" ?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/69800779","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-69800779","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":69800779,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODAwNzc5","user":{"login":"kstaken","id":283724,"node_id":"MDQ6VXNlcjI4MzcyNA==","avatar_url":"https://avatars0.githubusercontent.com/u/283724?v=4","gravatar_id":"","url":"https://api.github.com/users/kstaken","html_url":"https://github.com/kstaken","followers_url":"https://api.github.com/users/kstaken/followers","following_url":"https://api.github.com/users/kstaken/following{/other_user}","gists_url":"https://api.github.com/users/kstaken/gists{/gist_id}","starred_url":"https://api.github.com/users/kstaken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kstaken/subscriptions","organizations_url":"https://api.github.com/users/kstaken/orgs","repos_url":"https://api.github.com/users/kstaken/repos","events_url":"https://api.github.com/users/kstaken/events{/privacy}","received_events_url":"https://api.github.com/users/kstaken/received_events","type":"User","site_admin":false},"created_at":"2015-01-13T19:12:05Z","updated_at":"2015-01-13T19:12:05Z","author_association":"NONE","body":"Nothing in the logs that I can see. I've seen this happen maybe 4-5 times since we upgraded to 1.4.2. Don't remember seeing this particular issue on 1.3.0 or 1.3.5. The new allocation of this same shard is also taking a long time. Is there any way to see deeper into the translog activity? \n\nIf I look at the initializing shard on disk I see the translog files grow and then disappear to be replaced by a new file that repeats the cycle. There's also a second translog file sitting in the same directory that doesn't seem to change.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/69816044","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-69816044","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":69816044,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODE2MDQ0","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-01-13T20:42:40Z","updated_at":"2015-01-13T20:42:40Z","author_association":"MEMBER","body":"@kstaken can you enable trace logging on the `indices.recovery` log and post that information here?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70112439","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-70112439","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":70112439,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMTEyNDM5","user":{"login":"pickypg","id":1501235,"node_id":"MDQ6VXNlcjE1MDEyMzU=","avatar_url":"https://avatars2.githubusercontent.com/u/1501235?v=4","gravatar_id":"","url":"https://api.github.com/users/pickypg","html_url":"https://github.com/pickypg","followers_url":"https://api.github.com/users/pickypg/followers","following_url":"https://api.github.com/users/pickypg/following{/other_user}","gists_url":"https://api.github.com/users/pickypg/gists{/gist_id}","starred_url":"https://api.github.com/users/pickypg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pickypg/subscriptions","organizations_url":"https://api.github.com/users/pickypg/orgs","repos_url":"https://api.github.com/users/pickypg/repos","events_url":"https://api.github.com/users/pickypg/events{/privacy}","received_events_url":"https://api.github.com/users/pickypg/received_events","type":"User","site_admin":false},"created_at":"2015-01-15T16:23:40Z","updated_at":"2015-02-09T16:55:36Z","author_association":"MEMBER","body":"This recently happened with a user that was about to delete the transaction log a day later, but right before they were going to do it, then it finished.\n\nReady to apply the logger for next time though.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70116006","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-70116006","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":70116006,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMTE2MDA2","user":{"login":"kstaken","id":283724,"node_id":"MDQ6VXNlcjI4MzcyNA==","avatar_url":"https://avatars0.githubusercontent.com/u/283724?v=4","gravatar_id":"","url":"https://api.github.com/users/kstaken","html_url":"https://github.com/kstaken","followers_url":"https://api.github.com/users/kstaken/followers","following_url":"https://api.github.com/users/kstaken/following{/other_user}","gists_url":"https://api.github.com/users/kstaken/gists{/gist_id}","starred_url":"https://api.github.com/users/kstaken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kstaken/subscriptions","organizations_url":"https://api.github.com/users/kstaken/orgs","repos_url":"https://api.github.com/users/kstaken/repos","events_url":"https://api.github.com/users/kstaken/events{/privacy}","received_events_url":"https://api.github.com/users/kstaken/received_events","type":"User","site_admin":false},"created_at":"2015-01-15T16:42:15Z","updated_at":"2015-01-15T16:42:15Z","author_association":"NONE","body":"@bleskes I figured out the issue with the second allocation of the problem shard. When the shard got stuck the translog had grown so large  it couldn't replay it on recovery. It appeared to be replaying the translog at a rate that was about 50% of the rate new records were being added to it. It's conceivable that the shard appearing to be stuck originally may have been the result of the same thing. Now that I know what to look for I can confirm that the next time this happens.  The normal replicas seem to have no issue keeping up, is there throttling on the replay of translogs during recovery?\n\nBTW, to recover this shard I canceled the allocation and forced a fresh allocation on a new node. That seemed to allow the primary to clear the translog and then it was able to get back in sync.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71026021","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71026021","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71026021,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMDI2MDIx","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-01-22T14:17:11Z","updated_at":"2015-01-22T14:17:11Z","author_association":"MEMBER","body":"@kstaken sorry for the late response. I was travelling.\n\n> The normal replicas seem to have no issue keeping up, is there throttling on the replay of translogs during recovery?\n\nThere is no throttling but it is dealt with using a single thread. That's less then the amount of threads that are available for indexing. That said, the recovery works in two stages - it first acquires a snapshot of the current translog and performs that on the replica (while indexing is on going on the primary). Once that is done, it blocks the indexing on the primary and replicate the last operations.  So in theory it should always be able to catch up.\n\nI do share the feeling that this is due to a very large transaction log. If this happens again, do you mind sharing this information, issued once and then 5m later:  `GET _recovery` , `GET {index_name}/_stats?level=shards` and `GET /_nodes/_local/hot_threads` on the target node of the recovery.  This will tell us whether the recovery is really stuck or just making progress really slowly. The last hot threads call will help shed light on why it's slow (if that's the case).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71094073","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71094073","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71094073,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMDk0MDcz","user":{"login":"kstaken","id":283724,"node_id":"MDQ6VXNlcjI4MzcyNA==","avatar_url":"https://avatars0.githubusercontent.com/u/283724?v=4","gravatar_id":"","url":"https://api.github.com/users/kstaken","html_url":"https://github.com/kstaken","followers_url":"https://api.github.com/users/kstaken/followers","following_url":"https://api.github.com/users/kstaken/following{/other_user}","gists_url":"https://api.github.com/users/kstaken/gists{/gist_id}","starred_url":"https://api.github.com/users/kstaken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kstaken/subscriptions","organizations_url":"https://api.github.com/users/kstaken/orgs","repos_url":"https://api.github.com/users/kstaken/repos","events_url":"https://api.github.com/users/kstaken/events{/privacy}","received_events_url":"https://api.github.com/users/kstaken/received_events","type":"User","site_admin":false},"created_at":"2015-01-22T20:38:19Z","updated_at":"2015-01-22T20:38:19Z","author_association":"NONE","body":"@bleskes Thanks. I've had a couple more instances of the issue and all have been on the same shard and I've been able to confirm on that shard it's a case of translog replay not being able to keep up with the rate of indexing. If I stop indexing, recovery will complete. \n\nTurns out we have a hotspot on that shard due to routing, it had grown much too large at over 1billion docs and 220GB in size. We're in the process of re-indexing to deal with this but at this size the translog on the primary seemed to be growing at about 2x the rate of consumption on the recovering node and I'm pretty sure it would never catch up. I'm not sure if this is an actual issue now or just an example of why a 220GB shard is not a good thing to have.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71207805","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71207805","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71207805,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMjA3ODA1","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-01-23T15:18:23Z","updated_at":"2015-01-23T15:18:23Z","author_association":"MEMBER","body":"@kstaken thx. Replying in reverse order.\n\n>  I'm not sure if this is an actual issue now or just an example of why a 220GB shard is not a good thing to have.\n\nA 220GB shard is indeed unusually big but it _should_ work. I \n\n> this size the translog on the primary seemed to be growing at about 2x the rate of consumption on the recovering node.\n\nI see. For what it's worth, as long as there is an ongoing recovery the translog will not be trimmed as we need to make sure everything is replicated. Once replication is done, the translog will be dropped as one.\n\n> 2x the rate of consumption on the recovering node and I'm pretty sure it would never catch up.\n\nThe replica is guaranteed to catch up because once it's will pause indexing in order to complete the operation. We need to figure out why it's so slow to deal with the initial snapshot when it allows concurrent indexing. When you say 2x the rate of consumption, do you measure the operation count in `GET {index}/_recovery} output?\n\n>  If I stop indexing, recovery will complete.\n\nThat's interesting. Maybe indexing just put load on the machine. I've chasing this for the last couple of days and if you can share the following, it would be very helpful: \n\n1) Did you see something like this log message from the target shard, while the recovery was going on: \"now throttling indexing: numMergesInFlight={}, maxNumMerges={}\" ?\n2) Can you run `GET _nodes/_local/hot_threads` a couple of time on the target node and on the source node? I want to see if there is any clear hot spots?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71265388","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71265388","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71265388,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMjY1Mzg4","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-01-23T21:03:09Z","updated_at":"2015-01-23T21:03:09Z","author_association":"MEMBER","body":"@kstaken one more question - what environment are you running on? which OS?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71283566","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71283566","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71283566,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMjgzNTY2","user":{"login":"kstaken","id":283724,"node_id":"MDQ6VXNlcjI4MzcyNA==","avatar_url":"https://avatars0.githubusercontent.com/u/283724?v=4","gravatar_id":"","url":"https://api.github.com/users/kstaken","html_url":"https://github.com/kstaken","followers_url":"https://api.github.com/users/kstaken/followers","following_url":"https://api.github.com/users/kstaken/following{/other_user}","gists_url":"https://api.github.com/users/kstaken/gists{/gist_id}","starred_url":"https://api.github.com/users/kstaken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kstaken/subscriptions","organizations_url":"https://api.github.com/users/kstaken/orgs","repos_url":"https://api.github.com/users/kstaken/repos","events_url":"https://api.github.com/users/kstaken/events{/privacy}","received_events_url":"https://api.github.com/users/kstaken/received_events","type":"User","site_admin":false},"created_at":"2015-01-23T23:19:53Z","updated_at":"2015-01-23T23:19:53Z","author_association":"NONE","body":"@bleskes I've been measuring the output by looking at {index}/_status?recovery=true and looking at the translog.recovered on the replica vs. translog.operations on the primary. Is that a valid thing to do?\n\nWhen this has occurred there has been nothing in the logs indicating any kind of issue, not even the throttling message. \n\nIf I see the problem again I'll try to come up with the other things you asked for however, I'll be retiring this index in the next couple days once the re-indexing process completes. I thought I had seen the issue on other shards but that was before I was paying specific attention to this shard and the last four occurrences have all been on the same shard so I'm doubting that I've seen it elsewhere now. \n\nOur environment is Ubuntu 14.04 on physical hardware. ES 1.4.2 on Java 8. We have 72 ES nodes of two types, one with SSDs and one with HDs. The impacted index is on nodes with SSDs. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71285209","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71285209","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71285209,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMjg1MjA5","user":{"login":"kstaken","id":283724,"node_id":"MDQ6VXNlcjI4MzcyNA==","avatar_url":"https://avatars0.githubusercontent.com/u/283724?v=4","gravatar_id":"","url":"https://api.github.com/users/kstaken","html_url":"https://github.com/kstaken","followers_url":"https://api.github.com/users/kstaken/followers","following_url":"https://api.github.com/users/kstaken/following{/other_user}","gists_url":"https://api.github.com/users/kstaken/gists{/gist_id}","starred_url":"https://api.github.com/users/kstaken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kstaken/subscriptions","organizations_url":"https://api.github.com/users/kstaken/orgs","repos_url":"https://api.github.com/users/kstaken/repos","events_url":"https://api.github.com/users/kstaken/events{/privacy}","received_events_url":"https://api.github.com/users/kstaken/received_events","type":"User","site_admin":false},"created_at":"2015-01-23T23:35:28Z","updated_at":"2015-01-23T23:35:28Z","author_association":"NONE","body":"@bleskes I should also add that the node holding the primary here is indeed under very heavy load due to indexing. It will bounce around 60-90 in load average, however the load on the replicas is not elevated and is consistent with other nodes in the cluster. Stopping indexing will return the load to normal. Since it's a major hot spot, it's conceivable that shard could be receiving well over 10,000 indexing requests per second. The index overall is receiving 50K-100K/sec, sometimes more. Those requests are also 'create' requests with unique IDs and most will be dropped as duplicates. The actual write volume on the index overall is less than 1000/sec.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71288082","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71288082","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71288082,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMjg4MDgy","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2015-01-24T00:06:09Z","updated_at":"2015-01-24T00:06:09Z","author_association":"CONTRIBUTOR","body":"One change in 1.4.0 was to disable loading bloom filters by default since we made other performance improvements that should have made them unnecessary in most cases: https://github.com/elasticsearch/elasticsearch/pull/6959\n\nI wonder if that is causing the performance issues here?  Can you try setting index.codec.bloom.load to true and see if it makes a difference?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71313433","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71313433","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71313433,"node_id":"MDEyOklzc3VlQ29tbWVudDcxMzEzNDMz","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-01-24T11:55:06Z","updated_at":"2015-01-24T11:55:06Z","author_association":"MEMBER","body":"@kstaken thx. The index _status api is a good way, but it's deprecated and replaced with the [_recovery API](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-recovery.html#indices-recovery) . can you let us how it goes with @mikemccand 's suggestion?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71499181","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71499181","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71499181,"node_id":"MDEyOklzc3VlQ29tbWVudDcxNDk5MTgx","user":{"login":"kstaken","id":283724,"node_id":"MDQ6VXNlcjI4MzcyNA==","avatar_url":"https://avatars0.githubusercontent.com/u/283724?v=4","gravatar_id":"","url":"https://api.github.com/users/kstaken","html_url":"https://github.com/kstaken","followers_url":"https://api.github.com/users/kstaken/followers","following_url":"https://api.github.com/users/kstaken/following{/other_user}","gists_url":"https://api.github.com/users/kstaken/gists{/gist_id}","starred_url":"https://api.github.com/users/kstaken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kstaken/subscriptions","organizations_url":"https://api.github.com/users/kstaken/orgs","repos_url":"https://api.github.com/users/kstaken/repos","events_url":"https://api.github.com/users/kstaken/events{/privacy}","received_events_url":"https://api.github.com/users/kstaken/received_events","type":"User","site_admin":false},"created_at":"2015-01-26T17:24:10Z","updated_at":"2015-01-26T17:24:10Z","author_association":"NONE","body":"Unfortunately I'm not able to test re-enabling bloom filters as our re-indexing process completed over the weekend and the problem index was removed from usage. I'll have the index around for a couple more days and then it will be deleted completely as the variation in shard sizes is also causing disk allocation problems.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71509780","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71509780","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71509780,"node_id":"MDEyOklzc3VlQ29tbWVudDcxNTA5Nzgw","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-01-26T18:22:30Z","updated_at":"2015-01-26T18:22:30Z","author_association":"MEMBER","body":"@kstaken OK. Let us know if this happens again.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71525745","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71525745","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71525745,"node_id":"MDEyOklzc3VlQ29tbWVudDcxNTI1NzQ1","user":{"login":"kstaken","id":283724,"node_id":"MDQ6VXNlcjI4MzcyNA==","avatar_url":"https://avatars0.githubusercontent.com/u/283724?v=4","gravatar_id":"","url":"https://api.github.com/users/kstaken","html_url":"https://github.com/kstaken","followers_url":"https://api.github.com/users/kstaken/followers","following_url":"https://api.github.com/users/kstaken/following{/other_user}","gists_url":"https://api.github.com/users/kstaken/gists{/gist_id}","starred_url":"https://api.github.com/users/kstaken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kstaken/subscriptions","organizations_url":"https://api.github.com/users/kstaken/orgs","repos_url":"https://api.github.com/users/kstaken/repos","events_url":"https://api.github.com/users/kstaken/events{/privacy}","received_events_url":"https://api.github.com/users/kstaken/received_events","type":"User","site_admin":false},"created_at":"2015-01-26T19:52:52Z","updated_at":"2015-01-26T19:52:52Z","author_association":"NONE","body":"@bleskes sorry I can't test enabling bloom filters to confirm, but I did some more digging and looking at the historical CPU usage of the node holding the problem primary does show a dramatic increase in load right after it was upgraded to 1.4.2. Prior to 1.4.2 we didn't have any nodes that showed consistently high load, after the upgrade which ever node this shard resided on pushed 80-90 load non stop. \n\nDoes this imply we should enable bloom filters on the new index for the use case where we're generating the IDs?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/71526154","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-71526154","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":71526154,"node_id":"MDEyOklzc3VlQ29tbWVudDcxNTI2MTU0","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-01-26T19:55:00Z","updated_at":"2015-01-26T19:55:00Z","author_association":"MEMBER","body":"@kstaken it would be great if can do a \"before\" and \"after\" check regarding the effect bloom filters have on your index.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73217115","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-73217115","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":73217115,"node_id":"MDEyOklzc3VlQ29tbWVudDczMjE3MTE1","user":{"login":"drax68","id":763189,"node_id":"MDQ6VXNlcjc2MzE4OQ==","avatar_url":"https://avatars3.githubusercontent.com/u/763189?v=4","gravatar_id":"","url":"https://api.github.com/users/drax68","html_url":"https://github.com/drax68","followers_url":"https://api.github.com/users/drax68/followers","following_url":"https://api.github.com/users/drax68/following{/other_user}","gists_url":"https://api.github.com/users/drax68/gists{/gist_id}","starred_url":"https://api.github.com/users/drax68/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/drax68/subscriptions","organizations_url":"https://api.github.com/users/drax68/orgs","repos_url":"https://api.github.com/users/drax68/repos","events_url":"https://api.github.com/users/drax68/events{/privacy}","received_events_url":"https://api.github.com/users/drax68/received_events","type":"User","site_admin":false},"created_at":"2015-02-06T10:32:12Z","updated_at":"2015-02-06T10:32:12Z","author_association":"NONE","body":"Bumped into same issue during shards initialization on es 1.4.2, tested with enabled bloom filters - this doesn't helped. Only deletion of transactions logs helped to finish initialization.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73220092","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-73220092","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":73220092,"node_id":"MDEyOklzc3VlQ29tbWVudDczMjIwMDky","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-06T10:58:16Z","updated_at":"2015-02-06T10:58:16Z","author_association":"MEMBER","body":"@drax68 do you know how big the translog was before you deleted it? Did you see any index throttling messages in the log? Also, how long was it \"stuck\" in the TRANSLOG phase?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73221153","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-73221153","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":73221153,"node_id":"MDEyOklzc3VlQ29tbWVudDczMjIxMTUz","user":{"login":"drax68","id":763189,"node_id":"MDQ6VXNlcjc2MzE4OQ==","avatar_url":"https://avatars3.githubusercontent.com/u/763189?v=4","gravatar_id":"","url":"https://api.github.com/users/drax68","html_url":"https://github.com/drax68","followers_url":"https://api.github.com/users/drax68/followers","following_url":"https://api.github.com/users/drax68/following{/other_user}","gists_url":"https://api.github.com/users/drax68/gists{/gist_id}","starred_url":"https://api.github.com/users/drax68/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/drax68/subscriptions","organizations_url":"https://api.github.com/users/drax68/orgs","repos_url":"https://api.github.com/users/drax68/repos","events_url":"https://api.github.com/users/drax68/events{/privacy}","received_events_url":"https://api.github.com/users/drax68/received_events","type":"User","site_admin":false},"created_at":"2015-02-06T11:07:45Z","updated_at":"2015-02-06T11:07:45Z","author_association":"NONE","body":"4-6 Gb translog, on replica it's size was spinning around 200Mb. Shard size about 40Gb.\n\nIt was stuck for hours, then I have to delete translogs to complete initialization.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73466115","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-73466115","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":73466115,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDY2MTE1","user":{"login":"avleen","id":539525,"node_id":"MDQ6VXNlcjUzOTUyNQ==","avatar_url":"https://avatars1.githubusercontent.com/u/539525?v=4","gravatar_id":"","url":"https://api.github.com/users/avleen","html_url":"https://github.com/avleen","followers_url":"https://api.github.com/users/avleen/followers","following_url":"https://api.github.com/users/avleen/following{/other_user}","gists_url":"https://api.github.com/users/avleen/gists{/gist_id}","starred_url":"https://api.github.com/users/avleen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/avleen/subscriptions","organizations_url":"https://api.github.com/users/avleen/orgs","repos_url":"https://api.github.com/users/avleen/repos","events_url":"https://api.github.com/users/avleen/events{/privacy}","received_events_url":"https://api.github.com/users/avleen/received_events","type":"User","site_admin":false},"created_at":"2015-02-09T07:12:27Z","updated_at":"2015-02-09T07:12:27Z","author_association":"NONE","body":"This happened to us over the weekend too.\nFriday night we upgraded from 1.3.2 to 1.4.2.\nWe've seen multiple replica recoveries since then (either due to a cluster restart or a node failing and replicas needing to be rebuilt).\n\nIn most cases it's fine. But sometimes, just sometimes, the target host doesn't finish catching up the translog and it gets pretty big.\nRight now I have this happening, and the translog on the source host is 22Gb.\nOn the target host, the translog grows to ~200mb, and then rotates to a new file. \nThe source host was sending to the target at ~750kbit/sec, which seem really really slow. The servers have 10gbit, are on the same switch, and all have SSDs. This made no sense.\n\nUnfortunately I wasn't able to get `hot_threads` output this time, but when it happens again I will.\nIn the time it took me to finish fixing the issue and write this note, the translog on the source host grew to 24Gb.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73468701","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-73468701","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":73468701,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDY4NzAx","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-09T07:45:54Z","updated_at":"2015-02-09T07:45:54Z","author_association":"MEMBER","body":"@avleen any chance you can check in the logs of the node hosting the primary of the stuck shard for \"now throttling indexing\" (which means it's  #9394 )? if happen to have your logs in debug mode, check for lines with \"recovery_mapping_check\". If they are too far apart, it will indicate it's #9575\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73468835","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-73468835","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":73468835,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDY4ODM1","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-09T07:47:25Z","updated_at":"2015-02-09T07:47:25Z","author_association":"MEMBER","body":"@drax68 see my previous comment - I don't know if you still have the logs of the node with the primary shard, but if you do it would be great if you can check them.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73469041","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-73469041","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":73469041,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDY5MDQx","user":{"login":"avleen","id":539525,"node_id":"MDQ6VXNlcjUzOTUyNQ==","avatar_url":"https://avatars1.githubusercontent.com/u/539525?v=4","gravatar_id":"","url":"https://api.github.com/users/avleen","html_url":"https://github.com/avleen","followers_url":"https://api.github.com/users/avleen/followers","following_url":"https://api.github.com/users/avleen/following{/other_user}","gists_url":"https://api.github.com/users/avleen/gists{/gist_id}","starred_url":"https://api.github.com/users/avleen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/avleen/subscriptions","organizations_url":"https://api.github.com/users/avleen/orgs","repos_url":"https://api.github.com/users/avleen/repos","events_url":"https://api.github.com/users/avleen/events{/privacy}","received_events_url":"https://api.github.com/users/avleen/received_events","type":"User","site_admin":false},"created_at":"2015-02-09T07:49:41Z","updated_at":"2015-02-09T07:49:41Z","author_association":"NONE","body":"And it happened again.\n\nhot_threads from the target host:\n\n```\n::: [logdb50][xne7qsKqQnSeacIQnM_uyQ][logdb5][inet[logdb50]]{local=false, master=false}\n\n   92.8% (464ms out of 500ms) cpu usage by thread 'elasticsearch[logdb50][generic][T#353]'\n     7/10 snapshots sharing following 24 elements\n       sun.nio.ch.NativeThread.current(Native Method)\n       sun.nio.ch.NativeThreadSet.add(NativeThreadSet.java:46)\n       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:695)\n       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:684)\n       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:179)\n       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)\n       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)\n       org.apache.lucene.store.DataInput.readVInt(DataInput.java:122)\n       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)\n       org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:152)\n       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:506)\n       org.elasticsearch.common.lucene.uid.PerThreadIDAndVersionLookup.lookup(PerThreadIDAndVersionLookup.java:104)\n       org.elasticsearch.common.lucene.uid.Versions.loadDocIdAndVersion(Versions.java:150)\n       org.elasticsearch.common.lucene.uid.Versions.loadVersion(Versions.java:161)\n       org.elasticsearch.index.engine.internal.InternalEngine.loadCurrentVersionFromIndex(InternalEngine.java:1381)\n       org.elasticsearch.index.engine.internal.InternalEngine.innerCreate(InternalEngine.java:427)\n       org.elasticsearch.index.engine.internal.InternalEngine.create(InternalEngine.java:404)\n       org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryOperation(InternalIndexShard.java:779)\n       org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:433)\n       org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:412)\n       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\n       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n       java.lang.Thread.run(Thread.java:744)\n     3/10 snapshots sharing following 6 elements\n       org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:433)\n       org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:412)\n       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\n       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n       java.lang.Thread.run(Thread.java:744)\n\n    1.3% (6.7ms out of 500ms) cpu usage by thread 'elasticsearch[logdb50][[transport_server_worker.default]][T#7]{New I/O worker #34}'\n     10/10 snapshots sharing following 15 elements\n       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)\n       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)\n       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)\n       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n       java.lang.Thread.run(Thread.java:744)\n\n    0.8% (3.8ms out of 500ms) cpu usage by thread 'elasticsearch[logdb50][[transport_server_worker.default]][T#6]{New I/O worker #33}'\n     10/10 snapshots sharing following 15 elements\n       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)\n       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)\n       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)\n       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n       java.lang.Thread.run(Thread.java:744)\n```\n\nSource host:\n\n```\n::: [logdb40][aJwrteLzRRedjpeRH-XTuA][logdb40][inet[/:9300]]{local=false, master=false}\n\n    4.2% (20.7ms out of 500ms) cpu usage by thread 'elasticsearch[logdb40][generic][T#562]'\n     10/10 snapshots sharing following 21 elements\n       sun.misc.Unsafe.park(Native Method)\n       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n       java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\n       java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)\n       java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)\n       org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)\n       org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)\n       org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:45)\n       org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)\n       org.elasticsearch.indices.recovery.RecoverySource$1.sendSnapshot(RecoverySource.java:432)\n       org.elasticsearch.indices.recovery.RecoverySource$1.phase2(RecoverySource.java:300)\n       org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1132)\n       org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:654)\n       org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:137)\n       org.elasticsearch.indices.recovery.RecoverySource.access$2600(RecoverySource.java:74)\n       org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:464)\n       org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:450)\n       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\n       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n       java.lang.Thread.run(Thread.java:744)\n\n    0.1% (611micros out of 500ms) cpu usage by thread 'elasticsearch[logdb40][transport_client_worker][T#9]{New I/O worker #9}'\n     10/10 snapshots sharing following 15 elements\n       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)\n       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)\n       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)\n       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n       java.lang.Thread.run(Thread.java:744)\n\n    0.1% (410.3micros out of 500ms) cpu usage by thread 'elasticsearch[logdb40][transport_client_worker][T#10]{New I/O worker #10}'\n     10/10 snapshots sharing following 15 elements\n       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)\n       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)\n       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)\n       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)\n       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n       java.lang.Thread.run(Thread.java:744)\n```\n\nWe can't keep stopping nodes to delete their translog, as it just results in the problem jumping to a different host.\n\nNothing mentioning throttling at all :(\nOn these hosts, there is a lot more traffic between them right now, about 15mbit/sec. Indexing from logstash is temporarily off. I'm hoping that lets the translog catch up.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73469368","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-73469368","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":73469368,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDY5MzY4","user":{"login":"avleen","id":539525,"node_id":"MDQ6VXNlcjUzOTUyNQ==","avatar_url":"https://avatars1.githubusercontent.com/u/539525?v=4","gravatar_id":"","url":"https://api.github.com/users/avleen","html_url":"https://github.com/avleen","followers_url":"https://api.github.com/users/avleen/followers","following_url":"https://api.github.com/users/avleen/following{/other_user}","gists_url":"https://api.github.com/users/avleen/gists{/gist_id}","starred_url":"https://api.github.com/users/avleen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/avleen/subscriptions","organizations_url":"https://api.github.com/users/avleen/orgs","repos_url":"https://api.github.com/users/avleen/repos","events_url":"https://api.github.com/users/avleen/events{/privacy}","received_events_url":"https://api.github.com/users/avleen/received_events","type":"User","site_admin":false},"created_at":"2015-02-09T07:53:49Z","updated_at":"2015-02-09T07:53:49Z","author_association":"NONE","body":"Typical, computers making me look bad.\nJust after I sent that last message, this popped into the log for the target host:\n\n```\n[2015-02-09 07:50:32,376][INFO ][index.engine.internal    ] [logdb50] [logstash-2015.02.09][25] now throttling indexing: numMergesInFlight=9, maxNumMerges=8\n[2015-02-09 07:50:47,503][INFO ][index.engine.internal    ] [logdb50] [logstash-2015.02.09][25] stop throttling indexing: numMergesInFlight=7, maxNumMerges=8\n```\n\nIt looks like this came in right around the time the replica the translog caught up. It only happened one on this node, and possibly once on another node at the time when it got over this problem.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73469525","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-73469525","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":73469525,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDY5NTI1","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2015-02-09T07:55:35Z","updated_at":"2015-02-09T07:55:35Z","author_association":"MEMBER","body":"@avleen this might be part of the problem, but it only happened once, it's not a complete story.  I reached out through IRC, so we can iterate faster.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/73476714","html_url":"https://github.com/elastic/elasticsearch/issues/9226#issuecomment-73476714","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/9226","id":73476714,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDc2NzE0","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2015-02-09T09:06:48Z","updated_at":"2015-02-09T09:06:48Z","author_association":"CONTRIBUTOR","body":"I'm a little concerned that NIOFSDirectory appears in the hot thread above.  Elasticsearch normally tries to use MMapDirectory for the terms dictionary index files, for large segments.\n\n@avleen Have you explicitly set index.store.type: to \"niofs\"?  Or, have you enabled index.compound_format?\n","performed_via_github_app":null}]