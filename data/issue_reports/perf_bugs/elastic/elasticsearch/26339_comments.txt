[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/324302899","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-324302899","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":324302899,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNDMwMjg5OQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2017-08-23T11:39:33Z","updated_at":"2017-08-23T11:39:33Z","author_association":"MEMBER","body":"thx for trying 6.0.0 - I added you to the pioneer program. \r\n\r\n >  Seeing hotspots in LocalCheckpointTracker.margSeqNoAsCompleted(), generetaSeqNo() \r\n\r\nThose methods are indeed synchronization points as we assumed they would be much lighter than indexing. Can you share the output that make you conclude this?\r\n\r\n>      \r\n        \"index.translog.flush_threshold_size\": \"4gb\",\r\n        \"index.translog.durability\": \"async\",\r\n        \"index.translog.sync_interval\": \"240s\"\r\n\r\nThis indeed removes a common bottle neck, shifting it to another place. Can you try without these? I want to confirm things become closer then - as our own [benchmarks](https://elasticsearch-benchmarks.elastic.co/#tracks/geonames/release) suggest.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/324494136","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-324494136","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":324494136,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNDQ5NDEzNg==","user":{"login":"coderplay","id":107521,"node_id":"MDQ6VXNlcjEwNzUyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/107521?v=4","gravatar_id":"","url":"https://api.github.com/users/coderplay","html_url":"https://github.com/coderplay","followers_url":"https://api.github.com/users/coderplay/followers","following_url":"https://api.github.com/users/coderplay/following{/other_user}","gists_url":"https://api.github.com/users/coderplay/gists{/gist_id}","starred_url":"https://api.github.com/users/coderplay/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderplay/subscriptions","organizations_url":"https://api.github.com/users/coderplay/orgs","repos_url":"https://api.github.com/users/coderplay/repos","events_url":"https://api.github.com/users/coderplay/events{/privacy}","received_events_url":"https://api.github.com/users/coderplay/received_events","type":"User","site_admin":false},"created_at":"2017-08-23T23:54:01Z","updated_at":"2017-08-23T23:54:01Z","author_association":"NONE","body":"The degradation is caused by [this line](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java#L663) introduced by [this commit](https://github.com/elastic/elasticsearch/commit/75b4f408e05fc19de6fce775132239f1e22c99ac) whose original purpose was doing refactoring, but it added a lock in the critical path of indexing.  ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/324598959","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-324598959","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":324598959,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNDU5ODk1OQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-08-24T10:36:04Z","updated_at":"2017-08-24T10:36:04Z","author_association":"MEMBER","body":"Thanks for reporting this! We've reproduced the issue on our nightly benchmarking hardware with a few modifications:\r\n\r\n* 16GB heap configured (our machines have \"only\" 32GB)\r\n* 32 clients\r\n\r\nI have used the nyc_taxis track and I've seen the following median throughput:\r\n\r\n* 6.0.0-beta1: 92261 docs/s\r\n* 5.5.2: 98220 docs/s\r\n\r\nWhile the effect is not as pronounced as in your case, it is still significant. For 6.0.0-beta1, we are seeing lock contention in `LocalCheckpointTracker` where locks were held for roughly 4s in total during the benchmark (for comparison: locks on `TranslogWriter`,  which is the lock with the highest contention, were held for a total of 49 seconds). \r\n\r\nIn 80 out of 94 captured traces, the lock was held via `LocalCheckpointTracker#generateSeqNo()`. I'll implement this method in a lock-free manner. \r\n\r\n`LocalCheckpointTracker.markSeqNoAsCompleted(long)` were held 14 times for a total of 580ms. As this method is (a) a lot trickier to implement lock-free and (b) contributing less to contention (in our tests), I'll only reduce the scope of the lock a bit.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/324899499","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-324899499","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":324899499,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNDg5OTQ5OQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-08-25T12:03:11Z","updated_at":"2017-08-25T12:03:33Z","author_association":"MEMBER","body":"Repeating my comment on the related PR #26362 here:\r\n\r\nAfter running benchmarks in our nightly benchmarking environment, we see the following median throughput for the nyc_taxis track (24 clients):\r\n\r\n* 5.5.2: 91773 docs/s\r\n* With #26362: 82125 docs/s\r\n* Without #26362: 82258 docs/s\r\n\r\nIn summary: This did not improve the situation.\r\n\r\nWe do, however, see that with this PR the monitor lock of `org.elasticsearch.index.seqno.LocalCheckpointTracker` is now blocked for 10.3 seconds during the benchmark (before ~ 4 seconds). In the majority of cases, the affected method is `#markSeqNoAsCompleted(long)`. Hence, we'll look into improving `#markSeqNoAsCompleted(long)` too.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/325069843","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-325069843","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":325069843,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNTA2OTg0Mw==","user":{"login":"coderplay","id":107521,"node_id":"MDQ6VXNlcjEwNzUyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/107521?v=4","gravatar_id":"","url":"https://api.github.com/users/coderplay","html_url":"https://github.com/coderplay","followers_url":"https://api.github.com/users/coderplay/followers","following_url":"https://api.github.com/users/coderplay/following{/other_user}","gists_url":"https://api.github.com/users/coderplay/gists{/gist_id}","starred_url":"https://api.github.com/users/coderplay/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderplay/subscriptions","organizations_url":"https://api.github.com/users/coderplay/orgs","repos_url":"https://api.github.com/users/coderplay/repos","events_url":"https://api.github.com/users/coderplay/events{/privacy}","received_events_url":"https://api.github.com/users/coderplay/received_events","type":"User","site_admin":false},"created_at":"2017-08-26T01:26:40Z","updated_at":"2017-08-26T01:26:40Z","author_association":"NONE","body":"@danielmitterdorfer Try more powerful machine. More powerful machine will gain better parallelism if there is no lock contention.   The throughput difference will be larger. ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/325323592","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-325323592","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":325323592,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNTMyMzU5Mg==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-08-28T10:58:28Z","updated_at":"2017-08-28T10:58:28Z","author_association":"MEMBER","body":"> Try more powerful machine. More powerful machine will gain better parallelism if there is no lock contention. The throughput difference will be larger.\r\n\r\nI totally agree to this statement but for the time being, it's just important that we see a measurable difference due to the same hotspots that you are describing. When we're confident that the problem is fixed, I think it makes sense to run another benchmark on a more powerful machine.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/326558851","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-326558851","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":326558851,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNjU1ODg1MQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-09-01T11:37:37Z","updated_at":"2017-09-01T11:39:36Z","author_association":"MEMBER","body":"To summarize the work so far:\r\n\r\nI've reimplemented `LocalCheckpointTracker` so that it is *completely* lock-free. In the default case, it even avoids notification if it is certain that there are no waiting threads anyway (see `LocalCheckpointTracker#waitForOpsToComplete(long)`).\r\n\r\nI've ran the `nyc_taxis` track with the following challenge:\r\n\r\n```json\r\n{\r\n  \"name\": \"index-contention\",\r\n  \"description\": \"Indexes the whole document corpus with more clients\",\r\n  \"index-settings\": {\r\n    \"index.number_of_replicas\": 0,\r\n    \"index.number_of_shards\": 8,\r\n    \"index.refresh_interval\": \"100s\",\r\n    \"index.translog.flush_threshold_size\": \"4gb\",\r\n    \"index.translog.durability\": \"async\",\r\n    \"index.translog.sync_interval\": \"240s\"\r\n  },\r\n  \"schedule\": [\r\n    {\r\n      \"operation\": \"index\",\r\n      \"warmup-time-period\": 240,\r\n      \"clients\": 24\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n_Note: So far I could never finish any benchmark with the originally stated number of 48 clients without running into bulk rejections and I consider a benchmark with an error rate > 0 invalid. Hence, I've reduced the number of clients to 24._\r\n\r\nOn our nightly benchmarking hardware (4 cores, 32GB) with a heap size of 16GB and `indices.memory.index_buffer_size: 50%`, we see the following results (median indexing throughput):\r\n\r\n* 5.5.2: 90844 docs/s\r\n* 6.0.0-beta2-SNAPSHOT (commit hash elastic/elasticsearch@41675c1): 83611 docs/s\r\n* 6.0.0-beta2-SNAPSHOT + lock-free changes: 83208 docs/s\r\n\r\nSo for this configuration we do not see any noticeable difference between different implementations on 6.0.0-beta2 that cannot be attributed to run-to-run variance.\r\n\r\nOne significant contributor is contention in `TranslogWriter` due to frequent flushing of checkpoints. Therefore I also ran a benchmark with the index setting `index.translog.generation_threshold_size: \"2gb\"` (default is `64mb`) and we see indeed an improvement. I only tested the configuration \"6.0.0-beta2-SNAPSHOT + lock free changes\" which resulted in a median indexing throughput of 86937 docs/s. Still not there but much closer. We also see less contention in `TranslogWriter` as checkpoints are flushed less often.\r\n\r\nFinally, I also ran a benchmark on the same hardware as stated originally, i.e. `i3.16xl` with 8 data paths (formatted with ext4), a heap size of 31GB and `indices.memory.index_buffer_size: 50%`. The load generator (Rally) was on a `c3.8xlarge` in the same availability zone. I've removed all data and trimmed the disks before each run.\r\n\r\n* 5.5.2: 240115 docs/s\r\n* 6.0.0-beta2-SNAPSHOT (commit hash elastic/elasticsearch@41675c1): 144277 docs/s\r\n* 6.0.0-beta2-SNAPSHOT + lock-free changes: 156933 docs/s\r\n\r\nIn this case, we can see a noticeable improvement due the lock-free implementation of `LocalCheckpointTracker`. An additional run with `index.translog.generation_threshold_size: \"2gb\"` did not show any improvement though (median indexing throughput 153709 docs/s).\r\n\r\nAs far as I can tell at the moment, after resolving problems in `LocalCheckpointTracker` the main bottleneck seems to be caused by heavy contention in `TranslogWriter` but this is subject to further investigation.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/326973357","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-326973357","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":326973357,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNjk3MzM1Nw==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-09-04T14:14:37Z","updated_at":"2017-09-04T14:14:37Z","author_association":"MEMBER","body":"Further experiments have shown that a major contributor to higher contention in 6.0.0 is caused by a higher number threads in the system due to #20874. A `i3.16xl` has 64 cores which result in 64 bulk indexing threads on 6.0.0 whereas it was bounded to 32 bulk indexing threads on 5.x. Thus setting `processors: 32` in `config/elasticsearch.yml` we see now a much saner behavior w.r.t. to lock contention.\r\n\r\nWith the same setup as before (same instance *types* but different instances) we get the following median indexing throughput:\r\n\r\n* 5.5.2: 251121 docs/s\r\n* 6.0.0-beta2-SNAPSHOT + lock-free changes with default processor settings: 152846 docs/s\r\n* 6.0.0-beta2-SNAPSHOT + lock-free changes with `processors: 32`: 136893 docs/s\r\n\r\nThe top-contended lock in 6.0.0 is the monitor lock on `TranslogWriter`. Across the three benchmark candidates, this lock was contended the following number of times during the benchmark:\r\n\r\n* 5.5.2: 1,050\r\n* 6.0.0-beta2-SNAPSHOT + lock-free changes with default processor settings: 143,030\r\n* 6.0.0-beta2-SNAPSHOT + lock-free changes with `processors: 32`: 13,708\r\n\r\nWhile this measure has drastically reduced the number of times this lock is contended, we still see a significant difference in indexing throughput for these settings and hardware.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/327455823","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-327455823","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":327455823,"node_id":"MDEyOklzc3VlQ29tbWVudDMyNzQ1NTgyMw==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-09-06T11:32:22Z","updated_at":"2017-09-06T11:34:54Z","author_association":"MEMBER","body":"After further benchmarks have hinted that the cause may be Lucene, I've started bisecting. Indeed, we see a major drop in median indexing throughput after upgrading to Lucene 7 (measured on an `i3.16xlarge`):\r\n\r\n* last revision with Lucene 6.x (revision elastic/elasticsearch@f217eb8ad8): 241561 docs/s\r\n* first revision with Lucene 7.0 (revision elastic/elasticsearch@4632661bc7): 128667 docs/s\r\n\r\nHere is the achieved median indexing throughput on a 4 core machine for comparison:\r\n\r\n* last revision with Lucene 6.x: 83898 docs/s\r\n* first revision with Lucene 7.0 : 82558 docs/s\r\n\r\nSo the drop we see is significantly lower on machines with lower number of cores.\r\n\r\nThe cause is not yet clear; two possibilities are JIT compiler behavior or lock contention.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328079161","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-328079161","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":328079161,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODA3OTE2MQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-09-08T11:28:57Z","updated_at":"2017-09-08T11:28:57Z","author_association":"MEMBER","body":"The performance drop is caused by a [write to a(n unused) variable in Lucene](https://github.com/apache/lucene-solr/blob/6c3ece2/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java#L736). We've seen that a significant portion of time is spent in related code. Here is a snippet of a JMH microbenchmark with the `perfasm` profiler:\r\n\r\n```\r\n....[Hottest Region 1]..............................................................................\r\nC2, level 4, org.apache.lucene.index.DefaultIndexingChain$PerField::invert, version 1231 (792 bytes)\r\n\r\n                          0x00007f52d56b026a: xor    %r10d,%r10d        ;*invokevirtual isAssignableFrom\r\n                                                                        ; - org.apache.lucene.util.AttributeSource::addAttribute@28 (line 207)\r\n                                                                        ; - org.apache.lucene.document.Field$BinaryTokenStream::&lt;init&gt;@8 (line 512)\r\n                                                                        ; - org.apache.lucene.document.Field::tokenStream@82 (line 491)\r\n                                                                        ; - org.apache.lucene.index.DefaultIndexingChain$PerField::invert@99 (line 729)\r\n                          0x00007f52d56b026d: mov    $0x8,%r11d         ;*invokeinterface iterator\r\n                                                                        ; - org.apache.lucene.util.AttributeSource::getCurrentState@46 (line 254)\r\n                                                                        ; - org.apache.lucene.util.AttributeSource::clearAttributes@1 (line 269)\r\n                                                                        ; - org.apache.lucene.document.Field$BinaryTokenStream::incrementToken@10 (line 532)\r\n                                                                        ; - org.apache.lucene.index.DefaultIndexingChain$PerField::invert@153 (line 736)\r\n  0.00%    0.02%       ↗  0x00007f52d56b0273: test   %r10,%r10\r\n                  ╭    │  0x00007f52d56b0276: je     0x00007f52d56b0292  ;*getfield fieldsData\r\n                  │    │                                                ; - org.apache.lucene.document.Field::binaryValue@1 (line 441)\r\n                  │    │                                                ; - org.apache.lucene.document.Field::tokenStream@65 (line 487)\r\n                  │    │                                                ; - org.apache.lucene.index.DefaultIndexingChain$PerField::invert@99 (line 729)\r\n  0.00%    0.00%  │    │  0x00007f52d56b0278: mov    (%r11),%rsi        ;*getfield next\r\n                  │    │                                                ; - java.util.HashMap::getNode@137 (line 580)\r\n                  │    │                                                ; - java.util.LinkedHashMap::get@6 (line 440)\r\n                  │    │                                                ; - org.apache.lucene.util.AttributeSource::getAttribute@6 (line 245)\r\n                  │    │                                                ; - org.apache.lucene.index.DefaultIndexingChain$PerField::invert@143 (line 734)\r\n  0.09%    0.51%  │    │  0x00007f52d56b027b: mov    0x18(%rsi),%r8\r\n 23.70%    3.54%  │    │  0x00007f52d56b027f: mov    $0x7f4926a81d88,%rcx  ;   {metadata(&apos;org/apache/lucene/analysis/tokenattributes/CharTermAttribute&apos;)}\r\n```\r\n\r\nI've created a custom version of Lucene (and Elasticsearch) where the only difference is that I've eliminated this variable. Here are the results of the `nyc_taxis` benchmark (again same benchmark setup as always):\r\n\r\n* Baseline: last revision with Lucene 6.x (revision elastic/elasticsearch@f217eb8ad8): 218361 docs/s\r\n* first revision with Lucene 7.0 (revision elastic/elasticsearch@4632661bc7): 125418 docs/s\r\n* first revision with Lucene 7.0 (revision elastic/elasticsearch@4632661bc7) + unused variable removed: 221237 docs/s\r\n\r\nI've raised [LUCENE-7963](https://issues.apache.org/jira/browse/LUCENE-7963) together with a patch. As soon as we have a new Lucene snapshot build which contains my patch, we can iterate here. The next step is then to see whether we still need a lock-free implementation of `LocalCheckpointTracker` (i.e. #26362).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328097184","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-328097184","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":328097184,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODA5NzE4NA==","user":{"login":"nitsanw","id":2692655,"node_id":"MDQ6VXNlcjI2OTI2NTU=","avatar_url":"https://avatars2.githubusercontent.com/u/2692655?v=4","gravatar_id":"","url":"https://api.github.com/users/nitsanw","html_url":"https://github.com/nitsanw","followers_url":"https://api.github.com/users/nitsanw/followers","following_url":"https://api.github.com/users/nitsanw/following{/other_user}","gists_url":"https://api.github.com/users/nitsanw/gists{/gist_id}","starred_url":"https://api.github.com/users/nitsanw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nitsanw/subscriptions","organizations_url":"https://api.github.com/users/nitsanw/orgs","repos_url":"https://api.github.com/users/nitsanw/repos","events_url":"https://api.github.com/users/nitsanw/events{/privacy}","received_events_url":"https://api.github.com/users/nitsanw/received_events","type":"User","site_admin":false},"created_at":"2017-09-08T13:02:25Z","updated_at":"2017-09-08T13:02:25Z","author_association":"NONE","body":"@danielmitterdorfer If I read this correctly (and there's always a chance to misread assembly) the line blame here is falsely accused. At least the assembly line (not arguing about the unused local var). The line:\r\n``` 0x00007f52d56b027f: mov    $0x7f4926a81d88,%rcx ```\r\nI setting the RCX register to a constant, so costs nothing and cannot be the issue. It's likely that the issue is in fact one of the lines preceding it:\r\n```\r\n  0.00%    0.00%  │    │  0x00007f52d56b0278: mov    (%r11),%rsi        ;*getfield next\r\n                  │    │                                                ; - java.util.HashMap::getNode@137 (line 580)\r\n                  │    │                                                ; - java.util.LinkedHashMap::get@6 (line 440)\r\n                  │    │                                                ; - org.apache.lucene.util.AttributeSource::getAttribute@6 (line 245)\r\n                  │    │                                                ; - org.apache.lucene.index.DefaultIndexingChain$PerField::invert@143 (line 734)\r\n  0.09%    0.51%  │    │  0x00007f52d56b027b: mov    0x18(%rsi),%r8\r\n 23.70%    3.54%  │    │  0x00007f52d56b027f: mov    $0x7f4926a81d88,%rcx  ;   {metadata(&apos;org/apache/lucene/analysis/tokenattributes/CharTermAttribute&apos;)}\r\n  0.00%    0.00%  │    │  0x00007f52d56b0289: cmp    %rcx,%r8\r\n                  │    │  0x00007f52d56b028c: jne    0x00007f52d56b0949  ;*instanceof\r\n                  │    │                                                ; - org.apache.lucene.document.Field::binaryValue@4 (line 441)\r\n                  │    │                                                ; - org.apache.lucene.document.Field::tokenStream@65 (line 487)\r\n                  │    │                                                ; - org.apache.lucene.index.DefaultIndexingChain$PerField::invert@99 (line 729)\r\n```\r\nThe cost would be the code in here: https://github.com/apache/lucene-solr/blob/e2521b2a8baabdaf43b92192588f51e042d21e97/lucene/core/src/java/org/apache/lucene/util/AttributeSource.java#L244\r\nWhich is a HashMap lookup, which is probably a costly cache miss. The cache miss theory is supported by the poor CPI (blamed on the wrong instruction) of roughly 6.5 cycles per instruction. I would hazard a  guess the hash lookup is a loop (see [here](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java#l566)) , and this is not one cache miss but many that get blamed on the wrong instruction.\r\n\r\nThe line of code, while initialising an unused local variable, cannot be easily eliminated by the JVM as the lookup code may have a side effect (e.g. return the wrong type and trigger a ClassCastException), and it is hard to prove that a complex piece of code such as the one triggered to init the variable has no side effects.\r\n\r\nIn any case, removing the unused variable is (as backed by your results) the right solution to the issue :-)","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328425084","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-328425084","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":328425084,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODQyNTA4NA==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-09-11T06:04:27Z","updated_at":"2017-09-11T07:30:59Z","author_association":"MEMBER","body":"> The cost would be the code in here: https://github.com/apache/lucene-solr/blob/e2521b2a8baabdaf43b92192588f51e042d21e97/lucene/core/src/java/org/apache/lucene/util/AttributeSource.java#L244 [...] which is a HashMap lookup, which is probably a costly cache miss.\r\n\r\nI have to admit, I misread this instruction (reading assembly is not something I do every day ;) ). Your explanation is much more sound to me as to why we have such a huge cost.\r\n\r\n> The line of code, while initialising an unused local variable, cannot be easily eliminated by the JVM as the lookup code may have a side effect (e.g. return the wrong type and trigger a ClassCastException), and it is hard to prove that a complex piece of code such as the one triggered to init the variable has no side effects.\r\n\r\nThat makes perfect sense to me and also solves the second part of the puzzle. Thanks a lot for your explanations @nitsanw. I really appreciate it.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328512303","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-328512303","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":328512303,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODUxMjMwMw==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-09-11T12:22:02Z","updated_at":"2017-09-11T12:22:02Z","author_association":"MEMBER","body":"I've run final benchmarks now. It's the same benchmarking scenario as always but I'll summarize it here.\r\n\r\n### Benchmark Setup and Methodology\r\n\r\n* Load driver machine: c3.8xlarge, Ubuntu 16.04\r\n* Target machine: i3.16xlarge, Ubuntu 16.04\r\n\r\nElasticsearch is configured to use 8 data paths (ext4-formatted) and 31 GB heap size. Before each trial run, we wipe the disks and run TRIM.\r\n\r\n`config/elasticsearch.yml`:\r\n\r\n```\r\nnetwork.host: 0.0.0.0\r\npath.data: [/mnt/data0, /mnt/data1, /mnt/data2, /mnt/data3, /mnt/data4, /mnt/data5, /mnt/data6, /mnt/data7]\r\nindices.memory.index_buffer_size: 50%\r\n```\r\n\r\n`config/jvm.options`: out of the box configuration, except that we set:\r\n\r\n```\r\n-Xms31G\r\n-Xmx31G\r\n```\r\n\r\nWe use [Rally](https://github.com/elastic/rally) and the [`nyc_taxis` track](https://github.com/elastic/rally-tracks/tree/master/nyc_taxis) with the following challenge:\r\n\r\n```json\r\n{\r\n  \"name\": \"index-contention\",\r\n  \"description\": \"Indexes the whole document corpus with more clients\",\r\n  \"index-settings\": {\r\n    \"index.number_of_replicas\": 0,\r\n    \"index.number_of_shards\": 8,\r\n    \"index.refresh_interval\": \"100s\",\r\n    \"index.translog.flush_threshold_size\": \"4gb\",\r\n    \"index.translog.durability\": \"async\",\r\n    \"index.translog.sync_interval\": \"240s\"\r\n  },\r\n  \"schedule\": [\r\n    {\r\n      \"operation\": \"index\",\r\n      \"warmup-time-period\": 240,\r\n      \"clients\": 24\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Results\r\n\r\n|   Version                                                          |        `processors` setting   |   Min Indexing Throughput |   Median Indexing Throughput |   Max Indexing Throughput |\r\n|-------------------------------------------------------------------:|------------------------------:|--------------------------:|-----------------------------:|--------------------------:|\r\n| 5.5.2                                                              |\"default\" (was bounded to 32)  | 229286 docs/s             | 239590 docs/s                | 242889 docs/s             |\r\n| 6.0.0-beta2*                                                       |32                             | 230524 docs/s             | 236368 docs/s                | 242602 docs/s             |\r\n| 6.0.0-beta2*                                                       |\"default\" (unbounded, i.e. 64) | 227845 docs/s             | 238880 docs/s                | 246613 docs/s             |\r\n| 6.0.0-beta2* + https://github.com/elastic/elasticsearch/pull/26362 |32                             | 228910 docs/s             | 234151 docs/s                | 240606 docs/s             |\r\n| 6.0.0-beta2* + https://github.com/elastic/elasticsearch/pull/26362 |\"default\" (unbounded, i.e. 64) | 221387 docs/s             | 237433 docs/s                | 242800 docs/s             |\r\n\r\n\r\n*) includes the patch from [LUCENE-7963](https://issues.apache.org/jira/browse/LUCENE-7963)\r\n\r\n### Analysis\r\n\r\nWe see now comparable performance between 5.5.2 and 6.0.0-beta2. In JFR (Java flight recorder) we can see that the new unbounded default for `processors` causes significantly more contention than with 32 processors. This is also the reason we are not seeing much improvement between setting `processors` to 32 and keeping the default value. While this is an unfortunate situation I think it is out of the scope of this ticket to fix these contention issues.\r\n\r\nComparing the lock-free implementation of `LocalCheckpointTracker` (see https://github.com/elastic/elasticsearch/pull/26362) with the current implementation does not yield any benefits and that's why I will also close the PR without merging it. Flight recordings show that we just put more pressure on the monitor lock on `TranslogWriter`. So while it may be beneficial in the future to implement `LocalCheckpointTracker` lock-free for machines with a high core count, I think we first need to address other issues before we can see a benefit. \r\n\r\nI'll leave this ticket open until we've actually upgraded to a Lucene version that includes the patch from [LUCENE-7963](https://issues.apache.org/jira/browse/LUCENE-7963).\r\n\r\nThanks @muralikpbhat for bringing this up.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328604455","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-328604455","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":328604455,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODYwNDQ1NQ==","user":{"login":"muralikpbhat","id":9718411,"node_id":"MDQ6VXNlcjk3MTg0MTE=","avatar_url":"https://avatars3.githubusercontent.com/u/9718411?v=4","gravatar_id":"","url":"https://api.github.com/users/muralikpbhat","html_url":"https://github.com/muralikpbhat","followers_url":"https://api.github.com/users/muralikpbhat/followers","following_url":"https://api.github.com/users/muralikpbhat/following{/other_user}","gists_url":"https://api.github.com/users/muralikpbhat/gists{/gist_id}","starred_url":"https://api.github.com/users/muralikpbhat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/muralikpbhat/subscriptions","organizations_url":"https://api.github.com/users/muralikpbhat/orgs","repos_url":"https://api.github.com/users/muralikpbhat/repos","events_url":"https://api.github.com/users/muralikpbhat/events{/privacy}","received_events_url":"https://api.github.com/users/muralikpbhat/received_events","type":"User","site_admin":false},"created_at":"2017-09-11T17:44:05Z","updated_at":"2017-09-11T17:44:05Z","author_association":"NONE","body":"Thanks @danielmitterdorfer for taking this to a nice conclusion. Very interesting find of getAttribute() issue in [LUCENE-7963](https://issues.apache.org/jira/browse/LUCENE-7963)\r\n\r\n> In JFR (Java flight recorder) we can see that the new unbounded default for processors causes significantly more contention than with 32 processors. This is also the reason we are not seeing much improvement between setting processors to 32 and keeping the default value. \r\n\r\n>I think we first need to address other issues before we can see a benefit.\r\n\r\nSurprising that your lock-free implementation is not giving better performance with 64cores. Are you saying Translogwriter monitor lock needs to be fixed before we make this lock free or are there more issues than the Translogwriter?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328666915","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-328666915","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":328666915,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODY2NjkxNQ==","user":{"login":"coderplay","id":107521,"node_id":"MDQ6VXNlcjEwNzUyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/107521?v=4","gravatar_id":"","url":"https://api.github.com/users/coderplay","html_url":"https://github.com/coderplay","followers_url":"https://api.github.com/users/coderplay/followers","following_url":"https://api.github.com/users/coderplay/following{/other_user}","gists_url":"https://api.github.com/users/coderplay/gists{/gist_id}","starred_url":"https://api.github.com/users/coderplay/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderplay/subscriptions","organizations_url":"https://api.github.com/users/coderplay/orgs","repos_url":"https://api.github.com/users/coderplay/repos","events_url":"https://api.github.com/users/coderplay/events{/privacy}","received_events_url":"https://api.github.com/users/coderplay/received_events","type":"User","site_admin":false},"created_at":"2017-09-11T21:41:14Z","updated_at":"2017-09-11T21:41:14Z","author_association":"NONE","body":"My recent test on nyc_taxis with 5.5.2 is about 288K, but with different configuration. Will make another comparison later.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328745021","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-328745021","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":328745021,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODc0NTAyMQ==","user":{"login":"danielmitterdorfer","id":1699576,"node_id":"MDQ6VXNlcjE2OTk1NzY=","avatar_url":"https://avatars3.githubusercontent.com/u/1699576?v=4","gravatar_id":"","url":"https://api.github.com/users/danielmitterdorfer","html_url":"https://github.com/danielmitterdorfer","followers_url":"https://api.github.com/users/danielmitterdorfer/followers","following_url":"https://api.github.com/users/danielmitterdorfer/following{/other_user}","gists_url":"https://api.github.com/users/danielmitterdorfer/gists{/gist_id}","starred_url":"https://api.github.com/users/danielmitterdorfer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielmitterdorfer/subscriptions","organizations_url":"https://api.github.com/users/danielmitterdorfer/orgs","repos_url":"https://api.github.com/users/danielmitterdorfer/repos","events_url":"https://api.github.com/users/danielmitterdorfer/events{/privacy}","received_events_url":"https://api.github.com/users/danielmitterdorfer/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T05:54:23Z","updated_at":"2017-09-12T05:54:23Z","author_association":"MEMBER","body":"> Surprising that your lock-free implementation is not giving better performance with 64cores. Are you saying Translogwriter monitor lock needs to be fixed before we make this lock free or are there more issues than the Translogwriter?\r\n\r\nAs far as I can see from the most recent flight recordings, the reason is that `LocalCheckpointTracker` is not the bottleneck. Although there is measurable contention in the current implementation, making it lock-free just increases contention in `TranslogWriter`.\r\n\r\nIMHO, Elasticsearch's approach to scalability is to scale horizontally rather than vertically (i.e. many small nodes instead of a few large nodes). As a consequence of that design decision, we see contention on machines with a high number of cores. Or more specifically: The root cause is that multiple writers attempt to write concurrently to the translog and guarding it with a lock does not scale to machines with high core counts. A more scalable solution could be to apply the [single-writer principle](https://mechanical-sympathy.blogspot.de/2011/09/single-writer-principle.html).","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/328749748","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-328749748","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":328749748,"node_id":"MDEyOklzc3VlQ29tbWVudDMyODc0OTc0OA==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2017-09-12T06:19:32Z","updated_at":"2017-09-12T06:19:32Z","author_association":"CONTRIBUTOR","body":"> IMHO, Elasticsearch's approach to scalability is to scale horizontally rather than vertically (i.e. many small nodes instead of a few large nodes). As a consequence of that design decision, we see contention on machines with a high number of cores. Or more specifically: The root cause is that multiple writers attempt to write concurrently to the translog and guarding it with a lock does not scale to machines with high core counts. A more scalable solution could be to apply the single-writer principle.\r\n\r\nI had multiple attempts to this in the past that didn't yield massive speed improvements compared to the complexity it added. Lets say you'd use a ringbuffer to do this you'd need to copy the payload for every document which in-turn adds a reasonable complexity in terms of memory consumption and GC. Today the impl is basically writing to a buffered stream which is reusing it's buffer so we only pay the price of copying the bytes. It's all hidden behind the stream interface which is nice. IMO there needs to be a significant speed improvement to justify a Single Writer refactoring.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/330780544","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-330780544","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":330780544,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMDc4MDU0NA==","user":{"login":"coderplay","id":107521,"node_id":"MDQ6VXNlcjEwNzUyMQ==","avatar_url":"https://avatars0.githubusercontent.com/u/107521?v=4","gravatar_id":"","url":"https://api.github.com/users/coderplay","html_url":"https://github.com/coderplay","followers_url":"https://api.github.com/users/coderplay/followers","following_url":"https://api.github.com/users/coderplay/following{/other_user}","gists_url":"https://api.github.com/users/coderplay/gists{/gist_id}","starred_url":"https://api.github.com/users/coderplay/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderplay/subscriptions","organizations_url":"https://api.github.com/users/coderplay/orgs","repos_url":"https://api.github.com/users/coderplay/repos","events_url":"https://api.github.com/users/coderplay/events{/privacy}","received_events_url":"https://api.github.com/users/coderplay/received_events","type":"User","site_admin":false},"created_at":"2017-09-20T08:20:29Z","updated_at":"2017-09-20T08:27:07Z","author_association":"NONE","body":"### Configurations\r\n\r\n* Load driver machine: i3.16xlarge, Amazon Linux\r\n* Target machine: i3.16xlarge, Amazon Linux,  128GB heap configured for ES, G1 GC, MaxGCPauseMillis=1000,  indexing buffer size of 50%, data.path spread across 8 disks, 32 netty workers,  64 bulk workers.\r\n\r\nChallenge settings:\r\n```\r\n      \"index-settings\": {\r\n        \"index.number_of_shards\": 24,\r\n        \"index.codec\": \"best_compression\",\r\n        \"index.number_of_replicas\": 0,\r\n        \"index.refresh_interval\": \"100s\",\r\n        \"index.translog.flush_threshold_size\": \"4g\",\r\n        \"index.translog.durability\": \"async\",\r\n        \"index.translog.sync_interval\": \"240s\"\r\n      },\r\n      \"schedule\": [\r\n        {\r\n          \"operation\": \"index\",\r\n          \"warmup-time-period\": 240,\r\n          \"clients\": 48\r\n        }\r\n      ]\r\n```\r\n\r\n### Results\r\nVersion | Min Indexing Throughput | Median Indexing Throughput | Max Indexing Throughput\r\n--  | -- | -- | --\r\n5.5.2 | 268327 docs/s | 281017 docs/s | 288709 docs/s\r\n6.0.0-beta2  | 166966 docs/s | 168839 docs/s | 171388 docs/s\r\n6.0.0-beta2* | 271438 docs/s | 277353 docs/s | 285558 docs/s\r\n\r\n5.5.2 loosed max threads number limit.\r\n\r\n@danielmitterdorfer Still a nice work!\r\n\r\nThe `TranslogWriter` lock contention problem I believe has been around for a long time. 5.5.2 also has that problem.  Writings on the same index shard share the same WAL writer. `Single writer principle` is the ideal solution. In reality, we usually use `group commit` to solve this problem. \r\n* [group commit in mariadb](https://mariadb.com/kb/en/library/group-commit-for-the-binary-log/)\r\n* [group commit in Postgres](https://wiki.postgresql.org/wiki/Group_commit)\r\n* [group commit in MySQL](https://www.facebook.com/notes/mysql-at-facebook/group-commit/438641125932/)\r\n* [group commit in Hbase ](https://issues.apache.org/jira/browse/HBASE-1939)\r\n\r\nHbase uses disruptor ringbuffer achieved group commit. We probably can learn from it.\r\n\r\n@s1monw\r\nI don't think we need to copy the payload, since TransLog already created a new byte array in ReleasableBytesStreamOutput. \r\n```java\r\n  public Location add(final Operation operation) throws IOException {\r\n        final ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(bigArrays);\r\n...\r\n            final ReleasablePagedBytesReference bytes = out.bytes();\r\n...\r\n```\r\n just keep the reference object in the buffer and dump them to the WAL file in a batch should work.\r\n\r\nRegarding to the GC, elasticsearch already had serious YGC issue. The JVM allocates ~9.11GB objects per second in nyc_taxis benchmark, most of them are allocated by `org.elasticsearch.index.mapper.DocumentParser.internalParseDocument(Mapping, ParseContext$InternalParseContext, XContentParser)`.  \r\nThe nyc_taxis indexing takes ~630 secs in 5.5.2,  times 9.11 GB/s  , that's  5.74TB memory allocated just for a size of 74.3 GB dataset.  The GC couldn't be worse even we copy payloads.\r\n\r\n\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/331792496","html_url":"https://github.com/elastic/elasticsearch/issues/26339#issuecomment-331792496","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/26339","id":331792496,"node_id":"MDEyOklzc3VlQ29tbWVudDMzMTc5MjQ5Ng==","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2017-09-25T06:54:36Z","updated_at":"2017-09-25T06:54:36Z","author_association":"CONTRIBUTOR","body":"Fixed via #26744","performed_via_github_app":null}]