[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/371047642","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-371047642","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":371047642,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MTA0NzY0Mg==","user":{"login":"spinscale","id":667544,"node_id":"MDQ6VXNlcjY2NzU0NA==","avatar_url":"https://avatars2.githubusercontent.com/u/667544?v=4","gravatar_id":"","url":"https://api.github.com/users/spinscale","html_url":"https://github.com/spinscale","followers_url":"https://api.github.com/users/spinscale/followers","following_url":"https://api.github.com/users/spinscale/following{/other_user}","gists_url":"https://api.github.com/users/spinscale/gists{/gist_id}","starred_url":"https://api.github.com/users/spinscale/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spinscale/subscriptions","organizations_url":"https://api.github.com/users/spinscale/orgs","repos_url":"https://api.github.com/users/spinscale/repos","events_url":"https://api.github.com/users/spinscale/events{/privacy}","received_events_url":"https://api.github.com/users/spinscale/received_events","type":"User","site_admin":false},"created_at":"2018-03-07T07:23:44Z","updated_at":"2018-03-07T07:23:44Z","author_association":"MEMBER","body":"@elastic/es-distributed can you please take a look if this is valid or might have changed with 6.0?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/371275379","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-371275379","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":371275379,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MTI3NTM3OQ==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-03-07T20:34:25Z","updated_at":"2018-03-07T20:34:25Z","author_association":"CONTRIBUTOR","body":"I'd expect the recovery of a partitioned 10,000 shard cluster to be faster in 6.x, but not for the reasons described here. Please could you provide the logs from a decent proportion of the nodes (â‰¥ 5 from each side of the partition, including a selection of both master and data nodes) so we can see what was actually happening for those 10 minutes. I can provide an email address if you struggle to attach then here.\r\n\r\nAlso please describe the \"optimisations\" performed that reduced this from 10 minutes down to 1.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/371293814","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-371293814","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":371293814,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MTI5MzgxNA==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-03-07T21:37:32Z","updated_at":"2018-03-07T21:37:32Z","author_association":"CONTRIBUTOR","body":"Also 5.3.1 is approaching a year old and missing a few fixes that might have an impact on this. Is it feasible to upgrade to a more recent version and retry your experiments? Ideally the latest possible version (currently 6.2.2) but it'd be useful to look at the latest 5.x version (currently 5.6.8) if that's not possible.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/371305946","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-371305946","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":371305946,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MTMwNTk0Ng==","user":{"login":"dnhatn","id":13474362,"node_id":"MDQ6VXNlcjEzNDc0MzYy","avatar_url":"https://avatars3.githubusercontent.com/u/13474362?v=4","gravatar_id":"","url":"https://api.github.com/users/dnhatn","html_url":"https://github.com/dnhatn","followers_url":"https://api.github.com/users/dnhatn/followers","following_url":"https://api.github.com/users/dnhatn/following{/other_user}","gists_url":"https://api.github.com/users/dnhatn/gists{/gist_id}","starred_url":"https://api.github.com/users/dnhatn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnhatn/subscriptions","organizations_url":"https://api.github.com/users/dnhatn/orgs","repos_url":"https://api.github.com/users/dnhatn/repos","events_url":"https://api.github.com/users/dnhatn/events{/privacy}","received_events_url":"https://api.github.com/users/dnhatn/received_events","type":"User","site_admin":false},"created_at":"2018-03-07T22:20:32Z","updated_at":"2018-03-07T22:20:32Z","author_association":"MEMBER","body":"> Same shard, the same primary term, the same allocationId shard failed request processing did not remove the duplicate request , ShardEntry does not override equals and hashCode methods.\r\n\r\n@ywelsch and I discussed this issue. We will work on an improvement which avoids failing the same shard multiple times.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/371396872","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-371396872","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":371396872,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MTM5Njg3Mg==","user":{"login":"djjsindy","id":4013873,"node_id":"MDQ6VXNlcjQwMTM4NzM=","avatar_url":"https://avatars2.githubusercontent.com/u/4013873?v=4","gravatar_id":"","url":"https://api.github.com/users/djjsindy","html_url":"https://github.com/djjsindy","followers_url":"https://api.github.com/users/djjsindy/followers","following_url":"https://api.github.com/users/djjsindy/following{/other_user}","gists_url":"https://api.github.com/users/djjsindy/gists{/gist_id}","starred_url":"https://api.github.com/users/djjsindy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/djjsindy/subscriptions","organizations_url":"https://api.github.com/users/djjsindy/orgs","repos_url":"https://api.github.com/users/djjsindy/repos","events_url":"https://api.github.com/users/djjsindy/events{/privacy}","received_events_url":"https://api.github.com/users/djjsindy/received_events","type":"User","site_admin":false},"created_at":"2018-03-08T06:51:30Z","updated_at":"2018-03-08T06:51:30Z","author_association":"NONE","body":"@spinscale @DaveCTurner @dnhatn \r\nCould you provide your email address ? \r\nI will send it to you some log and detailed analysis ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/371437651","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-371437651","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":371437651,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MTQzNzY1MQ==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-03-08T09:53:50Z","updated_at":"2018-03-08T09:53:50Z","author_association":"CONTRIBUTOR","body":"@djjsindy please use david.turner@elastic.co and I will share with the rest of the team. I expect we will also want to summarise and discuss any analysis here, which might include things like index names, shard counts, and so on. If there is any information like this that you want not to be shared in public then please indicate as such in your email.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/371769326","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-371769326","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":371769326,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MTc2OTMyNg==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-03-09T10:05:43Z","updated_at":"2018-03-09T10:05:43Z","author_association":"CONTRIBUTOR","body":"@djjsindy I received your email but it only contained a handful of log messages, showing nothing unexpected. I responded:\r\n\r\n> Please could we have copies of full logs for the full duration of the outage so we can see what was taking so long to recover. The lines that you quote in your email show nothing unexpectedly slow, but only cover the very start of the outage. Ideally logs from all of the master nodes and some of the data nodes. I note that you have TRACE logging enabled for the ClusterService which is great - the logs will be quite large but this extra detail will help us a lot.\r\n\r\nCopying this here in case you didn't get my response.\r\n\r\nAdditionally, we just pushed 90bde12 (5.6), e0da114 (6.x), and 033a83b (master) which should make a difference to how fast the task summary `toString()` method takes. If you're able, please can you try this change out and let us know if it helps?","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/372405121","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-372405121","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":372405121,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MjQwNTEyMQ==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-03-12T17:57:21Z","updated_at":"2018-03-12T17:57:21Z","author_association":"CONTRIBUTOR","body":"Thanks for the logs. The issue you are facing is as follows. The master detects the failure of some of the nodes (but, crucially, not all of them) and publishes a cluster state update to remove them. When committing a cluster state update, each receiving node attempts to establish connections to all the nodes that are listed in the new cluster state, which includes all the failed nodes that the master has not yet detected as failed. Each such connection attempt times out after 30 seconds because of the network partition. In 5.3.x it looks like these attempts are made in sequence; #22984 (released in v5.4.0) improves this a little so they happen up to 5-at-a-time, but never to better than 30 seconds. Once all the connection attempts have timed out, the node can finish applying the cluster state update.\r\n\r\nMeanwhile, the master node has detected the failure of the rest of the nodes and publishes the next update. Once the other nodes have finished failing to connect to all their disconnected peers and applied the first update, the second update seems to be applied reasonably quickly.\r\n\r\nThis explains why waiting before sending the first cluster state update improves the situation dramatically: if the first cluster state update is delayed for long enough to capture all of the failed nodes then its recipients do not waste any time trying to connect to their failed peers and can apply it reasonably quickly.\r\n\r\nHowever, this is unlikely to be the solution we choose: it means that Elasticsearch will block writes for the defined waiting time on the failure of just a single node. We need to think about this more deeply.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/372615699","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-372615699","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":372615699,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MjYxNTY5OQ==","user":{"login":"djjsindy","id":4013873,"node_id":"MDQ6VXNlcjQwMTM4NzM=","avatar_url":"https://avatars2.githubusercontent.com/u/4013873?v=4","gravatar_id":"","url":"https://api.github.com/users/djjsindy","html_url":"https://github.com/djjsindy","followers_url":"https://api.github.com/users/djjsindy/followers","following_url":"https://api.github.com/users/djjsindy/following{/other_user}","gists_url":"https://api.github.com/users/djjsindy/gists{/gist_id}","starred_url":"https://api.github.com/users/djjsindy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/djjsindy/subscriptions","organizations_url":"https://api.github.com/users/djjsindy/orgs","repos_url":"https://api.github.com/users/djjsindy/repos","events_url":"https://api.github.com/users/djjsindy/events{/privacy}","received_events_url":"https://api.github.com/users/djjsindy/received_events","type":"User","site_admin":false},"created_at":"2018-03-13T10:18:52Z","updated_at":"2018-03-13T10:18:52Z","author_association":"NONE","body":"@DaveCTurner \r\nIndeed, as you analyzed the situation: Version 5.4 uses 5 management threads to create new connections. When the number of the nodes in the cluster is very large, this concurrency mode is not enough, which will cause the data node to process cluster state very long, this will affect the next cluster state processing.\r\n\r\nAbout write blocking, my sync data process will continue to retry when it encounters an error.  The time from the start of write blocking to the write operation can be performed normally will be longer. It includes the time to write retry.\r\n\r\nMy opinion:\r\n\r\n- In the network partition,master should handles disconnected events earlier than shard failed events .   Because the node disconnection event occurs at the same time, these node disconnected events are handled in one batch.\r\n\r\nHelp me see if my opinion is feasible?  If my suggestion is feasible, I will try to create a pull request.\r\n\r\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/372637166","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-372637166","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":372637166,"node_id":"MDEyOklzc3VlQ29tbWVudDM3MjYzNzE2Ng==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-03-13T11:42:27Z","updated_at":"2018-03-13T11:42:27Z","author_association":"CONTRIBUTOR","body":"> In the network partition,master should handles disconnected events earlier than shard failed events . Because the node disconnection event occurs at the same time, these node disconnected events are handled in one batch.\r\n\r\nIt's not clear that the shard-failed events have anything to do with this. As far as I can tell, it's just about the node disconnections being split across multiple updates.\r\n\r\nHowever, I don't like the idea of trying to get all the node disconnection events to occur at the same time. It might be possible in the kind of clean partition you are simulating, but it would leave us open to the same kind of problem in more complicated scenarios. Fundamentally, there are no natural events triggered in the kind of network partition you are simulating, so we must rely on timeouts to detect node disconnection, and I think anything involving timeouts is going to have pathological behaviours similar to the one we're trying to avoid.\r\n\r\nI think I would prefer better handling of node disconnections that are split across multiple updates instead. If applying a new cluster state did not try and synchronously connect to all the nodes listed in the new cluster state then we would be able to move onto subsequent cluster states much more quickly, removing further batches of failed nodes as their failures are detected.\r\n\r\nThis sounds nontrivial to achieve, for at least two reasons:\r\n\r\n1. Connecting is a blocking operation at quite a low level in Elasticsearch, but I think it'd need to be made asynchronous so it doesn't consume a whole thread for each node, and so it can be cancelled safely.\r\n\r\n2. Fully exposing the applied cluster state before all the nodes are connected is racy: what happens if we try and talk to a node in a just-applied cluster state but we're still establishing a connection? Throwing an error is bad because we'll end up trying to remove that node from the cluster again; blocking until the connection is established is also bad because in the situation we're looking at here everything will get stuck until the connections time out; queueing the request up just defers the problem until the queue's full.\r\n\r\nI'm raising this for discussion with the wider team, as it'd be good to get some more ideas.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/372747885","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-372747885","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":372747885,"node_id":"MDEyOklzc3VlQ29tbWVudDM3Mjc0Nzg4NQ==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-03-13T17:22:37Z","updated_at":"2018-03-13T17:22:37Z","author_association":"CONTRIBUTOR","body":"We had a good discussion on this subject, and came up with three clear ideas that should help improve Elasticsearch's behaviour in this situation: #29022, #29023 and #29025.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374452664","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-374452664","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":374452664,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDQ1MjY2NA==","user":{"login":"djjsindy","id":4013873,"node_id":"MDQ6VXNlcjQwMTM4NzM=","avatar_url":"https://avatars2.githubusercontent.com/u/4013873?v=4","gravatar_id":"","url":"https://api.github.com/users/djjsindy","html_url":"https://github.com/djjsindy","followers_url":"https://api.github.com/users/djjsindy/followers","following_url":"https://api.github.com/users/djjsindy/following{/other_user}","gists_url":"https://api.github.com/users/djjsindy/gists{/gist_id}","starred_url":"https://api.github.com/users/djjsindy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/djjsindy/subscriptions","organizations_url":"https://api.github.com/users/djjsindy/orgs","repos_url":"https://api.github.com/users/djjsindy/repos","events_url":"https://api.github.com/users/djjsindy/events{/privacy}","received_events_url":"https://api.github.com/users/djjsindy/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T02:36:42Z","updated_at":"2018-03-20T02:36:42Z","author_association":"NONE","body":"@DaveCTurner \r\nThank you for your discussion.We discussed last week and very much appreciated this universal solution. \r\n1. It might be possible in the kind of clean partition. Our business is very large and our applications are often deployed in multiple racks and multiple cities. This can ensure the disaster recovery of the rack level or city level. For example, the rack outage or network disconnection, the speed of our application recovery is very important.\r\n2.  \" I don't like the idea of trying to get all the node disconnection events to occur at the same time. It might be possible in the kind of clean partition you are simulating, but it would leave us open to the same kind of problem in more complicated scenarios\"  -- Can you elaborate on this complex scenarios? ","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/374533463","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-374533463","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":374533463,"node_id":"MDEyOklzc3VlQ29tbWVudDM3NDUzMzQ2Mw==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-03-20T09:40:05Z","updated_at":"2018-03-20T09:40:05Z","author_association":"CONTRIBUTOR","body":"You're welcome @djjsindy. Thank you in turn for your help in digging into the issue.\r\n\r\nNote that although we opened those issues, no work on them is currently scheduled so we have marked them with the `adoptme` label. PRs are welcome.\r\n\r\n> \" I don't like the idea of trying to get all the node disconnection events to occur at the same time. It might be possible in the kind of clean partition you are simulating, but it would leave us open to the same kind of problem in more complicated scenarios\" -- Can you elaborate on this complex scenarios?\r\n\r\nSure. Something like a single-node failure a short time before a whole-rack failure would be troublesome: no matter how long you wait after the single-node failure, there's always a chance that you'd decide to proceed with the cluster-state update to remove it at exactly the wrong moment, ending up the very situation we were trying to avoid. It'd be less likely, but in a sense that makes it worse: it'd be much more of a struggle to reproduce and diagnose it.\r\n\r\nI'm closing this issue as there's no further action required here.","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/399781967","html_url":"https://github.com/elastic/elasticsearch/issues/28920#issuecomment-399781967","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/28920","id":399781967,"node_id":"MDEyOklzc3VlQ29tbWVudDM5OTc4MTk2Nw==","user":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"created_at":"2018-06-24T19:40:20Z","updated_at":"2018-06-24T19:40:20Z","author_association":"CONTRIBUTOR","body":"Recently there have been a couple of threads on the discussion forums that look closely related to this:\r\n\r\n- https://discuss.elastic.co/t/cluster-instability-if-master-leader-is-stopped/136180\r\n- https://discuss.elastic.co/t/large-pending-tasks-backlog/136116","performed_via_github_app":null}]