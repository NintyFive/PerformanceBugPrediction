[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218386004","html_url":"https://github.com/elastic/elasticsearch/issues/18253#issuecomment-218386004","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18253","id":218386004,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODM4NjAwNA==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2016-05-11T07:44:26Z","updated_at":"2016-05-11T07:44:26Z","author_association":"MEMBER","body":"Did you try with a smaller batch size ? 100k seems big and could potentially use a lot of memory on the es side. What happens with a batch size of 1k ? You should also check the GC logs to see if your nodes have enough memory to handle a scan with big batches. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218439140","html_url":"https://github.com/elastic/elasticsearch/issues/18253#issuecomment-218439140","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18253","id":218439140,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODQzOTE0MA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-05-11T12:02:15Z","updated_at":"2016-05-11T12:02:15Z","author_association":"CONTRIBUTOR","body":"Do you realise that you're asking for 100,000 results X number of primary shards?  That's huge.  I'd reduce this to eg 5000 docs and remeasure.\n\nAlso, the `scroll` (you said `scan` but i think you meant `scroll`) needs to be long enough for you to process your docs and come back for the next tranche.  \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218492233","html_url":"https://github.com/elastic/elasticsearch/issues/18253#issuecomment-218492233","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18253","id":218492233,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODQ5MjIzMw==","user":{"login":"jsnod","id":1916150,"node_id":"MDQ6VXNlcjE5MTYxNTA=","avatar_url":"https://avatars2.githubusercontent.com/u/1916150?v=4","gravatar_id":"","url":"https://api.github.com/users/jsnod","html_url":"https://github.com/jsnod","followers_url":"https://api.github.com/users/jsnod/followers","following_url":"https://api.github.com/users/jsnod/following{/other_user}","gists_url":"https://api.github.com/users/jsnod/gists{/gist_id}","starred_url":"https://api.github.com/users/jsnod/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jsnod/subscriptions","organizations_url":"https://api.github.com/users/jsnod/orgs","repos_url":"https://api.github.com/users/jsnod/repos","events_url":"https://api.github.com/users/jsnod/events{/privacy}","received_events_url":"https://api.github.com/users/jsnod/received_events","type":"User","site_admin":false},"created_at":"2016-05-11T15:18:50Z","updated_at":"2016-05-11T15:18:50Z","author_association":"CONTRIBUTOR","body":"To clarify, the batch size in my benchmark code is not the batch size for `scan` it's simply how often I print out the timer value (every 100k docs).  I am simply calling `scan` (https://elasticsearch-py.readthedocs.io/en/master/helpers.html#scan) with default values, and iterating over the results. (see **Steps to reproduce** in my original post).\n\nRegarding the `scroll` value, it's plenty long, as I'm not doing any processing of the results, simply iterating over them.  In any case, I've tried multiple values there and it has no affect.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218518151","html_url":"https://github.com/elastic/elasticsearch/issues/18253#issuecomment-218518151","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18253","id":218518151,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODUxODE1MQ==","user":{"login":"jimczi","id":15977469,"node_id":"MDQ6VXNlcjE1OTc3NDY5","avatar_url":"https://avatars0.githubusercontent.com/u/15977469?v=4","gravatar_id":"","url":"https://api.github.com/users/jimczi","html_url":"https://github.com/jimczi","followers_url":"https://api.github.com/users/jimczi/followers","following_url":"https://api.github.com/users/jimczi/following{/other_user}","gists_url":"https://api.github.com/users/jimczi/gists{/gist_id}","starred_url":"https://api.github.com/users/jimczi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimczi/subscriptions","organizations_url":"https://api.github.com/users/jimczi/orgs","repos_url":"https://api.github.com/users/jimczi/repos","events_url":"https://api.github.com/users/jimczi/events{/privacy}","received_events_url":"https://api.github.com/users/jimczi/received_events","type":"User","site_admin":false},"created_at":"2016-05-11T16:44:18Z","updated_at":"2016-05-11T16:44:18Z","author_association":"MEMBER","body":"@jsnod sorry for the confusion. It seems that it's the opposite, the scan implementation in elasticsearch-py uses a ridiculously small batch size: 10. I might be wrong:\nhttps://github.com/elastic/elasticsearch-py/blob/6e94bf76adbbbb2fe5b927a3933ed535cd738387/elasticsearch/helpers/__init__.py\n@HonzaKral can you confirm ?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218722978","html_url":"https://github.com/elastic/elasticsearch/issues/18253#issuecomment-218722978","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18253","id":218722978,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODcyMjk3OA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-05-12T10:47:56Z","updated_at":"2016-05-12T10:47:56Z","author_association":"CONTRIBUTOR","body":"@jsnod I've tried this out locally in 1.7.3.  I indexed 2 million empty docs then scrolled through them (with `search_type=scan`).\n\nWith size `10`, these are the times each batch took in seconds:\n\n```\n100000     9.558165\n200000     9.448229\n300000     9.986946\n400000     10.793542\n500000     11.296383\n600000     12.203433\n700000     12.712313\n800000     13.569650\n900000     14.012085\n1000000    14.735239\n1100000    15.399333\n1200000    16.301194\n1300000    16.838166\n1400000    17.399276\n1500000    18.171454\n1600000    18.972609\n1700000    19.450955\n1800000    20.162497\n1900000    21.228723\n2000000    20.770031\n```\n\nWith size `1000` (ie num_shards \\* 1000), I got a much flatter graph:\n\n```\n100000     4.770898\n200000     4.679908\n300000     4.660660\n400000     4.612432\n500000     4.797696\n600000     4.670094\n700000     4.723260\n800000     4.754570\n900000     4.747548\n1000000    4.748295\n1100000    4.680469\n1200000    4.788206\n1300000    4.975887\n1400000    4.822974\n1500000    4.824638\n1600000    4.745880\n1700000    4.884137\n1800000    4.884558\n1900000    4.747686\n2000000    4.783460\n```\n\nThe best value for `size` depends on hardware, doc size etc.  I tried `5000` too and got similar results to size `1000`.  \n\nSo I'd recommend setting size to about `1000` (or at least experimenting with values around there).  Failing that, you can break your docs down into tranches (eg a filter on created_date) and run several reindexing jobs.  \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218729900","html_url":"https://github.com/elastic/elasticsearch/issues/18253#issuecomment-218729900","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18253","id":218729900,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODcyOTkwMA==","user":{"login":"HonzaKral","id":32132,"node_id":"MDQ6VXNlcjMyMTMy","avatar_url":"https://avatars0.githubusercontent.com/u/32132?v=4","gravatar_id":"","url":"https://api.github.com/users/HonzaKral","html_url":"https://github.com/HonzaKral","followers_url":"https://api.github.com/users/HonzaKral/followers","following_url":"https://api.github.com/users/HonzaKral/following{/other_user}","gists_url":"https://api.github.com/users/HonzaKral/gists{/gist_id}","starred_url":"https://api.github.com/users/HonzaKral/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HonzaKral/subscriptions","organizations_url":"https://api.github.com/users/HonzaKral/orgs","repos_url":"https://api.github.com/users/HonzaKral/repos","events_url":"https://api.github.com/users/HonzaKral/events{/privacy}","received_events_url":"https://api.github.com/users/HonzaKral/received_events","type":"User","site_admin":false},"created_at":"2016-05-12T11:23:49Z","updated_at":"2016-05-12T11:23:49Z","author_association":"MEMBER","body":"@jimferenczi The `elasticsearch-py` client doesn't set any default value for the size so it uses the defaults. You can always raise it by passing in a larger number as argument.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/218897798","html_url":"https://github.com/elastic/elasticsearch/issues/18253#issuecomment-218897798","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/18253","id":218897798,"node_id":"MDEyOklzc3VlQ29tbWVudDIxODg5Nzc5OA==","user":{"login":"jsnod","id":1916150,"node_id":"MDQ6VXNlcjE5MTYxNTA=","avatar_url":"https://avatars2.githubusercontent.com/u/1916150?v=4","gravatar_id":"","url":"https://api.github.com/users/jsnod","html_url":"https://github.com/jsnod","followers_url":"https://api.github.com/users/jsnod/followers","following_url":"https://api.github.com/users/jsnod/following{/other_user}","gists_url":"https://api.github.com/users/jsnod/gists{/gist_id}","starred_url":"https://api.github.com/users/jsnod/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jsnod/subscriptions","organizations_url":"https://api.github.com/users/jsnod/orgs","repos_url":"https://api.github.com/users/jsnod/repos","events_url":"https://api.github.com/users/jsnod/events{/privacy}","received_events_url":"https://api.github.com/users/jsnod/received_events","type":"User","site_admin":false},"created_at":"2016-05-12T21:56:35Z","updated_at":"2016-05-12T21:56:35Z","author_association":"CONTRIBUTOR","body":"EUREKA!  Simply passing in a `size=1000` kwarg into `scan()` has given me results similar to those posted by @clintongormley above:\n\n```\n    ...\n    Scanned 6,700,000 docs (batch time: 0:00:08 / total time: 0:12:52 )\n    Scanned 6,800,000 docs (batch time: 0:00:08 / total time: 0:13:00 )\n    Scanned 6,900,000 docs (batch time: 0:00:08 / total time: 0:13:09 )\n    Scanned 7,000,000 docs (batch time: 0:00:08 / total time: 0:13:17 )\n    Scanned 7,100,000 docs (batch time: 0:00:08 / total time: 0:13:26 )\n    Scanned 7,200,000 docs (batch time: 0:00:08 / total time: 0:13:34 )\n    Scanned 7,300,000 docs (batch time: 0:00:08 / total time: 0:13:43 )\n    Scanned 7,400,000 docs (batch time: 0:00:08 / total time: 0:13:52 )\n    Scanned 7,500,000 docs (batch time: 0:00:08 / total time: 0:14:00 )\n    Scanned 7,600,000 docs (batch time: 0:00:08 / total time: 0:14:09 )\n    Scanned 7,700,000 docs (batch time: 0:00:08 / total time: 0:14:18 )\n    Scanned 7,800,000 docs (batch time: 0:00:08 / total time: 0:14:27 )\n    Scanned 7,900,000 docs (batch time: 0:00:08 / total time: 0:14:36 )\n    Scanned 8,000,000 docs (batch time: 0:00:08 / total time: 0:14:44 )\n    Scanned 8,100,000 docs (batch time: 0:00:09 / total time: 0:14:53 )\n    Scanned 8,200,000 docs (batch time: 0:00:08 / total time: 0:15:02 )\n    Scanned 8,300,000 docs (batch time: 0:00:09 / total time: 0:15:12 )\n    Scanned 8,400,000 docs (batch time: 0:00:09 / total time: 0:15:21 )\n    Scanned 8,500,000 docs (batch time: 0:00:09 / total time: 0:15:30 )\n    Scanned 8,600,000 docs (batch time: 0:00:09 / total time: 0:15:39 )\n    Scanned 8,700,000 docs (batch time: 0:00:09 / total time: 0:15:48 )\n    Scanned 8,800,000 docs (batch time: 0:00:09 / total time: 0:15:58 )\n    Scanned 8,900,000 docs (batch time: 0:00:09 / total time: 0:16:07 )\n    Scanned 9,000,000 docs (batch time: 0:00:09 / total time: 0:16:17 )\n    Scanned 9,100,000 docs (batch time: 0:00:09 / total time: 0:16:26 )\n    Scanned 9,200,000 docs (batch time: 0:00:09 / total time: 0:16:36 )\n    Scanned 9,300,000 docs (batch time: 0:00:09 / total time: 0:16:45 )\n    Scanned 9,400,000 docs (batch time: 0:00:09 / total time: 0:16:55 )\n    Scanned 9,500,000 docs (batch time: 0:00:09 / total time: 0:17:04 )\n    Scanned 9,600,000 docs (batch time: 0:00:09 / total time: 0:17:14 )\n    Scanned 9,700,000 docs (batch time: 0:00:09 / total time: 0:17:24 )\n    Scanned 9,800,000 docs (batch time: 0:00:09 / total time: 0:17:33 )\n    Scanned 9,900,000 docs (batch time: 0:00:09 / total time: 0:17:43 )\n    ...\n```\n\nWhat previously took 6 hours now takes about 18 minutes!\n\nAfter pulling my hair out for over a week, it seems insane that `elasticsearch-py` uses a default `size=10` when calling `scan()`.  We were effectively DDoS'ing ES with 100x the number of queries than if we had passed in `size=1000`.  A default of 10 makes sense for `search()` but presumably when calling `scan()` you are iterating over a relatively large set.  I will close this ticket and open a new one at `elasticsearch-py` to discuss upping the default size for `scan()` or at least updating the documentation to explain why using a much larger `size` would be beneficial, especially when calling `reindex()`.\n\nThanks ES team for getting us past this bump... onward to 2.X!\n","performed_via_github_app":null}]