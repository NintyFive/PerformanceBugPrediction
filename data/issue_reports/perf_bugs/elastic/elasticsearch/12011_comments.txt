[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/118638036","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-118638036","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":118638036,"node_id":"MDEyOklzc3VlQ29tbWVudDExODYzODAzNg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-07-05T16:52:28Z","updated_at":"2015-07-05T16:52:28Z","author_association":"CONTRIBUTOR","body":"Hi @womwombat \n\nMy first guess would be heavy I/O caused by the optimize process.  You say you're using EBS volumes, without provisioned IOPS?  EBS is pretty slow unless you have provisioned IOPS, and you've set the merge throttling to a too high value for EBS.\n\nSo I think the optimize uses all the I/O which stops Elasticsearch from obtaining the file system lock quickly enough.  Try reducing the throttling.\n\nas a side note, you have `gateway.recover_after_nodes` set to 2, but `expected_nodes` set to 1.  This doesn't make sense.  It waits for the expected nodes and only falls back to recover_after_nodes if it is still waiting when the recover_after_time period has expired.  You should set expected nodes to 3 in your case (or however many nodes you plan on having)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/120033587","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-120033587","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":120033587,"node_id":"MDEyOklzc3VlQ29tbWVudDEyMDAzMzU4Nw==","user":{"login":"womwombat","id":13163142,"node_id":"MDQ6VXNlcjEzMTYzMTQy","avatar_url":"https://avatars3.githubusercontent.com/u/13163142?v=4","gravatar_id":"","url":"https://api.github.com/users/womwombat","html_url":"https://github.com/womwombat","followers_url":"https://api.github.com/users/womwombat/followers","following_url":"https://api.github.com/users/womwombat/following{/other_user}","gists_url":"https://api.github.com/users/womwombat/gists{/gist_id}","starred_url":"https://api.github.com/users/womwombat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/womwombat/subscriptions","organizations_url":"https://api.github.com/users/womwombat/orgs","repos_url":"https://api.github.com/users/womwombat/repos","events_url":"https://api.github.com/users/womwombat/events{/privacy}","received_events_url":"https://api.github.com/users/womwombat/received_events","type":"User","site_admin":false},"created_at":"2015-07-09T15:26:04Z","updated_at":"2015-07-09T15:26:04Z","author_association":"NONE","body":"Hi,\n\nAs a side note, this issue arise suddently, we've been running this cluster for weeks (~ 8 weeks at least), but following your wise advise we have set the indices.store.throttle.max_bytes_per_sec: 5mb.\n\nWe have fixed what was a typo in the expected_nodes / gateway_recover_after_nodes too.\n\nHowever the issue still goes on :/\n\n```\n[2015-07-08 08:30:49,271][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][2]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][2] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][2], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:30:49,317][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89042\n[2015-07-08 08:30:54,359][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][0]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][0] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][0], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:30:59,360][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][5]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][5] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][5], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:30:59,372][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89043\n[2015-07-08 08:31:04,413][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][4]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][4] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][4], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:31:09,414][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][3]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][3] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][3], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:31:14,415][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][1]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][1] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][1], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:31:14,424][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89044\n[2015-07-08 08:31:14,466][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]\n[2015-07-08 08:31:14,474][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89045\n[2015-07-08 08:31:38,228][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:31:38,228][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:31:38,228][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:31:38,228][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:32:05,411][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89046\n[2015-07-08 08:32:05,412][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [NONE] to [ALL]\n[2015-07-08 08:32:05,418][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89047\n[2015-07-08 08:32:10,460][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][1]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][1] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][1], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:32:15,461][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][5]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][5] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][5], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:32:20,462][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][2]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][2] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][2], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:32:20,475][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89048\n[2015-07-08 08:32:25,516][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][4]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][4] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][4], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:32:30,517][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][0]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][0] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][0], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:32:35,517][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][3]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][3] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][3], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:32:35,532][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89049\n[2015-07-08 08:32:38,118][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:32:38,118][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:32:38,118][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:32:38,118][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:32:40,573][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][1]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][1] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][1], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:32:45,574][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][5]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [index_a][5] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][5], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-07-08 08:32:45,584][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89050\n[2015-07-08 08:32:45,626][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]\n[2015-07-08 08:32:45,662][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89051\n[2015-07-08 08:33:39,685][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:33:39,686][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:33:39,686][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:33:39,686][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:34:41,144][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:34:41,145][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:34:41,145][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:34:41,145][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:35:37,612][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:35:37,612][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:35:37,612][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:35:37,612][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:36:41,024][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:36:41,025][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:36:41,025][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:36:41,025][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:37:14,212][INFO ][node                     ] [elasticsearch-nodes2.localdomain] stopping ...\n[2015-07-08 08:37:14,254][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.\njava.util.concurrent.RejectedExecutionException: Worker has already been shutdown\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)\n    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)\n    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)\n    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)\n    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)\n    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)\n    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)\n    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)\n    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:360)\n    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:355)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)\n    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)\n    at org.elasticsearch.cluster.service.InternalClusterService.doStop(InternalClusterService.java:174)\n    at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)\n    at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:307)\n    at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:331)\n    at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:82)\n[2015-07-08 08:37:14,256][DEBUG][discovery.zen.fd         ] [elasticsearch-nodes2.localdomain] [master] stopping fault detection against master [[elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_\nstorage_nodes=1}], reason [zen disco stop]\n[2015-07-08 08:37:14,296][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.\njava.util.concurrent.RejectedExecutionException: Worker has already been shutdown\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)\n    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)\n    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)\n    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)\n    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)\n    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)\n    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)\n    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)\n    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:360)\n    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:355)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$2.handleException(TransportShardReplicationOperationAction.java:481)\n    at org.elasticsearch.transport.TransportService$2.run(TransportService.java:178)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-07-08 08:37:14,370][INFO ][node                     ] [elasticsearch-nodes2.localdomain] stopped\n[2015-07-08 08:37:14,370][INFO ][node                     ] [elasticsearch-nodes2.localdomain] closing ...\n[2015-07-08 08:37:14,378][DEBUG][com.amazonaws.http.IdleConnectionReaper] Reaper thread:\njava.lang.InterruptedException: sleep interrupted\n    at java.lang.Thread.sleep(Native Method)\n    at com.amazonaws.http.IdleConnectionReaper.run(IdleConnectionReaper.java:112)\n[2015-07-08 08:37:14,378][DEBUG][com.amazonaws.http.IdleConnectionReaper] Shutting down reaper thread.\n```\n\nAnd when we stop/start the node:\n\n```\n[2015-07-08 08:37:57,146][INFO ][node                     ] [elasticsearch-nodes2.localdomain] version[1.6.0], pid[33261], build[cdd3ac4/2015-06-09T13:36:34Z]\n[2015-07-08 08:37:57,146][INFO ][node                     ] [elasticsearch-nodes2.localdomain] initializing ...\n[2015-07-08 08:37:57,163][INFO ][plugins                  ] [elasticsearch-nodes2.localdomain] loaded [cloud-aws], sites [HQ, whatson, kopf]\n[2015-07-08 08:37:57,203][INFO ][env                      ] [elasticsearch-nodes2.localdomain] using [1] data paths, mounts [[/srv/data (/dev/mapper/lvm--raid--0-lvm0)]], net usable_space [475.9gb], net total_space [499.6gb], types [xfs]\n[2015-07-08 08:38:00,074][INFO ][node                     ] [elasticsearch-nodes2.localdomain] initialized\n[2015-07-08 08:38:00,074][INFO ][node                     ] [elasticsearch-nodes2.localdomain] starting ...\n[2015-07-08 08:38:00,254][INFO ][transport                ] [elasticsearch-nodes2.localdomain] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/10.210.14.19:9300]}\n[2015-07-08 08:38:00,273][INFO ][discovery                ] [elasticsearch-nodes2.localdomain] cluster-es/t7ZN91B7Se2qT6NsRRau5g\n[2015-07-08 08:38:04,343][INFO ][cluster.service          ] [elasticsearch-nodes2.localdomain] detected_master [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}, added {[elastics\nearch-nodes3.localdomain][Bg5OX3aoTK-pWNImAkf-vw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1},[elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_z\none=eu-west-1c, max_local_storage_nodes=1},}, reason: zen-disco-receive(from master [[elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}])\n[2015-07-08 08:38:04,350][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]\n[2015-07-08 08:38:04,350][INFO ][indices.recovery         ] [elasticsearch-nodes2.localdomain] updating [indices.recovery.translog_size] from [512kb] to [2mb]\n[2015-07-08 08:38:04,351][INFO ][indices.store            ] [elasticsearch-nodes2.localdomain] updating indices.store.throttle.max_bytes_per_sec from [5mb] to [100mb], note, type is [MERGE]\n[2015-07-08 08:38:04,394][INFO ][http                     ] [elasticsearch-nodes2.localdomain] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/10.210.14.19:9200]}\n[2015-07-08 08:38:04,394][INFO ][node                     ] [elasticsearch-nodes2.localdomain] started\n[2015-07-08 08:39:14,425][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:39:14,425][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:39:14,426][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:39:14,426][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:39:35,655][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89054\n[2015-07-08 08:39:35,655][INFO ][cluster.service          ] [elasticsearch-nodes2.localdomain] removed {[elasticsearch-nodes3.localdomain][Bg5OX3aoTK-pWNImAkf-vw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1},}, reason: zen-disco-rece\nive(from master [[elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}])\n[2015-07-08 08:39:35,745][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89055\n[2015-07-08 08:39:36,153][DEBUG][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_b][4] starting recovery from local ...\n[2015-07-08 08:39:36,155][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] using existing shard data, translog id [1436278480889]\n[2015-07-08 08:39:36,158][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] try recover from translog file translog-1436278480889 locations: [/srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/4/translog]\n[2015-07-08 08:39:36,159][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] Translog file found in /srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/4/translog - renaming\n[2015-07-08 08:39:36,159][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] Renamed translog from translog-1436278480889 to translog-1436278480889.recovering\n[2015-07-08 08:39:36,167][DEBUG][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_b][5] starting recovery from local ...\n[2015-07-08 08:39:36,170][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] using existing shard data, translog id [1436278480893]\n[2015-07-08 08:39:36,171][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] try recover from translog file translog-1436278480893 locations: [/srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/5/translog]\n[2015-07-08 08:39:36,171][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] Translog file found in /srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/5/translog - renaming\n[2015-07-08 08:39:36,171][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] Renamed translog from translog-1436278480893 to translog-1436278480893.recovering\n[2015-07-08 08:39:36,527][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] recovering translog file: /srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/5/translog/translog-1436278480893.recovering length: 17\n[2015-07-08 08:39:36,527][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] recovering translog file: /srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/4/translog/translog-1436278480889.recovering length: 17\n[2015-07-08 08:39:36,527][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] ignoring translog EOF exception, the last operation was not properly written\njava.io.EOFException\n    at org.elasticsearch.common.io.stream.InputStreamStreamInput.readByte(InputStreamStreamInput.java:43)\n    at org.elasticsearch.common.io.stream.StreamInput.readInt(StreamInput.java:116)\n    at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:59)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:267)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-07-08 08:39:36,527][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] ignoring translog EOF exception, the last operation was not properly written\njava.io.EOFException\n    at org.elasticsearch.common.io.stream.InputStreamStreamInput.readByte(InputStreamStreamInput.java:43)\n    at org.elasticsearch.common.io.stream.StreamInput.readInt(StreamInput.java:116)\n    at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:59)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:267)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-07-08 08:39:36,532][TRACE][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery completed from local, took [379ms]\n    index    : files           [13] with total_size [3gb], took[6ms]\n             : recovered_files [0] with total_size [0b]\n             : reusing_files   [13] with total_size [3gb]\n    start    : took [368ms], check_index [0s]\n    translog : number_of_operations [0], took [4ms]\n[2015-07-08 08:39:36,532][TRACE][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery completed from local, took [364ms]\n    index    : files           [13] with total_size [3.1gb], took[3ms]\n             : recovered_files [0] with total_size [0b]\n             : reusing_files   [13] with total_size [3.1gb]\n    start    : took [356ms], check_index [0s]\n    translog : number_of_operations [0], took [4ms]\n[2015-07-08 08:39:36,536][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89056\n[2015-07-08 08:39:59,521][DEBUG][com.amazonaws.internal.SdkSSLSocket] shutting down output of ec2.eu-west-1.amazonaws.com/178.236.7.129:443\n[2015-07-08 08:39:59,521][DEBUG][com.amazonaws.internal.SdkSSLSocket] closing ec2.eu-west-1.amazonaws.com/178.236.7.129:443\n[2015-07-08 08:40:24,346][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89057\n[2015-07-08 08:40:24,346][INFO ][cluster.service          ] [elasticsearch-nodes2.localdomain] added {[elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1},}, reason: zen-disco-receiv\ne(from master [[elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}])\n[2015-07-08 08:40:35,973][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:40:35,973][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:40:35,973][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:40:35,973][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:40:41,857][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:40:41,857][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:40:41,857][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:40:41,857][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-07-08 08:40:49,191][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89058\n[2015-07-08 08:40:49,192][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [NONE] to [ALL]\n[2015-07-08 08:40:49,205][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89059\n[2015-07-08 08:40:49,305][DEBUG][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_a][1] starting recovery from local ...\n[2015-07-08 08:40:49,306][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_a][1] no translog id set (indexShouldExist [false])\n[2015-07-08 08:40:49,318][TRACE][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery completed from local, took [13ms]\n    index    : files           [0] with total_size [0b], took[1ms]\n             : recovered_files [0] with total_size [0b]\n             : reusing_files   [0] with total_size [0b]\n    start    : took [11ms], check_index [0s]\n    translog : number_of_operations [0], took [0s]\n[2015-07-08 08:40:49,322][DEBUG][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_a][5] starting recovery from local ...\n[2015-07-08 08:40:49,324][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_a][5] cleaning existing shard, shouldn't exists\n[2015-07-08 08:40:49,618][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89060\n[2015-07-08 08:40:49,640][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] started recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc\nal_storage_nodes=1}, id [1]\n[2015-07-08 08:40:49,641][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_b][3] [1]\n[2015-07-08 08:40:49,645][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo\ncal_storage_nodes=1}\n[2015-07-08 08:40:49,657][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] Got exception on recovery\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)\n    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-07-08 08:40:49,657][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] will retrying recovery with id [1] in [500ms] (reason [source node does not have the shard listed in its state as allocated on the node])\n[2015-07-08 08:40:49,844][DEBUG][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_a][5] failed to list file details\njava.io.FileNotFoundException: segments_d\n    at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:255)\n    at org.apache.lucene.store.FileSwitchDirectory.fileLength(FileSwitchDirectory.java:147)\n    at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:63)\n    at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:63)\n    at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:63)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:171)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-07-08 08:40:49,844][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_a][5] no translog id set (indexShouldExist [false])\n[2015-07-08 08:40:49,848][TRACE][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery completed from local, took [526ms]\n    index    : files           [0] with total_size [0b], took[522ms]\n             : recovered_files [0] with total_size [0b]\n             : reusing_files   [0] with total_size [0b]\n    start    : took [3ms], check_index [0s]\n    translog : number_of_operations [0], took [0s]\n[2015-07-08 08:40:49,855][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89061\n[2015-07-08 08:40:49,872][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] started recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc\nal_storage_nodes=1}, id [2]\n[2015-07-08 08:40:49,872][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][2] [2]\n[2015-07-08 08:40:49,876][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo\ncal_storage_nodes=1}\n[2015-07-08 08:40:49,885][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] Got exception on recovery\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)\n    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-07-08 08:40:49,885][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] will retrying recovery with id [2] in [500ms] (reason [source node does not have the shard listed in its state as allocated on the node])\n[2015-07-08 08:40:50,157][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_b][3] [1]\n[2015-07-08 08:40:50,162][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo\ncal_storage_nodes=1}\n[2015-07-08 08:40:50,173][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89062\n[2015-07-08 08:40:50,318][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] starting recovery to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}, mark_as_relocated false\n[2015-07-08 08:40:50,326][INFO ][indices.recovery         ] [elasticsearch-nodes2.localdomain] Recovery with sync ID 12837074 numDocs: 12837074 vs. true\n[2015-07-08 08:40:50,326][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] skipping [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1} - identical sync id [AU5pGODmRtbfpQ748m1P] found on both source and target\n[2015-07-08 08:40:50,327][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: took [3.7ms]\n[2015-07-08 08:40:50,328][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: start\n[2015-07-08 08:40:50,340][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89063\n[2015-07-08 08:40:50,386][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][2] [2]\n[2015-07-08 08:40:50,389][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo\ncal_storage_nodes=1}\n[2015-07-08 08:40:50,795][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] Got exception on recovery\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][2] Phase[1] Execution failed\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)\n    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)\n    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][2] Failed to transfer [0] files with total size of [0b]\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)\n    ... 10 more\nCaused by: java.lang.IllegalStateException: try to recover [index_a][2] from primary shard with sync id but number of docs differ: 12627628 (elasticsearch-nodes1.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)\n    ... 11 more\n[2015-07-08 08:40:50,796][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] failing recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc\nal_storage_nodes=1}, id [2]. Send shard failure: [true]\n[2015-07-08 08:40:50,799][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][2]] marking and sending shard failed due to [failed recovery]\norg.elasticsearch.indices.recovery.RecoveryFailedException: [index_a][2]: Recovery failed from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1} into [elasticse\narch-nodes2.localdomain][t7ZN91B7Se2qT6NsRRau5g][elasticsearch-nodes2.localdomain][inet[/10.210.14.19:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)\n    at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)\n    at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][2] Phase[1] Execution failed\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)\n    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)\n    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][2] Failed to transfer [0] files with total size of [0b]\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)\n    ... 10 more\nCaused by: java.lang.IllegalStateException: try to recover [index_a][2] from primary shard with sync id but number of docs differ: 12627628 (elasticsearch-nodes1.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)\n    ... 11 more\n[2015-07-08 08:40:51,257][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: start took [929.1ms]\n[2015-07-08 08:40:51,257][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: updating current mapping to master\n[2015-07-08 08:40:51,261][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: sending transaction log operations\n[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] no translog operations (id: [1436278480889]) to send to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_avai\nlability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] sending final batch of [0][0b] (total: [0], id: [1436278480889]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][in\net[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: took [239.7micros]\n[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: sending transaction log operations\n[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] no translog operations (id: [1436278480889]) to send to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_avai\nlability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] sending final batch of [0][0b] (total: [0], id: [1436278480889]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][in\net[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:40:51,264][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: took [1.5ms]\n[2015-07-08 08:40:55,355][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] marking recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc\nal_storage_nodes=1} as done, id [1]\n[2015-07-08 08:40:55,355][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] recovery completed from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_l\nocal_storage_nodes=1}, took[5.7s]\n   phase1: recovered_files [0] with total_size of [0b], took [1ms], throttling_wait [0s]\n         : reusing_files   [0] with total_size of [0b]\n   phase2: start took [252ms]\n         : recovered [0] transaction log operations, took [0s]\n   phase3: recovered [0] transaction log operations, took [2ms]\n[2015-07-08 08:40:55,356][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89064\n[2015-07-08 08:40:55,385][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89065\n[2015-07-08 08:40:55,400][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] started recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc\nal_storage_nodes=1}, id [3]\n[2015-07-08 08:40:55,401][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_b][2] [3]\n[2015-07-08 08:40:55,403][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo\ncal_storage_nodes=1}\n[2015-07-08 08:40:55,404][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] starting recovery to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}, mark_as_relocated false\n[2015-07-08 08:40:55,406][INFO ][indices.recovery         ] [elasticsearch-nodes2.localdomain] Recovery with sync ID 12940164 numDocs: 12940164 vs. true\n[2015-07-08 08:40:55,406][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] skipping [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1} - identical sync id [AU5pGM3poRUqHcJOE4ov] found on both source and target\n[2015-07-08 08:40:55,406][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: took [2ms]\n[2015-07-08 08:40:55,406][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: start\n[2015-07-08 08:40:55,407][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] Got exception on recovery\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)\n    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-07-08 08:40:55,407][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] will retrying recovery with id [3] in [500ms] (reason [source node does not have the shard listed in its state as allocated on the node])\n[2015-07-08 08:40:55,424][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89066\n[2015-07-08 08:40:55,438][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] started recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc\nal_storage_nodes=1}, id [4]\n[2015-07-08 08:40:55,438][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][3] [4]\n[2015-07-08 08:40:55,440][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo\ncal_storage_nodes=1}\n[2015-07-08 08:40:55,443][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] Got exception on recovery\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)\n    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-07-08 08:40:55,443][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] will retrying recovery with id [4] in [500ms] (reason [source node does not have the shard listed in its state as allocated on the node])\n[2015-07-08 08:40:55,613][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: start took [207.2ms]\n[2015-07-08 08:40:55,614][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: updating current mapping to master\n[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: sending transaction log operations\n[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] no translog operations (id: [1436278480893]) to send to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_avai\nlability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] sending final batch of [0][0b] (total: [0], id: [1436278480893]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][in\net[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: took [164.1micros]\n[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: sending transaction log operations\n[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] no translog operations (id: [1436278480893]) to send to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_avai\nlability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] sending final batch of [0][0b] (total: [0], id: [1436278480893]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][in\net[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:40:55,618][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: took [936micros]\n[2015-07-08 08:40:55,907][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_b][2] [3]\n[2015-07-08 08:40:55,910][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo\ncal_storage_nodes=1}\n[2015-07-08 08:40:55,944][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][3] [4]\n[2015-07-08 08:40:55,946][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo\ncal_storage_nodes=1}\n[2015-07-08 08:40:56,315][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] Got exception on recovery\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][3] Phase[1] Execution failed\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)\n    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)\n    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][3] Failed to transfer [0] files with total size of [0b]\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)\n    ... 10 more\nCaused by: java.lang.IllegalStateException: try to recover [index_a][3] from primary shard with sync id but number of docs differ: 12599260 (elasticsearch-nodes1.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)\n    ... 11 more\n[2015-07-08 08:40:56,316][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] failing recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc\nal_storage_nodes=1}, id [4]. Send shard failure: [true]\n[2015-07-08 08:40:56,316][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][3]] marking and sending shard failed due to [failed recovery]\norg.elasticsearch.indices.recovery.RecoveryFailedException: [index_a][3]: Recovery failed from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1} into [elasticse\narch-nodes2.localdomain][t7ZN91B7Se2qT6NsRRau5g][elasticsearch-nodes2.localdomain][inet[/10.210.14.19:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)\n    at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)\n    at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][3] Phase[1] Execution failed\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)\n    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)\n    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][3] Failed to transfer [0] files with total size of [0b]\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)\n    ... 10 more\nCaused by: java.lang.IllegalStateException: try to recover [index_a][3] from primary shard with sync id but number of docs differ: 12599260 (elasticsearch-nodes1.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)\n    ... 11 more\n[2015-07-08 08:41:00,460][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] marking recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc\nal_storage_nodes=1} as done, id [3]\n[2015-07-08 08:41:00,461][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] recovery completed from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_l\nocal_storage_nodes=1}, took[5s]\n   phase1: recovered_files [0] with total_size of [0b], took [1ms], throttling_wait [0s]\n         : reusing_files   [0] with total_size of [0b]\n   phase2: start took [159ms]\n         : recovered [0] transaction log operations, took [0s]\n   phase3: recovered [0] transaction log operations, took [2ms]\n[2015-07-08 08:41:00,461][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89067\n[2015-07-08 08:41:00,487][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] starting recovery to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}, mark_as_relocated false\n[2015-07-08 08:41:00,489][INFO ][indices.recovery         ] [elasticsearch-nodes2.localdomain] Recovery with sync ID 0 numDocs: 12635618 vs. true\n[2015-07-08 08:41:00,501][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89068\n[2015-07-08 08:41:00,520][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][4] started recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loca\nl_storage_nodes=1}, id [5]\n[2015-07-08 08:41:00,520][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][4] [5]\n[2015-07-08 08:41:00,522][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][4] starting recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loc\nal_storage_nodes=1}\n[2015-07-08 08:41:01,589][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][4] Got exception on recovery\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][4] Phase[1] Execution failed\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)\n    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)\n    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][4] Failed to transfer [0] files with total size of [0b]\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)\n    ... 10 more\nCaused by: java.lang.IllegalStateException: try to recover [index_a][4] from primary shard with sync id but number of docs differ: 12533048 (elasticsearch-nodes3.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)\n    ... 11 more\n[2015-07-08 08:41:01,590][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][4] failing recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loca\nl_storage_nodes=1}, id [5]. Send shard failure: [true]\n[2015-07-08 08:41:01,590][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][4]] marking and sending shard failed due to [failed recovery]\norg.elasticsearch.indices.recovery.RecoveryFailedException: [index_a][4]: Recovery failed from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1} into [elasticsea\nrch-nodes2.localdomain][t7ZN91B7Se2qT6NsRRau5g][elasticsearch-nodes2.localdomain][inet[/10.210.14.19:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)\n    at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)\n    at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]][internal:index/shard/recovery/start_recovery]\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][4] Phase[1] Execution failed\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)\n    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)\n    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][4] Failed to transfer [0] files with total size of [0b]\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)\n    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)\n    ... 10 more\nCaused by: java.lang.IllegalStateException: try to recover [index_a][4] from primary shard with sync id but number of docs differ: 12533048 (elasticsearch-nodes3.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)\n    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)\n    ... 11 more\n[2015-07-08 08:41:05,539][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89069\n[2015-07-08 08:41:05,569][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89070\n[2015-07-08 08:41:05,587][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89071\n[2015-07-08 08:41:05,601][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] started recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loca\nl_storage_nodes=1}, id [6]\n[2015-07-08 08:41:05,601][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][0] [6]\n[2015-07-08 08:41:05,601][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] starting recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loc\nal_storage_nodes=1}\n[2015-07-08 08:41:05,606][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] starting recovery to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}, mark_as_relocated false\n[2015-07-08 08:41:05,607][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: recovering [segments_1], does not exists in remote\n[2015-07-08 08:41:05,607][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: recovering_files [1] with total_size [79b], reusing_files [0] with total_size [0b]\n[2015-07-08 08:41:05,634][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] starting recovery to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}, mark_as_relocated false\n[2015-07-08 08:41:05,636][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_28.cfe], does not exists in remote\n[2015-07-08 08:41:05,636][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_28.si], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_28.cfs], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_29.si], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_29.cfs], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_29.cfe], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_2b.si], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_2b.cfe], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_2b.cfs], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_2a.cfs], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_2a.cfe], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [_2a.si], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering [segments_f], does not exists in remote\n[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: recovering_files [13] with total_size [5.3mb], reusing_files [0] with total_size [0b]\n[2015-07-08 08:41:05,638][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89072\n[2015-07-08 08:41:05,653][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] started recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc\nal_storage_nodes=1}, id [7]\n[2015-07-08 08:41:05,653][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][3] [7]\n[2015-07-08 08:41:05,655][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo\ncal_storage_nodes=1}\n[2015-07-08 08:41:05,809][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89073\n[2015-07-08 08:41:06,045][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: took [410.1ms]\n[2015-07-08 08:41:06,045][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase2] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: start\n[2015-07-08 08:41:06,054][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase2] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: start took [8.3ms]\n[2015-07-08 08:41:06,054][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase2] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: updating current mapping to master\n[2015-07-08 08:41:06,057][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase2] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: sending transaction log operations\n[2015-07-08 08:41:06,077][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: took [471.4ms]\n[2015-07-08 08:41:06,078][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: start\n[2015-07-08 08:41:06,082][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: start took [3.5ms]\n[2015-07-08 08:41:06,082][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: updating current mapping to master\n[2015-07-08 08:41:06,085][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: sending transaction log operations\n[2015-07-08 08:41:06,094][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241kb] (total: [18462], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain\n][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,131][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.3kb] (total: [55022], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,349][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][240.4kb] (total: [19222], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,473][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.8kb] (total: [56302], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,506][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][240.3kb] (total: [19792], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,634][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.5kb] (total: [20205], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,670][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.5kb] (total: [56735], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,742][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][240.9kb] (total: [20205], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,789][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][245.1kb] (total: [56735], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,833][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.7kb] (total: [20205], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,877][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [56832], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,924][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.5kb] (total: [20640], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:06,963][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.2kb] (total: [57160], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,019][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][240.7kb] (total: [21338], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,043][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.3kb] (total: [57992], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,107][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][243kb] (total: [21499], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain\n][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,111][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.5kb] (total: [57992], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,172][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.2kb] (total: [57992], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,192][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.7kb] (total: [21499], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,234][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.3kb] (total: [57992], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,278][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][242.2kb] (total: [21499], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,294][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.6kb] (total: [57992], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,356][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.5kb] (total: [58611], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,365][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.7kb] (total: [22220], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,485][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.7kb] (total: [22528], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,487][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.8kb] (total: [59122], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,555][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.9kb] (total: [59661], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,571][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][242.2kb] (total: [23207], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,620][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.7kb] (total: [59661], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,657][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.4kb] (total: [23207], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,685][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.6kb] (total: [59661], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,741][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][242kb] (total: [23207], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain\n][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,749][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.3kb] (total: [59661], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,808][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.6kb] (total: [59661], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,825][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.9kb] (total: [23207], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,909][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][242.1kb] (total: [23567], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,953][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [60497], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:07,990][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending final batch of [439][200.2kb] (total: [24338], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.loca\nldomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,039][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.1kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,051][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase2] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: took [1.9s]\n[2015-07-08 08:41:08,051][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase3] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: sending transaction log operations\n[2015-07-08 08:41:08,062][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][352.6kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,097][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][242.4kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,103][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.3kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,129][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][301.7kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,157][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.6kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,164][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,183][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain\n][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,209][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.5kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma\nin][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,225][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,287][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.5kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,295][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending final batch of [436][105.5kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.loca\nldomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,349][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.9kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,413][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,476][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [61991], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,500][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase3] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca\nl_storage_nodes=1}: took [448.8ms]\n[2015-07-08 08:41:08,562][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.1kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.1kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,707][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.4kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,775][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.9kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,841][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.1kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,905][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:08,969][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.8kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:09,030][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.3kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:09,167][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.4kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:09,243][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.2kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:09,256][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89074\n[2015-07-08 08:41:09,307][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89075\n[2015-07-08 08:41:09,310][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:09,386][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][239.9kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:09,445][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.8kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:09,720][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.4kb] (total: [62854], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:09,810][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [63120], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:09,895][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.7kb] (total: [63977], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:09,979][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.1kb] (total: [63977], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:10,063][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.8kb] (total: [63977], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:10,145][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.3kb] (total: [63977], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:10,268][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [64410], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:10,499][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.8kb] (total: [65269], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:10,630][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.9kb] (total: [65277], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:10,773][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89076\n[2015-07-08 08:41:10,799][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.6kb] (total: [65305], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:10,893][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.4kb] (total: [65341], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:10,975][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [65363], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,080][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.1kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,189][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.1kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,294][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending final batch of [995][370.1kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.loca\nldomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,393][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: took [5.3s]\n[2015-07-08 08:41:11,393][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: sending transaction log operations\n[2015-07-08 08:41:11,399][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][299.2kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,406][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] marking recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc\nal_storage_nodes=1} as done, id [7]\n[2015-07-08 08:41:11,406][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] recovery completed from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_l\nocal_storage_nodes=1}, took[5.7s]\n   phase1: recovered_files [12] with total_size of [6.9mb], took [992ms], throttling_wait [0s]\n         : reusing_files   [0] with total_size of [0b]\n   phase2: start took [13ms]\n         : recovered [38341] transaction log operations, took [3.4s]\n   phase3: recovered [7286] transaction log operations, took [881ms]\n[2015-07-08 08:41:11,409][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89077\n[2015-07-08 08:41:11,440][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.4kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,479][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][304.5kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,521][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,561][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.9kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,597][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.1kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.8kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,674][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][354.1kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,714][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain\n][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,743][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][535.3kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma\nin][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:11,773][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending final batch of [395][991.8kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.loca\nldomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}\n[2015-07-08 08:41:12,092][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local\n_storage_nodes=1}: took [699.4ms]\n[2015-07-08 08:41:12,684][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89078\n[2015-07-08 08:41:12,859][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] marking recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loca\nl_storage_nodes=1} as done, id [6]\n[2015-07-08 08:41:12,859][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] recovery completed from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_lo\ncal_storage_nodes=1}, took[7.2s]\n   phase1: recovered_files [1] with total_size of [108b], took [13ms], throttling_wait [0s]\n         : reusing_files   [0] with total_size of [0b]\n   phase2: start took [4ms]\n         : recovered [54870] transaction log operations, took [5.8s]\n   phase3: recovered [10795] transaction log operations, took [668ms]\n[2015-07-08 08:41:12,862][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89079\n[2015-07-08 08:41:12,934][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89080\n[2015-07-08 08:41:19,194][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89081\n[2015-07-08 08:46:20,640][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89082\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/122241136","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-122241136","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":122241136,"node_id":"MDEyOklzc3VlQ29tbWVudDEyMjI0MTEzNg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-07-17T10:41:02Z","updated_at":"2015-07-17T10:41:02Z","author_association":"CONTRIBUTOR","body":"Hi @womwombat \n\nWe had a long talk about this internally, and need some more info to track down what is happening here.  Please could you give us hot threads output while you see the \"failed to lock\" message happening, as follows:\n\n```\ncurl -XGET \"http://localhost:9202/_nodes/hot_threads?threads=10000&ignore_idle_threads=false\"\n```\n- Could we have the logs from the moment you start the optimize, through closing the index, and reopening the other index.\n- Do you use scroll requests? If so, what scroll timeout do you set?\n- Could you try removing optimize from your process and see if that helps?\n- Another wild suggestion - do your servers use Haswell CPUs? Wondering if you are suffering from the futex bug https://github.com/elastic/elasticsearch/issues/11526\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/128643925","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-128643925","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":128643925,"node_id":"MDEyOklzc3VlQ29tbWVudDEyODY0MzkyNQ==","user":{"login":"womwombat","id":13163142,"node_id":"MDQ6VXNlcjEzMTYzMTQy","avatar_url":"https://avatars3.githubusercontent.com/u/13163142?v=4","gravatar_id":"","url":"https://api.github.com/users/womwombat","html_url":"https://github.com/womwombat","followers_url":"https://api.github.com/users/womwombat/followers","following_url":"https://api.github.com/users/womwombat/following{/other_user}","gists_url":"https://api.github.com/users/womwombat/gists{/gist_id}","starred_url":"https://api.github.com/users/womwombat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/womwombat/subscriptions","organizations_url":"https://api.github.com/users/womwombat/orgs","repos_url":"https://api.github.com/users/womwombat/repos","events_url":"https://api.github.com/users/womwombat/events{/privacy}","received_events_url":"https://api.github.com/users/womwombat/received_events","type":"User","site_admin":false},"created_at":"2015-08-07T08:49:54Z","updated_at":"2015-08-07T08:49:54Z","author_association":"NONE","body":"Hi,\n- The log bellow is the only log I have, maybe you want TRACE from something else ?\n- we don't use scroll requests\n- we can't right now, but we'll do this next week\n- our servers are not using haswell CPU \n- We have migrated all the nodes form AWS to on premises brand-new servers, with 64G ram, 256G SSD, and the pb still occurs in 1.7.0\n- here is a full log of one of the event yesterday\n\n```\n[2015-08-06 19:36:37,742][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][4]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][4] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][4], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-08-06 19:36:47,848][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-08-06 19:36:57,931][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][4]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][4] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][4], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n\n----\n\nThere're 3000 lines of this error, I've skipped them for readability\n\n----\n\n\n[2015-08-06 20:00:15,485][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-08-06 20:00:25,722][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-08-06 20:00:35,742][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-08-06 20:00:38,747][DEBUG][action.bulk              ] [node-01] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]\n[2015-08-06 20:00:44,367][INFO ][node                     ] [node-01] stopping ...\n[2015-08-06 20:00:44,396][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.\njava.util.concurrent.RejectedExecutionException: Worker has already been shutdown\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)\n    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)\n    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)\n    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)\n    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)\n    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)\n    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)\n    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)\n    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:360)\n    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:355)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)\n    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)\n    at org.elasticsearch.cluster.service.InternalClusterService.doStop(InternalClusterService.java:174)\n    at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)\n    at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:310)\n    at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:334)\n    at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:82)\n[2015-08-06 20:00:44,400][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.\njava.util.concurrent.RejectedExecutionException: Worker has already been shutdown\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)\n    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)\n    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)\n    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)\n    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)\n    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)\n    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)\n    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)\n    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.doRun(TransportSearchQueryThenFetchAction.java:152)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-08-06 20:00:44,425][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.\njava.util.concurrent.RejectedExecutionException: Worker has already been shutdown\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)\n    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)\n    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)\n    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)\n    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)\n    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)\n    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)\n    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)\n    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.doRun(TransportSearchQueryThenFetchAction.java:152)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-08-06 20:00:45,808][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-08-06 20:00:45,828][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.\njava.util.concurrent.RejectedExecutionException: Worker has already been shutdown\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)\n    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)\n    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)\n    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)\n    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)\n    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)\n    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)\n    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)\n    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)\n    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)\n    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:360)\n    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:355)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)\n    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)\n    at org.elasticsearch.cluster.service.InternalClusterService.add(InternalClusterService.java:236)\n    at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:146)\n    at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:96)\n    at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:88)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.retry(TransportShardReplicationOperationAction.java:501)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.retryBecauseUnavailable(TransportShardReplicationOperationAction.java:655)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.doRun(TransportShardReplicationOperationAction.java:362)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onNewClusterState(TransportShardReplicationOperationAction.java:504)\n    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.postAdded(ClusterStateObserver.java:201)\n    at org.elasticsearch.cluster.service.InternalClusterService$1.run(InternalClusterService.java:248)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[2015-08-06 20:00:46,409][INFO ][node                     ] [node-01] stopped\n[2015-08-06 20:00:46,409][INFO ][node                     ] [node-01] closing ...\n[2015-08-06 20:00:56,428][WARN ][cluster.action.index     ] [node-01] [my_custom_index_b] failed to lock all shards for index - timed out after 30 seconds\n[2015-08-06 20:00:56,440][INFO ][node                     ] [node-01] closed\n[2015-08-06 20:00:58,249][INFO ][node                     ] [node-01] version[1.7.0], pid[27455], build[929b973/2015-07-16T14:31:07Z]\n[2015-08-06 20:00:58,249][INFO ][node                     ] [node-01] initializing ...\n[2015-08-06 20:00:58,362][INFO ][plugins                  ] [node-01] loaded [], sites [HQ, kopf, whatson]\n[2015-08-06 20:00:58,411][INFO ][env                      ] [node-01] using [1] data paths, mounts [[/srv (/dev/sdb)]], net usable_space [219.4gb], net total_space [237.7gb], types [xfs]\n[2015-08-06 20:01:01,277][INFO ][node                     ] [node-01] initialized\n[2015-08-06 20:01:01,278][INFO ][node                     ] [node-01] starting ...\n[2015-08-06 20:01:01,485][INFO ][transport                ] [node-01] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.211:9300]}\n[2015-08-06 20:01:01,502][INFO ][discovery                ] [node-01] my_cluster/65f-xeTqT3msqPDbYlbtew\n[2015-08-06 20:01:11,165][INFO ][cluster.service          ] [node-01] detected_master [node-02][-_LoaCn7TaqMjAkRDaHTQA][node-02][inet[/192.168.1.212:9300]], added {[node-02][-_LoaCn7TaqMjAkRDaHTQA][node-02][\ninet[/192.168.1.212:9300]],[node-03][zdxOnF2qQrG1rJ4czVMXUA][node-03][inet[/192.168.1.213:9300]],}, reason: zen-disco-receive(from master [[node-02][-_LoaCn7TaqMjAkRDaHTQA][node-02][inet[/192.168.1.212:9300]]])\n[2015-08-06 20:01:11,235][INFO ][http                     ] [node-01] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.211:9200]}\n[2015-08-06 20:01:11,235][INFO ][node                     ] [node-01] started\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/128679291","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-128679291","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":128679291,"node_id":"MDEyOklzc3VlQ29tbWVudDEyODY3OTI5MQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-08-07T11:28:54Z","updated_at":"2015-08-07T11:28:54Z","author_association":"CONTRIBUTOR","body":"Hi @womwombat \n\nThanks for the logs.  Restarting was the right thing to do here.  Could you look back in the logs on that server to before the problem started, to find out what triggered this issue?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/128682464","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-128682464","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":128682464,"node_id":"MDEyOklzc3VlQ29tbWVudDEyODY4MjQ2NA==","user":{"login":"womwombat","id":13163142,"node_id":"MDQ6VXNlcjEzMTYzMTQy","avatar_url":"https://avatars3.githubusercontent.com/u/13163142?v=4","gravatar_id":"","url":"https://api.github.com/users/womwombat","html_url":"https://github.com/womwombat","followers_url":"https://api.github.com/users/womwombat/followers","following_url":"https://api.github.com/users/womwombat/following{/other_user}","gists_url":"https://api.github.com/users/womwombat/gists{/gist_id}","starred_url":"https://api.github.com/users/womwombat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/womwombat/subscriptions","organizations_url":"https://api.github.com/users/womwombat/orgs","repos_url":"https://api.github.com/users/womwombat/repos","events_url":"https://api.github.com/users/womwombat/events{/privacy}","received_events_url":"https://api.github.com/users/womwombat/received_events","type":"User","site_admin":false},"created_at":"2015-08-07T11:54:54Z","updated_at":"2015-08-07T11:54:54Z","author_association":"NONE","body":"Hi,\n\nThanks for your help :)\n\nThe log above was on node-01, here is log on node-02 (looks like there's what's your looking for)\n\n```\n[2015-08-06 19:36:02,594][INFO ][cluster.metadata         ] [node-02] [my_custom_index_b] deleting index\n[2015-08-06 19:36:32,663][INFO ][cluster.metadata         ] [node-02] [my_custom_index_b] creating index, cause [api], templates [], shards [6]/[1], mappings []\n[2015-08-06 19:36:37,745][WARN ][cluster.action.shard     ] [node-02] [my_custom_index_b][4] received shard failed for [my_custom_index_b][4], node[n8Ry5JsZT4WiXqr16ux4dg], [P], s[INITIALIZING], unassigned_info[[reason=INDEX_CREATED], at[2015-08-06T17:36\n:32.664Z]], indexUUID [mcr4fgrtRwaHgITxhd3qrg], reason [shard failure [failed to create shard][IndexShardCreationException[[my_custom_index_b][4] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [my_custom_index_b][4], timed out after 50\n00ms]; ]]\n[2015-08-06 19:36:42,803][WARN ][indices.cluster          ] [node-02] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n```\n\nAnd on node-03:\n\n```\n[2015-08-06 19:38:27,714][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:39:29,515][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:40:29,701][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:41:30,813][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:41:30,928][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:42:31,519][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:42:31,600][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:43:32,715][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:44:32,801][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:44:32,884][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:45:33,165][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:45:33,165][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:46:33,894][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:46:33,968][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:47:34,267][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:47:34,293][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:48:34,505][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:48:34,577][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:49:35,501][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:52:36,197][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:53:36,611][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:54:36,775][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:58:41,294][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 19:59:38,439][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 20:00:42,834][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv\nice. timeout setting [1m], time since start [1m]\n[2015-08-06 20:00:45,853][INFO ][cluster.service          ] [node-03] removed {[node-01][n8Ry5JsZT4WiXqr16ux\n4dg][node-01][inet[/192.168.1.211:9300]],}, reason: zen-disco-receive(from master [[node-02][-_LoaCn7TaqMjA\nkRDaHTQA][node-02][inet[/192.168.1.212:9300]]])\n[2015-08-06 20:01:11,144][INFO ][cluster.service          ] [node-03] added {[node-01][65f-xeTqT3msqPDbYlbte\nw][node-01][inet[/192.168.1.211:9300]],}, reason: zen-disco-receive(from master [[node-02][-_LoaCn7TaqMjAkR\nDaHTQA][node-02][inet[/192.168.1.212:9300]]])\n```\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/128692856","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-128692856","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":128692856,"node_id":"MDEyOklzc3VlQ29tbWVudDEyODY5Mjg1Ng==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-08-07T12:55:36Z","updated_at":"2015-08-07T12:55:36Z","author_association":"CONTRIBUTOR","body":"So essentially the shards for the new index can't be allocated because the shard from the old index is still hanging around.  Something is holding on to it, but not sure what.  eg a scroll request would be one option but it could be many things.  That hot threads output (while the node is in this state) might help here, plus as much information as you can give us about any exceptions that you see (not necessarily related to allocation) plus queries that you run etc etc.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/128966255","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-128966255","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":128966255,"node_id":"MDEyOklzc3VlQ29tbWVudDEyODk2NjI1NQ==","user":{"login":"womwombat","id":13163142,"node_id":"MDQ6VXNlcjEzMTYzMTQy","avatar_url":"https://avatars3.githubusercontent.com/u/13163142?v=4","gravatar_id":"","url":"https://api.github.com/users/womwombat","html_url":"https://github.com/womwombat","followers_url":"https://api.github.com/users/womwombat/followers","following_url":"https://api.github.com/users/womwombat/following{/other_user}","gists_url":"https://api.github.com/users/womwombat/gists{/gist_id}","starred_url":"https://api.github.com/users/womwombat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/womwombat/subscriptions","organizations_url":"https://api.github.com/users/womwombat/orgs","repos_url":"https://api.github.com/users/womwombat/repos","events_url":"https://api.github.com/users/womwombat/events{/privacy}","received_events_url":"https://api.github.com/users/womwombat/received_events","type":"User","site_admin":false},"created_at":"2015-08-08T11:14:37Z","updated_at":"2015-08-08T11:14:37Z","author_association":"NONE","body":"Hi,\n\nThe hot thread output is rather large (598ko) so I paste it here http://justpaste.it/es-hot-thread\n\nRegards,\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/131971810","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-131971810","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":131971810,"node_id":"MDEyOklzc3VlQ29tbWVudDEzMTk3MTgxMA==","user":{"login":"zombiepig01","id":3710495,"node_id":"MDQ6VXNlcjM3MTA0OTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3710495?v=4","gravatar_id":"","url":"https://api.github.com/users/zombiepig01","html_url":"https://github.com/zombiepig01","followers_url":"https://api.github.com/users/zombiepig01/followers","following_url":"https://api.github.com/users/zombiepig01/following{/other_user}","gists_url":"https://api.github.com/users/zombiepig01/gists{/gist_id}","starred_url":"https://api.github.com/users/zombiepig01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zombiepig01/subscriptions","organizations_url":"https://api.github.com/users/zombiepig01/orgs","repos_url":"https://api.github.com/users/zombiepig01/repos","events_url":"https://api.github.com/users/zombiepig01/events{/privacy}","received_events_url":"https://api.github.com/users/zombiepig01/received_events","type":"User","site_admin":false},"created_at":"2015-08-17T21:45:33Z","updated_at":"2015-08-17T21:45:33Z","author_association":"NONE","body":"I am seeing this same issue on my (small) cluster.  I bulk-index about 200M documents with index.refresh_interval:-1 and index.number_of_replicas:0.  Shortly after setting index.number_of_replicas:1, I started seeing the failed shard errors.  Seems to be repeatable.  Let me know if I can send any info that might help.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/137176129","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-137176129","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":137176129,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNzE3NjEyOQ==","user":{"login":"rakesh91","id":1933540,"node_id":"MDQ6VXNlcjE5MzM1NDA=","avatar_url":"https://avatars2.githubusercontent.com/u/1933540?v=4","gravatar_id":"","url":"https://api.github.com/users/rakesh91","html_url":"https://github.com/rakesh91","followers_url":"https://api.github.com/users/rakesh91/followers","following_url":"https://api.github.com/users/rakesh91/following{/other_user}","gists_url":"https://api.github.com/users/rakesh91/gists{/gist_id}","starred_url":"https://api.github.com/users/rakesh91/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rakesh91/subscriptions","organizations_url":"https://api.github.com/users/rakesh91/orgs","repos_url":"https://api.github.com/users/rakesh91/repos","events_url":"https://api.github.com/users/rakesh91/events{/privacy}","received_events_url":"https://api.github.com/users/rakesh91/received_events","type":"User","site_admin":false},"created_at":"2015-09-02T17:16:20Z","updated_at":"2015-09-02T17:16:20Z","author_association":"NONE","body":"Even i am seeing the same issue, after upgrade from 1.4 to 1.6 version and made replication 0 from 1. So I upgraded to 1.7, then this issue stopped. But, now cluster keeps going down, by giving \"shard failure\" error and eventually node getting disconnected. \n\nHad never seen this issue while it was in 1.4 version.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/137948324","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-137948324","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":137948324,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNzk0ODMyNA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-09-05T12:05:00Z","updated_at":"2015-09-05T12:05:00Z","author_association":"CONTRIBUTOR","body":"@rakesh91 @zombiepig01  what exceptions are you seeing in your logs?\n\nare you deleting an index then creating a new index with the same name?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/138101221","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-138101221","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":138101221,"node_id":"MDEyOklzc3VlQ29tbWVudDEzODEwMTIyMQ==","user":{"login":"rakesh91","id":1933540,"node_id":"MDQ6VXNlcjE5MzM1NDA=","avatar_url":"https://avatars2.githubusercontent.com/u/1933540?v=4","gravatar_id":"","url":"https://api.github.com/users/rakesh91","html_url":"https://github.com/rakesh91","followers_url":"https://api.github.com/users/rakesh91/followers","following_url":"https://api.github.com/users/rakesh91/following{/other_user}","gists_url":"https://api.github.com/users/rakesh91/gists{/gist_id}","starred_url":"https://api.github.com/users/rakesh91/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rakesh91/subscriptions","organizations_url":"https://api.github.com/users/rakesh91/orgs","repos_url":"https://api.github.com/users/rakesh91/repos","events_url":"https://api.github.com/users/rakesh91/events{/privacy}","received_events_url":"https://api.github.com/users/rakesh91/received_events","type":"User","site_admin":false},"created_at":"2015-09-06T17:16:17Z","updated_at":"2015-09-06T17:16:17Z","author_association":"NONE","body":"@clintongormley when cluster go down, in master \n\n[2015-09-04 15:40:43,172][WARN ][cluster.action.shard     ] [esmaster2] [logstash-2015.09.04][9] received shard failed for [logstash-2015.09.04][9], node[2eOY2_zxRwiNtbZRmVxL5g], [P], s[INITIALIZING], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-09-04T10:10:34.254Z]], indexUUID [pGvcrpH3SzSlX46yYiqQcw], reason [shard failure [failed to create shard][IndexShardCreationException[[logstash-2015.09.04][9] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [logstash-2015.09.04][9], timed out after 5000ms]; ]]\n\nNo I am not deleting the index and re-create with same name, but i delete 28 days old index.\n\nUpon cluster restart, it will be fine, but few times it will fix on its own\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/140840508","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-140840508","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":140840508,"node_id":"MDEyOklzc3VlQ29tbWVudDE0MDg0MDUwOA==","user":{"login":"matthewgard1","id":1737766,"node_id":"MDQ6VXNlcjE3Mzc3NjY=","avatar_url":"https://avatars1.githubusercontent.com/u/1737766?v=4","gravatar_id":"","url":"https://api.github.com/users/matthewgard1","html_url":"https://github.com/matthewgard1","followers_url":"https://api.github.com/users/matthewgard1/followers","following_url":"https://api.github.com/users/matthewgard1/following{/other_user}","gists_url":"https://api.github.com/users/matthewgard1/gists{/gist_id}","starred_url":"https://api.github.com/users/matthewgard1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/matthewgard1/subscriptions","organizations_url":"https://api.github.com/users/matthewgard1/orgs","repos_url":"https://api.github.com/users/matthewgard1/repos","events_url":"https://api.github.com/users/matthewgard1/events{/privacy}","received_events_url":"https://api.github.com/users/matthewgard1/received_events","type":"User","site_admin":false},"created_at":"2015-09-16T18:49:38Z","updated_at":"2015-09-16T18:49:38Z","author_association":"NONE","body":"upgraded from 1.4.4 -> 1.7.2\n/_bulk (indexing)\n\nlooping errors : \n\nshard-failed ([INDEX-2015-34][1], node[9xgitQcHRR2kPZ7JYpKBtg], [P], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-09-16T18:26:29.047Z], details[shard failure [failed to create shard][IndexShardCreationException[[INDEX-2015-34][1] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [INDEX-2015-34][1], timed out after 5000ms]; ]]]), reason [shard failure [failed to create shard][IndexShardCreationException[[INDEX-2015-34][1] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [INDEX-2015-34][1], timed out after 5000ms]; ]] \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/146514305","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-146514305","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":146514305,"node_id":"MDEyOklzc3VlQ29tbWVudDE0NjUxNDMwNQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-10-08T11:48:21Z","updated_at":"2015-10-08T11:48:21Z","author_association":"CONTRIBUTOR","body":"If you're still seeing this \"LockObtainFailedException\", please get a hot threads dump with the following command:\n\n```\ncurl -XGET -s localhost:9200/_nodes/hot_threads?threads=10000\n```\n\nIn particular, we're looking for output like this:\n\n```\n 0.0% (0s out of 500ms) cpu usage by thread 'elasticsearch[host-23][[index-abc-123][0]: Lucene Merge Thread #111975]'\n 10/10 snapshots sharing following 14 elements\n java.lang.Object.wait(Native Method)\n java.lang.Thread.join(Thread.java:1245)\n java.lang.Thread.join(Thread.java:1319)\n org.apache.lucene.index.ConcurrentMergeScheduler.sync(ConcurrentMergeScheduler.java:281)\n org.apache.lucene.index.ConcurrentMergeScheduler.close(ConcurrentMergeScheduler.java:262)\n org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.close(ConcurrentMergeSchedulerProvider.java:141)\n org.elasticsearch.index.merge.EnableMergeScheduler.close(EnableMergeScheduler.java:56)\n org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2114)\n org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2089)\n org.apache.lucene.index.IndexWriter.tragicEvent(IndexWriter.java:4686)\n org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3839)\n org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:409)\n org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)\n org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:486)\n```\n\nThis is related to this Lucene bug (https://issues.apache.org/jira/browse/LUCENE-6670) where an OOM is thrown on a merge thread.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164148668","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-164148668","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":164148668,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDE0ODY2OA==","user":{"login":"krisb78","id":222033,"node_id":"MDQ6VXNlcjIyMjAzMw==","avatar_url":"https://avatars3.githubusercontent.com/u/222033?v=4","gravatar_id":"","url":"https://api.github.com/users/krisb78","html_url":"https://github.com/krisb78","followers_url":"https://api.github.com/users/krisb78/followers","following_url":"https://api.github.com/users/krisb78/following{/other_user}","gists_url":"https://api.github.com/users/krisb78/gists{/gist_id}","starred_url":"https://api.github.com/users/krisb78/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krisb78/subscriptions","organizations_url":"https://api.github.com/users/krisb78/orgs","repos_url":"https://api.github.com/users/krisb78/repos","events_url":"https://api.github.com/users/krisb78/events{/privacy}","received_events_url":"https://api.github.com/users/krisb78/received_events","type":"User","site_admin":false},"created_at":"2015-12-12T13:10:03Z","updated_at":"2015-12-12T13:10:03Z","author_association":"NONE","body":"I seem to be suffering from the same problem. \n\nInterestingly enough, I managed to make it go away on 1.7.4 (https://github.com/elastic/elasticsearch/issues/12926) by removing all nested fields (https://www.elastic.co/guide/en/elasticsearch/reference/2.1/nested.html) from my mappings and reindexing all the data.\n\nI upgraded to 2.0.0 a couple of days ago and then to 2.1.0 yesterday. Today I noticed that the problem returned - could the upgrade have caused it?\n\nI can see shards stuck in INITIALIZING again:\n\n```\n[LIVE] krisba@cubitsearch-client-1:~$ curl localhost:9200/_cat/shards | grep INIT\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0joint_user_summary_v1_all               2 r INITIALIZING                   10.0.1.238 cubitsearch-4\njoint_user_summary_v1_all               2 r INITIALIZING                   10.0.1.239 cubitsearch-5\njoint_user_summary_v1_all               2 r INITIALIZING                   10.0.1.236 cubitsearch-2\njoint_user_summary_v1_all               1 r INITIALIZING                   10.0.1.237 cubitsearch-3\njoint_user_summary_v1_all               4 r INITIALIZING                   10.0.1.238 cubitsearch-4\njoint_user_summary_v1_all               4 r INITIALIZING                   10.0.1.239 cubitsearch-5\njoint_user_summary_v1_all               0 r INITIALIZING                   10.0.1.237 cubitsearch-3\njoint_user_summary_v1_all               0 r INITIALIZING                   10.0.1.236 cubitsearch-2\n```\n\nHere's a log sample from cubitsearch-4:\n\n```\nDec 12 12:53:40 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][0]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\nDec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\nDec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\nDec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\nDec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\nDec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\nDec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\nDec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\nDec 12 12:53:40 cubitsearch-4 elasticsearch: #011... 7 more\nDec 12 12:53:41 cubitsearch-4 elasticsearch: [2015-12-12 12:53:41,541][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][1], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8697], s[INITIALIZING], a[id=KIkQeU0rTCSzZtFPl0YtlQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:42:23.164Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms]; ]]]\nDec 12 12:53:41 cubitsearch-4 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011... 7 more\nDec 12 12:53:41 cubitsearch-4 elasticsearch: [2015-12-12 12:53:41,549][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][0], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8605], s[INITIALIZING], a[id=IQeeXvCCTweMQpf2O-ncWw], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:43:05.921Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][0], timed out after 5000ms]; ]]]\nDec 12 12:53:41 cubitsearch-4 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][0]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][0]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\nDec 12 12:53:41 cubitsearch-4 elasticsearch: #011... 7 more\nDec 12 12:53:42 cubitsearch-4 elasticsearch: [2015-12-12 12:53:42,608][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][1], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8697], s[INITIALIZING], a[id=KIkQeU0rTCSzZtFPl0YtlQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:42:23.164Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms]; ]]]\nDec 12 12:53:42 cubitsearch-4 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011... 7 more\nDec 12 12:53:42 cubitsearch-4 elasticsearch: [2015-12-12 12:53:42,610][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][0], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8605], s[INITIALIZING], a[id=IQeeXvCCTweMQpf2O-ncWw], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:43:05.921Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][0], timed out after 5000ms]; ]]]\nDec 12 12:53:42 cubitsearch-4 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][0]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][0]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\nDec 12 12:53:42 cubitsearch-4 elasticsearch: #011... 7 more\nDec 12 12:53:44 cubitsearch-4 elasticsearch: [2015-12-12 12:53:44,514][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][1], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8697], s[INITIALIZING], a[id=KIkQeU0rTCSzZtFPl0YtlQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:42:23.164Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms]; ]]]\nDec 12 12:53:44 cubitsearch-4 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\nDec 12 12:53:44 cubitsearch-4 elasticsearch: #011... 7 more\nDec 12 12:53:44 cubitsearch-4 elasticsearch: [2015-12-12 12:53:44,515][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][0], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8605], s[INITIALIZING], a[id=IQeeXvCCTweMQpf2O-ncWw], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:43:05.921Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][0], timed out after 5000ms]; ]]]\n```\n\nAnd here's the output of `curl -XGET -s localhost:9200/_nodes/hot_threads?threads=10000`:\n\n[hot_threads.txt](https://github.com/elastic/elasticsearch/files/60134/hot_threads.txt)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164172767","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-164172767","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":164172767,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDE3Mjc2Nw==","user":{"login":"krisb78","id":222033,"node_id":"MDQ6VXNlcjIyMjAzMw==","avatar_url":"https://avatars3.githubusercontent.com/u/222033?v=4","gravatar_id":"","url":"https://api.github.com/users/krisb78","html_url":"https://github.com/krisb78","followers_url":"https://api.github.com/users/krisb78/followers","following_url":"https://api.github.com/users/krisb78/following{/other_user}","gists_url":"https://api.github.com/users/krisb78/gists{/gist_id}","starred_url":"https://api.github.com/users/krisb78/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krisb78/subscriptions","organizations_url":"https://api.github.com/users/krisb78/orgs","repos_url":"https://api.github.com/users/krisb78/repos","events_url":"https://api.github.com/users/krisb78/events{/privacy}","received_events_url":"https://api.github.com/users/krisb78/received_events","type":"User","site_admin":false},"created_at":"2015-12-12T18:05:50Z","updated_at":"2015-12-12T18:05:50Z","author_association":"NONE","body":"Ho-hum. It fixed itself...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164437888","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-164437888","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":164437888,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDQzNzg4OA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-12-14T13:32:05Z","updated_at":"2015-12-14T13:32:05Z","author_association":"CONTRIBUTOR","body":"@krisb78 these log messages are just saying that you are trying to search on these shards before they have recovered.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164595850","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-164595850","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":164595850,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDU5NTg1MA==","user":{"login":"zombiepig01","id":3710495,"node_id":"MDQ6VXNlcjM3MTA0OTU=","avatar_url":"https://avatars3.githubusercontent.com/u/3710495?v=4","gravatar_id":"","url":"https://api.github.com/users/zombiepig01","html_url":"https://github.com/zombiepig01","followers_url":"https://api.github.com/users/zombiepig01/followers","following_url":"https://api.github.com/users/zombiepig01/following{/other_user}","gists_url":"https://api.github.com/users/zombiepig01/gists{/gist_id}","starred_url":"https://api.github.com/users/zombiepig01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zombiepig01/subscriptions","organizations_url":"https://api.github.com/users/zombiepig01/orgs","repos_url":"https://api.github.com/users/zombiepig01/repos","events_url":"https://api.github.com/users/zombiepig01/events{/privacy}","received_events_url":"https://api.github.com/users/zombiepig01/received_events","type":"User","site_admin":false},"created_at":"2015-12-14T23:42:08Z","updated_at":"2015-12-14T23:42:08Z","author_association":"NONE","body":"@clintongormley -- I recently started seeing those messages again:  here is the thread dump:\n\nhttps://gist.github.com/zombiepig01/41bad73814d27fe3a018\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/164971801","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-164971801","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":164971801,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDk3MTgwMQ==","user":{"login":"jindov","id":10844917,"node_id":"MDQ6VXNlcjEwODQ0OTE3","avatar_url":"https://avatars3.githubusercontent.com/u/10844917?v=4","gravatar_id":"","url":"https://api.github.com/users/jindov","html_url":"https://github.com/jindov","followers_url":"https://api.github.com/users/jindov/followers","following_url":"https://api.github.com/users/jindov/following{/other_user}","gists_url":"https://api.github.com/users/jindov/gists{/gist_id}","starred_url":"https://api.github.com/users/jindov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jindov/subscriptions","organizations_url":"https://api.github.com/users/jindov/orgs","repos_url":"https://api.github.com/users/jindov/repos","events_url":"https://api.github.com/users/jindov/events{/privacy}","received_events_url":"https://api.github.com/users/jindov/received_events","type":"User","site_admin":false},"created_at":"2015-12-16T03:03:29Z","updated_at":"2015-12-16T03:03:29Z","author_association":"NONE","body":"I have a large index, about 150GB, this index is stucked whern restart es (running 1 node), this is log:\n\n```\n[logstash-2015.12.15][[logstash-2015.12.15][4]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [logstash-2015.12.15][[logstash-2015.12.15][4]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n[2015-12-16 09:57:24,397][DEBUG][action.admin.indices.stats] [log03] [indices:monitor/stats] failed to execute operation for shard [[logstash-2015.12.15][3], node[gEc7JgdmSVyu-EzzdNn7bA], [P], v[7], s[INITIALIZING], a[id=Q8evo1x3Rgy0Ytpd7zpLGQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-16T02:54:37.577Z]]]\n[logstash-2015.12.15][[logstash-2015.12.15][3]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [logstash-2015.12.15][[logstash-2015.12.15][3]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n[2015-12-16 09:57:24,398][DEBUG][action.admin.indices.stats] [log03] [indices:monitor/stats] failed to execute operation for shard [[logstash-2015.12.15][1], node[gEc7JgdmSVyu-EzzdNn7bA], [P], v[7], s[INITIALIZING], a[id=Av2uA3StSOW_8S72m7EqOw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-16T02:54:37.577Z]]]\n[logstash-2015.12.15][[logstash-2015.12.15][1]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [logstash-2015.12.15][[logstash-2015.12.15][1]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n[2015-12-16 09:57:24,398][DEBUG][action.admin.indices.stats] [log03] [indices:monitor/stats] failed to execute operation for shard [[logstash-2015.12.15][2], node[gEc7JgdmSVyu-EzzdNn7bA], [P], v[7], s[INITIALIZING], a[id=3s_-6jJ7Q8WxfAQsFZBJMw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-16T02:54:37.577Z]]]\n[logstash-2015.12.15][[logstash-2015.12.15][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [logstash-2015.12.15][[logstash-2015.12.15][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)\n        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)\n        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:131)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)\n        ... 7 more\n[2015-12-16 09:57:24,399][DEBUG][action.admin.indices.stats] [log03] [indices:monitor/stats] failed to execute operation for shard [[logstash-2015.12.15][0], node[gEc7JgdmSVyu-EzzdNn7bA], [P], v[7], s[INITIALIZING], a[id=H4oJgM5dR0GS73NiK7Glsg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-16T02:54:37.577Z]]]\n[logstash-2015.12.15][[logstash-2015.12.15][0]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)\n        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)\n        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: [logstash-2015.12.15][[logstash-2015.12.15][0]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]\n```\n\nis it a bug?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/165076456","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-165076456","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":165076456,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTA3NjQ1Ng==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-12-16T11:36:04Z","updated_at":"2015-12-16T11:36:04Z","author_association":"CONTRIBUTOR","body":"@jindov see https://github.com/elastic/elasticsearch/issues/12011#issuecomment-164437888\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/172494582","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-172494582","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":172494582,"node_id":"MDEyOklzc3VlQ29tbWVudDE3MjQ5NDU4Mg==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-01-18T10:43:43Z","updated_at":"2016-01-18T10:43:43Z","author_association":"CONTRIBUTOR","body":"The original issue here `LockObtainFailedException: Can't lock shard` seems to have been resolved in recent releases.  Closing for now.  Please reopen if you're seeing the same thing on 2.1 or higher.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/201303811","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-201303811","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":201303811,"node_id":"MDEyOklzc3VlQ29tbWVudDIwMTMwMzgxMQ==","user":{"login":"kovrus","id":7442373,"node_id":"MDQ6VXNlcjc0NDIzNzM=","avatar_url":"https://avatars2.githubusercontent.com/u/7442373?v=4","gravatar_id":"","url":"https://api.github.com/users/kovrus","html_url":"https://github.com/kovrus","followers_url":"https://api.github.com/users/kovrus/followers","following_url":"https://api.github.com/users/kovrus/following{/other_user}","gists_url":"https://api.github.com/users/kovrus/gists{/gist_id}","starred_url":"https://api.github.com/users/kovrus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kovrus/subscriptions","organizations_url":"https://api.github.com/users/kovrus/orgs","repos_url":"https://api.github.com/users/kovrus/repos","events_url":"https://api.github.com/users/kovrus/events{/privacy}","received_events_url":"https://api.github.com/users/kovrus/received_events","type":"User","site_admin":false},"created_at":"2016-03-25T14:18:44Z","updated_at":"2016-03-25T14:18:44Z","author_association":"NONE","body":"@clintongormley do you have any clue what was the cause of the issue and what could have fixed it in > 2.1 ?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/201321821","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-201321821","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":201321821,"node_id":"MDEyOklzc3VlQ29tbWVudDIwMTMyMTgyMQ==","user":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"created_at":"2016-03-25T14:57:49Z","updated_at":"2016-03-25T14:57:49Z","author_association":"MEMBER","body":"@kovrus we had some deadlocks fixed - those were preventing the shard lock from being freed. Do see this with a version >2.1?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/223273994","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-223273994","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":223273994,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMzI3Mzk5NA==","user":{"login":"krisb78","id":222033,"node_id":"MDQ6VXNlcjIyMjAzMw==","avatar_url":"https://avatars3.githubusercontent.com/u/222033?v=4","gravatar_id":"","url":"https://api.github.com/users/krisb78","html_url":"https://github.com/krisb78","followers_url":"https://api.github.com/users/krisb78/followers","following_url":"https://api.github.com/users/krisb78/following{/other_user}","gists_url":"https://api.github.com/users/krisb78/gists{/gist_id}","starred_url":"https://api.github.com/users/krisb78/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krisb78/subscriptions","organizations_url":"https://api.github.com/users/krisb78/orgs","repos_url":"https://api.github.com/users/krisb78/repos","events_url":"https://api.github.com/users/krisb78/events{/privacy}","received_events_url":"https://api.github.com/users/krisb78/received_events","type":"User","site_admin":false},"created_at":"2016-06-02T12:15:43Z","updated_at":"2016-06-02T12:15:43Z","author_association":"NONE","body":"I'm hitting this old chestnut again. I upgraded from 2.3.0 to 2.3.3 and all indices are fine (green) except the largest one, which is still yellow.\n\nShards are spending considerable time in the VERIFY_INDEX stage, but then they fail to initialise:\n\nOn the node that tries to initialise a shard, I'm seeing:\n\n```\nJun  2 12:07:00 cubitsearch-5 elasticsearch: [2016-06-02 12:07:00,089][WARN ][indices.cluster          ] [cubitsearch-5] [[joint_user_summary_v1_all][2]] marking and sending shard failed due to [failed to create shard]\nJun  2 12:07:00 cubitsearch-5 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][2]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][2], timed out after 5000ms];\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1_all][2], timed out after 5000ms\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)\nJun  2 12:07:00 cubitsearch-5 elasticsearch: #011... 10 more\nJun  2 12:07:05 cubitsearch-5 elasticsearch: [2016-06-02 12:07:05,223][WARN ][indices.cluster          ] [cubitsearch-5] [[joint_user_summary_v1_all][4]] marking and sending shard failed due to [failed to create shard]\nJun  2 12:07:05 cubitsearch-5 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][4]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][4], timed out after 5000ms];\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1_all][4], timed out after 5000ms\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)\nJun  2 12:07:05 cubitsearch-5 elasticsearch: #011... 10 more\nJun  2 12:07:10 cubitsearch-5 elasticsearch: [2016-06-02 12:07:10,318][WARN ][indices.cluster          ] [cubitsearch-5] [[joint_user_summary_v1_all][1]] marking and sending shard failed due to [failed to create shard]\nJun  2 12:07:10 cubitsearch-5 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms];\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)\nJun  2 12:07:10 cubitsearch-5 elasticsearch: #011... 10 more\nJun  2 12:07:15 cubitsearch-5 elasticsearch: [2016-06-02 12:07:15,421][WARN ][indices.cluster          ] [cubitsearch-5] [[joint_user_summary_v1_all][1]] marking and sending shard failed due to [failed to create shard]\nJun  2 12:07:15 cubitsearch-5 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms];\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)\nJun  2 12:07:15 cubitsearch-5 elasticsearch: #011... 10 more\nJun  2 12:07:20 cubitsearch-5 elasticsearch: [2016-06-02 12:07:20,830][WARN ][indices.cluster          ] [cubitsearch-5] [[joint_user_summary_v1_all][2]] marking and sending shard failed due to [failed to create shard]\nJun  2 12:07:20 cubitsearch-5 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][2]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][2], timed out after 5000ms];\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1_all][2], timed out after 5000ms\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)\nJun  2 12:07:20 cubitsearch-5 elasticsearch: #011... 10 more\n```\n\nThe node that \"sends\" the shard to cubitsearch-5 says:\n\n```\nJun  2 11:28:28 cubitsearch-2 elasticsearch: [2016-06-02 11:28:28,961][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [2214039ms] ago, timed out [1314039ms] ago, action [internal:index/shard/recovery/prepare_translog], node [{cubitsearch-5}{Dxpr53kUS9qN41nLnLUOXA}{10.0.1.239}{10.0.1.239:9300}{max_local_storage_nodes=1, master=false}], id [479687]\nJun  2 11:34:15 cubitsearch-2 elasticsearch: [2016-06-02 11:34:15,131][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [1849861ms] ago, timed out [949861ms] ago, action [internal:index/shard/recovery/prepare_translog], node [{cubitsearch-5}{Dxpr53kUS9qN41nLnLUOXA}{10.0.1.239}{10.0.1.239:9300}{max_local_storage_nodes=1, master=false}], id [482623]\nJun  2 11:47:14 cubitsearch-2 elasticsearch: [2016-06-02 11:47:14,682][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [1537976ms] ago, timed out [637976ms] ago, action [internal:index/shard/recovery/prepare_translog], node [{cubitsearch-5}{Dxpr53kUS9qN41nLnLUOXA}{10.0.1.239}{10.0.1.239:9300}{max_local_storage_nodes=1, master=false}], id [487423]\nJun  2 12:06:51 cubitsearch-2 elasticsearch: [2016-06-02 12:06:51,344][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [1807416ms] ago, timed out [907416ms] ago, action [internal:index/shard/recovery/prepare_translog], node [{cubitsearch-5}{Dxpr53kUS9qN41nLnLUOXA}{10.0.1.239}{10.0.1.239:9300}{max_local_storage_nodes=1, master=false}], id [507910]\nJun  2 12:09:32 cubitsearch-2 elasticsearch: [2016-06-02 12:09:32,897][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [1975624ms] ago, timed out [1075624ms] ago, action [internal:index/shard/recovery/prepare_translog], node [{cubitsearch-5}{Dxpr53kUS9qN41nLnLUOXA}{10.0.1.239}{10.0.1.239:9300}{max_local_storage_nodes=1, master=false}], id [507493]\n```\n\nI tried restarting the nodes a couple of times already and managed to get some more shards to initialise, but some of them are still missing. \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/223286581","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-223286581","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":223286581,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMzI4NjU4MQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-06-02T13:11:21Z","updated_at":"2016-06-02T13:11:21Z","author_association":"CONTRIBUTOR","body":"@krisb78 what operations are happening on your cluster at the same time? esp, any scrolls?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/223286638","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-223286638","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":223286638,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMzI4NjYzOA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2016-06-02T13:11:37Z","updated_at":"2016-06-02T13:11:37Z","author_association":"CONTRIBUTOR","body":"delete index and recreate?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/223290869","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-223290869","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":223290869,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMzI5MDg2OQ==","user":{"login":"krisb78","id":222033,"node_id":"MDQ6VXNlcjIyMjAzMw==","avatar_url":"https://avatars3.githubusercontent.com/u/222033?v=4","gravatar_id":"","url":"https://api.github.com/users/krisb78","html_url":"https://github.com/krisb78","followers_url":"https://api.github.com/users/krisb78/followers","following_url":"https://api.github.com/users/krisb78/following{/other_user}","gists_url":"https://api.github.com/users/krisb78/gists{/gist_id}","starred_url":"https://api.github.com/users/krisb78/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krisb78/subscriptions","organizations_url":"https://api.github.com/users/krisb78/orgs","repos_url":"https://api.github.com/users/krisb78/repos","events_url":"https://api.github.com/users/krisb78/events{/privacy}","received_events_url":"https://api.github.com/users/krisb78/received_events","type":"User","site_admin":false},"created_at":"2016-06-02T13:27:32Z","updated_at":"2016-06-02T13:27:32Z","author_association":"NONE","body":"Already check the scrolls, nothing like that is happening.\n\nWhat seems to have helped a bit was deleting this index manually  (i.e., removing the index directory) before restarting a node.  I managed to get shards to initialise on 3 nodes by doing that, but the remaining 2 won't budge.\n\nI'm holding off from deleting & recreating the index for now - I have done it to fix this issue before, but I wouldn't want to have to do that on every upgrade...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/223291666","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-223291666","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":223291666,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMzI5MTY2Ng==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-06-02T13:30:26Z","updated_at":"2016-06-02T13:30:26Z","author_association":"CONTRIBUTOR","body":"@krisb78 is it only one node that is causing problems? if so you can try restarting it. Can I the output of `localhost:9200/_cat/recovery`\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/223302704","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-223302704","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":223302704,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMzMwMjcwNA==","user":{"login":"krisb78","id":222033,"node_id":"MDQ6VXNlcjIyMjAzMw==","avatar_url":"https://avatars3.githubusercontent.com/u/222033?v=4","gravatar_id":"","url":"https://api.github.com/users/krisb78","html_url":"https://github.com/krisb78","followers_url":"https://api.github.com/users/krisb78/followers","following_url":"https://api.github.com/users/krisb78/following{/other_user}","gists_url":"https://api.github.com/users/krisb78/gists{/gist_id}","starred_url":"https://api.github.com/users/krisb78/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krisb78/subscriptions","organizations_url":"https://api.github.com/users/krisb78/orgs","repos_url":"https://api.github.com/users/krisb78/repos","events_url":"https://api.github.com/users/krisb78/events{/privacy}","received_events_url":"https://api.github.com/users/krisb78/received_events","type":"User","site_admin":false},"created_at":"2016-06-02T14:08:19Z","updated_at":"2016-06-02T14:08:19Z","author_association":"NONE","body":"It was all nodes, really...\n\nHere's what _cat/recovery is saying about this problematic index:\n\n```\njoint_user_summary_v1_all                 0 1861391 replica done         10.0.1.239 10.0.1.237 n/a n/a 147 100.0% 17955289909 100.0% 147 17955289909 0      100.0% 0\njoint_user_summary_v1_all                 0 2303300 replica done         10.0.1.239 10.0.1.235 n/a n/a 147 100.0% 17955289909 100.0% 147 17955289909 0      100.0% 0\njoint_user_summary_v1_all                 1 1931733 replica done         10.0.1.239 10.0.1.236 n/a n/a 162 100.0% 17654494329 100.0% 162 17654494329 0      100.0% 0\njoint_user_summary_v1_all                 1 1459125 replica done         10.0.1.239 10.0.1.237 n/a n/a 162 100.0% 17654494329 100.0% 162 17654494329 0      100.0% 0\njoint_user_summary_v1_all                 1 856778  replica done         10.0.1.239 10.0.1.235 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0\njoint_user_summary_v1_all                 2 463469  replica done         10.0.1.239 10.0.1.236 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0\njoint_user_summary_v1_all                 2 472860  replica done         10.0.1.239 10.0.1.237 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0\njoint_user_summary_v1_all                 2 1059848 replica done         10.0.1.239 10.0.1.235 n/a n/a 124 100.0% 13853113660 100.0% 124 13853113660 0      100.0% 0\njoint_user_summary_v1_all                 3 1717997 replica done         10.0.1.239 10.0.1.236 n/a n/a 128 100.0% 14859107452 100.0% 128 14859107452 0      100.0% 0\njoint_user_summary_v1_all                 3 900335  replica done         10.0.1.239 10.0.1.237 n/a n/a 128 100.0% 14859107452 100.0% 128 14859107452 0      100.0% 0\njoint_user_summary_v1_all                 3 183371  replica verify_index 10.0.1.236 10.0.1.239 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0\njoint_user_summary_v1_all                 3 1272129 replica done         10.0.1.239 10.0.1.235 n/a n/a 128 100.0% 14859107452 100.0% 128 14859107452 0      100.0% 0\njoint_user_summary_v1_all                 4 1611829 replica done         10.0.1.239 10.0.1.236 n/a n/a 125 100.0% 16055840146 100.0% 125 16055840146 0      100.0% 0\njoint_user_summary_v1_all                 4 1250961 replica done         10.0.1.239 10.0.1.237 n/a n/a 125 100.0% 16055840146 100.0% 125 16055840146 0      100.0% 0\njoint_user_summary_v1_all                 4 708638  replica verify_index 10.0.1.236 10.0.1.238 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0\njoint_user_summary_v1_all                 4 708630  replica verify_index 10.0.1.236 10.0.1.239 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0\njoint_user_summary_v1_all                 4 753186  replica done         10.0.1.239 10.0.1.235 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0\n```\n\nMy theory is that for some reason `prepare_translog` takes too long:\n\n```\nJun  2 13:37:09 cubitsearch-5 elasticsearch: Caused by: ReceiveTimeoutTransportException[[cubitsearch-5][10.0.1.239:9300][internal:index/shard/recovery/prepare_translog] request_id [709272] timed out after [900001ms]]\n```\n\nOnce this timeout happens, the `Can't lock shard` errors follow until the node is restarted.\n\nLooking at the code, this timeout is dictated by `indices.recovery.internal_action_timeout` or `indices.recovery.internal_action_long_timeout`.\n\nThe documentation doesn't say that these settings are customisable, but I tried to change them anyway now to see if it changes anything.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/223305689","html_url":"https://github.com/elastic/elasticsearch/issues/12011#issuecomment-223305689","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12011","id":223305689,"node_id":"MDEyOklzc3VlQ29tbWVudDIyMzMwNTY4OQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2016-06-02T14:18:04Z","updated_at":"2016-06-02T14:18:04Z","author_association":"CONTRIBUTOR","body":"@bleskes does this ring any bell? I think we cancel both sides of recovery on such a failure?\n","performed_via_github_app":null}]