[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/40229009","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-40229009","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":40229009,"node_id":"MDEyOklzc3VlQ29tbWVudDQwMjI5MDA5","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2014-04-11T17:33:15Z","updated_at":"2014-04-11T17:33:15Z","author_association":"CONTRIBUTOR","body":"I guess this behavior is expected. Operating systems generally write and read entire blocks of data at once. Let's take a block size of 4KB, this means that reading a random byte on disk might require to load a full 4-KB block into memory. So if we read 20000 random documents from disk, this means that we could expect the disk to read about 20000 \\* 4096 bytes ~ 80MB.\n\nThis doesn't happen with query execution since query execution reads are sequential, so two bytes that are read consecutively are likely to be in the same block. On the other hand, stored fields reads are typically random.\n\nThis phenomenon is probably amplified a bit by the fact that Lucene stores data into blocks of about 16KB (uncompressed, so assuming a compression of 60%, which is quite common, this would make the block take about 10KB on disk). But I wanted to talk about the operating system first to make clear that it is just amplifying a bit something that is already happening at the operating system level.\n\nDatabases in general and Elasticsearch in particular are not good at fetching high numbers of random records. If you want to get such large numbers of documents from Elasticsearch, I would recommend using SCAN[1] whenever possible.\n\n[1] http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-search-type.html#scan\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/40236629","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-40236629","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":40236629,"node_id":"MDEyOklzc3VlQ29tbWVudDQwMjM2NjI5","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"created_at":"2014-04-11T18:11:51Z","updated_at":"2014-04-11T18:11:51Z","author_association":"CONTRIBUTOR","body":"You missed the part about 1/20 mobile users so 20000 \\* 4096 bytes ~ 80MB turns out to be more like 4MB per pack. Since all search requests are sequential and take 200ms+, there are at most 4MB \\* 1000ms / 200ms ~ 20MB which is an order of magnitude lower than 220MB. Also, fs cache should help when you request the same block repeatedly, but that cold not happen in this case actually.\n\nDoesn't elasticsearch know document ids if it could respond with total hits per query?\n\nI'll try to backup and rebuild the whole index from scratch anyway, just to be sure.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/40242410","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-40242410","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":40242410,"node_id":"MDEyOklzc3VlQ29tbWVudDQwMjQyNDEw","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2014-04-11T19:08:20Z","updated_at":"2014-04-11T19:08:20Z","author_association":"CONTRIBUTOR","body":"Indeed, I missed the 1/20 thing. How large are your documents on average?\n\n>  Also, fs cache should help when you request the same block repeatedly\n\nIndeed, but you have 16GB of RAM, and 10 of them go to the JVM so the FS cache has at most 6GB for 70GB of index. The chances of a cache miss are very high.\n\n> Doesn't elasticsearch know document ids if it could respond with total hits per query?\n\nElasticsearch internally use Lucene doc IDs to count matches, which are just the ordinals of the document within a segment. For example, if a segment has 3 documents, the first will have `0` as an identifier, the second `1`, etc. On the other hand, the `_id` of documents are stored in Lucene stored fields, just like `_source`.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/40243764","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-40243764","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":40243764,"node_id":"MDEyOklzc3VlQ29tbWVudDQwMjQzNzY0","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"created_at":"2014-04-11T19:22:14Z","updated_at":"2014-04-11T19:22:14Z","author_association":"CONTRIBUTOR","body":"Docs are around 1-2kb on average, mobile users are usually 2kb.\n\nTurns out that `\"fields\": []` could only save me network bandwidth if `_id` is stored in fields.\n\nAnyway, reindexing is in progress :)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/40335903","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-40335903","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":40335903,"node_id":"MDEyOklzc3VlQ29tbWVudDQwMzM1OTAz","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"created_at":"2014-04-14T06:15:01Z","updated_at":"2014-04-14T06:15:01Z","author_association":"CONTRIBUTOR","body":"Well, I reindexed dataset, it's 63.4g on disk after two days of updates.\n\nI disabled updates, cleared caches, removed `is_mobile` condition, and changed `size` to 1000:\n\n``` json\n{\"size\":1000,\"fields\":[],\"query\":{\"filtered\":{\"filter\":{\"bool\":{\"must\":[{\"range\":{\"user_id\":{\"gt\":0,\"lte\":1000},\"_cache\":false}}]}}}}}\n```\n\nI queried packs sequentially with ids up to 400 000, here is output from my script:\n\n```\n1000: 1000/1000 results in 614.9781ms\n2000: 1000/1000 results in 205.9588ms\n3000: 1000/1000 results in 151.7389ms\n4000: 1000/1000 results in 207.2759ms\n5000: 1000/1000 results in 201.7291ms\n6000: 1000/1000 results in 173.6081ms\n7000: 1000/1000 results in 169.4219ms\n8000: 1000/1000 results in 199.9931ms\n9000: 1000/1000 results in 202.5528ms\n10000: 1000/1000 results in 181.7811ms\n11000: 1000/1000 results in 178.7648ms\n12000: 1000/1000 results in 199.1701ms\n13000: 1000/1000 results in 204.5979ms\n14000: 1000/1000 results in 156.7390ms\n15000: 1000/1000 results in 172.9791ms\n16000: 1000/1000 results in 253.1419ms\n17000: 1000/1000 results in 165.1721ms\n18000: 1000/1000 results in 174.8121ms\n19000: 1000/1000 results in 189.6560ms\n...\n391000: 1000/1000 results in 169.0459ms\n392000: 1000/1000 results in 183.8639ms\n393000: 1000/1000 results in 184.2752ms\n394000: 1000/1000 results in 200.5951ms\n395000: 1000/1000 results in 163.5761ms\n396000: 1000/1000 results in 162.3139ms\n397000: 1000/1000 results in 134.5751ms\n398000: 1000/1000 results in 185.7009ms\n399000: 1000/1000 results in 167.1751ms\n400000: 1000/1000 results in 160.0850ms\n401000: 1000/1000 results in 141.7110ms\n402000: 1000/1000 results in 162.6661ms\n```\n\nAverage time is 168.206ms, for last 50 requests it's 159.316, that's 6.25 rps.\n\n6.25 rps \\* 1000 items in pack \\* 4kb block = 25600000.0 bytes = 24.4140625MB.\n\nLet's look at the pictures:\n\n![one](http://puu.sh/882Tu.png)\n![two](http://puu.sh/882Ul.png)\n![three](http://puu.sh/882UP.png)\n![four](http://puu.sh/882Vs.png)\n\nAnd `iostat -x -d 10` before:\n\n```\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00   321.20    1.50   54.60     7.20  3900.80   139.32     0.30    5.40   56.67    3.99   0.70   3.90\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00   108.80    0.00   13.60     0.00   865.60   127.29     0.20   14.85    0.00   14.85   1.40   1.90\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     7.20    0.00    6.50     0.00   625.60   192.49     0.09   13.08    0.00   13.08   5.23   3.40\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     7.30    0.40   10.20     1.60   876.00   165.58     0.05    4.81    0.00    5.00   1.70   1.80\n```\n\nIn progress of searches:\n\n```\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00    33.80  191.30   24.90 18828.00  1283.20   186.04     0.79    3.63    2.59   11.65   0.62  13.40\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00    14.70 2231.10    5.40 230444.00    96.40   206.16     5.09    2.28    2.28    0.00   0.42  94.30\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.90 2495.00    0.90 227958.40    11.20   182.68     5.57    2.23    2.23    2.22   0.38  94.20\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.70 2495.50    0.60 227886.40     5.20   182.60     5.60    2.25    2.25    0.00   0.38  94.00\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               1.00     0.60 2483.90    1.60 226898.40    24.00   182.60     5.55    2.23    2.23    0.00   0.38  93.30\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.50 2469.10    0.60 227619.20     4.40   184.33     5.47    2.22    2.22    0.00   0.38  93.80\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda              49.70    77.50 2712.10   16.50 227116.00   387.20   166.75     5.96    2.18    2.19    0.97   0.34  93.90\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00    21.40 1645.90    6.50 150786.40   112.80   182.64     3.66    2.22    2.22    0.62   0.38  62.10\n```\n\nAnd after:\n\n```\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00    15.40    3.20   12.20    26.00   118.40    18.75     0.05    3.38    1.88    3.77   0.39   0.60\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.20    0.00    1.20     0.00    48.40    80.67     0.02   13.33    0.00   13.33  13.33   1.60\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     2.00    2.30    0.80    21.60    11.20    21.16     0.28   91.61   95.65   80.00  16.13   5.00\n```\n\nWell, on average there were 227886.40KB read from disk in 2495.50 read requests per second, this is quite far away from expected 24.5 megabytes. Average read size of 91KB is worrying.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/41646260","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-41646260","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":41646260,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNjQ2MjYw","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"created_at":"2014-04-29T06:51:03Z","updated_at":"2014-04-29T06:51:03Z","author_association":"CONTRIBUTOR","body":"I tried to update to 1.1.1, that didn't help. Then I tried to switch to `niofs`, that helped. I wanted to measure effect and switched back to `mmapfs` and all of a sudden reads become faster. Maybe that's because db on disk now takes 58gb (79m docs, 18m deleted, no merges happened during tests below).\n\nAfter clearing caches (both es and page) this is what I see now with `mmapfs`:\n\n```\n1000: 1000/1000 results in 574.1880ms\n2000: 1000/1000 results in 136.8101ms\n3000: 1000/1000 results in 131.5050ms\n4000: 1000/1000 results in 196.9988ms\n5000: 1000/1000 results in 129.2441ms\n6000: 1000/1000 results in 114.4180ms\n7000: 1000/1000 results in 125.1080ms\n8000: 1000/1000 results in 125.5410ms\n9000: 1000/1000 results in 137.8851ms\n10000: 1000/1000 results in 421.0851ms\n11000: 1000/1000 results in 132.7579ms\n12000: 1000/1000 results in 139.8439ms\n13000: 1000/1000 results in 147.3451ms\n14000: 1000/1000 results in 128.7601ms\n15000: 1000/1000 results in 124.6841ms\n16000: 1000/1000 results in 160.6929ms\n17000: 1000/1000 results in 102.9320ms\n18000: 1000/1000 results in 124.1219ms\n19000: 1000/1000 results in 136.3180ms\n20000: 1000/1000 results in 132.3440ms\n21000: 1000/1000 results in 119.6802ms\n22000: 1000/1000 results in 131.2690ms\n23000: 1000/1000 results in 119.5798ms\n24000: 1000/1000 results in 105.7951ms\n25000: 1000/1000 results in 74.1820ms\n26000: 1000/1000 results in 74.0089ms\n27000: 1000/1000 results in 70.2801ms\n28000: 1000/1000 results in 105.9880ms\n29000: 1000/1000 results in 69.2880ms\n30000: 1000/1000 results in 78.7618ms\n31000: 1000/1000 results in 107.4059ms\n32000: 1000/1000 results in 105.9480ms\n33000: 1000/1000 results in 171.9649ms\n34000: 1000/1000 results in 142.6880ms\n35000: 1000/1000 results in 128.8581ms\n36000: 1000/1000 results in 121.4101ms\n37000: 1000/1000 results in 125.2351ms\n38000: 1000/1000 results in 115.6292ms\n...\n382000: 1000/1000 results in 82.3901ms\n383000: 1000/1000 results in 72.6929ms\n384000: 1000/1000 results in 92.5410ms\n385000: 1000/1000 results in 92.0448ms\n386000: 1000/1000 results in 67.9412ms\n387000: 1000/1000 results in 85.4239ms\n388000: 1000/1000 results in 79.0410ms\n389000: 1000/1000 results in 85.2480ms\n390000: 1000/1000 results in 71.1071ms\n391000: 1000/1000 results in 88.0740ms\n392000: 1000/1000 results in 98.4831ms\n393000: 1000/1000 results in 101.9111ms\n394000: 1000/1000 results in 90.3878ms\n395000: 1000/1000 results in 62.3178ms\n396000: 1000/1000 results in 87.1019ms\n397000: 1000/1000 results in 83.6191ms\n398000: 1000/1000 results in 99.5011ms\n399000: 1000/1000 results in 88.6149ms\n400000: 1000/1000 results in 92.9849ms\n401000: 1000/1000 results in 81.6429ms\n402000: 1000/1000 results in 93.7450ms\n403000: 1000/1000 results in 85.8731ms\n404000: 1000/1000 results in 76.5259ms\n405000: 1000/1000 results in 68.8400ms\n406000: 1000/1000 results in 80.6811ms\n407000: 1000/1000 results in 91.7079ms\n408000: 1000/1000 results in 76.2289ms\n409000: 1000/1000 results in 81.4910ms\n410000: 1000/1000 results in 71.6860ms\n```\n\nReading from non-zero offset works quite well too:\n\n```\n10001000: 1000/1000 results in 167.1340ms\n10002000: 1000/1000 results in 93.1280ms\n10003000: 1000/1000 results in 74.7740ms\n10004000: 1000/1000 results in 111.3100ms\n10005000: 1000/1000 results in 72.1872ms\n10006000: 1000/1000 results in 90.1690ms\n10007000: 1000/1000 results in 97.9781ms\n10008000: 1000/1000 results in 93.0998ms\n10009000: 1000/1000 results in 89.1528ms\n10010000: 1000/1000 results in 96.3719ms\n10011000: 1000/1000 results in 77.0710ms\n10012000: 1000/1000 results in 69.8390ms\n10013000: 998/998 results in 65.2430ms\n10014000: 1000/1000 results in 94.7330ms\n10015000: 0/0 results in 3.0620ms\n10016000: 999/999 results in 90.8070ms\n10017000: 1000/1000 results in 93.3099ms\n10018000: 1000/1000 results in 89.5600ms\n10019000: 1000/1000 results in 88.2859ms\n10020000: 1000/1000 results in 91.9721ms\n10021000: 1000/1000 results in 88.1851ms\n10022000: 999/999 results in 80.3919ms\n10023000: 1000/1000 results in 82.9122ms\n10024000: 999/999 results in 92.6580ms\n10025000: 1000/1000 results in 91.4960ms\n10026000: 1000/1000 results in 106.1878ms\n10027000: 1000/1000 results in 88.2740ms\n...\n10424000: 999/999 results in 84.5432ms\n10425000: 1000/1000 results in 77.2529ms\n10426000: 1000/1000 results in 81.6329ms\n10427000: 1000/1000 results in 76.0500ms\n10428000: 1000/1000 results in 77.8770ms\n10429000: 999/999 results in 92.4761ms\n10430000: 1000/1000 results in 89.7510ms\n10431000: 1000/1000 results in 90.7910ms\n10432000: 999/999 results in 66.0632ms\n10433000: 1000/1000 results in 93.5569ms\n10434000: 1000/1000 results in 105.3710ms\n10435000: 1000/1000 results in 71.0671ms\n10436000: 1000/1000 results in 90.0891ms\n10437000: 1000/1000 results in 81.4419ms\n10438000: 1000/1000 results in 69.7391ms\n10439000: 1000/1000 results in 73.6330ms\n10440000: 1000/1000 results in 85.9270ms\n```\n\nThere are corresponding iostat outputs:\n\n```\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.10     1.90 2943.40    4.10 220123.20    34.40   149.39     6.15    2.09    2.09    0.24   0.30  89.60\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.30 3571.60    0.40 216859.60     2.80   121.42     6.61    1.85    1.85    0.00   0.25  89.50\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.90 4053.20    0.50 211587.60     5.60   104.40     7.39    1.82    1.82    0.00   0.22  88.60\n\n Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.70 3688.40    2.50 181295.20    31.20    98.26     6.73    1.83    1.83    0.80   0.20  75.60\n```\n\n```\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     1.20 3504.20    0.70 184944.00     8.00   105.54     6.73    1.92    1.91   24.29   0.22  78.30\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.90 4225.50    0.60 211811.20    33.60   100.26     7.81    1.85    1.85    0.00   0.21  88.20\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00    10.30 4370.50    6.00 209556.40    84.80    95.80     7.78    1.78    1.78    0.67   0.20  87.70\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     1.70 4254.30    0.60 197814.00     9.20    92.99     7.60    1.79    1.79    0.00   0.19  82.60\n```\n\nSame for `niofs`:\n\n```\n1000: 1000/1000 results in 496.1381ms\n2000: 1000/1000 results in 74.4381ms\n3000: 1000/1000 results in 73.3318ms\n4000: 1000/1000 results in 79.1049ms\n5000: 1000/1000 results in 72.4850ms\n6000: 1000/1000 results in 63.8230ms\n7000: 1000/1000 results in 61.4109ms\n8000: 1000/1000 results in 70.1060ms\n9000: 1000/1000 results in 63.4592ms\n10000: 1000/1000 results in 74.9750ms\n11000: 1000/1000 results in 67.6420ms\n12000: 1000/1000 results in 74.9049ms\n13000: 1000/1000 results in 73.5261ms\n14000: 1000/1000 results in 60.2379ms\n15000: 1000/1000 results in 54.9819ms\n16000: 1000/1000 results in 67.2531ms\n17000: 1000/1000 results in 58.7561ms\n18000: 1000/1000 results in 68.6781ms\n19000: 1000/1000 results in 73.9031ms\n20000: 1000/1000 results in 69.7479ms\n21000: 1000/1000 results in 62.8011ms\n22000: 1000/1000 results in 65.1691ms\n23000: 1000/1000 results in 69.0689ms\n24000: 1000/1000 results in 55.1078ms\n25000: 1000/1000 results in 40.6590ms\n26000: 1000/1000 results in 39.3720ms\n27000: 1000/1000 results in 37.1220ms\n28000: 1000/1000 results in 52.6612ms\n29000: 1000/1000 results in 45.9480ms\n30000: 1000/1000 results in 40.6678ms\n31000: 1000/1000 results in 62.4180ms\n32000: 1000/1000 results in 67.7090ms\n33000: 1000/1000 results in 117.3420ms\n34000: 1000/1000 results in 83.6558ms\n35000: 1000/1000 results in 80.6820ms\n36000: 1000/1000 results in 62.2151ms\n37000: 1000/1000 results in 72.4199ms\n38000: 1000/1000 results in 68.6040ms\n39000: 1000/1000 results in 75.7370ms\n...\n378000: 1000/1000 results in 52.8140ms\n379000: 1000/1000 results in 40.0920ms\n380000: 1000/1000 results in 47.5650ms\n381000: 1000/1000 results in 51.1720ms\n382000: 1000/1000 results in 49.6240ms\n383000: 1000/1000 results in 43.7841ms\n384000: 1000/1000 results in 48.2061ms\n385000: 1000/1000 results in 51.2450ms\n386000: 1000/1000 results in 45.0971ms\n387000: 1000/1000 results in 46.7429ms\n388000: 1000/1000 results in 46.7849ms\n389000: 1000/1000 results in 49.4158ms\n390000: 1000/1000 results in 37.8931ms\n391000: 1000/1000 results in 47.8899ms\n392000: 1000/1000 results in 52.6190ms\n393000: 1000/1000 results in 56.3581ms\n394000: 1000/1000 results in 54.5709ms\n395000: 1000/1000 results in 39.5730ms\n396000: 1000/1000 results in 44.1380ms\n397000: 1000/1000 results in 51.1749ms\n398000: 1000/1000 results in 59.4790ms\n399000: 1000/1000 results in 57.6718ms\n400000: 1000/1000 results in 48.2280ms\n401000: 1000/1000 results in 51.0840ms\n```\n\n```\n1001000: 1000/1000 results in 74.7449ms\n1002000: 1000/1000 results in 53.4320ms\n1003000: 1000/1000 results in 54.7791ms\n1004000: 1000/1000 results in 52.4430ms\n1005000: 1000/1000 results in 46.8009ms\n1006000: 1000/1000 results in 47.2221ms\n1007000: 1000/1000 results in 44.2469ms\n1008000: 1000/1000 results in 42.1610ms\n1009000: 1000/1000 results in 39.5839ms\n1010000: 1000/1000 results in 40.0970ms\n1011000: 1000/1000 results in 40.9501ms\n1012000: 1000/1000 results in 38.7561ms\n1013000: 1000/1000 results in 40.1518ms\n1014000: 1000/1000 results in 43.7870ms\n1015000: 1000/1000 results in 40.2260ms\n1016000: 1000/1000 results in 35.7931ms\n1017000: 1000/1000 results in 41.6169ms\n1018000: 1000/1000 results in 43.9320ms\n1019000: 1000/1000 results in 45.1200ms\n1020000: 1000/1000 results in 40.1652ms\n1021000: 1000/1000 results in 39.2389ms\n1022000: 1000/1000 results in 41.3420ms\n1023000: 1000/1000 results in 50.7259ms\n1024000: 1000/1000 results in 44.7628ms\n1025000: 1000/1000 results in 35.8820ms\n1026000: 1000/1000 results in 43.0739ms\n1027000: 1000/1000 results in 36.7758ms\n1028000: 1000/1000 results in 36.2608ms\n1029000: 1000/1000 results in 38.4901ms\n1030000: 1000/1000 results in 37.1521ms\n1031000: 1000/1000 results in 40.0848ms\n1032000: 1000/1000 results in 43.8211ms\n1033000: 1000/1000 results in 51.0271ms\n1034000: 1000/1000 results in 40.8111ms\n1035000: 1000/1000 results in 44.4250ms\n1036000: 1000/1000 results in 38.4030ms\n1037000: 1000/1000 results in 38.2321ms\n1038000: 1000/1000 results in 35.8500ms\n1039000: 1000/1000 results in 31.7190ms\n...\n1425000: 1000/1000 results in 47.9991ms\n1426000: 1000/1000 results in 46.0401ms\n1427000: 1000/1000 results in 40.9181ms\n1428000: 1000/1000 results in 44.3571ms\n1429000: 1000/1000 results in 45.2061ms\n1430000: 1000/1000 results in 48.6059ms\n1431000: 1000/1000 results in 43.0620ms\n1432000: 1000/1000 results in 37.7240ms\n1433000: 1000/1000 results in 35.8810ms\n1434000: 1000/1000 results in 42.1000ms\n1435000: 1000/1000 results in 48.0080ms\n1436000: 1000/1000 results in 123.9400ms\n1437000: 1000/1000 results in 35.1648ms\n1438000: 1000/1000 results in 53.0531ms\n1439000: 1000/1000 results in 37.2989ms\n1440000: 1000/1000 results in 41.7180ms\n1441000: 1000/1000 results in 43.6361ms\n1442000: 1000/1000 results in 45.9149ms\n1443000: 1000/1000 results in 52.1169ms\n```\n\n```\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.40 2112.90    0.80 39818.00     4.80    37.68     1.09    0.52    0.52    0.00   0.12  25.10\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     1.10 7294.50    2.60 146462.80    14.80    40.15     3.84    0.53    0.53    0.00   0.10  74.90\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     1.60 5995.10    1.30 119229.60    16.40    39.77     3.35    0.56    0.56    0.00   0.10  61.90\n```\n\n```\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     1.50 6329.80    0.60 147168.80    12.00    46.50     4.26    0.67    0.67    0.00   0.12  75.20\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     1.00 4711.00    1.00 165078.80    23.60    70.08     4.32    0.92    0.92    0.00   0.17  78.60\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.50  989.20    0.70 37534.40     4.80    75.84     0.66    0.66    0.63   42.86   0.23  22.30\n```\n\n`niofs` still better in terms of io utilization and speed: 97.1836ms and 86.3708ms vs 66.6095ms and 49.4988ms on average for runs above.\n\nI also measured more real-life workloads: fetch 4.5m docs from 20 parallel clients. Restart, drop all caches, wait for indexing to catch up, measure 2 times. Here's what I've got:\n\n`mmapfs`:\n1. Avg search time: 3245.3082466ms, total search time: 4053390ms\n2. Avg search time: 3232.51321057ms, total search time: 4037409ms\n\nio for both measurements:\n\n![io](http://puu.sh/8rX3Q.png)\n\n`niofs`:\n1. Avg search time: 1692.49319456ms, total search time: 2113924ms\n2. Avg search time: 1717.33706966ms, total search time: 2144954ms\n\nio for both measurements:\n\n![io](http://puu.sh/8rXhi.png)\n\nThat is 2x difference in speed with similar io utilization.\n\nAny idea what could be the reason for misbehaving `mmapfs`?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/41647496","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-41647496","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":41647496,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNjQ3NDk2","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2014-04-29T07:13:55Z","updated_at":"2014-04-29T07:13:55Z","author_association":"CONTRIBUTOR","body":"This is a big difference. What Java version are you using? In case it is an old version, it would be interesting to try it with Java [7u55](http://www.elasticsearch.org/blog/java-1-7u55-safe-use-elasticsearch-lucene/) or maybe even a Java 8 build, [which seems to have better mmap performance](http://search-lucene.com/m/WwzTb2mGQps).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/41647729","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-41647729","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":41647729,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNjQ3NzI5","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"created_at":"2014-04-29T07:17:46Z","updated_at":"2014-04-29T07:17:46Z","author_association":"CONTRIBUTOR","body":"```\njava -version\njava version \"1.7.0_17\"\nJava(TM) SE Runtime Environment (build 1.7.0_17-b02)\nJava HotSpot(TM) 64-Bit Server VM (build 23.7-b01, mixed mode)\n```\n\nI'll try to update and let you know.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/67309435","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-67309435","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":67309435,"node_id":"MDEyOklzc3VlQ29tbWVudDY3MzA5NDM1","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"created_at":"2014-12-17T11:23:23Z","updated_at":"2014-12-17T11:23:23Z","author_association":"CONTRIBUTOR","body":"I checked es 1.4.1 with different java versions and different storage types. Testing looked like this:\n1. Set java version and store type in config\n2. Restart Elasticsearch\n3. Wait until indexing is up to date with queue\n4. Create task to iterate the data (1 or 20 parallel workers, 6.6m docs, max 5k docs per result set)\n\nNo reindexing happened between the tests. Index is 77gb, 90m docs.\n\nResults are here:\n- java 1.7.0_17 + default store type:\n  - 20 workers: 3849ms avg, 6609816ms total\n  - 1 worker: 241ms avg, 414071ms total\n- java 1.7.0_17 + niofs store type:\n  - 20 workers: 3259ms avg, 5596472ms total\n  - 1 worker: 242ms avg, 416553ms total\n- java 1.8.0_25 + default store type:\n  - 20 workers: 3612ms avg, 6202648ms total\n  - 1 worker: 248ms avg, 426188ms total\n- java 1.8.0_25 + niofs store type:\n  - 20 workers: 3378ms avg, 5801105ms total\n  - 1 worker: 242ms avg, 415881ms total\n\nAll results are pretty close, but worse than with niofs and 1.1.1. I haven't tried downgrading to 1.1.1, though. Any thoughts?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/67329207","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-67329207","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":67329207,"node_id":"MDEyOklzc3VlQ29tbWVudDY3MzI5MjA3","user":{"login":"jpountz","id":299848,"node_id":"MDQ6VXNlcjI5OTg0OA==","avatar_url":"https://avatars2.githubusercontent.com/u/299848?v=4","gravatar_id":"","url":"https://api.github.com/users/jpountz","html_url":"https://github.com/jpountz","followers_url":"https://api.github.com/users/jpountz/followers","following_url":"https://api.github.com/users/jpountz/following{/other_user}","gists_url":"https://api.github.com/users/jpountz/gists{/gist_id}","starred_url":"https://api.github.com/users/jpountz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jpountz/subscriptions","organizations_url":"https://api.github.com/users/jpountz/orgs","repos_url":"https://api.github.com/users/jpountz/repos","events_url":"https://api.github.com/users/jpountz/events{/privacy}","received_events_url":"https://api.github.com/users/jpountz/received_events","type":"User","site_admin":false},"created_at":"2014-12-17T14:29:00Z","updated_at":"2014-12-17T14:29:00Z","author_association":"CONTRIBUTOR","body":"I just did some tests to try to see if there would be errors on the ES side, but things look ok. First in the simple case at least elasticsearch looks up each document only once. Then I hacked the source code to track the number of bytes that are read from the directory and things seem to work as expected as well: I tried various document sizes ranging from 200b to 20KB and the number of bytes read per document was between 9KB and 40KB, which is expected due to the way lucene stores stored fields. Given that the NIO directory has an internal buffer of 16KB to save system calls, I believe it could go up to 40+16=56KB in practice.\n\nRegarding your new numbers, the new store now uses niofs for cold parts of the index (eg. stored fields) and mmap for hot parts (eg. the inverted index and doc values) so that would explain why the numbers in your last comment are similar. Regarding your previous experiments with `mmap`, I'm wondering if the slowness could be partially explained with fragmentation. (?)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/70066152","html_url":"https://github.com/elastic/elasticsearch/issues/5781#issuecomment-70066152","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/5781","id":70066152,"node_id":"MDEyOklzc3VlQ29tbWVudDcwMDY2MTUy","user":{"login":"bobrik","id":89186,"node_id":"MDQ6VXNlcjg5MTg2","avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","gravatar_id":"","url":"https://api.github.com/users/bobrik","html_url":"https://github.com/bobrik","followers_url":"https://api.github.com/users/bobrik/followers","following_url":"https://api.github.com/users/bobrik/following{/other_user}","gists_url":"https://api.github.com/users/bobrik/gists{/gist_id}","starred_url":"https://api.github.com/users/bobrik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bobrik/subscriptions","organizations_url":"https://api.github.com/users/bobrik/orgs","repos_url":"https://api.github.com/users/bobrik/repos","events_url":"https://api.github.com/users/bobrik/events{/privacy}","received_events_url":"https://api.github.com/users/bobrik/received_events","type":"User","site_admin":false},"created_at":"2015-01-15T10:25:31Z","updated_at":"2015-01-15T10:25:31Z","author_association":"CONTRIBUTOR","body":"Well, I'm going to close this for now since current performance is mostly ok for us.\n","performed_via_github_app":null}]