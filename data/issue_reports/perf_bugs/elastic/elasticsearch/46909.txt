{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/46909","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46909/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46909/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/46909/events","html_url":"https://github.com/elastic/elasticsearch/issues/46909","id":496267639,"node_id":"MDU6SXNzdWU0OTYyNjc2Mzk=","number":46909,"title":"Cluster stuck for few mins blocked by zen-disco-node-left","user":{"login":"entrop-tankos","id":37574231,"node_id":"MDQ6VXNlcjM3NTc0MjMx","avatar_url":"https://avatars3.githubusercontent.com/u/37574231?v=4","gravatar_id":"","url":"https://api.github.com/users/entrop-tankos","html_url":"https://github.com/entrop-tankos","followers_url":"https://api.github.com/users/entrop-tankos/followers","following_url":"https://api.github.com/users/entrop-tankos/following{/other_user}","gists_url":"https://api.github.com/users/entrop-tankos/gists{/gist_id}","starred_url":"https://api.github.com/users/entrop-tankos/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/entrop-tankos/subscriptions","organizations_url":"https://api.github.com/users/entrop-tankos/orgs","repos_url":"https://api.github.com/users/entrop-tankos/repos","events_url":"https://api.github.com/users/entrop-tankos/events{/privacy}","received_events_url":"https://api.github.com/users/entrop-tankos/received_events","type":"User","site_admin":false},"labels":[{"id":881394071,"node_id":"MDU6TGFiZWw4ODEzOTQwNzE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Cluster%20Coordination","name":":Distributed/Cluster Coordination","color":"0e8a16","default":false,"description":"Cluster formation and cluster state publication, including cluster membership and fault detection."}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2019-09-20T10:11:20Z","updated_at":"2019-12-02T10:22:00Z","closed_at":"2019-09-20T12:31:35Z","author_association":"NONE","active_lock_reason":null,"body":"Elasticsearch version: 6.8.2\r\nCluster: a huge one, 360 data nodes, 60 coordinators, 40 masters. About ~200tb of data\r\nPlugins installed: none\r\nJVM version: 1.8.0_102-b14\r\nOS: CentOS 7.6\r\n\r\nA few words about the cluster:\r\nI'm driving a big elasticsearch cluster across 4 data centers with 3 of them with data nodes (120 each).\r\nIndexes I'm storing contain 180 primary and 180 replica shards. Each replica shard is stored in a different data center,\r\nso it's ok for the cluster to stay yellow if it looses a data center for some reason.\r\n\r\nNow about the bug:\r\nWhen a data center goes down, nodes allocated there stop to respond (145 nodes).\r\n\r\nRight after \"pulling the plug\" in one data center the master behaves like this:\r\nIt detects 1 node as down (1 of 145) and creates about 140 pending tasks to commit to the cluster that this node is gone.\r\nAnd these tasks become blockers. While the master is waiting for respond from dead nodes it doesn't mark currently lost primary shards as stale. This produces\r\na huge delay for all indexing operations on the cluster (3-5mins).\r\n\r\n\r\n\r\n\r\nHow this can be reproduces:\r\n1) Here is the start point:\r\n\r\n```\r\ncurl -X GET \"localhost:9200/_cluster/health?pretty\"\r\n{\r\n  \"cluster_name\" : \"name\",\r\n  \"status\" : \"green\",\r\n  \"timed_out\" : false,\r\n  \"number_of_nodes\" : 460,\r\n  \"number_of_data_nodes\" : 360,\r\n  \"active_primary_shards\" : 5400,\r\n  \"active_shards\" : 10800,\r\n  \"relocating_shards\" : 0,\r\n  \"initializing_shards\" : 0,\r\n  \"unassigned_shards\" : 0,\r\n  \"delayed_unassigned_shards\" : 0,\r\n  \"number_of_pending_tasks\" : 0,\r\n  \"number_of_in_flight_fetch\" : 0,\r\n  \"task_max_waiting_in_queue_millis\" : 0,\r\n  \"active_shards_percent_as_number\" : 100.0\r\n}\r\n```\r\n\r\n2) Now I disconnect 1 of 4 data centers (just the 120 data nodes):\r\n```\r\ncurl -X GET \"localhost:9200/_cluster/health?pretty\"\r\n{\r\n  \"cluster_name\" : \"graylog\",\r\n  \"status\" : \"yellow\",\r\n  \"timed_out\" : false,\r\n  \"number_of_nodes\" : 459,\r\n  \"number_of_data_nodes\" : 359,\r\n  \"active_primary_shards\" : 5400,\r\n  \"active_shards\" : 10770,\r\n  \"relocating_shards\" : 0,\r\n  \"initializing_shards\" : 0,\r\n  \"unassigned_shards\" : 30,\r\n  \"delayed_unassigned_shards\" : 30,\r\n  \"number_of_pending_tasks\" : 145,\r\n  \"number_of_in_flight_fetch\" : 240,\r\n  \"task_max_waiting_in_queue_millis\" : 27831,\r\n  \"active_shards_percent_as_number\" : 99.72222222222223\r\n}\r\n```\r\n\r\nI have 30 shards per node. You can see here that it detected 1 node and 30 shards as down, but there are much more nodes down.\r\n\r\nPending tasks:\r\n```\r\ncurl -s -X GET \"localhost:9200/_cluster/pending_tasks?pretty\"\r\n{\r\n  \"tasks\" : [\r\n    {\r\n      \"insert_order\" : 129525,\r\n      \"priority\" : \"IMMEDIATE\",\r\n      \"source\" : \"zen-disco-node-left({50.data.elasticsearch.dc.domain.org}{s9fwQI5KSoegZuulZV8mUA}{oOFnfDL6RQS_UbnC-CnIOg}{10.21.131.177}{10.21.131.177:9300}{zone=dc, xpack.installed=true}), reason(left)\",\r\n      \"executing\" : true,\r\n      \"time_in_queue_millis\" : 38637,\r\n      \"time_in_queue\" : \"38.6s\"\r\n    },\r\n    {\r\n      \"insert_order\" : 129526,\r\n      \"priority\" : \"IMMEDIATE\",\r\n      \"source\" : \"zen-disco-node-left({45.data.elasticsearch.dc.domain.org}{WOanEsQcSLeYyNwMjAikzQ}{A6BeDVPqQH28ClYYNe-cbQ}{10.21.131.172}{10.21.131.172:9300}{zone=dc, xpack.installed=true}), reason(left)\",\r\n      \"executing\" : false,\r\n      \"time_in_queue_millis\" : 38626,\r\n      \"time_in_queue\" : \"38.6s\"\r\n    },\r\n\r\n.........\r\nabout 140 tasks with big time_in_queue and \"zen-disco-node-left\"\r\n.........\r\n\r\n ]\r\n}\r\n```\r\n\r\n3) In about 1-2 mins the \"zen-disco-node-left\" tasks are gone and the cluster is able to accept new documents.\r\n\r\n \r\n\r\nWhat would be cool to face instead of this behavior:\r\n\r\nAs far as I understand, the tasks are processed by the TaskBatcher in a single thread one after another. \r\nIt would be very cool to detect dead nodes in a async way and to cancel pending tasks for this nodes.","closed_by":{"login":"DaveCTurner","id":5058284,"node_id":"MDQ6VXNlcjUwNTgyODQ=","avatar_url":"https://avatars3.githubusercontent.com/u/5058284?v=4","gravatar_id":"","url":"https://api.github.com/users/DaveCTurner","html_url":"https://github.com/DaveCTurner","followers_url":"https://api.github.com/users/DaveCTurner/followers","following_url":"https://api.github.com/users/DaveCTurner/following{/other_user}","gists_url":"https://api.github.com/users/DaveCTurner/gists{/gist_id}","starred_url":"https://api.github.com/users/DaveCTurner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaveCTurner/subscriptions","organizations_url":"https://api.github.com/users/DaveCTurner/orgs","repos_url":"https://api.github.com/users/DaveCTurner/repos","events_url":"https://api.github.com/users/DaveCTurner/events{/privacy}","received_events_url":"https://api.github.com/users/DaveCTurner/received_events","type":"User","site_admin":false},"performed_via_github_app":null}