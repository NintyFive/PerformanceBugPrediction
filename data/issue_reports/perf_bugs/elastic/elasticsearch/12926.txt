{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/12926","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12926/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12926/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/12926/events","html_url":"https://github.com/elastic/elasticsearch/issues/12926","id":101375198,"node_id":"MDU6SXNzdWUxMDEzNzUxOTg=","number":12926,"title":"Shards stuck in INITIALIZING","user":{"login":"krisb78","id":222033,"node_id":"MDQ6VXNlcjIyMjAzMw==","avatar_url":"https://avatars3.githubusercontent.com/u/222033?v=4","gravatar_id":"","url":"https://api.github.com/users/krisb78","html_url":"https://github.com/krisb78","followers_url":"https://api.github.com/users/krisb78/followers","following_url":"https://api.github.com/users/krisb78/following{/other_user}","gists_url":"https://api.github.com/users/krisb78/gists{/gist_id}","starred_url":"https://api.github.com/users/krisb78/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krisb78/subscriptions","organizations_url":"https://api.github.com/users/krisb78/orgs","repos_url":"https://api.github.com/users/krisb78/repos","events_url":"https://api.github.com/users/krisb78/events{/privacy}","received_events_url":"https://api.github.com/users/krisb78/received_events","type":"User","site_admin":false},"labels":[{"id":152510590,"node_id":"MDU6TGFiZWwxNTI1MTA1OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Recovery","name":":Distributed/Recovery","color":"0e8a16","default":false,"description":"Anything around constructing a new shard, either from a local or a remote source."},{"id":111624690,"node_id":"MDU6TGFiZWwxMTE2MjQ2OTA=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/feedback_needed","name":"feedback_needed","color":"d4c5f9","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2015-08-17T09:11:46Z","updated_at":"2015-08-17T13:02:04Z","closed_at":"2015-08-17T12:54:37Z","author_association":"NONE","active_lock_reason":null,"body":"I have a fairly large index (~3.5 million documents, with nested fields, some of the docs are quite large) where the documents are being updated a lot.\n\nA while ago the cluster went into the yellow state and I'm seeing a number of shards stuck in the \"INITIALIZING\" state:\n\n```\njoint_user_summary_v1                   0 r INITIALIZING                   127.0.1.1 cubitsearch-1\njoint_user_summary_v1                   7 r INITIALIZING                   127.0.1.1 cubitsearch-5\njoint_user_summary_v1                   7 r INITIALIZING                   127.0.1.1 cubitsearch-2\njoint_user_summary_v1                   3 r INITIALIZING                   127.0.1.1 cubitsearch-1\njoint_user_summary_v1                   1 r INITIALIZING                   127.0.1.1 cubitsearch-4\njoint_user_summary_v1                   1 r INITIALIZING                   127.0.1.1 cubitsearch-3\njoint_user_summary_v1                   5 r INITIALIZING                   127.0.1.1 cubitsearch-2\njoint_user_summary_v1                   6 r INITIALIZING                   127.0.1.1 cubitsearch-3\njoint_user_summary_v1                   6 r INITIALIZING                   127.0.1.1 cubitsearch-5\n```\n\nNaturally, I can't optimize this index anymore:\n\n```\n{\"_shards\":{\"total\":30,\"successful\":5,\"failed\":6,\"failures\":[{\"index\":\"joint_user_summary_v1\",\"shard\":0,\"status\":500,\"reason\":\"BroadcastShardOperationFailedException[[joint_user_summary_v1][0] ]; nested: RemoteTransportException[[cubitsearch-2][inet[/10.0.1.236:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][0] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][0] recovery is in progress, flush is not allowed]; \"},{\"index\":\"joint_user_summary_v1\",\"shard\":1,\"status\":500,\"reason\":\"BroadcastShardOperationFailedException[[joint_user_summary_v1][1] ]; nested: RemoteTransportException[[cubitsearch-2][inet[/10.0.1.236:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][1] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][1] recovery is in progress, flush is not allowed]; \"},{\"index\":\"joint_user_summary_v1\",\"shard\":3,\"status\":500,\"reason\":\"BroadcastShardOperationFailedException[[joint_user_summary_v1][3] ]; nested: RemoteTransportException[[cubitsearch-3][inet[/10.0.1.237:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][3] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][3] recovery is in progress, flush is not allowed]; \"},{\"index\":\"joint_user_summary_v1\",\"shard\":5,\"status\":500,\"reason\":\"BroadcastShardOperationFailedException[[joint_user_summary_v1][5] ]; nested: RemoteTransportException[[cubitsearch-1][inet[/10.0.1.235:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][5] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][5] recovery is in progress, flush is not allowed]; \"},{\"index\":\"joint_user_summary_v1\",\"shard\":6,\"status\":500,\"reason\":\"BroadcastShardOperationFailedException[[joint_user_summary_v1][6] ]; nested: RemoteTransportException[[cubitsearch-1][inet[/10.0.1.235:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][6] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][6] recovery is in progress, flush is not allowed]; \"},{\"index\":\"joint_user_summary_v1\",\"shard\":7,\"status\":500,\"reason\":\"BroadcastShardOperationFailedException[[joint_user_summary_v1][7] ]; nested: RemoteTransportException[[cubitsearch-3][inet[/10.0.1.237:9300]][indices:admin/optimize[s]]]; nested: OptimizeFailedEngineException[[joint_user_summary_v1][7] force merge failed]; nested: FlushNotAllowedEngineException[[joint_user_summary_v1][7] recovery is in progress, flush is not allowed]; \"}]}}\n```\n\nHere's an excerpt from the log file that may shed some light on the issue:\n\n```\nAug 17 09:06:31 cubitsearch-3 elasticsearch: [2015-08-17 09:06:31,525][DEBUG][action.bulk              ] [cubitsearch-3] observer: timeout notification from cluster service. timeout setting [60ms], time since start [60ms]\nAug 17 09:06:31 cubitsearch-3 elasticsearch: [2015-08-17 09:06:31,525][DEBUG][action.bulk              ] [cubitsearch-3] observer: timeout notification from cluster service. timeout setting [60ms], time since start [60ms]\nAug 17 09:06:31 cubitsearch-3 elasticsearch: [2015-08-17 09:06:31,939][WARN ][indices.cluster          ] [cubitsearch-3] [[joint_user_summary_v1][1]] marking and sending shard failed due to [failed recovery]\nAug 17 09:06:31 cubitsearch-3 elasticsearch: org.elasticsearch.indices.recovery.RecoveryFailedException: [joint_user_summary_v1][1]: Recovery failed from [cubitsearch-2][U2eMGyQ7Rhic1lHqzIoLXA][cubitsearch-2][inet[/10.0.1.236:9300]]{max_local_storage_nodes=1, master=false} into [cubitsearch-3][IEP7xQczReKinA78HfLC3Q][cubitsearch-3][inet[/10.0.1.237:9300]]{max_local_storage_nodes=1, master=false}\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: Caused by: org.elasticsearch.transport.RemoteTransportException: [cubitsearch-2][inet[/10.0.1.236:9300]][internal:index/shard/recovery/start_recovery]\nAug 17 09:06:31 cubitsearch-3 elasticsearch: Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [joint_user_summary_v1][1] Phase[2] Execution failed\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:902)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: Caused by: org.elasticsearch.transport.ReceiveTimeoutTransportException: [cubitsearch-3][inet[/10.0.1.237:9300]][internal:index/shard/recovery/prepare_translog] request_id [14280416] timed out after [900000ms]\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)\nAug 17 09:06:31 cubitsearch-3 elasticsearch: #011... 3 more\n```\n\nI was hoping that upgrading to the latest version (1.7.1) would fix the problem, but unfortunately it persists.\n","closed_by":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"performed_via_github_app":null}