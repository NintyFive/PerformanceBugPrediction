{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/19646","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19646/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19646/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/19646/events","html_url":"https://github.com/elastic/elasticsearch/issues/19646","id":168036837,"node_id":"MDU6SXNzdWUxNjgwMzY4Mzc=","number":19646,"title":"Cluster hangs and node disconnects due to exessive traffic on transport layer network card stopping pings","user":{"login":"Cardy165","id":9974991,"node_id":"MDQ6VXNlcjk5NzQ5OTE=","avatar_url":"https://avatars0.githubusercontent.com/u/9974991?v=4","gravatar_id":"","url":"https://api.github.com/users/Cardy165","html_url":"https://github.com/Cardy165","followers_url":"https://api.github.com/users/Cardy165/followers","following_url":"https://api.github.com/users/Cardy165/following{/other_user}","gists_url":"https://api.github.com/users/Cardy165/gists{/gist_id}","starred_url":"https://api.github.com/users/Cardy165/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Cardy165/subscriptions","organizations_url":"https://api.github.com/users/Cardy165/orgs","repos_url":"https://api.github.com/users/Cardy165/repos","events_url":"https://api.github.com/users/Cardy165/events{/privacy}","received_events_url":"https://api.github.com/users/Cardy165/received_events","type":"User","site_admin":false},"labels":[{"id":836504707,"node_id":"MDU6TGFiZWw4MzY1MDQ3MDc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Distributed/Distributed","name":":Distributed/Distributed","color":"0e8a16","default":false,"description":"A catch all label for anything in the Distributed Area. If you aren't sure, use this one."},{"id":111416437,"node_id":"MDU6TGFiZWwxMTE0MTY0Mzc=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/discuss","name":"discuss","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2016-07-28T08:06:08Z","updated_at":"2018-02-13T19:54:17Z","closed_at":"2016-07-28T19:14:52Z","author_association":"NONE","active_lock_reason":null,"body":"<!--\nGitHub is reserved for bug reports and feature requests. The best place\nto ask a general question is at the Elastic Discourse forums at\nhttps://discuss.elastic.co. If you are in fact posting a bug report or\na feature request, please include one and only one of the below blocks\nin your new issue. Note that whether you're filing a bug report or a\nfeature request, ensure that your submission is for an\n[OS that we support](https://www.elastic.co/support/matrix#show_os).\nBug reports on an OS that we do not support or feature requests\nspecific to an OS that we do not support will be closed.\n-->\n\n<!--\nIf you are filing a bug report, please remove the below feature\nrequest block and provide responses for all of the below items.\n-->\n\n**Elasticsearch version**:\nIssue Tested on v 2.3.1 & v2.3.4 of Elasticsearch\n\n**JVM version**:\n      \"version\" : \"1.8.0_92\",\n        \"vm_name\" : \"Java HotSpot(TM) 64-Bit Server VM\",\n        \"vm_version\" : \"25.92-b14\",\n\n**OS version**:\nCentOS Linux release 7.2.1511 (Core)\n\n**Description of the problem including expected versus actual behavior**:\n\n_Background information_\nOur clusters consist of approximately 1500 or so indexes (5 shards per index), we are running a group of  aggregated queries across 1175 of the available indexes. On the test system in question (although the issue affects both our development and much more powerful live environment) there is between 1.5 and 2.5 TB of data (including 1 replica per shard).\n\n**Expected**\nThe frontend of our system issues complex queries and often runs multiple queries at once. The queries are complex with a number of aggregations. The queries normally run on the ES backend and the frontend code then renders the results to the users.\n\nCluster state remains Green.\n\n**Actual**\nStaring the queries from the frontend with the cluster running and occasionally logging GC entries.\n\nThe cluster starts processing as expected. After a short amount of time one or more cluster nodes get disconnected from the cluster. This is seen in the logs on both the data node and the master instance it was attempting to communicate with.\n\n_Master log entry_\n\n```\n[2016-07-27 15:51:14,198][DEBUG][action.admin.cluster.node.stats] [qa-es-01-master] failed to execute on node [0Ob8bwWzRJ2MmFoteZ-o1g]\nReceiveTimeoutTransportException[[qa-es-02-01][192.168.253.2:9300][cluster:monitor/nodes/stats[n]] request_id [177174] timed out after [15000ms]]\n        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:679)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n[2016-07-27 15:52:06,645][INFO ][cluster.routing.allocation] [qa-es-01-master] Cluster health status changed from [GREEN] to [RED] (reason: [[{qa-es-02-01}{0Ob8bwWzRJ2MmFoteZ-o1g}{192.168.253.2}{192.168.253.2:9300}{host=qa-es-02, master=false}] failed]).\n[2016-07-27 15:52:06,650][INFO ][cluster.service          ] [qa-es-01-master] removed {{qa-es-02-01}{0Ob8bwWzRJ2MmFoteZ-o1g}{192.168.253.2}{192.168.253.2:9300}{host=qa-es-02, master=false},}, reason: zen-disco-node_failed({qa-es-02-01}{0Ob8bwWzRJ2MmFoteZ-o1g}{192.168.253.2}{192.168.253.2:9300}{host=qa-es-02, master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout\n```\n\n_Failed node log entry_\n\n```\n[2016-07-27 15:52:28,547][WARN ][monitor.jvm              ] [qa-es-02-01] [gc][old][8175][2] duration [1.9m], collections [1]/[1.9m], total [1.9m]/[1.9m], memory [23.3gb]->[17.2gb]/[23.8gb], all_pools {[young] [838.6mb]->[19.2mb]/[865.3mb]}{[survivor] [108.1mb]->[0b]/[108.1mb]}{[old] [22.4gb]->[17.2gb]/[22.9gb]}\n[2016-07-27 15:52:28,618][INFO ][discovery.zen            ] [qa-es-02-01] master_left [{qa-es-01-master}{bt2JqIWeRvaLg1MdExafRQ}{192.168.253.1}{192.168.253.1:9302}{host=qa-es-01, data=false, master=true}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]\n[2016-07-27 15:52:28,621][WARN ][discovery.zen            ] [qa-es-02-01] master left (reason = failed to ping, tried [3] times, each with  maximum [30s] timeout), current nodes: {{magnesium}{ucSxWsLbS_CFHzrgL-9bLw}{10.91.119.10}{10.91.119.10:9300}{data=false, master=false},{qa-es-02-master}{N-b9DErbTz2-N7DOYVx3yQ}{192.168.253.2}{192.168.253.2:9302}{host=qa-es-02, data=false, master=true},{qa-es-03-02}{M5sKFMoNSEKQFTFQjwQqyA}{192.168.253.3}{192.168.253.3:9301}{host=qa-es-03, master=false},{qa-es-01-01}{oM7dd8SjTNyTTXPn-65k6g}{192.168.253.1}{192.168.253.1:9300}{host=qa-es-01, master=false},{qa-es-03-master}{fQD12LrVRHWNmdTBHP-tNg}{192.168.253.3}{192.168.253.3:9302}{host=qa-es-03, data=false, master=true},{qa-es-01-02}{pgqJ5ybRQH2RGnxK7VkYrA}{192.168.253.1}{192.168.253.1:9301}{host=qa-es-01, master=false},{qa-es-03-01}{pCwkC8fCQj-B-wIWxCCxww}{192.168.253.3}{192.168.253.3:9300}{host=qa-es-03, master=false},{qa-es-02-01}{0Ob8bwWzRJ2MmFoteZ-o1g}{192.168.253.2}{192.168.253.2:9300}{host=qa-es-02, master=false},{qa-es-02-02}{Vi5LgogOQNehMD5Yze5l5g}{192.168.253.2}{192.168.253.2:9301}{host=qa-es-02, master=false},}\n[2016-07-27 15:52:28,625][INFO ][cluster.service          ] [qa-es-02-01] removed {{qa-es-01-master}{bt2JqIWeRvaLg1MdExafRQ}{192.168.253.1}{192.168.253.1:9302}{host=qa-es-01, data=false, master=true},}, reason: zen-disco-master_failed ({qa-es-01-master}{bt2JqIWeRvaLg1MdExafRQ}{192.168.253.1}{192.168.253.1:9302}{host=qa-es-01, data=false, master=true})\n```\n\nEach physical host has 3 nodes running on it. 1 Master node and 2 data nodes, host awareness is set for the nodes also. \n\nThe cluster state changes to red (even though there are replicas available), normally from this point on the cluster will respond to queries such as /_cluster/health and /_nodes  but the failed nodes will not rejoin the cluster.\n\nIf i try to use the OS command to stop nodes that have timed out the command is ignored and just hangs. I have to kill -9 the process to stop the instance, this needs to be done on both the master and the node that failed. Usually even killing those 2 instances does not help, the cluster continues to throw errors about being unable to ping \n\nEven once stopped and restarting the individual instances they still fail to connect back to the rest of the cluster reporting timeouts to other instance's IPs however I can ping all the IPs of the cluster. \n\nI eventually realised that the problem is the bandwidth available in the network card used for the transport layer. Once this becomes saturated with traffic from the cluster the pings between nodes become queued at the network interface. By the time they are processed the other instances have already timed out the expected ping.\n\nTo confirm this was the issue I reset our cluster and modified it so rather than having 2 network cards em1 for the http traffic and em2 for the transport layer traffic i setup the machine so that it has\n\nem1 - http traffic (1 Gb/s)\nem2 and em3 as bonded interface bond0 with mode 0 round robin giving me a single (2Gb/s) interface.\n\nRunning the same query as above allowed the query to run without issue and there were no errors on the cluster. This would seem like the solution however the number of layers of aggregation in our system can change dynamically and even with the 2 x 1Gb/s interfaces acting as one adding another aggregation then caused the same original problem. \n\n**Steps to reproduce**:\n1. On a cluster with a large amount of data and a large number of indexes and shards start a complex aggregation query. The query needs to create enough traffic to overwhelm the network card's available bandwidth.\n2. Because of the saturation of the network card used for the transport layer the cluster pings fail to arrive in a timely manor resulting in the node being removed from the cluster by the master and the node thinking the master has gone away.\n\n**Provide logs (if relevant)**:\n\n**Thoughts on a solution**\nIn previous experience with clusters would normally have some disk quorum device which arbitrates similar issues, I think with elasticsearch this isn't required but being able to define a dedicated LAN (possibly LANs to allow redundancy) like you can split http and transport traffic would remove this problem entirely. As an example a solution such as:\n\nem1 - HTTP Traffic \nem2 - Interconnect between nodes traffic (possibly including cluster state traffic)\nem3 - transport layer traffic (i.e. results of searches, indexing etc......)\n\nIn this sense bonding or teaming of cards a the OS level would allow a user to provide resiliency whilst the segregation of traffic will protect the cluster from saturation of the transport layer with network IO. \n\nI also have a post on the discussion forum for the same issue: [https://discuss.elastic.co/t/cluster-nodes-get-disconnected-and-out-of-sync-due-to-ping-timeouts-caused-by-transport-load/56505/](https://discuss.elastic.co/t/cluster-nodes-get-disconnected-and-out-of-sync-due-to-ping-timeouts-caused-by-transport-load/56505/)\n\nKind Regards\n\nLee\n","closed_by":{"login":"bleskes","id":1006375,"node_id":"MDQ6VXNlcjEwMDYzNzU=","avatar_url":"https://avatars1.githubusercontent.com/u/1006375?v=4","gravatar_id":"","url":"https://api.github.com/users/bleskes","html_url":"https://github.com/bleskes","followers_url":"https://api.github.com/users/bleskes/followers","following_url":"https://api.github.com/users/bleskes/following{/other_user}","gists_url":"https://api.github.com/users/bleskes/gists{/gist_id}","starred_url":"https://api.github.com/users/bleskes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bleskes/subscriptions","organizations_url":"https://api.github.com/users/bleskes/orgs","repos_url":"https://api.github.com/users/bleskes/repos","events_url":"https://api.github.com/users/bleskes/events{/privacy}","received_events_url":"https://api.github.com/users/bleskes/received_events","type":"User","site_admin":false},"performed_via_github_app":null}