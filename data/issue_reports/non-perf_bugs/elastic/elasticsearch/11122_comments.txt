[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/101316525","html_url":"https://github.com/elastic/elasticsearch/issues/11122#issuecomment-101316525","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11122","id":101316525,"node_id":"MDEyOklzc3VlQ29tbWVudDEwMTMxNjUyNQ==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-05-12T15:12:55Z","updated_at":"2015-05-12T15:12:55Z","author_association":"CONTRIBUTOR","body":"@mikemccand I made this a blocker\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/101339594","html_url":"https://github.com/elastic/elasticsearch/issues/11122#issuecomment-101339594","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11122","id":101339594,"node_id":"MDEyOklzc3VlQ29tbWVudDEwMTMzOTU5NA==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2015-05-12T16:27:55Z","updated_at":"2015-05-12T16:27:55Z","author_association":"CONTRIBUTOR","body":"> @mikemccand I made this a blocker\n\nOK, I agree.  I can try to fix this ...\n\nI think there are at least 2 cases here: 1) new index (shard) allocated to this node, and 2) shard relocated/recovered from another node.\n\nFor case 2), I think IndexService.createShard needs to be told how large the incoming shard will be (it's not today?), since we must already know this somewhere, but I don't know how to get this information in IndicesClusterStateService.applyInitializingShard when it calls createShard.\n\nFor case 1) we need to be a bit smarter when our node is allocated more than 1 shard for the same index, and use some heuristic e.g. \"the size of the overall index will be 50% of total free space across all path.data paths\", and budget each shard as 1/Nth of that.\n\nMaybe there are other cases, e.g. restoring from a snapshot?\n\nFor both of these cases we need some way to record \"reserved space\" against each path.data, much like how a credit card company pre-charges you to \"authorize\" a purchase but not charge you until you actually need to pay.  Maybe this \"tracking of disk space that will be consumed soon\" needs to be in NodeEnvironment?\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/101347511","html_url":"https://github.com/elastic/elasticsearch/issues/11122#issuecomment-101347511","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11122","id":101347511,"node_id":"MDEyOklzc3VlQ29tbWVudDEwMTM0NzUxMQ==","user":{"login":"dakrone","id":19060,"node_id":"MDQ6VXNlcjE5MDYw","avatar_url":"https://avatars3.githubusercontent.com/u/19060?v=4","gravatar_id":"","url":"https://api.github.com/users/dakrone","html_url":"https://github.com/dakrone","followers_url":"https://api.github.com/users/dakrone/followers","following_url":"https://api.github.com/users/dakrone/following{/other_user}","gists_url":"https://api.github.com/users/dakrone/gists{/gist_id}","starred_url":"https://api.github.com/users/dakrone/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakrone/subscriptions","organizations_url":"https://api.github.com/users/dakrone/orgs","repos_url":"https://api.github.com/users/dakrone/repos","events_url":"https://api.github.com/users/dakrone/events{/privacy}","received_events_url":"https://api.github.com/users/dakrone/received_events","type":"User","site_admin":false},"created_at":"2015-05-12T16:57:45Z","updated_at":"2015-05-12T16:57:45Z","author_association":"MEMBER","body":"> For case 2), I think IndexService.createShard needs to be told how large the incoming shard will be (it's not today?), since we must already know this somewhere, but I don't know how to get this information in IndicesClusterStateService.applyInitializingShard when it calls createShard.\n\nI think we could average the shard sizes for all other shards in the cluster. This information can be retrieved from the `ClusterInfoService`, which will periodically fetch it (on the master node).\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/101409366","html_url":"https://github.com/elastic/elasticsearch/issues/11122#issuecomment-101409366","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11122","id":101409366,"node_id":"MDEyOklzc3VlQ29tbWVudDEwMTQwOTM2Ng==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2015-05-12T20:21:35Z","updated_at":"2015-05-12T20:21:35Z","author_association":"CONTRIBUTOR","body":"Thanks for the pointer @dakrone, I'll try to use that for the case when there are already some shards in the cluster.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/101651763","html_url":"https://github.com/elastic/elasticsearch/issues/11122#issuecomment-101651763","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11122","id":101651763,"node_id":"MDEyOklzc3VlQ29tbWVudDEwMTY1MTc2Mw==","user":{"login":"mikemccand","id":796508,"node_id":"MDQ6VXNlcjc5NjUwOA==","avatar_url":"https://avatars0.githubusercontent.com/u/796508?v=4","gravatar_id":"","url":"https://api.github.com/users/mikemccand","html_url":"https://github.com/mikemccand","followers_url":"https://api.github.com/users/mikemccand/followers","following_url":"https://api.github.com/users/mikemccand/following{/other_user}","gists_url":"https://api.github.com/users/mikemccand/gists{/gist_id}","starred_url":"https://api.github.com/users/mikemccand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikemccand/subscriptions","organizations_url":"https://api.github.com/users/mikemccand/orgs","repos_url":"https://api.github.com/users/mikemccand/repos","events_url":"https://api.github.com/users/mikemccand/events{/privacy}","received_events_url":"https://api.github.com/users/mikemccand/received_events","type":"User","site_admin":false},"created_at":"2015-05-13T12:41:25Z","updated_at":"2015-05-13T12:41:25Z","author_association":"CONTRIBUTOR","body":"At the end of the day, the fix here will be heuristicky, since we essentially must \"guess\" how big this shard will grow to in the future.  So, there will be adversarial cases which fill up one data path while others remain very empty...\n\nMaybe we should (separately) make the separate path.datas on a given node visible to the cluster state?  I think we don't do this today?  E.g. DiskThresholdDecider looks at the net free bytes on a node, so it won't notice if one path is nearly full and another is very empty?  It will see that node has still having plenty of space?\n\nIf we did that, it would handle the adversarial cases where we \"guessed wrong\", and accidentally put N tiny shards on one path and N huge shards on another ... we'd be able to correct it later by relocating shards from the nearly full path.data ...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/101654493","html_url":"https://github.com/elastic/elasticsearch/issues/11122#issuecomment-101654493","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11122","id":101654493,"node_id":"MDEyOklzc3VlQ29tbWVudDEwMTY1NDQ5Mw==","user":{"login":"s1monw","id":973334,"node_id":"MDQ6VXNlcjk3MzMzNA==","avatar_url":"https://avatars0.githubusercontent.com/u/973334?v=4","gravatar_id":"","url":"https://api.github.com/users/s1monw","html_url":"https://github.com/s1monw","followers_url":"https://api.github.com/users/s1monw/followers","following_url":"https://api.github.com/users/s1monw/following{/other_user}","gists_url":"https://api.github.com/users/s1monw/gists{/gist_id}","starred_url":"https://api.github.com/users/s1monw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/s1monw/subscriptions","organizations_url":"https://api.github.com/users/s1monw/orgs","repos_url":"https://api.github.com/users/s1monw/repos","events_url":"https://api.github.com/users/s1monw/events{/privacy}","received_events_url":"https://api.github.com/users/s1monw/received_events","type":"User","site_admin":false},"created_at":"2015-05-13T12:58:42Z","updated_at":"2015-05-13T12:58:46Z","author_association":"CONTRIBUTOR","body":"I mean we have a general problem here that we don't know on which node we should put the shard if the index is freshly created. I wonder if we should do the same thing we do for shard allocation as well for disk allocaiton and make sure shards of an index are balanced across disks if there is enough space on all the disks and as a second heuristic use the number of shards in total on that node to balance. \n","performed_via_github_app":null}]