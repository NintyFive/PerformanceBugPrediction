[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/116000588","html_url":"https://github.com/elastic/elasticsearch/issues/11899#issuecomment-116000588","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11899","id":116000588,"node_id":"MDEyOklzc3VlQ29tbWVudDExNjAwMDU4OA==","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2015-06-27T10:00:08Z","updated_at":"2015-06-27T10:00:08Z","author_association":"MEMBER","body":"Please use discuss.elastic.co\n\nTry to reduce your bulk size. I think I don't insert more than 10k docs per bulk (small docs). So, some mb at most per bulk.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/118270453","html_url":"https://github.com/elastic/elasticsearch/issues/11899#issuecomment-118270453","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11899","id":118270453,"node_id":"MDEyOklzc3VlQ29tbWVudDExODI3MDQ1Mw==","user":{"login":"rpalsaxena","id":8288236,"node_id":"MDQ6VXNlcjgyODgyMzY=","avatar_url":"https://avatars1.githubusercontent.com/u/8288236?v=4","gravatar_id":"","url":"https://api.github.com/users/rpalsaxena","html_url":"https://github.com/rpalsaxena","followers_url":"https://api.github.com/users/rpalsaxena/followers","following_url":"https://api.github.com/users/rpalsaxena/following{/other_user}","gists_url":"https://api.github.com/users/rpalsaxena/gists{/gist_id}","starred_url":"https://api.github.com/users/rpalsaxena/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rpalsaxena/subscriptions","organizations_url":"https://api.github.com/users/rpalsaxena/orgs","repos_url":"https://api.github.com/users/rpalsaxena/repos","events_url":"https://api.github.com/users/rpalsaxena/events{/privacy}","received_events_url":"https://api.github.com/users/rpalsaxena/received_events","type":"User","site_admin":false},"created_at":"2015-07-03T08:11:31Z","updated_at":"2015-07-03T08:11:31Z","author_association":"NONE","body":"But elasticsearch is made for handling big size data some GBs of data , even wiki and github are also using it .\n\nI think reducing the input size is not a good solution , but I somehow managed to insert the data using python ..\n\nEven then if someone can suggest me best possible solution then kindly comment \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/118275996","html_url":"https://github.com/elastic/elasticsearch/issues/11899#issuecomment-118275996","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11899","id":118275996,"node_id":"MDEyOklzc3VlQ29tbWVudDExODI3NTk5Ng==","user":{"login":"dadoonet","id":274222,"node_id":"MDQ6VXNlcjI3NDIyMg==","avatar_url":"https://avatars3.githubusercontent.com/u/274222?v=4","gravatar_id":"","url":"https://api.github.com/users/dadoonet","html_url":"https://github.com/dadoonet","followers_url":"https://api.github.com/users/dadoonet/followers","following_url":"https://api.github.com/users/dadoonet/following{/other_user}","gists_url":"https://api.github.com/users/dadoonet/gists{/gist_id}","starred_url":"https://api.github.com/users/dadoonet/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dadoonet/subscriptions","organizations_url":"https://api.github.com/users/dadoonet/orgs","repos_url":"https://api.github.com/users/dadoonet/repos","events_url":"https://api.github.com/users/dadoonet/events{/privacy}","received_events_url":"https://api.github.com/users/dadoonet/received_events","type":"User","site_admin":false},"created_at":"2015-07-03T08:34:23Z","updated_at":"2015-07-03T08:34:23Z","author_association":"MEMBER","body":"Yes. I can confirm that elasticsearch can handle billions of documents, trillions of documents.\nIt does not mean that you can pass all those docs within one single bulk request.\n\nHave a look BTW at `http.max_content_length`. By default it's limited to 100mb (on purpose).\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/modules-http.html\n","performed_via_github_app":null}]