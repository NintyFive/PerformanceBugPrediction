[{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/106899278","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-106899278","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":106899278,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNjg5OTI3OA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-05-29T18:37:52Z","updated_at":"2015-05-29T18:37:52Z","author_association":"CONTRIBUTOR","body":"Hi @l15k4 \n\nYou're really going to have to provide a lot more information than this, because 20M documents is tiny, Elasticsearch should handle this with ease.    I suggest you start by discussing your issue in the forum https://discuss.elastic.co/c/elasticsearch which is intended to help solve user issues.  If you find something that you think is a bug, please feel free to open a new ticket with the relevant information\n\nthanks\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107155434","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-107155434","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":107155434,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE1NTQzNA==","user":{"login":"l15k4","id":518855,"node_id":"MDQ6VXNlcjUxODg1NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/518855?v=4","gravatar_id":"","url":"https://api.github.com/users/l15k4","html_url":"https://github.com/l15k4","followers_url":"https://api.github.com/users/l15k4/followers","following_url":"https://api.github.com/users/l15k4/following{/other_user}","gists_url":"https://api.github.com/users/l15k4/gists{/gist_id}","starred_url":"https://api.github.com/users/l15k4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l15k4/subscriptions","organizations_url":"https://api.github.com/users/l15k4/orgs","repos_url":"https://api.github.com/users/l15k4/repos","events_url":"https://api.github.com/users/l15k4/events{/privacy}","received_events_url":"https://api.github.com/users/l15k4/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T10:43:25Z","updated_at":"2015-05-31T11:12:20Z","author_association":"NONE","body":"Hi @clintongormley ,\n\nThe cluster has default configuration, running on 2 aws dual-core EC2 instances. Each ES node has 2GB RAM. I'm indexing in bulks (100 records / 2.5MB overall) concurrently from (1 to 4) threads and I have disabled `refresh_interval` for this index...\n\nIt's an index-only cluster, ES 1.5.1...\n\nIs there any particular configuration setting that has the biggest impact on this issue? RAM doesn't seem to be an issue according to Marvel Overview. Thank you\n\nI can think of these configs that might help : \n\n```\nindices.memory.index_buffer_size: 40%\nindices.store.throttle.type: none\nindex.translog.interval: 10s\nindex.gateway.local.sync: 10s\nindex.translog.flush_threshold_size: 1024mb\n```\n\nI'n not sure if I should also change `indices.recovery` settings...\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107157361","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-107157361","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":107157361,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE1NzM2MQ==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T11:21:20Z","updated_at":"2015-05-31T11:21:20Z","author_association":"CONTRIBUTOR","body":"HI @l15k4 \n\nI'd be interested to see:\n- what's in the logs (esp looking for slow GCs)\n- what your nodes info and stats say\n- what hot threads say\n\nAnd I'd investigate all of those before changing settings like the ones you suggest.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/107174905","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-107174905","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":107174905,"node_id":"MDEyOklzc3VlQ29tbWVudDEwNzE3NDkwNQ==","user":{"login":"l15k4","id":518855,"node_id":"MDQ6VXNlcjUxODg1NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/518855?v=4","gravatar_id":"","url":"https://api.github.com/users/l15k4","html_url":"https://github.com/l15k4","followers_url":"https://api.github.com/users/l15k4/followers","following_url":"https://api.github.com/users/l15k4/following{/other_user}","gists_url":"https://api.github.com/users/l15k4/gists{/gist_id}","starred_url":"https://api.github.com/users/l15k4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l15k4/subscriptions","organizations_url":"https://api.github.com/users/l15k4/orgs","repos_url":"https://api.github.com/users/l15k4/repos","events_url":"https://api.github.com/users/l15k4/events{/privacy}","received_events_url":"https://api.github.com/users/l15k4/received_events","type":"User","site_admin":false},"created_at":"2015-05-31T13:10:03Z","updated_at":"2015-05-31T13:17:38Z","author_association":"NONE","body":"@clintongormley \n\nthere are just these log lines : \n\n```\n[2015-05-31 04:16:56,935][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:16:56,987][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n[2015-05-31 04:17:11,461][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][3] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:17:11,478][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][3] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n[2015-05-31 04:17:13,757][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:17:13,768][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n[2015-05-31 04:17:21,697][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:17:21,744][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n[2015-05-31 04:17:37,832][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:17:37,973][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n[2015-05-31 04:18:04,276][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:18:04,306][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n[2015-05-31 04:18:26,656][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:18:26,692][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][2] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n[2015-05-31 04:18:30,538][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][0] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:18:30,567][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][0] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n[2015-05-31 04:18:51,568][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:18:51,620][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n[2015-05-31 04:19:35,157][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:19:35,230][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][1] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n[2015-05-31 04:19:35,524][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n[2015-05-31 04:19:36,150][INFO ][index.engine             ] [Ikthalon] [mi_2015_05][4] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n```\n\n[node info](http://pastebin.com/raw.php?i=as6kFMcS)\n[node stats](http://pastebin.com/raw.php?i=jpdxPyzV)\n[hot threads](http://pastebin.com/raw.php?i=C04ABS4Y)\n\nBtw it usually performs well, with ~ 80% CPU utilization on both nodes but after ~ 2-3 minutes it slows down to ~ 20% CPU utilization... RAM utilization remains the same ~ 65%\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/108028980","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-108028980","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":108028980,"node_id":"MDEyOklzc3VlQ29tbWVudDEwODAyODk4MA==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-06-02T17:48:16Z","updated_at":"2015-06-02T17:48:16Z","author_association":"CONTRIBUTOR","body":"It looks like your disks are slow, which is causing your merges to slow down.  You can either add more nodes (to spread the load across more disks) or get faster disks (eg SSD or provisioned IOPS)\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/108089651","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-108089651","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":108089651,"node_id":"MDEyOklzc3VlQ29tbWVudDEwODA4OTY1MQ==","user":{"login":"l15k4","id":518855,"node_id":"MDQ6VXNlcjUxODg1NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/518855?v=4","gravatar_id":"","url":"https://api.github.com/users/l15k4","html_url":"https://github.com/l15k4","followers_url":"https://api.github.com/users/l15k4/followers","following_url":"https://api.github.com/users/l15k4/following{/other_user}","gists_url":"https://api.github.com/users/l15k4/gists{/gist_id}","starred_url":"https://api.github.com/users/l15k4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l15k4/subscriptions","organizations_url":"https://api.github.com/users/l15k4/orgs","repos_url":"https://api.github.com/users/l15k4/repos","events_url":"https://api.github.com/users/l15k4/events{/privacy}","received_events_url":"https://api.github.com/users/l15k4/received_events","type":"User","site_admin":false},"created_at":"2015-06-02T20:30:20Z","updated_at":"2015-06-02T21:05:56Z","author_association":"NONE","body":"@clintongormley  there is plenty of disk IO throughput available, really. The indexing is way faster if I shut one node down and use just one. Keeping the replica synced is the real bottleneck here imho.\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/108290860","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-108290860","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":108290860,"node_id":"MDEyOklzc3VlQ29tbWVudDEwODI5MDg2MA==","user":{"login":"l15k4","id":518855,"node_id":"MDQ6VXNlcjUxODg1NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/518855?v=4","gravatar_id":"","url":"https://api.github.com/users/l15k4","html_url":"https://github.com/l15k4","followers_url":"https://api.github.com/users/l15k4/followers","following_url":"https://api.github.com/users/l15k4/following{/other_user}","gists_url":"https://api.github.com/users/l15k4/gists{/gist_id}","starred_url":"https://api.github.com/users/l15k4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l15k4/subscriptions","organizations_url":"https://api.github.com/users/l15k4/orgs","repos_url":"https://api.github.com/users/l15k4/repos","events_url":"https://api.github.com/users/l15k4/events{/privacy}","received_events_url":"https://api.github.com/users/l15k4/received_events","type":"User","site_admin":false},"created_at":"2015-06-03T10:26:57Z","updated_at":"2015-06-03T10:26:57Z","author_association":"NONE","body":"In other words, I can index 8000 documents/second into small indices but when the index reaches 20-30 million records, ES cluster suddenly slows down to 200-300 docs/s **with very low RAM and CPU utilization** and if I split the documents into indices up to 20 million records it is indexing 8000 documents/second next 12 hours without problem... \n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/108332227","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-108332227","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":108332227,"node_id":"MDEyOklzc3VlQ29tbWVudDEwODMzMjIyNw==","user":{"login":"clintongormley","id":56599,"node_id":"MDQ6VXNlcjU2NTk5","avatar_url":"https://avatars0.githubusercontent.com/u/56599?v=4","gravatar_id":"","url":"https://api.github.com/users/clintongormley","html_url":"https://github.com/clintongormley","followers_url":"https://api.github.com/users/clintongormley/followers","following_url":"https://api.github.com/users/clintongormley/following{/other_user}","gists_url":"https://api.github.com/users/clintongormley/gists{/gist_id}","starred_url":"https://api.github.com/users/clintongormley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/clintongormley/subscriptions","organizations_url":"https://api.github.com/users/clintongormley/orgs","repos_url":"https://api.github.com/users/clintongormley/repos","events_url":"https://api.github.com/users/clintongormley/events{/privacy}","received_events_url":"https://api.github.com/users/clintongormley/received_events","type":"User","site_admin":false},"created_at":"2015-06-03T11:49:34Z","updated_at":"2015-06-03T11:49:34Z","author_association":"CONTRIBUTOR","body":"@l15k4 Merging is a cost of indexing.  You have to pay it in order to continue indexing efficiently, but you start paying it only as your segments start getting bigger.  I suggest that you ask about your options in the forum: https://discuss.elastic.co/c/elasticsearch\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/135987909","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-135987909","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":135987909,"node_id":"MDEyOklzc3VlQ29tbWVudDEzNTk4NzkwOQ==","user":{"login":"bitonp","id":2499269,"node_id":"MDQ6VXNlcjI0OTkyNjk=","avatar_url":"https://avatars1.githubusercontent.com/u/2499269?v=4","gravatar_id":"","url":"https://api.github.com/users/bitonp","html_url":"https://github.com/bitonp","followers_url":"https://api.github.com/users/bitonp/followers","following_url":"https://api.github.com/users/bitonp/following{/other_user}","gists_url":"https://api.github.com/users/bitonp/gists{/gist_id}","starred_url":"https://api.github.com/users/bitonp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bitonp/subscriptions","organizations_url":"https://api.github.com/users/bitonp/orgs","repos_url":"https://api.github.com/users/bitonp/repos","events_url":"https://api.github.com/users/bitonp/events{/privacy}","received_events_url":"https://api.github.com/users/bitonp/received_events","type":"User","site_admin":false},"created_at":"2015-08-29T13:38:23Z","updated_at":"2015-08-29T13:38:23Z","author_association":"NONE","body":"2G Ram seems very low. Java requires half of that, so thats 1G gone... and Isuspect you are swapping the rest.\nAdd to that the fact that you have data and readers on teh same nodes, its probably gettiing a bit insane. we are on AWS and regularly index 140m-250m per day, using 32G machines. we have Kibana reading for 20+ graphs simultabeously... indexing queries off mysql databases. I suspect that arger machines will solve yoour issue.. but I could also be a couple of months late picking up on this.. sorry\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/149975789","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-149975789","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":149975789,"node_id":"MDEyOklzc3VlQ29tbWVudDE0OTk3NTc4OQ==","user":{"login":"l15k4","id":518855,"node_id":"MDQ6VXNlcjUxODg1NQ==","avatar_url":"https://avatars1.githubusercontent.com/u/518855?v=4","gravatar_id":"","url":"https://api.github.com/users/l15k4","html_url":"https://github.com/l15k4","followers_url":"https://api.github.com/users/l15k4/followers","following_url":"https://api.github.com/users/l15k4/following{/other_user}","gists_url":"https://api.github.com/users/l15k4/gists{/gist_id}","starred_url":"https://api.github.com/users/l15k4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l15k4/subscriptions","organizations_url":"https://api.github.com/users/l15k4/orgs","repos_url":"https://api.github.com/users/l15k4/repos","events_url":"https://api.github.com/users/l15k4/events{/privacy}","received_events_url":"https://api.github.com/users/l15k4/received_events","type":"User","site_admin":false},"created_at":"2015-10-21T17:50:29Z","updated_at":"2015-10-21T17:52:12Z","author_association":"NONE","body":"@clintongormley I did, I'm disabling refresh interval, setting throttling to none and optimizing indices after bulk indexing, so that next bulk indexing starts on merged segments, see : \n\n15:05:25,915][INFO ][indices.store ] [es-884a70a1805147db95b8a28e776cd59a] updating indices.store.throttle.type from [merge] to [none]\n15:05:25,966][INFO ][index.shard ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][4] updating refresh_interval from [1s] to [-1]\n15:05:25,971][INFO ][index.shard ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][0] updating refresh_interval from [1s] to [-1]\n15:05:25,974][INFO ][index.shard ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][2] updating refresh_interval from [1s] to [-1]\n15:05:25,974][INFO ][index.shard ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][3] updating refresh_interval from [1s] to [-1]\n15:05:25,974][INFO ][index.shard ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][1] updating refresh_interval from [1s] to [-1]\n15:05:26,731][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][4] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n15:05:26,807][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][0] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n15:05:26,865][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][2] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n15:05:26,922][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][4] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n15:05:26,979][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][3] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n15:05:27,018][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][1] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n15:05:27,060][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][2] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n15:05:27,067][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][0] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n15:05:27,139][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][3] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n15:05:27,330][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][1] stop throttling indexing: numMergesInFlight=2, maxNumMerges=3\n15:05:29,024][INFO ][index.engine ] [es-884a70a1805147db95b8a28e776cd59a] [mi_2015_10_w4][3] now throttling indexing: numMergesInFlight=4, maxNumMerges=3\n\n@bitonp I meant 2GB for Heap ... anyway this isn't related to memory, there is always enough of memory.... GC is choking though, a lot of time is spent by GC young space\n","performed_via_github_app":null},{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/comments/158945699","html_url":"https://github.com/elastic/elasticsearch/issues/11425#issuecomment-158945699","issue_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11425","id":158945699,"node_id":"MDEyOklzc3VlQ29tbWVudDE1ODk0NTY5OQ==","user":{"login":"jsh2134","id":1028443,"node_id":"MDQ6VXNlcjEwMjg0NDM=","avatar_url":"https://avatars1.githubusercontent.com/u/1028443?v=4","gravatar_id":"","url":"https://api.github.com/users/jsh2134","html_url":"https://github.com/jsh2134","followers_url":"https://api.github.com/users/jsh2134/followers","following_url":"https://api.github.com/users/jsh2134/following{/other_user}","gists_url":"https://api.github.com/users/jsh2134/gists{/gist_id}","starred_url":"https://api.github.com/users/jsh2134/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jsh2134/subscriptions","organizations_url":"https://api.github.com/users/jsh2134/orgs","repos_url":"https://api.github.com/users/jsh2134/repos","events_url":"https://api.github.com/users/jsh2134/events{/privacy}","received_events_url":"https://api.github.com/users/jsh2134/received_events","type":"User","site_admin":false},"created_at":"2015-11-23T14:21:59Z","updated_at":"2015-11-23T14:21:59Z","author_association":"NONE","body":"@l15k4 having the same issue. We are trying to index 400M records and right around 100M records, throughput grinds to a halt, we are using larger nodes (m4.2xlarges) but once the throughput drops so does CPU and Load. Have you had any further success?\n","performed_via_github_app":null}]