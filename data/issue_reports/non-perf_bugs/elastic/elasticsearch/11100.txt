{"url":"https://api.github.com/repos/elastic/elasticsearch/issues/11100","repository_url":"https://api.github.com/repos/elastic/elasticsearch","labels_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11100/labels{/name}","comments_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11100/comments","events_url":"https://api.github.com/repos/elastic/elasticsearch/issues/11100/events","html_url":"https://github.com/elastic/elasticsearch/issues/11100","id":75349483,"node_id":"MDU6SXNzdWU3NTM0OTQ4Mw==","number":11100,"title":"Cluster state update takes a long time due to lots of aliases","user":{"login":"msimos","id":11743255,"node_id":"MDQ6VXNlcjExNzQzMjU1","avatar_url":"https://avatars3.githubusercontent.com/u/11743255?v=4","gravatar_id":"","url":"https://api.github.com/users/msimos","html_url":"https://github.com/msimos","followers_url":"https://api.github.com/users/msimos/followers","following_url":"https://api.github.com/users/msimos/following{/other_user}","gists_url":"https://api.github.com/users/msimos/gists{/gist_id}","starred_url":"https://api.github.com/users/msimos/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/msimos/subscriptions","organizations_url":"https://api.github.com/users/msimos/orgs","repos_url":"https://api.github.com/users/msimos/repos","events_url":"https://api.github.com/users/msimos/events{/privacy}","received_events_url":"https://api.github.com/users/msimos/received_events","type":"User","site_admin":false},"labels":[{"id":163824881,"node_id":"MDU6TGFiZWwxNjM4MjQ4ODE=","url":"https://api.github.com/repos/elastic/elasticsearch/labels/:Core/Features/Indices%20APIs","name":":Core/Features/Indices APIs","color":"0e8a16","default":false,"description":"APIs to create and manage indices"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2015-05-11T20:36:48Z","updated_at":"2018-02-13T20:37:45Z","closed_at":"2015-05-15T22:08:45Z","author_association":"MEMBER","active_lock_reason":null,"body":"When taking a snapshot or creating an index takes a very long time. For example when taking a snapshot takes 18 hours. The cluster has 23 nodes & 655 indexes with 400 alias per index. Each user is an alias (to an index). They track the current Index (in zookeeper). Once the current index has 400 alias, they create a new index. When looking at the pending tasks while the snapshot is running we see a lot of long run tasks like:\n\n```\n{\n\"insert_order\" : 30872,\n\"priority\" : \"NORMAL\",\n\"source\" : \"update snapshot state\",\n\"executing\" : false,\n\"time_in_queue_millis\" : 62424008,\n\"time_in_queue\" : \"17.3h\"\n}\n```\n\nThis is an example of the alias that is created:\n\n```\n\"63e5786c2bea7e51a2d2359de2bc42419bf1c8e96d14c40336e7a0aa594dfb80\" : {\n    \"filter\" : {\n        \"term\" : {\n            \"user_id\" : \"63e5786c2bea7e51a2d2359de2bc42419bf1c8e96d14c40336e7a0aa594dfb80\"\n        }\n    },\n    \"index_routing\" : \"63e5786c2bea7e51a2d2359de2bc42419bf1c8e96d14c40336e7a0aa594dfb80\",\n    \"search_routing\" : \"63e5786c2bea7e51a2d2359de2bc42419bf1c8e96d14c40336e7a0aa594dfb80\"\n},\n```\n\nAccording to Igor, publishing of cluster state updates always takes about 14-15 seconds no matter what type of update it is. Here is one example when updating aliases:\n\n```\n[03:23:37,861][DEBUG][cluster.service ] [Mad Dog Rassitano] processing [index-aliases]: execute\n[03:23:40,144][DEBUG][cluster.service ] [Mad Dog Rassitano] cluster state updated, version [39957], source [index-aliases]\n[03:23:40,144][DEBUG][cluster.service ] [Mad Dog Rassitano] publishing cluster state version 39957\n[03:23:55,545][DEBUG][cluster.service ] [Mad Dog Rassitano] set local cluster state to version 39957\n[03:23:55,554][DEBUG][cluster.service ] [Mad Dog Rassitano] processing [index-aliases]: done applying updated cluster_state (version: 39957)\n```\n\nHere is another example of a node leaving the cluster:\n\n```\n[07:37:21,822][DEBUG][cluster.service ] [Mad Dog Rassitano] processing [zen-disco-node_failed([Death-Stalker][l045BOE8RWC3Wm6DDZJERA][polaris-prod-135-w-esnode-9t65fuj427hgh][inet[/10.0.0.67:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout]: execute\n[07:37:58,920][DEBUG][cluster.service ] [Mad Dog Rassitano] cluster state updated, version [40189], source [zen-disco-node_failed([Death-Stalker][l045BOE8RWC3Wm6DDZJERA][polaris-prod-135-w-esnode-9t65fuj427hgh][inet[/10.0.0.67:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout]\n[07:37:58,921][INFO ][cluster.service ] [Mad Dog Rassitano] removed {[Death-Stalker][l045BOE8RWC3Wm6DDZJERA][polaris-prod-135-w-esnode-9t65fuj427hgh][inet[/10.0.0.67:9300]]{master=false},}, reason: zen-disco-node_failed([Death-Stalker][l045BOE8RWC3Wm6DDZJERA][polaris-prod-135-w-esnode-9t65fuj427hgh][inet[/10.0.0.67:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout\n[07:37:58,921][DEBUG][cluster.service ] [Mad Dog Rassitano] publishing cluster state version 40189\n[07:38:12,901][DEBUG][cluster.service ] [Mad Dog Rassitano] set local cluster state to version 40189\n[07:38:12,942][DEBUG][cluster.service ] [Mad Dog Rassitano] processing [zen-disco-node_failed([Death-Stalker][l045BOE8RWC3Wm6DDZJERA][polaris-prod-135-w-esnode-9t65fuj427hgh][inet[/10.0.0.67:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout]: done applying updated cluster_state (version: 40189)\n```\n\nSlow cluster state propagation causes issues with index creation and slow snapshots.\n\nThe hot threads provided shows a lot of activity in reading cluster state from the network with a majority of the hot threads being busy reading aliases:\n\n```\nthreads-0007: 79.9% (399.7ms out of 500ms) cpu usage by thread 'elasticsearch[Dormammu][transport_client_worker][T#2]{New I/O worker #2}'\nthreads-0007- 4/10 snapshots sharing following 31 elements\nthreads-0007- org.elasticsearch.common.hppc.ObjectObjectOpenHashMap.get(ObjectObjectOpenHashMap.java:564)\nthreads-0007- org.elasticsearch.common.collect.ImmutableOpenMap$Builder.get(ImmutableOpenMap.java:255)\nthreads-0007- org.elasticsearch.cluster.metadata.MetaData.<init>(MetaData.java:222)\nthreads-0007- org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1245)\nthreads-0007- org.elasticsearch.cluster.metadata.MetaData$Builder.readFrom(MetaData.java:1382)\nthreads-0007- org.elasticsearch.cluster.ClusterState$Builder.readFrom(ClusterState.java:620)\nthreads-0007- org.elasticsearch.action.admin.cluster.state.ClusterStateResponse.readFrom(ClusterStateResponse.java:58)\nthreads-0007- org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:150)\nthreads-0007- org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:127)\nthreads-0007- org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\nthreads-0007- org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\nthreads-0007- org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\nthreads-0007- org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\nthreads-0007- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\nthreads-0007- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\nthreads-0007- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)\nthreads-0007- org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\nthreads-0007- org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\nthreads-0007- org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n\nthreads-0008: 88.7% (443.7ms out of 500ms) cpu usage by thread 'elasticsearch[Tower][clusterService#updateTask][T#1]'\nthreads-0008- 2/10 snapshots sharing following 11 elements\nthreads-0008- org.elasticsearch.common.hppc.ObjectObjectOpenHashMap.get(ObjectObjectOpenHashMap.java:564)\nthreads-0008- org.elasticsearch.common.collect.ImmutableOpenMap$Builder.get(ImmutableOpenMap.java:255)\nthreads-0008- org.elasticsearch.cluster.metadata.MetaData.<init>(MetaData.java:222)\nthreads-0008- org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1245)\nthreads-0008- org.elasticsearch.cluster.ClusterState$Builder.metaData(ClusterState.java:525)\nthreads-0008- org.elasticsearch.discovery.zen.ZenDiscovery$8.execute(ZenDiscovery.java:690)\nthreads-0008- org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)\nthreads-0008- org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\nthreads-0008- java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\nthreads-0008- java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\nthreads-0008- java.lang.Thread.run(Unknown Source)\nthreads-0008- 4/10 snapshots sharing following 12 elements\nthreads-0008- org.elasticsearch.common.hppc.AbstractIterator.hasNext(AbstractIterator.java:34)\nthreads-0008- org.elasticsearch.common.hppc.AbstractObjectCollection.toArray(AbstractObjectCollection.java:86)\nthreads-0008- org.elasticsearch.common.hppc.ObjectArrayList.toArray(ObjectArrayList.java:44)\nthreads-0008- org.elasticsearch.cluster.metadata.MetaData.<init>(MetaData.java:232)\nthreads-0008- org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1245)\nthreads-0008- org.elasticsearch.cluster.ClusterState$Builder.metaData(ClusterState.java:525)\nthreads-0008- org.elasticsearch.discovery.zen.ZenDiscovery$8.execute(ZenDiscovery.java:690)\n\nthreads-0008: 100.0% (500ms out of 500ms) cpu usage by thread 'elasticsearch[Freak][transport_server_worker][T#6]{New I/O worker #15}'\nthreads-0008- 6/10 snapshots sharing following 33 elements\nthreads-0008- org.elasticsearch.cluster.metadata.MetaData.<init>(MetaData.java:205)\nthreads-0008- org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1245)\nthreads-0008- org.elasticsearch.cluster.metadata.MetaData$Builder.readFrom(MetaData.java:1382)\nthreads-0008- org.elasticsearch.cluster.ClusterState$Builder.readFrom(ClusterState.java:620)\nthreads-0008- org.elasticsearch.discovery.zen.publish.PublishClusterStateAction$PublishClusterStateRequestHandler.messageReceived(PublishClusterStateAction.java:175)\nthreads-0008- org.elasticsearch.discovery.zen.publish.PublishClusterStateAction$PublishClusterStateRequestHandler.messageReceived(PublishClusterStateAction.java:156)\nthreads-0008- org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)\nthreads-0008- org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)\nthreads-0008- org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\nthreads-0008- org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\nthreads-0008- org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\nthreads-0008- org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\nthreads-0008- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\nthreads-0008- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\nthreads-0008- org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)\nthreads-0008- org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\nthreads-0008- org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\nthreads-0008- org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\nthreads-0008- org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\n```\n\nI realize this issue is related to these two issues:\n\nhttps://github.com/elastic/elasticsearch/pull/10212 \nhttps://github.com/elastic/elasticsearch/pull/10295\n\nHowever I am looking for suggestions as how to improve the performance of the cluster state propagation. \n","closed_by":{"login":"msimos","id":11743255,"node_id":"MDQ6VXNlcjExNzQzMjU1","avatar_url":"https://avatars3.githubusercontent.com/u/11743255?v=4","gravatar_id":"","url":"https://api.github.com/users/msimos","html_url":"https://github.com/msimos","followers_url":"https://api.github.com/users/msimos/followers","following_url":"https://api.github.com/users/msimos/following{/other_user}","gists_url":"https://api.github.com/users/msimos/gists{/gist_id}","starred_url":"https://api.github.com/users/msimos/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/msimos/subscriptions","organizations_url":"https://api.github.com/users/msimos/orgs","repos_url":"https://api.github.com/users/msimos/repos","events_url":"https://api.github.com/users/msimos/events{/privacy}","received_events_url":"https://api.github.com/users/msimos/received_events","type":"User","site_admin":false},"performed_via_github_app":null}